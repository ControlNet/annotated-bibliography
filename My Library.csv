"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"2JBPGV5S","journalArticle","1991","Kramer, Mark A.","Nonlinear principal component analysis using autoassociative neural networks","AIChE Journal","","1547-5905","https://doi.org/10.1002/aic.690370209","https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690370209","Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.","1991","2021-03-16 13:41:25","2021-03-16 13:41:25","2021-03-16 13:41:25","233-243","","2","37","","","","","","","","","","en","Copyright © 1991 American Institute of Chemical Engineers","","","","Wiley Online Library","","_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690370209","<div data-schema-version=""2""><p>Annotation</p> <p>Auto Encoder (Nonlinear Principal Component Analysis NLPCA)</p> <p>Engineers are often confronted with the problem of extracting information about poorly-known process of data</p> <p>The dimensionality reduction is closely related to feature extraction and it can capture the information contained in the original data.</p> <p>The proposed method is the using multi-layer neural networks with same input and output and a bottleneck. The training target is to minimize the reconstruction error. Compared to PCA, the nonlinearity improve the performance of feature extraction.</p> <p>Datasets are generated by some simple functions.</p> <p>The results of experiments shows the reconstruction error is less than PCA and ANN without mapping layer.</p> <p>+ve: Can find and eliminates nonlinear correlations in the data. Can remove redundant information. More effective than PCA.</p> <p>-ve: Limited by the practicalities of computing functional approximations from limited data.</p> <p>The origin of auto encoder. A great milestone.</p> <p></p></div>","F:\Google Drive - Monash\Bibliography\Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf; C:\Users\smczx\Zotero\storage\TA7ULUCD\aic.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMJ59NJI","journalArticle","1998","Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P.","Gradient-based learning applied to document recognition","Proceedings of the IEEE","","1558-2256","10.1109/5.726791","","Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.","1998-11","2021-03-16 13:50:37","2021-03-16 15:33:19","","2278-2324","","11","86","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Proceedings of the IEEE","<div data-schema-version=""2""><p>Annotation</p> <p>Convolutional Neural Networks (CNN) and LeNet</p> <p>To find a better pattern recognition systems can be built by relying more on automatic learning and less on hand-designed heuristics.</p> <p>The new method is made possible by recent progress in machine learning and computer technology. The hand-crafted feature extraction can be advantageously replaced by carefully designed learning machines that operate directly on pixel images.</p> <p>The architecture of CNN is based on convolutional layers, pooling layers, and some fully-connected layers. Convolutional layers and pooling layers are effective for extracting features from 2D images, and they can also share the parameters with nearby numbers which can dramatically reduce the parameters, and it is beneficial for model performance.</p> <p>The dataset is the called MNIST, with 60000 training images and 10000 test images, combined by SD-1 and SD-2 datasets.</p> <p>The classification error of LeNet is lower compared to other traditional methods and fully-connected NN.</p> <p>+ve: Better classification performance than traditional methods and fully-connected NN. Suitable for hardware implementations with low memory requirements. More robust for shape variance and noise.</p> <p>-ve: Longer training time.</p> <p>The origin of CNN, huge contribution to computer vision area.</p></div>","C:\Users\smczx\Zotero\storage\8RPNJDWS\726791.html; F:\Google Drive - Monash\Bibliography\Lecun et al_1998_Gradient-based learning applied to document recognition.pdf","","","2D shape variability; back-propagation; backpropagation; Character recognition; cheque reading; complex decision surface synthesis; convolution; convolutional neural network character recognizers; document recognition; document recognition systems; Feature extraction; field extraction; gradient based learning technique; gradient-based learning; graph transformer networks; GTN; handwritten character recognition; handwritten digit recognition task; Hidden Markov models; high-dimensional patterns; language modeling; Machine learning; Multi-layer neural network; multilayer neural networks; multilayer perceptrons; multimodule systems; Neural networks; optical character recognition; Optical character recognition software; Optical computing; Pattern recognition; performance measure minimization; Principal component analysis; segmentation recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJPJAB46","journalArticle","2012","Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R.","Improving neural networks by preventing co-adaptation of feature detectors","arXiv:1207.0580 [cs]","","","","http://arxiv.org/abs/1207.0580","When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.","2012-07-03","2021-03-16 13:52:06","2021-04-07 06:44:01","2021-03-16 13:52:06","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1207.0580","<div data-schema-version=""2""><p>Annotation</p> <p>Dropout</p> <p>The overfitting, do worse on the test data than on the training data, happens.</p> <p>Finding a similar method with ""mean network"", which is formed by many separate networks, to reduce the test set error.</p> <p>The dropout is used as disable some proportion of hidden units in training process to prevent the hidden units replies to others. Normally, the drop probability is set to 0.5.</p> <p>Datasets are MNIST, TIMIT benchmark, CIFAR-10, ImageNet, and Reuters.</p> <p>The main metrics for comparison is the classification error for test set. The performance of dropout network is similar to ""mean network"", and better than the network without dropout.</p> <p>+ve: Almost all dropout probabilities can improve the generalization performance of the models. Dropout is simpler to implement.</p> <p>-ve: Some extreme probabilities will cause the performance worse.</p> </div>","C:\Users\smczx\Zotero\storage\PCYSDJFS\1207.html; F:\Google Drive - Monash\Bibliography\Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf","","","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLKIMCG8","journalArticle","2009","Bengio, Y.","Learning Deep Architectures for AI","Foundations and Trends® in Machine Learning","","1935-8237, 1935-8245","10.1561/2200000006","http://www.nowpublishers.com/article/Details/MAL-006","Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.","2009","2021-03-16 13:56:40","2021-04-06 15:02:00","2021-03-16 13:56:40","1-127","","1","2","","FNT in Machine Learning","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""2""><p>Annotation </p> <p>Survey</p> <p>This paper introduced many architectures for deep learning. In chapter 2, the researchers explain that a deeper structure can extract the feature more accuracy. Besides, in chapter 5, the paper introduced the convolutional neural network and auto encoder, and they also introduced an optimization principle to initialize parameters for each layer by unsupervised learning, which the auto encoder is included in.</p></div>","F:\Google Drive - Monash\Bibliography\Bengio_2009_Learning Deep Architectures for AI.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WMKXI6M","journalArticle","2006","Hinton, G. E.; Salakhutdinov, R. R.","Reducing the Dimensionality of Data with Neural Networks","Science","","0036-8075, 1095-9203","10.1126/science.1127647","https://science.sciencemag.org/content/313/5786/504","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.","2006-07-28","2021-03-16 14:00:23","2021-03-16 14:00:23","2021-03-16 14:00:23","504-507","","5786","313","","","","","","","","","","en","American Association for the Advancement of Science","","","","science.sciencemag.org","","Publisher: American Association for the Advancement of Science Section: Report PMID: 16873662","<div data-schema-version=""1""><p>Annotation</p> <p>Using RBM to pretrain the Auto encoder for dimensionality reduction.</p> <p>Multi-layer auto encoder is hard to train.</p> <p>Use a very different type of algorithm to pretrain the model and prove it can be generalized to other datasets.</p> <p>The method introduced in this paper uses RBM as the algorithm to pretrain the weights of an autoencoder. For each layer of the autoencoder, the weights will be pretrained as RBM, and pretrain the next layer. With the RBM pretrained weights, the autoencoder has better performance for reconstruction.</p> <p>MNIST hand-written digits and Olivetti face dataset</p> <p>No quantitative comparison.</p> <p>+ve: Better reconstruction results than PCA. This pretraining can also be used for classification and regression. Pretraining helps generalization. The autoencoders can map in bi-direction between data and code spaces. Can apply to very large datasets.</p> <p>-ve:</p> <p>Provide a new pretrain method for auto encoder to improve the performance. Combine the RBM and the neural networks.</p> <p></p> <p></p></div>","F:\Google Drive - Monash\Bibliography\Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf; ; C:\Users\smczx\Zotero\storage\MVMYC94D\504.html","http://www.ncbi.nlm.nih.gov/pubmed/16873662","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CA2QE2FS","journalArticle","2010","Vincent, Pascal; Larochelle, Hugo; Lajoie, Isabelle; Bengio, Yoshua; Manzagol, Pierre-Antoine","Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion","The Journal of Machine Learning Research","","1532-4435","","","We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.","2010-12-01","2021-03-16 14:03:28","2021-03-16 14:03:28","","3371–3408","","","11","","J. Mach. Learn. Res.","Stacked Denoising Autoencoders","","","","","","","","","","","","3/1/2010","","","<div data-schema-version=""2""><p>Annotation</p> <p>Denoising autoencoder (DAE)</p> <p>To overcome the gap between the stacking RBMs and stacking autoencoders. And find out what can shape a good, useful representation.</p> <p>The authors were looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns. And expected the latent representation is robust and stable, also can perform denoising task.</p> <p>The method proposed is the DAE which adds the noise to input data, and train the autoencoder to reconstruct the original data without noise. The noise added can be Gaussian noise or masking noise. With the noise inside, DAE can learn the features more stable and robust. So, an autoencoder with noisy data input can encode as higher level representations which have better performance than without noise added. The stacked DAE is the pretrained method to improve the performance of autoencoder.</p> <p>The dataset used is the 12 x 12 patches from Olshausen for DAE. The MNIST, and tzanetakis audio genre classification data set for stacked DAE.</p> <p>They compared the test error rate for classification task with other methods, and the SDAE (stacked DAE) performs the best.</p> <p>+ve: Better performance than DBNs. Establish the value of using the denoising criterion as an unsupervised objective for useful higher level presentations.</p> <p>-ve: The DAEs used in the paper are shallow.</p> <p>This paper introduced a simple way (add noise to input data) to improve the performance of encoder, which is easy to implement and prove the value for denoising task in the AE training. </p> <p></p></div>","F:\Google Drive - Monash\Bibliography\Vincent et al_2010_Stacked Denoising Autoencoders.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"454BKCFN","journalArticle","2021","Doersch, Carl","Tutorial on Variational Autoencoders","arXiv:1606.05908 [cs, stat]","","","","http://arxiv.org/abs/1606.05908","In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.","2021-01-03","2021-03-16 14:05:22","2021-04-06 15:06:51","2021-03-16 14:05:22","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1606.05908","<div data-schema-version=""1""><p>Annotation</p> <p>This paper introduces a variant of autoencoder, which is the variational autoencoder. For the ordinary autoencoder, the code is a n-dimensional vector. As for VAE, the code trained is a Gaussian distribution. In the network structure, the mean and the variance of the distribution will be trained. And then, the generator resample the code by the Gaussian distribution encoded. Compared with the vanilla autoencoder, VAE has better performance.</p></div>","C:\Users\smczx\Zotero\storage\GBJ99QGT\1606.html; F:\Google Drive - Monash\Bibliography\Doersch_2021_Tutorial on Variational Autoencoders.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8PJBJU3","journalArticle","2016","Makhzani, Alireza; Shlens, Jonathon; Jaitly, Navdeep; Goodfellow, Ian; Frey, Brendan","Adversarial Autoencoders","arXiv:1511.05644 [cs]","","","","http://arxiv.org/abs/1511.05644","In this paper, we propose the ""adversarial autoencoder"" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.","2016-05-24","2021-03-16 14:06:13","2021-04-08 05:20:40","2021-03-16 14:06:13","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1511.05644","<div data-schema-version=""2""><p>Annotation</p> <p>Adversarial Autoencoder (AAE)</p> <p>The MCMC methods computing the gradient becomes more imprecise in training progress.</p> <p>Some generative models can avoid the difficulties of the training by being trained via direct back-propagation.</p> <p>The AAE can convert an autoencoder as a generative model. Both the reconstruction loss and adversarial loss are used in training, fitting the distribution of latent representation to any prior distribution (for example, normal distribution). The authors proposed several structures for unsupervised, semi-supervised and supervised lerning. In other word, the AAE has a discriminator classifing the latent representation if it is from encoder output or prior distribution. Therefore, with the discriminator loss, the encoder should confuse the discriminator to think all latent representations are from predefined distribution, which means these two distribution are identical. So the distribution of latent representation can be controlled.</p> <p>MNIST, Toronto face dataset (TFD) and SVHN.</p> <p>Use Log-likelihood of test data to compare with other generative networks. The results are the best. Use classification error for comparing the semi-supervised AAE structure with other generative networks. The results are the best compared to the others. Use clustering error rate to compare the clustering performance.</p> <p>+ve: Can use the prior distribution by its samples rather than the explicit functional form. Can be used in different situations (unsupervised, semi-unsupervised, clustering, ...). </p> <p>-ve:</p> <p>AAE is a great example for the improvement of AE, as it combined the concept of AE and GAN to regularize the distribution of latent representations.</p></div>","C:\Users\smczx\Zotero\storage\HFBFFJF8\1511.html; F:\Google Drive - Monash\Bibliography\Makhzani et al_2016_Adversarial Autoencoders.pdf","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQKA9BPK","journalArticle","2014","Shlens, Jonathon","A Tutorial on Principal Component Analysis","arXiv:1404.1100 [cs, stat]","","","","http://arxiv.org/abs/1404.1100","Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.","2014-04-03","2021-03-16 14:08:59","2021-04-06 15:04:42","2021-03-16 14:08:59","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1404.1100","<div data-schema-version=""1""><p>Annotation</p> <p>This paper introduces how the PCA works and how to apply the PCA into datasets. The PCA was invented in 1901, and is a transformation to a new space with smaller dimension by scalar projection. Take a low dimension (2D) dataset as an example, the PCA means transforming the dataset to a line (1D) which has the projection of the dataset with the highest variance. If the transformed dimension is larger than 1, the new axis is located with the highest variance of dataset projection in the orthogonal plane of other axises.</p> <p><br> </p></div>","C:\Users\smczx\Zotero\storage\CR8D93IJ\1404.html; F:\Google Drive - Monash\Bibliography\Shlens_2014_A Tutorial on Principal Component Analysis.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPDBRJGH","journalArticle","1995","Cootes, T. F.; Taylor, C. J.; Cooper, D. H.; Graham, J.","Active Shape Models-Their Training and Application","Computer Vision and Image Understanding","","1077-3142","10.1006/cviu.1995.1004","https://www.sciencedirect.com/science/article/pii/S1077314285710041","Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our Active Shape Models can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images.","1995-01-01","2021-03-16 14:10:07","2021-03-16 14:10:07","2021-03-16 14:10:07","38-59","","1","61","","Computer Vision and Image Understanding","","","","","","","","en","","","","","ScienceDirect","","","<div data-schema-version=""2""><p>Annotation</p> <p>Active Shape Model (ASM)</p> <p>The problem of locating examples of known objects in images.</p> <p>The shape of the objects of same class may vary. Using flexible models or deformable templates can be used to allow for some degree of variability in the shape of the imaged objects.</p> <p>The method is to use mean template and a deformable transformation applied to point distribution model to fit the target shape. And the shape is constrained by PCA.</p> <p>The shapes of registers.</p> <p>No quantitative comparison with other methods.</p> <p>+ve: Low time and space complexity. Can be applied to a wide range of image interpretation tasks.</p> <p>-ve: Require annotations for training images. Not robust to noise, clutter and occlusion.</p> <p>A traditional method for shape fitting with the constrain of PCA and the training with transformation.</p></div>","F:\Google Drive - Monash\Bibliography\Cootes et al_1995_Active Shape Models-Their Training and Application.pdf; C:\Users\smczx\Zotero\storage\A9CBSJ67\S1077314285710041.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NUJHIL5","journalArticle","2001","Cootes, T. F.; Edwards, G. J.; Taylor, C. J.","Active appearance models","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/34.927467","","We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.","2001-06","2021-03-16 14:11:16","2021-04-08 05:27:36","","681-685","","6","23","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","<div data-schema-version=""2""><p>Annotation</p> <p>Active Appearance Models (AAM)</p> <p>Previous methods are very slow or easily become trapped in local minima to fit photo-realistic appearance. Some previous methods cannot fit because the shape-normalized texture map cannot be reconstructed.</p> <p>The authors thought the hypothesis of the optimization problem is similar to each time so can be similarities offline, which can find the directions of rapid convergence.</p> <p>The AAM is a further improvement of ASM, which combining both the shape and texture for fitting the model.</p> <p>Datasets are 100 hand-labeled face images as training set and another 100 as test set.</p> <p>No quantitative comparison to other methods.</p> <p>+ve: Good reliability and robustness of image appearance search.</p> <p>-ve: The training speed of AAM is slower than ASM.</p> <p>The improvement of ASM combining the shape and texture.</p></div>","F:\Google Drive - Monash\Bibliography\Cootes et al_2001_Active appearance models.pdf; C:\Users\smczx\Zotero\storage\2Y22Q7QV\927467.html","","","active appearance models; Active shape model; Deformable models; deformable template; gray-level variation; Image generation; image matching; Image reconstruction; Image segmentation; image texture; Iterative algorithms; iterative method; iterative methods; learning; learning (artificial intelligence); model matching; optimisation; Optimization methods; Robustness; Shape control; shape matching; statistical analysis; statistical models; Surface fitting; texture matching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TFE7ABP5","report","2018","Rhue, Lauren","Racial Influence on Automated Perceptions of Emotions","","","","","https://papers.ssrn.com/abstract=3281765","The practical applications of artificial intelligence are expanding into various elements of society, leading to a growing interest in the potential biases of such algorithms. Facial analysis, one application of artificial intelligence, is increasingly used in real-word situations. For example, some organizations tell candidates to answer predefined questions in a recorded video and use facial recognition to analyze the potential applicant faces. In addition, some companies are developing facial recognition software to scan the faces in crowds and assess threats, specifically mentioning doubt and anger as emotions that indicate threats.  This study provides evidence that facial recognition software interprets emotions differently based on the person’s race. Using a publically available data set of professional basketball players’ pictures, I compare the emotional analysis from two different facial recognition services, Face   and Microsoft's Face API. Both services interpret black players as having more negative emotions than white players; however, there are two different mechanisms. Face   consistently interprets black players as angrier than white players, even controlling for their degree of smiling. Microsoft registers contempt instead of anger, and it interprets black players as more contemptuous when their facial expressions are ambiguous. As the players’ smile widens, the disparity disappears. This finding has implications for individuals, organizations, and society, and it contributes to the growing literature of bias and/or disparate impact in AI.","2018-11-09","2021-03-16 14:12:55","2021-04-06 15:05:45","2021-03-16 14:12:55","","","","","","","","","","","","Social Science Research Network","Rochester, NY","en","","SSRN Scholarly Paper","","","papers.ssrn.com","","DOI: 10.2139/ssrn.3281765","<div data-schema-version=""1""><p>Annotation</p> <p>Racial influence on expression recognition</p></div>","F:\Google Drive - Monash\Bibliography\Rhue_2018_Racial Influence on Automated Perceptions of Emotions.pdf; C:\Users\smczx\Zotero\storage\LJCMBRU6\papers.html","","Unreviewed","artificial intelligence; bias; coarsened exact matching; econometrics; facial recognition; race","","","","","","","","","","","","","","","","","","","ID 3281765","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZHFBZTZ","conferencePaper","2011","Rudovic, O.; Pantic, M.","Shape-constrained Gaussian process regression for facial-point-based head-pose normalization","2011 International Conference on Computer Vision","","","10.1109/ICCV.2011.6126407","","Given the facial points extracted from an image of a face in an arbitrary pose, the goal of facial-point-based head-pose normalization is to obtain the corresponding facial points in a predefined pose (e.g., frontal). This involves inference of complex and high-dimensional mappings due to the large number of the facial points employed, and due to differences in head-pose and facial expression. Most regression-based approaches for learning such mappings focus on modeling correlations only between the inputs (i.e., the facial points in a non-frontal pose) and the outputs (i.e., the facial points in the frontal pose), but not within the inputs and the outputs of the model. This makes these models prone to errors due to noise and outliers in test data, often resulting in anatomically impossible facial configurations formed by their predictions. To address this, we propose Shape-constrained Gaussian Process (SC-GP) regression for facial-point-based head-pose normalization. Specifically, a deformable face-shape model is used to learn a face-shape prior, which is placed on both the input and the output of GP regression in order to constrain the model predictions to anatomically feasible facial configurations. Our extensive experiments on both synthetic and real image data show that the proposed approach generalizes well across poses and handles successfully noise and outliers in test data. In addition, the proposed model outperforms previously proposed approaches to facial-point-based head-pose normalization.","2011-11","2021-03-16 14:14:23","2021-04-06 15:01:43","","1495-1502","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2380-7504","<div data-schema-version=""1""><p>Annotation</p> <p>Gaussian process regression and apply PCA to manipulate</p></div>","C:\Users\smczx\Zotero\storage\T82G2MK8\6126407.html; F:\Google Drive - Monash\Bibliography\Rudovic_Pantic_2011_Shape-constrained Gaussian process regression for facial-point-based head-pose.pdf","","Unreviewed","Principal component analysis; Deformable models; arbitrary pose; Computational modeling; Data models; deformable face-shape model; face recognition; facial point extraction; facial-point-based head-pose normalization; feature extraction; Gaussian processes; high-dimensional mappings; pose estimation; regression analysis; Shape; shape-constrained Gaussian process regression; Three dimensional displays; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 International Conference on Computer Vision","","","","","","","","","","","","","","",""
"9LTVQVP7","journalArticle","2019","Li, S.; Deng, W.","Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition","IEEE Transactions on Image Processing","","1941-0042","10.1109/TIP.2018.2868382","","Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30 000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI, and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning-based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community.","2019-01","2021-03-16 14:16:39","2021-04-06 15:06:02","","356-370","","1","28","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Image Processing","<div data-schema-version=""1""><p>Annotation</p> <p>RAF-DB</p></div>","F:\Google Drive - Monash\Bibliography\Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf","","Unreviewed","convolution; Machine learning; learning (artificial intelligence); face recognition; basic emotion; CK+ database; compound emotion; Compounds; cross-database study; crowdsourcing annotation; Databases; deep learning; deep learning-based methods; deep locality-preserving convolutional neural network method; descriptor encodings; DLP-CNN; emotion labels; emotion recognition; expectation-maximisation algorithm; expectation-maximization algorithm; express compound; Expression recognition; Face; Face recognition; facial behavior posed; facial expression database; facial images; feedforward neural nets; inter-class scatter; laboratory-controlled emotions; locality closeness; mixture emotions; multimodal expressions recognition; RAF database; RAF-DB; Reactive power; real-world affective face database; real-world emotions; Reliability; unconstrained facial expression recognition; visual databases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7ZNACBK","journalArticle","2015","Simonyan, Karen; Zisserman, Andrew","Very Deep Convolutional Networks for Large-Scale Image Recognition","arXiv:1409.1556 [cs]","","","","http://arxiv.org/abs/1409.1556","In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.","2015-04-10","2021-03-16 14:24:33","2021-04-08 07:19:57","2021-03-16 14:24:33","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1409.1556","<div data-schema-version=""2""><p>Annotation</p> <p>VGG</p> <p>Improve the previous CNN structure for better accuracy for ILSVRC.</p> <p>The depth of the structure of CNN can be discussed for better performance of the model.</p> <p>They proposed two best VGG-16 and VGG-19 structures. Instead of using big kernel size, the VGG only use 3x3 kernel sizes and 16 layers or 19 layers.</p> <p>ILSVRC-2014</p> <p>The metric is test set classification error. Their score is better than the best of ILSVRC-2013.</p> <p>+ve: More depth is beneficial for the performance. VGG structures has widely usage for other tasks.</p> <p>-ve:</p> <p>Provide a famous VGG structure for CNN area, which indicates the depth contributes more than width of CNN.</p></div>","C:\Users\smczx\Zotero\storage\9VVQ6FR8\1409.html; F:\Google Drive - Monash\Bibliography\Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGZ5ML22","conferencePaper","2016","He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian","Deep Residual Learning for Image Recognition","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html","Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","2016","2021-03-16 14:25:33","2021-05-01 10:30:11","","770-778","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>ResNet and Residual block</p> <p>Will the model be better with more layers? How to solve the problem of gradients vanishment and explosion.</p> <p>The hypothesis is the model constructed with some identity mappings can produce a better training error than shallower models.</p> <p>In the residual block, there is an identity mapping to the output of the block and sum them. In the residual networks (ResNet), &nbsp;they are formed by multiple residual blocks with different number of layers. </p> <p>ImageNet 2012 classification dataset. CIFAR-10. PASCAL VOC 2007 and 2012 and COCO.</p> <p>The comparison between the models of 18 layers and 34 layers indicates the ResNet can produce better performance when the number of layers is deeper. Then the ResNets are compared with other networks, leading to a better result. The compared applied to CIFAR-10 between different layer versions of ResNet shows extremely deep network does not produce a better result (ResNet-1202 vs ResNet-110). Also, the experiments for object detection on PASCAL VOC 2007, 2012 and COCO shows the ResNet has good generalization performance.</p> <p>+ve: Produce better performance with deeper ResNet. Has good generalization performance.</p> <p>-ve:</p> <p>A great method that release the limitation of the depth of model design to let the researchers available for any depth they want, which powered the community has better flexbility of networks architecture.</p> </div>","F:\Google Drive - Monash\Bibliography\He et al_2016_Deep Residual Learning for Image Recognition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"5DX7TJDY","journalArticle","2014","Lin, Min; Chen, Qiang; Yan, Shuicheng","Network In Network","arXiv:1312.4400 [cs]","","","","http://arxiv.org/abs/1312.4400","We propose a novel deep network structure called ""Network In Network"" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","2014-03-04","2021-03-16 14:27:20","2021-04-08 06:08:21","2021-03-16 14:27:20","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1312.4400","<div data-schema-version=""2""><p>Annotation</p> <p>NIN</p> <p>The traditional convolutional layer as generalized linear model (GLM) has low level of abstraction.</p> <p>By replacing the GLM in convolutional layer with a non-linear function approximator can improve the &nbsp;abstraction ability of local model.</p> <p>The authors proposed two concepts, using MLP to replace the GLM in convolutional layer, and the usage of global average pooling layer. They use convolution layers with 1x1 kernel to simulate the MLP inside. And the global average pooling layer is used to replace the fully-connected layers in traditional CNN.</p> <p>CIFAR-10, CIFAR-100, SVHN, MNIST.</p> <p>The metric is test set classification error. In these 4 datasets, the NIN has an outstanding performance compared to other structures. Also the global average pooling as a regularizer is also better than fully connected + dropout set.</p> <p>+ve: The Mlpconv allows more complex and learnable interaction of cross channel information. Global average pooling is more meaningful and interpretable. Global average pooling is more robust to spatial translations of the input.</p> <p>-ve:</p> <p>The ideas in this paper is amazing, and many famous structures such as ResNet and GoogLeNet use these ideas.</p> <p></p></div>; Comment: 10 pages, 4 figures, for iclr2014","C:\Users\smczx\Zotero\storage\GBJXNLGT\1312.html; F:\Google Drive - Monash\Bibliography\Lin et al_2014_Network In Network.pdf","","","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQEVWNAP","conferencePaper","2015","Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew","Going Deeper With Convolutions","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html","We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification.","2015","2021-03-16 14:27:59","2021-04-08 09:13:54","2021-03-16 14:27:59","1-9","","","","","","","","","","","","","","","","","","www.cv-foundation.org","","","<div data-schema-version=""2""><p>Annotation</p> <p>Inception Module and GoogLeNet (InceptionV1)</p> <p>The mobile and embedded computting has limited power for models. A high-efficiency method is required.</p> <p>Decide to develop a high-efficiency method keeping a computational budget of 1.5 billion operations.</p> <p>The Inception module has multiple routes for input features. Each route has different kernel size of convolutions and max-pooling. And then, concatenate the outputs as the result of this module. Otherwise, the 1x1 convolutions can be used to reduce the number of channels. GoogLeNet is the model formed by Inception modules.</p> <p>ILSVRC 2014</p> <p>The metric is Top-5 error in ILSVRC 2014. The GoogLeNet ranked 1st in this competition.</p> <p>+ve: A significant quality improvement with slightly increase of computational requirements. Also available for object detection.</p> <p>-ve:</p> <p>The Inception Module is the new direction that the CNN can be not sequence. And, the high-efficiency also is an important metrics to judge the usability of a neural network structure.</p></div>","C:\Users\smczx\Zotero\storage\PTFRKBPB\Szegedy_Going_Deeper_With_2015_CVPR_paper.html; F:\Google Drive - Monash\Bibliography\Szegedy et al_2015_Going Deeper With Convolutions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"Y99BNDVT","journalArticle","2012","Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey","ImageNet Classification with Deep Convolutional Neural Networks","Advances in neural information processing systems","","","10.1145/3065386","","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.","2012-01-01","2021-03-16 14:39:49","2021-04-06 15:04:35","","1097–1105","","","25","","Neural Information Processing Systems","","","","","","","","","","","","","ResearchGate","","","<div data-schema-version=""2""><p>Annotation</p> <p>AlexNet / ReLU</p></div>","F:\Google Drive - Monash\Bibliography\Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf; ","https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7T2LWIMS","conferencePaper","2020","Pidhorskyi, Stanislav; Adjeroh, Donald A.; Doretto, Gianfranco","Adversarial Latent Autoencoders","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html","Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.","2020","2021-03-16 14:44:03","2021-04-06 15:06:27","2021-03-16 14:44:03","14104-14113","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf; C:\Users\smczx\Zotero\storage\8TBAEC8L\Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"ZS3LF4TU","journalArticle","2017","Arjovsky, Martin; Chintala, Soumith; Bottou, Léon","Wasserstein GAN","arXiv:1701.07875 [cs, stat]","","","","http://arxiv.org/abs/1701.07875","We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.","2017-12-06","2021-03-16 14:44:26","2021-04-06 15:05:33","2021-03-16 14:44:26","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1701.07875","<div data-schema-version=""1""><p>Annotation</p> <p>WGAN</p></div>","F:\Google Drive - Monash\Bibliography\Arjovsky et al_2017_Wasserstein GAN.pdf; C:\Users\smczx\Zotero\storage\PWZ8UTT5\1701.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMPBRIPK","journalArticle","2020","Zheng, Xin; Guo, Yanqing; Huang, Huaibo; Li, Yi; He, Ran","A Survey of Deep Facial Attribute Analysis","International Journal of Computer Vision","","1573-1405","10.1007/s11263-020-01308-z","https://doi.org/10.1007/s11263-020-01308-z","Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.","2020-09-01","2021-03-16 14:46:55","2021-04-06 15:06:45","2021-03-16 14:46:55","2002-2034","","8","128","","Int J Comput Vis","","","","","","","","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Introduction to all facial analysis</p></div>","F:\Google Drive - Monash\Bibliography\Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNGWI5TW","journalArticle","2016","Radford, Alec; Metz, Luke; Chintala, Soumith","Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","arXiv:1511.06434 [cs]","","","","http://arxiv.org/abs/1511.06434","In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","2016-01-07","2021-03-16 14:49:34","2021-04-06 15:05:28","2021-03-16 14:49:34","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1511.06434","<div data-schema-version=""2""><p>Annotation</p> <p>DCGAN</p></div>; Comment: Under review as a conference paper at ICLR 2016","C:\Users\smczx\Zotero\storage\DQWLAZHB\1511.html; F:\Google Drive - Monash\Bibliography\Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf","","Unreviewed","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTCMR42I","conferencePaper","2015","Ioffe, Sergey; Szegedy, Christian","Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v37/ioffe15.html","Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the t...","2015-06-01","2021-03-16 14:50:26","2021-05-01 10:31:18","","448-456","","","","","","Batch Normalization","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","<div data-schema-version=""2""><p>Annotation</p> <p>Batch normalization and InceptionV2</p> <p>The hidden layers in neural networks need to continuously adapt to the new distribution or will cause the covariate shift (Shimodaira, 2000).</p> <p>The optimizer will train fast when the distribution of input remains more stable. </p> <p>They proposed a method called Batch Normalization. The BN transform applied to the mini-batch of the input. The BN layer will calculate the mini-batch mean and variance to normalize the input batch to similar to N(0, 1). And also will learn a scale and shift function to map the output better.</p> <p>MNIST</p> <p>The metric is classification error. The training is faster and the result is more accurate than the model without BN.</p> <p>+ve: The training speed is faster. The performance of model is better.</p> <p>-ve: Not explored the full range of other possibilities of BN.</p> <p>The BN contributes a lot for modern deep models to improve the speed in training and the performance. In the many models today, the BN layers will apply after each convolutional layers to improve the performance.</p> </div>","F:\Google Drive - Monash\Bibliography\Ioffe_Szegedy_2015_Batch Normalization.pdf; C:\Users\smczx\Zotero\storage\T2BGYQWB\ioffe15.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"VP88VTAW","conferencePaper","2020","Mildenhall, Ben; Srinivasan, Pratul; Tancik, Matthew; Barron, Jonathan; Ramamoorthi, Ravi; Ng, Ren","NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","European Conference on Computer Vision","978-3-030-58451-1","","10.1007","","We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","2020","2021-03-16 14:54:40","2021-03-16 15:32:14","","405-421","","","","","","","","","","","","Springer, Cham","","","","","","","","DOI: 10.1007/978-3-030-58452-8_24","","F:\Google Drive - Monash\Bibliography\Mildenhall et al_2020_NeRF.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","European Conference on Computer Vision","","","","","","","","","","","","","","",""
"C3G62U2M","journalArticle","2020","Pumarola, Albert; Corona, Enric; Pons-Moll, Gerard; Moreno-Noguer, Francesc","D-NeRF: Neural Radiance Fields for Dynamic Scenes","arXiv:2011.13961 [cs]","","","","http://arxiv.org/abs/2011.13961","Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.","2020-11-27","2021-03-16 14:59:24","2021-03-16 15:32:20","2021-03-16 14:59:24","","","","","","","D-NeRF","","","","","","","","","","","","arXiv.org","","arXiv: 2011.13961","","C:\Users\smczx\Zotero\storage\4M84DPVT\2011.html; F:\Google Drive - Monash\Bibliography\Pumarola et al_2020_D-NeRF.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLLHIHTD","conferencePaper","2016","Johnson, Justin; Alahi, Alexandre; Fei-Fei, Li","Perceptual Losses for Real-Time Style Transfer and Super-Resolution","Computer Vision – ECCV 2016","978-3-319-46475-6","","10.1007/978-3-319-46475-6_43","","We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.","2016","2021-03-18 05:21:38","2021-04-06 15:05:03","","694-711","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Perceptual loss is the difference (MSE) between features of hidden layers in loss network from y_true and y_pred.</p></div>","F:\Google Drive - Monash\Bibliography\Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf","","Unreviewed","Deep learning; Style transfer; Super-resolution","Leibe, Bastian; Matas, Jiri; Sebe, Nicu; Welling, Max","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCKMHJ8W","conferencePaper","2020","Hong, Ming; Xie, Yuan; Li, Cuihua; Qu, Yanyun","Distilling Image Dehazing With Heterogeneous Task Imitation","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html","","2020","2021-03-18 05:07:28","2021-04-06 15:06:16","2021-03-18 05:07:28","3462-3471","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation and an autoencoder teacher to train a dehazing network with representation mimicking loss.</p></div>","F:\Google Drive - Monash\Bibliography\Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf; C:\Users\smczx\Zotero\storage\ABU8RUMW\Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"YEH4KREU","conferencePaper","2019","Gao, Qinquan; Zhao, Yan; Li, Gen; Tong, Tong","Image Super-Resolution Using Knowledge Distillation","Computer Vision – ACCV 2018","978-3-030-20890-5","","10.1007/978-3-030-20890-5_34","","The significant improvements in image super-resolution (SR) in recent years is majorly resulted from the use of deeper and deeper convolutional neural networks (CNN). However, both computational time and memory consumption simultaneously increase with the utilization of very deep CNN models, posing challenges to deploy SR models in realtime on computationally limited devices. In this work, we propose a novel strategy that uses a teacher-student network to improve the image SR performance. The training of a small but efficient student network is guided by a deep and powerful teacher network. We have evaluated the performance using different ways of knowledge distillation. Through the validations on four datasets, the proposed method significantly improves the SR performance of a student network without changing its structure. This means that the computational time and the memory consumption do not increase during the testing stage while the SR performance is significantly improved.","2019","2021-03-18 05:04:35","2021-04-06 15:05:55","","527-541","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Perform super resolution with knowledge distillation.</p></div>","F:\Google Drive - Monash\Bibliography\Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf","","Unreviewed","Super-resolution; Convolutional neural networks; Knowledge distillation; Teacher-student network","Jawahar, C. V.; Li, Hongdong; Mori, Greg; Schindler, Konrad","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94Q7XD2W","conferencePaper","2018","Woo, Sanghyun; Park, Jongchan; Lee, Joon-Young; Kweon, In So","CBAM: Convolutional Block Attention Module","Proceedings of the European Conference on Computer Vision (ECCV)","","","","https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html","","2018","2021-03-18 03:50:54","2021-04-06 15:05:47","2021-03-18 03:50:54","3-19","","","","","","CBAM","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>With Channel Attention Module and Spatial Attention Module, the CBAM can be inserted into conv blocks to improve the CNN performance.</p></div>","C:\Users\smczx\Zotero\storage\JEERU7SI\Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html; F:\Google Drive - Monash\Bibliography\Woo et al_2018_CBAM.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV)","","","","","","","","","","","","","","",""
"SC6MFJRV","journalArticle","2020","Pihlgren, Gustav Grund; Sandin, Fredrik; Liwicki, Marcus","Pretraining Image Encoders without Reconstruction via Feature Prediction Loss","arXiv:2003.07441 [cs]","","","","http://arxiv.org/abs/2003.07441","This work investigates three methods for calculating loss for autoencoder-based pretraining of image encoders: The commonly used reconstruction loss, the more recently introduced deep perceptual similarity loss, and a feature prediction loss proposed here; the latter turning out to be the most efficient choice. Standard auto-encoder pretraining for deep learning tasks is done by comparing the input image and the reconstructed image. Recent work shows that predictions based on embeddings generated by image autoencoders can be improved by training with perceptual loss, i.e., by adding a loss network after the decoding step. So far the autoencoders trained with loss networks implemented an explicit comparison of the original and reconstructed images using the loss network. However, given such a loss network we show that there is no need for the time-consuming task of decoding the entire image. Instead, we propose to decode the features of the loss network, hence the name ""feature prediction loss"". To evaluate this method we perform experiments on three standard publicly available datasets (LunarLander-v2, STL-10, and SVHN) and compare six different procedures for training image encoders (pixel-wise, perceptual similarity, and feature prediction losses; combined with two variations of image and feature encoding/decoding). The embedding-based prediction results show that encoders trained with feature prediction loss is as good or better than those trained with the other two losses. Additionally, the encoder is significantly faster to train using feature prediction loss in comparison to the other losses. The method implementation used in this work is available online: https://github.com/guspih/Perceptual-Autoencoders","2020-07-15","2021-03-18 03:03:43","2021-04-06 15:06:29","2021-03-18 03:03:43","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2003.07441","<div data-schema-version=""1""><p>Annotation</p> <p>Another way to train the Autoencoder with other pretrained feature extractor models.</p></div>","C:\Users\smczx\Zotero\storage\TYYGD6EZ\2003.html; F:\Google Drive - Monash\Bibliography\Pihlgren et al_2020_Pretraining Image Encoders without Reconstruction via Feature Prediction Loss.pdf","","Unreviewed","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLTXINZX","conferencePaper","2020","Viazovetskyi, Yuri; Ivashkin, Vladimir; Kashin, Evgeny","StyleGAN2 Distillation for Feed-Forward Image Manipulation","Computer Vision – ECCV 2020","978-3-030-58542-6","","10.1007/978-3-030-58542-6_11","","StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces’ transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.","2020","2021-03-18 02:37:32","2021-04-06 15:06:32","","170-186","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""1""><p>Annotation</p> <p>Using StyleGAN to generate pairs for pix2pixHD to be trained as image translation task.</p></div>","F:\Google Drive - Monash\Bibliography\Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf","","Unreviewed","Computer vision; Distillation; StyleGAN2; Synthetic data","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Vision – ECCV 2020","","","","","","","","","","","","","","",""
"8YP6UX9S","journalArticle","2019","Wang, J.; Gou, L.; Zhang, W.; Yang, H.; Shen, H.","DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation","IEEE Transactions on Visualization and Computer Graphics","","1941-0506","10.1109/TVCG.2019.2903943","","Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.","2019-06","2021-03-18 01:51:56","2021-04-06 15:06:09","","2168-2180","","6","25","","","DeepVID","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Visualization and Computer Graphics","<div data-schema-version=""1""><p>Annotation</p> <p>Using VAE and knowledge distillation to generate neighbours of interesting points.</p></div>","C:\Users\smczx\Zotero\storage\ZF8Z5MGH\8667661.html; F:\Google Drive - Monash\Bibliography\Wang et al_2019_DeepVID.pdf","","Unreviewed","Neural networks; learning (artificial intelligence); Data models; Training; Deep learning; Analytical models; deep generative model; deep learning experts; Deep neural networks; Deep Neural Networks; deep visual diagnosis; deep visual interpretation; DeepVID; DNN; generative model; image classification; image classifiers; knowledge distillation; model interpretation; neural nets; safety-critical applications; Semantics; visual analytics; Visual analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTVLVRAI","journalArticle","2015","Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff","Distilling the Knowledge in a Neural Network","arXiv:1503.02531 [cs, stat]","","","","http://arxiv.org/abs/1503.02531","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","2015-03-09","2021-03-17 12:52:08","2021-04-06 15:04:46","2021-03-17 12:52:08","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1503.02531","<div data-schema-version=""1""><p>Annotation</p> <p>Knowledge Distillation</p></div>; Comment: NIPS 2014 Deep Learning Workshop","C:\Users\smczx\Zotero\storage\8KILIT88\1503.html; F:\Google Drive - Monash\Bibliography\Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EWLEGZ2W","conferencePaper","2020","Li, Zeqi; Jiang, Ruowei; Aarabi, Parham","Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation","Computer Vision – ECCV 2020","978-3-030-58574-7","","10.1007/978-3-030-58574-7_39","","Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher’s feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively.","2020","2021-03-22 05:27:10","2021-04-06 15:06:23","","648-663","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation to perform image-image translation with proposed semantic relation preserving matrix and semantic preserving distillation loss.</p></div>","F:\Google Drive - Monash\Bibliography\Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf","","Unreviewed","Knowledge distillation; Generative adversarial networks; Image-to-image translation; Model compression","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Vision – ECCV 2020","","","","","","","","","","","","","","",""
"QYH9LAF7","conferencePaper","2019","Zhai, Mengyao; Chen, Lei; Tung, Frederick; He, Jiawei; Nawhal, Megha; Mori, Greg","Lifelong GAN: Continual Learning for Conditional Image Generation","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html","","2019","2021-03-22 05:51:57","2021-04-06 15:06:13","2021-03-22 05:51:57","2759-2768","","","","","","Lifelong GAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>Lifelong GAN. Using knowledge distillation to perform continous training BicycleGAN for image translation avoiding forgeting.</p></div>","C:\Users\smczx\Zotero\storage\MCSJBUDY\Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html; F:\Google Drive - Monash\Bibliography\Zhai et al_2019_Lifelong GAN.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"HRBB3NBJ","journalArticle","2018","Zhu, Jun-Yan; Zhang, Richard; Pathak, Deepak; Darrell, Trevor; Efros, Alexei A.; Wang, Oliver; Shechtman, Eli","Toward Multimodal Image-to-Image Translation","arXiv:1711.11586 [cs, stat]","","","","http://arxiv.org/abs/1711.11586","Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.","2018-10-23","2021-03-22 07:05:57","2021-04-06 15:05:49","2021-03-22 07:05:57","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1711.11586","<div data-schema-version=""1""><p>Annotation</p> <p>BicycleGAN</p></div>; Comment: NIPS 2017 Final paper. v4 updated acknowledgment. Website: https://junyanz.github.io/BicycleGAN/","C:\Users\smczx\Zotero\storage\4JC6HZMQ\1711.html; F:\Google Drive - Monash\Bibliography\Zhu et al_2018_Toward Multimodal Image-to-Image Translation.pdf","","Unreviewed","Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRK93TD9","conferencePaper","2016","Larsen, Anders Boesen Lindbo; Sønderby, Søren Kaae; Larochelle, Hugo; Winther, Ole","Autoencoding beyond pixels using a learned similarity metric","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v48/larsen16.html","We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GA...","2016-06-11","2021-03-22 07:10:53","2021-04-06 15:05:07","2021-03-22 07:10:53","1558-1566","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","<div data-schema-version=""1""><p>Annotation</p> <p>VAE-GAN is introduced to combine VAE and GAN to improve the GAN training.</p></div>","F:\Google Drive - Monash\Bibliography\Larsen et al_2016_Autoencoding beyond pixels using a learned similarity metric.pdf; C:\Users\smczx\Zotero\storage\4IHWY4CY\larsen16.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"5PPVC36J","conferencePaper","2017","Bao, Jianmin; Chen, Dong; Wen, Fang; Li, Houqiang; Hua, Gang","CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html","","2017","2021-03-22 07:12:39","2021-04-06 15:05:36","2021-03-22 07:12:39","2745-2754","","","","","","CVAE-GAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Intoducing CVAE-GAN (Conditional VAE-GAN) by using label to improve the performance of VAE-GAN.</p></div>","F:\Google Drive - Monash\Bibliography\Bao et al_2017_CVAE-GAN.pdf; C:\Users\smczx\Zotero\storage\HJSQ369H\Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"4FLMXLVV","journalArticle","2020","Yuan, M.; Peng, Y.","CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis","IEEE Transactions on Multimedia","","1941-0077","10.1109/TMM.2019.2951463","","Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.","2020-08","2021-03-22 08:05:43","2021-04-06 15:06:40","","1955-1968","","8","22","","","CKD","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Multimedia","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation to perform text to image synthesis task.</p></div>","C:\Users\smczx\Zotero\storage\DTFEDK7M\8890866.html; F:\Google Drive - Monash\Bibliography\Yuan_Peng_2020_CKD.pdf","","Unreviewed","Neural networks; learning (artificial intelligence); knowledge distillation; Semantics; Generative adversarial networks; approximate the real data distribution; CKD; complex RGB pixel space; consistent semantic contents; cross-task knowledge distillation approach; highly challenging task; huge pixel space; Image color analysis; image colour analysis; image contents; image semantic understanding; Image synthesis; multiple image semantic understanding tasks; multistage knowledge distillation paradigm; pattern recognition; photo-realistic images; realistic images; semantic consistency issue; semantic representation; semantic space; synthetic images; T2IS; Task analysis; text analysis; text description; text-to-image synthesis; Text-to-image synthesis; transfer learning; visual quality issue; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ES8WZY5Q","conferencePaper","2019","Tung, Frederick; Mori, Greg","Similarity-Preserving Knowledge Distillation","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html","","2019","2021-03-22 08:16:09","2021-04-06 15:06:07","2021-03-22 08:16:09","1365-1374","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Using corsine similarity matrix to represent the relation in each batch, to replace the feature map to be learned in knowledge distillation.</p></div>","C:\Users\smczx\Zotero\storage\NXTWWDCS\Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html; F:\Google Drive - Monash\Bibliography\Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"J956Z7GT","journalArticle","2020","Jing, L.; Tian, Y.","Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/TPAMI.2020.2992393","","Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.","2020","2021-03-29 04:54:01","2021-03-29 04:55:56","","1-1","","","","","","Self-supervised Visual Feature Learning with Deep Neural Networks","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","","C:\Users\smczx\Zotero\storage\X8PV7RTB\9086055.html; F:\Google Drive - Monash\Bibliography\Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf","","Unread","Feature extraction; Training; Task analysis; Visualization; Annotations; Convolutional Neural Network; Deep Learning; Learning systems; Self-supervised Learning; Transfer Learning; Unsupervised Learning; Videos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YN6MUWWP","conferencePaper","2017","Huang, Xun; Belongie, Serge","Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html","","2017","2021-03-29 04:43:34","2021-04-09 01:53:06","2021-03-29 04:43:34","1501-1510","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Huang_Belongie_2017_Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization.pdf; C:\Users\smczx\Zotero\storage\ESXAEPLT\Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"AVFLWRB6","conferencePaper","2019","Karras, Tero; Laine, Samuli; Aila, Timo","A Style-Based Generator Architecture for Generative Adversarial Networks","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html","","2019","2021-03-29 04:41:33","2021-04-01 05:28:10","2021-03-29 04:41:33","4401-4410","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf; C:\Users\smczx\Zotero\storage\8LKELHY9\Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"L8IB9M3W","conferencePaper","2020","Wang, Huan; Li, Yijun; Wang, Yuehai; Hu, Haoji; Yang, Ming-Hsuan","Collaborative Distillation for Ultra-Resolution Universal Style Transfer","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html","Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.","2020","2021-03-29 04:33:48","2021-03-29 04:35:29","2021-03-29 04:33:48","1860-1869","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\smczx\Zotero\storage\VZ2DNRMF\Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html; F:\Google Drive - Monash\Bibliography\Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"FG3L3HGN","journalArticle","2019","Brock, Andrew; Donahue, Jeff; Simonyan, Karen","Large Scale GAN Training for High Fidelity Natural Image Synthesis","arXiv:1809.11096 [cs, stat]","","","","http://arxiv.org/abs/1809.11096","Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.","2019-02-25","2021-03-29 04:21:24","2021-03-29 04:24:47","2021-03-29 04:21:24","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1809.11096","","C:\Users\smczx\Zotero\storage\KYRR8X9E\1809.html; F:\Google Drive - Monash\Bibliography\Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis2.pdf","","Unread","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TD5237DC","journalArticle","2017","Zhang, Xiangyu; Zhou, Xinyu; Lin, Mengxiao; Sun, Jian","ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices","arXiv:1707.01083 [cs]","","","","http://arxiv.org/abs/1707.01083","We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.","2017-12-07","2021-03-29 04:17:08","2021-03-29 04:17:23","2021-03-29 04:17:08","","","","","","","ShuffleNet","","","","","","","","","","","","arXiv.org","","arXiv: 1707.01083","","C:\Users\smczx\Zotero\storage\NJXA8YRI\1707.html; F:\Google Drive - Monash\Bibliography\Zhang et al_2017_ShuffleNet.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SZH46QB","conferencePaper","2020","Yin, Hongxu; Molchanov, Pavlo; Alvarez, Jose M.; Li, Zhizhong; Mallya, Arun; Hoiem, Derek; Jha, Niraj K.; Kautz, Jan","Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html","We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ""invert"" a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.","2020","2021-03-29 04:15:29","2021-03-29 04:16:21","2021-03-29 04:15:29","8715-8724","","","","","","Dreaming to Distill","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Yin et al_2020_Dreaming to Distill.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"UWGZZ5T8","journalArticle","2020","Li, Kang; Yu, Lequan; Wang, Shujun; Heng, Pheng-Ann","Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","10.1609/aaai.v34i01.5421","https://ojs.aaai.org/index.php/AAAI/article/view/5421","","2020-04-03","2021-03-29 00:40:26","2021-04-06 15:06:21","2021-03-29 00:40:26","775-783","","01","34","","AAAI","","","","","","","","en","Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","Number: 01","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation to improve the performance of image segmentation.</p></div>","F:\Google Drive - Monash\Bibliography\Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf; C:\Users\smczx\Zotero\storage\K5LHY756\5421.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBQELRWF","journalArticle","2020","Zhu, M.; Li, J.; Wang, N.; Gao, X.","Knowledge Distillation for Face Photo-Sketch Synthesis","IEEE Transactions on Neural Networks and Learning Systems","","2162-2388","10.1109/TNNLS.2020.3030536","","Significant progress has been made with face photo-sketch synthesis in recent years due to the development of deep convolutional neural networks, particularly generative adversarial networks (GANs). However, the performance of existing methods is still limited because of the lack of training data (photo-sketch pairs). To address this challenge, we investigate the effect of knowledge distillation (KD) on training neural networks for the face photo-sketch synthesis task and propose an effective KD model to improve the performance of synthetic images. In particular, we utilize a teacher network trained on a large amount of data in a related task to separately learn knowledge of the face photo and knowledge of the face sketch and simultaneously transfer this knowledge to two student networks designed for the face photo-sketch synthesis task. In addition to assimilating the knowledge from the teacher network, the two student networks can mutually transfer their own knowledge to further enhance their learning. To further enhance the perception quality of the synthetic image, we propose a KD+ model that combines GANs with KD. The generator can produce images with more realistic textures and less noise under the guide of knowledge. Extensive experiments and a user study demonstrate the superiority of our models over the state-of-the-art methods.","2020","2021-03-29 00:18:36","2021-04-06 15:06:48","","1-14","","","","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Neural Networks and Learning Systems","<div data-schema-version=""1""><p>Annotation</p> <p>Using Knowledge Distillation and GAN to translate images between face photo and sketch.</p></div>","C:\Users\smczx\Zotero\storage\FZIX6WUE\9240973.html; F:\Google Drive - Monash\Bibliography\Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf","","Unreviewed","Data models; Face recognition; Task analysis; Face photo-sketch synthesis; Faces; Gallium nitride; generative adversarial networks (GANs); knowledge distillation (KD); Knowledge engineering; teacher-student model.; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7UITLDP","journalArticle","2019","Aguinaldo, Angeline; Chiang, Ping-Yeh; Gain, Alex; Patil, Ameya; Pearson, Kolten; Feizi, Soheil","Compressing GANs using Knowledge Distillation","arXiv:1902.00159 [cs, stat]","","","","http://arxiv.org/abs/1902.00159","Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super resolution, and synthetic data generation. State-of-the-art GANs often use tens of millions of parameters, making them expensive to deploy for applications in low SWAP (size, weight, and power) hardware, such as mobile devices, and for applications with real time capabilities. There has been no work found to reduce the number of parameters used in GANs. Therefore, we propose a method to compress GANs using knowledge distillation techniques, in which a smaller ""student"" GAN learns to mimic a larger ""teacher"" GAN. We show that the distillation methods used on MNIST, CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1, 58:1, and 87:1, respectively, while retaining the quality of the generated image. From our experiments, we observe a qualitative limit for GAN's compression. Moreover, we observe that, with a fixed parameter budget, compressed GANs outperform GANs trained using standard training methods. We conjecture that this is partially owing to the optimization landscape of over-parameterized GANs which allows efficient training using alternating gradient descent. Thus, training an over-parameterized GAN followed by our proposed compression scheme provides a high quality generative model with a small number of parameters.","2019-01-31","2021-03-26 07:04:30","2021-04-06 15:05:51","2021-03-26 07:04:30","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1902.00159","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation to compress the generator of GAN.</p></div>","F:\Google Drive - Monash\Bibliography\Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf; C:\Users\smczx\Zotero\storage\QQ42XCWA\1902.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDJH6KVN","journalArticle","1901","F.R.S, Karl Pearson","LIII. On lines and planes of closest fit to systems of points in space","The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science","","1941-5982","10.1080/14786440109462720","https://doi.org/10.1080/14786440109462720","","1901-11-01","2021-04-08 05:23:21","2021-04-08 05:24:13","2021-04-08 05:23:21","559-572","","11","2","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/14786440109462720","<div data-schema-version=""1""><p>Annotation</p> <p>PCA original paper</p></div>","F:\Google Drive - Monash\Bibliography\F.R.S_1901_LIII.pdf; C:\Users\smczx\Zotero\storage\NTCZUUMU\14786440109462720.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"698AHGVY","journalArticle","2014","Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua","Generative Adversarial Networks","arXiv:1406.2661 [cs, stat]","","","","http://arxiv.org/abs/1406.2661","We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","2014-06-10","2021-04-08 09:17:51","2021-04-08 09:17:51","2021-04-08 09:17:51","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1406.2661","<div data-schema-version=""2""><p>Annotation</p> <p>Generative Adversarial Networks (GAN)</p> <p>It is difficult to approximate because the difficulty of utilizing the benefits of picewise linear units in the generative context.</p> <p>It is possible to use a adversarial structures to overcome the difficulties.</p> <p>The GAN framework has two models, generative model and discriminative model. The generator input a noise vector try to confuse the discriminator, and the discriminator can classify the image is from real dataset or generator.</p> <p>MNIST, Toronto Face Database (TFD), CIFAR-10</p> <p>The metric is window-based log-likelihood etimates. The adversarial nets is better than other generative networks.</p> <p>+ve: Has potential in the future. No inference is needed during learning. A wide variety of functions can be incorporated into the model. The adversarial models does not directly receive any information from dataset. GAN can represent very sharp distributions.</p> <p>-ve: The discriminator must be synchronized well with generator during training (Hard to train). </p> <p>The development of GAN broaden the area of image synthesis and translation area. GAN has the advantages of generating very realistic data, so will be widely used in the future generation tasks. However, GAN is hard to train, but there will be future works to improve that.</p></div>","C:\Users\smczx\Zotero\storage\VPGCMDSE\1406.html; F:\Google Drive - Monash\Bibliography\Goodfellow et al_2014_Generative Adversarial Networks.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YWSL99D4","journalArticle","2020","Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil","An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","arXiv:2010.11929 [cs]","","","","http://arxiv.org/abs/2010.11929","While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","2020-10-22","2021-04-09 06:01:35","2021-05-01 10:31:54","","","","","","","","An Image is Worth 16x16 Words","","","","","","","","","","","","arXiv.org","","arXiv: 2010.11929","<div data-schema-version=""2""><p>Annotation</p> <p>Vision Transformer (ViT)</p> <p>In previous, there is few methods using transformer to replace CNN using in the computer vision image analysis tasks. Or it is still hard to be accelerated.</p> <p>The authors assume to use image patches to replace NLP tokens in transformer.</p> <p>The structure of ViT is very similar to the transformer encoder part in NLP area. For the input image, it will be divided as patches, and the patches are like tokens added by trainable positional encoding to be sent into transformer. There is another trainable token in the first place to be sent into the transformer. In the output of the transformer encoder, the output for that trainable token is used for classification.</p> <p>ILSVRC-2012 ImageNet dataset, ImageNet-21k and JFT are used.</p> <p>In the large dataset, the transformers with more parameters have advantages comparing to the less one and CNN.</p> <p>+ve: Better performance with pretraining in large dataset compared to CNN.</p> <p>-ve: Perform bad when data is not enough. Didn't try to implement ViT to other computer vision tasks. Need to continue exploring self-supervised pre-training methods.</p> <p>In my opinion, this paper provides a great method for using the original structure of transformer. The tokens in NLP is replaced with image patches in computer vision. From the result of image classification, we can expect this structure works in other CV tasks.</p> </div>; Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer","C:\Users\smczx\Zotero\storage\JWTXXQUI\2010.html; F:\Google Drive - Monash\Bibliography\Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTBA5S6Z","journalArticle","2021","Vaswani, Ashish; Ramachandran, Prajit; Srinivas, Aravind; Parmar, Niki; Hechtman, Blake; Shlens, Jonathon","Scaling Local Self-Attention for Parameter Efficient Visual Backbones","arXiv:2103.12731 [cs]","","","","http://arxiv.org/abs/2103.12731","Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.","2021-03-30","2021-04-09 05:56:57","2021-04-09 11:22:10","2021-04-09 05:56:57","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2103.12731","Comment: CVPR 2021 Oral","C:\Users\smczx\Zotero\storage\Y8LLUYWL\2103.html; F:\Google Drive - Monash\Bibliography\Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NL79FS5","journalArticle","2019","Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon","Stand-Alone Self-Attention in Vision Models","arXiv:1906.05909 [cs]","","","","http://arxiv.org/abs/1906.05909","Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.","2019-06-13","2021-04-09 05:56:03","2021-04-09 11:22:36","2021-04-09 05:56:03","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1906.05909","","C:\Users\smczx\Zotero\storage\ZHYHSP49\1906.html; F:\Google Drive - Monash\Bibliography\Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T6VFNVXY","conferencePaper","2017","Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Łukasz; Polosukhin, Illia","Attention is all you need","Proceedings of the 31st International Conference on Neural Information Processing Systems","978-1-5108-6096-4","","","","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.","2017-12-04","2021-04-12 07:30:28","2021-05-01 10:29:18","","6000–6010","","","","","","","NIPS'17","","","","Curran Associates Inc.","Red Hook, NY, USA","","","","","","ACM Digital Library","","","<div data-schema-version=""2""><p>Annotation</p> <p>Self-attention and transformer</p> <p>The previous recurrent model structures cannot compute parallelly.</p> <p>The attention mechanisms can allow the dependencies without regard to their distances in the sequences. The transformer structure can parallel the computation, and reach a better quarlity.</p> <p>They proposed the Transformer with Multi-Head Attention blocks. The tansformer is a encoder-decoder structure, and the attention layers are the main part of this structure. The Multi-head attention layer has 3 imput, Query, Key and Value. The Q, K and V will be used to calculate the output of attention layer.</p> <p>The standard WMT 2014 English-German dataset and WMT 2014 English-French dataset.</p> <p>The results shows the performance is better than other models and the training cost is less than others.</p> <p>+ve: Faster train. Better performance.</p> <p>-ve: Only used for text data. No locality focus.</p> <p>This paper is the origin of self-attention and transformer, which replaces the traditional RNN based structures for NLP tasks, with faster training speed and better perforance. Based on this method, lots of work also focus on the transformer for computer vision.</p> </div>","F:\Google Drive - Monash\Bibliography\Vaswani et al_2017_Attention is all you need.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 31st International Conference on Neural Information Processing Systems","","","","","","","","","","","","","","",""
"MFY9NNH8","journalArticle","2019","Aneja, Deepali; Li, Wilmot","Real-Time Lip Sync for Live 2D Animation","arXiv:1910.08685 [cs]","","","","http://arxiv.org/abs/1910.08685","The emergence of commercial tools for real-time performance-based 2D animation has enabled 2D characters to appear on live broadcasts and streaming platforms. A key requirement for live animation is fast and accurate lip sync that allows characters to respond naturally to other actors or the audience through the voice of a human performer. In this work, we present a deep learning based interactive system that automatically generates live lip sync for layered 2D characters using a Long Short Term Memory (LSTM) model. Our system takes streaming audio as input and produces viseme sequences with less than 200ms of latency (including processing time). Our contributions include specific design decisions for our feature definition and LSTM configuration that provide a small but useful amount of lookahead to produce accurate lip sync. We also describe a data augmentation procedure that allows us to achieve good results with a very small amount of hand-animated training data (13-20 minutes). Extensive human judgement experiments show that our results are preferred over several competing methods, including those that only support offline (non-live) processing. Video summary and supplementary results at GitHub link: https://github.com/deepalianeja/CharacterLipSync2D","2019-10-18","2021-04-22 10:34:19","2021-04-22 11:15:36","2021-04-22 10:34:19","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1910.08685","","F:\Google Drive - Monash\Bibliography\Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf; C:\Users\smczx\Zotero\storage\7I4RYARH\1910.html","","Unread","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8WSAZGX","conferencePaper","2019","Tian, Guanzhong; Yuan, Yi; Liu, Yong","Audio2Face: Generating Speech/Face Animation from Single Audio with Attention-Based Bidirectional LSTM Networks","2019 IEEE International Conference on Multimedia Expo Workshops (ICMEW)","","","10.1109/ICMEW.2019.00069","","We propose an end to end deep learning approach for generating real-time facial animation from just audio. Specifically, our deep architecture employs deep bidirectional long short-term memory network and attention mechanism to discover the latent representations of time-varying contextual information within the speech and recognize the significance of different information contributed to certain face status. Therefore, our model is able to drive different levels of facial movements at inference and automatically keep up with the corresponding pitch and latent speaking style in the input audio, with no assumption or further human intervention. Evaluation results show that our method could not only generate accurate lip movements from audio, but also successfully regress the speaker's time-varying facial movements.","2019-07","2021-04-22 10:24:08","2021-04-22 10:30:21","","366-371","","","","","","Audio2Face","","","","","","","","","","","","IEEE Xplore","","","","C:\Users\smczx\Zotero\storage\YQE2BYHQ\8795082.html; F:\Google Drive - Monash\Bibliography\Tian et al_2019_Audio2Face.pdf","","Unread","Animation, Long short-term memory network, Attention mechanism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE International Conference on Multimedia Expo Workshops (ICMEW)","","","","","","","","","","","","","","",""
"BR6PGMGD","conferencePaper","2020","Kumar, Neeraj; Goel, Srishti; Narang, Ankur; Hasan, Mujtaba","Robust One Shot Audio to Video Generation","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops","","","","https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html","","2020","2021-04-22 07:05:29","2021-04-22 14:02:10","2021-04-22 07:05:29","770-771","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>OneShotA2V, using audio and a single unseen image to generate video.</p> <p>The previous methods cannot generate smooth transition or need high-compute time to generate video for a new unseen speaker.</p> <p>Using multiple multi-level discriminators can generate synchronized and realistic video. And using multiple loss can learn better.</p> <p>The OneShotA2V structure is an adversarial networks with a generator and 3 discriminators. the generator is similar to SPADE architecture and do transfer learning from deepspeech2 network. The multi-scale frame discriminator classify the real and fake frame images in 3 scales. The multi-scale temporal discriminator can classify the real or fake image based on previous frame in time-series. The synchronization discriminator uses SyncNet ensure the lip in the face matching the audio. In the training, adversarial loss, reconstruction loss, feature-matching loss, percepture loss (VGG19), contrastive loss (for SyncNet), and blink loss.</p> <p>GRID dataset and LOMBARD GRID.</p> <p>The metrics used is PSNR, SSIM, CPBD, WER and ACD. The results are better than RSDGAN and Speech2Vid.</p> <p>+ve: Use speech features for lower word error rate. Generation performance is better. Robust without other datasets.</p> <p>-ve: No emotion included to train. Not very sophisticated. </p> <p>This paper provide a clear and efficient techinuq for ""image + audio -&gt; video"" generation. With using multi-scale multiple discriminators and multiple losses to improve the quarlity of output.</p> </div>","F:\Google Drive - Monash\Bibliography\Kumar et al_2020_Robust One Shot Audio to Video Generation.pdf; C:\Users\smczx\Zotero\storage\Q9YPKPYG\Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops","","","","","","","","","","","","","","",""
"VC4TDGY7","journalArticle","2017","Suwajanakorn, Supasorn; Seitz, Steven M.; Kemelmacher-Shlizerman, Ira","Synthesizing Obama: learning lip sync from audio","ACM Transactions on Graphics","","0730-0301","10.1145/3072959.3073640","http://doi.org/10.1145/3072959.3073640","Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.","2017-07-20","2021-04-22 11:33:13","2021-04-22 11:33:40","2021-04-22 11:33:13","95:1–95:13","","4","36","","ACM Trans. Graph.","Synthesizing Obama","","","","","","","","","","","","July 2017","","","","F:\Google Drive - Monash\Bibliography\Suwajanakorn et al_2017_Synthesizing Obama.pdf","","Unread","audio; audiovisual speech; big data; face synthesis; lip sync; LSTM; RNN; uncanny valley; videos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPZXSPBG","conferencePaper","2018","Wang, Ting-Chun; Liu, Ming-Yu; Zhu, Jun-Yan; Liu, Guilin; Tao, Andrew; Kautz, Jan; Catanzaro, Bryan","Video-to-Video Synthesis","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2018/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf","We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)","2018","2021-04-22 11:47:08","2021-04-22 11:51:47","","1144-1156","","","31","","","","","","","","Curran Associates, Inc.","","","","","","","","","","","C:\Users\smczx\Zotero\storage\TS53RTJY\d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html; F:\Google Drive - Monash\Bibliography\Wang et al_2018_Video-to-Video Synthesis.pdf","","Unread","","Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GP9G4I4X","conferencePaper","2017","Saito, Masaki; Matsumoto, Eiichi; Saito, Shunta","Temporal Generative Adversarial Nets With Singular Value Clipping","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html","In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.","2017","2021-04-22 11:57:13","2021-04-23 02:41:12","2021-04-22 11:57:13","2830-2839","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf; C:\Users\smczx\Zotero\storage\IZS7A9V9\Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"NVF5W5CD","conferencePaper","2016","Vondrick, Carl; Pirsiavash, Hamed; Torralba, Antonio","Generating Videos with Scene Dynamics","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf","We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.","2016","2021-04-22 12:06:12","2021-04-22 12:07:23","","613-621","","","29","","","","","","","","Curran Associates, Inc.","","","","","","","","","","","C:\Users\smczx\Zotero\storage\LUFQTG57\04025959b191f8f9de3f924f0940515f-Abstract.html; F:\Google Drive - Monash\Bibliography\Vondrick et al_2016_Generating Videos with Scene Dynamics.pdf","","Unread","","Lee, D.; Sugiyama, M.; Luxburg, U.; Guyon, I.; Garnett, R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WX2RB87G","conferencePaper","2019","Park, Taesung; Liu, Ming-Yu; Wang, Ting-Chun; Zhu, Jun-Yan","Semantic Image Synthesis With Spatially-Adaptive Normalization","We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html","","2019","2021-04-22 12:35:13","2021-04-22 12:35:44","2021-04-22 12:35:13","2337-2346","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Park et al_2019_Semantic Image Synthesis With Spatially-Adaptive Normalization.pdf; C:\Users\smczx\Zotero\storage\G8CYMQAZ\Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"9VG323DT","conferencePaper","2016","Amodei, Dario; Ananthanarayanan, Sundaram; Anubhai, Rishita; Bai, Jingliang; Battenberg, Eric; Case, Carl; Casper, Jared; Catanzaro, Bryan; Cheng, Qiang; Chen, Guoliang; Chen, Jie; Chen, Jingdong; Chen, Zhijie; Chrzanowski, Mike; Coates, Adam; Diamos, Greg; Ding, Ke; Du, Niandong; Elsen, Erich; Engel, Jesse; Fang, Weiwei; Fan, Linxi; Fougner, Christopher; Gao, Liang; Gong, Caixia; Hannun, Awni; Han, Tony; Johannes, Lappi; Jiang, Bing; Ju, Cai; Jun, Billy; LeGresley, Patrick; Lin, Libby; Liu, Junjie; Liu, Yang; Li, Weigao; Li, Xiangang; Ma, Dongpeng; Narang, Sharan; Ng, Andrew; Ozair, Sherjil; Peng, Yiping; Prenger, Ryan; Qian, Sheng; Quan, Zongfeng; Raiman, Jonathan; Rao, Vinay; Satheesh, Sanjeev; Seetapun, David; Sengupta, Shubho; Srinet, Kavya; Sriram, Anuroop; Tang, Haiyuan; Tang, Liliang; Wang, Chong; Wang, Jidong; Wang, Kaifu; Wang, Yi; Wang, Zhijian; Wang, Zhiqian; Wu, Shuang; Wei, Likai; Xiao, Bo; Xie, Wen; Xie, Yan; Yogatama, Dani; Yuan, Bin; Zhan, Jun; Zhu, Zhenyao","Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v48/amodei16.html","We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-eng...","2016-06-11","2021-04-22 12:39:27","2021-04-22 12:39:44","2021-04-22 12:39:27","173-182","","","","","","Deep Speech 2","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","","F:\Google Drive - Monash\Bibliography\Amodei et al_2016_Deep Speech 2.pdf; C:\Users\smczx\Zotero\storage\XUINVS2H\amodei16.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"KCEFSMA2","conferencePaper","2018","Wang, Ting-Chun; Liu, Ming-Yu; Zhu, Jun-Yan; Tao, Andrew; Kautz, Jan; Catanzaro, Bryan","High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html","We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.","2018","2021-04-22 13:32:51","2021-04-23 02:41:15","2021-04-22 13:32:51","8798-8807","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\smczx\Zotero\storage\3FUDQRMP\Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html; F:\Google Drive - Monash\Bibliography\Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"VMXHCLUD","conferencePaper","2017","Arandjelovic, Relja; Zisserman, Andrew","Look, Listen and Learn","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html","We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel ""Audio-Visual Correspondence"" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.","2017","2021-04-22 16:07:57","2021-04-22 16:10:13","2021-04-22 16:07:57","609-617","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Arandjelovic_Zisserman_2017_Look, Listen and Learn.pdf; C:\Users\smczx\Zotero\storage\AU3848SY\Arandjelovic_Look_Listen_and_ICCV_2017_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"973HNL3T","conferencePaper","2018","Parmar, Niki; Vaswani, Ashish; Uszkoreit, Jakob; Kaiser, Lukasz; Shazeer, Noam; Ku, Alexander; Tran, Dustin","Image Transformer","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v80/parmar18a.html","Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual seq...","2018-07-03","2021-04-23 10:52:07","2021-04-23 10:54:09","2021-04-23 10:52:07","4055-4064","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","F:\Google Drive - Monash\Bibliography\Parmar et al_2018_Image Transformer2.pdf; C:\Users\smczx\Zotero\storage\5JCVU8L8\parmar18a.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"BSDAK6I6","conferencePaper","2019","Bello, Irwan; Zoph, Barret; Vaswani, Ashish; Shlens, Jonathon; Le, Quoc V.","Attention Augmented Convolutional Networks","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html","","2019","2021-04-23 10:22:17","2021-05-01 10:31:37","","3286-3295","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Bello et al_2019_Attention Augmented Convolutional Networks.pdf; C:\Users\smczx\Zotero\storage\4VKKC22S\Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"36MVED78","journalArticle","2021","Khan, Salman; Naseer, Muzammal; Hayat, Munawar; Zamir, Syed Waqas; Khan, Fahad Shahbaz; Shah, Mubarak","Transformers in Vision: A Survey","arXiv:2101.01169 [cs]","","","","http://arxiv.org/abs/2101.01169","Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.","2021-02-22","2021-04-23 10:41:54","2021-04-23 10:44:12","2021-04-23 10:41:54","","","","","","","Transformers in Vision","","","","","","","","","","","","arXiv.org","","arXiv: 2101.01169","Comment: 28 pages","C:\Users\smczx\Zotero\storage\GWYVPP4U\2101.html; F:\Google Drive - Monash\Bibliography\Khan et al_2021_Transformers in Vision.pdf","","Unread","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8BUHT44I","bookSection","2021","Sharma, Garima; Dhall, Abhinav","A Survey on Automatic Multimodal Emotion Recognition in the Wild","Advances in Data Science: Methodologies and Applications","978-3-030-51870-7","","","https://doi.org/10.1007/978-3-030-51870-7_3","Affective computing has been an active area of research for the past two decades. One of the major component of affective computing is automatic emotion recognition. This chapter gives a detailed overview of different emotion recognition techniques and the predominantly used signal modalities. The discussion starts with the different emotion representations and their limitations. Given that affective computing is a data-driven research area, a thorough comparison of standard emotion labelled databases is presented. Based on the source of the data, feature extraction and analysis techniques are presented for emotion recognition. Further, applications of automatic emotion recognition are discussed along with current and important issues such as privacy and fairness.","2021","2021-04-23 07:21:27","2021-04-23 07:23:59","2021-04-23 07:21:27","35-64","","","","","","","Intelligent Systems Reference Library","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","DOI: 10.1007/978-3-030-51870-7_3","","F:\Google Drive - Monash\Bibliography\Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf","","Unread","","Phillips-Wren, Gloria; Esposito, Anna; Jain, Lakhmi C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFC7X2G5","conferencePaper","2019","Chung, Soo-Whan; Chung, Joon Son; Kang, Hong-Goo","Perfect Match: Improved Cross-modal Embeddings for Audio-visual Synchronisation","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","10.1109/ICASSP.2019.8682524","","This paper proposes a new strategy for learning powerful cross-modal embeddings for audio-to-video synchronisation. Here, we set up the problem as one of cross-modal retrieval, where the objective is to find the most relevant audio segment given a short video clip. The method builds on the recent advances in learning representations from cross-modal self-supervision. The main contributions of this paper are as follows: (1) we propose a new learning strategy where the embeddings are learnt via a multi-way matching problem, as opposed to a binary classification (matching or non-matching) problem as proposed by recent papers; (2) we demonstrate that performance of this method far exceeds the existing baselines on the synchronisation task; (3) we use the learnt embeddings for visual speech recognition in self-supervision, and show that the performance matches the representations learnt end-to-end in a fully-supervised manner.","2019-05","2021-04-23 07:11:46","2021-04-23 07:12:18","","3965-3969","","","","","","Perfect Match","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","F:\Google Drive - Monash\Bibliography\Chung et al_2019_Perfect Match.pdf; C:\Users\smczx\Zotero\storage\CCLS68BR\8682524.html","","Unread","Training; Task analysis; Visualization; audio-visual synchronisation; cross-modal embedding; Cross-modal supervision; Lips; self-supervised learning; Speech recognition; Streaming media; Synchronization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","","","","","","","","","","","","",""
"FHCWTIXV","conferencePaper","2017","Chung, Joon Son; Zisserman, Andrew","Out of Time: Automated Lip Sync in the Wild","Computer Vision – ACCV 2016 Workshops","978-3-319-54427-4","","10.1007/978-3-319-54427-4_19","","The goal of this work is to determine the audio-video synchronisation between mouth motion and speech in a video.We propose a two-stream ConvNet architecture that enables the mapping between the sound and the mouth images to be trained end-to-end from unlabelled data. The trained network is used to determine the lip-sync error in a video.We apply the network to two further tasks: active speaker detection and lip reading. On both tasks we set a new state-of-the-art on standard benchmark datasets.","2017","2021-04-23 07:10:44","2021-04-23 07:12:14","","251-263","","","","","","Out of Time","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","F:\Google Drive - Monash\Bibliography\Chung_Zisserman_2017_Out of Time.pdf","","Unread","Convolutional Neural Network; Audio Stream; Equal Error Rate; Mouth Shape; Phoneme Recognition","Chen, Chu-Song; Lu, Jiwen; Ma, Kai-Kuang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JBYFZKE","journalArticle","2021","Fan, Haoqi; Xiong, Bo; Mangalam, Karttikeya; Li, Yanghao; Yan, Zhicheng; Malik, Jitendra; Feichtenhofer, Christoph","Multiscale Vision Transformers","arXiv:2104.11227 [cs]","","","","http://arxiv.org/abs/2104.11227","We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast","2021-04-22","2021-04-23 05:56:08","2021-05-01 10:28:57","","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2104.11227","<div data-schema-version=""2""><p>Annotation</p> <p>Multi-scale ViT (MViT)</p> <p>The previous vision transformer (ViT) only used one scale to analyze the images, lack of the thoughts of ""pyramid"" strategies, which need to go to different scales to get variety of features from different scales.</p> <p>The ""pyramid"" strategies is a multi-scale processing, which means working in a lower resolution can cover the features with better ""sense"" of context. The ""depth"" of modern deep neural networks can help. Authors hope to connect the multi-scale feature hierarchies concept with the transformer. And this concept is beneficial to the transformer structures.</p> <p>They proposed Multiscale Vision Transformer (MViT) and it is built by Multi-Head Pooling Attention (MHPA) layers. Compared to the Muti-Head Attention layer, the MHPA has pooling layers to each route (identity, Q, K and V). In the MViT, the dimension of feature maps will decrease through the MHPA layers to detect multi-scale features.</p> <p>The dataset used is Kinetics-400 and Kinetics-600 for image classification. And Something-Something-v2, Charades and AVA for transfer learning.+</p> <p>The results shows the MViT has better accuracy for video classification than others. Also it has less parameters and less computational complexity than ViT. In image recognition with transfer learning, the MViT also performs the best comparing to other models.</p> <p>+ve: Expand the feature complexity. Big advantage than single-scale vision transformer.</p> <p>-ve:</p> <p>In my opinion, this MViT brings the pyramid concepts to transformer models, also provide a method to work with video. With higher performance and lower computational complexity and parameters. This MViT is very useful for video analysis.</p> </div>; Comment: Technical report","C:\Users\smczx\Zotero\storage\3CH8Q456\2104.html; F:\Google Drive - Monash\Bibliography\Fan et al_2021_Multiscale Vision Transformers.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVB9JEQB","journalArticle","2021","Zheng, Sixiao; Lu, Jiachen; Zhao, Hengshuang; Zhu, Xiatian; Luo, Zekun; Wang, Yabiao; Fu, Yanwei; Feng, Jianfeng; Xiang, Tao; Torr, Philip H. S.; Zhang, Li","Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers","arXiv:2012.15840 [cs]","","","","http://arxiv.org/abs/2012.15840","Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.","2021-03-30","2021-04-23 05:30:54","2021-04-23 05:35:30","2021-04-23 05:30:54","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2012.15840","Comment: CVPR 2021. Project page at https://fudan-zvg.github.io/SETR/","C:\Users\smczx\Zotero\storage\E42HK8FU\2012.html; F:\Google Drive - Monash\Bibliography\Zheng et al_2021_Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQX3QM86","conferencePaper","2020","Carion, Nicolas; Massa, Francisco; Synnaeve, Gabriel; Usunier, Nicolas; Kirillov, Alexander; Zagoruyko, Sergey","End-to-End Object Detection with Transformers","Computer Vision – ECCV 2020","978-3-030-58452-8","","10.1007/978-3-030-58452-8_13","","We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.","2020","2021-04-23 04:39:37","2021-04-23 04:39:51","","213-229","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","F:\Google Drive - Monash\Bibliography\Carion et al_2020_End-to-End Object Detection with Transformers.pdf","","Unread","","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUPV2IWH","conferencePaper","2019","Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","","","10.18653/v1/N19-1423","https://www.aclweb.org/anthology/N19-1423","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","2019-06","2021-04-27 14:50:52","2021-04-27 14:51:20","2021-04-27 14:50:52","4171–4186","","","","","","BERT","","","","","Association for Computational Linguistics","Minneapolis, Minnesota","","","","","","ACLWeb","","","","F:\Google Drive - Monash\Bibliography\Devlin et al_2019_BERT.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2019","","","","","","","","","","","","","","",""
"FI3WQZCW","journalArticle","1997","Hochreiter, Sepp; Schmidhuber, Jürgen","Long Short-Term Memory","Neural Computation","","0899-7667","10.1162/neco.1997.9.8.1735","https://doi.org/10.1162/neco.1997.9.8.1735","Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.","1997-11-15","2021-05-01 07:07:59","2021-05-01 07:08:23","2021-05-01 07:07:59","1735-1780","","8","9","","Neural Computation","","","","","","","","","","","","","Silverchair","","","<div data-schema-version=""2""><p>Annotation</p> <p>LSTM</p> </div>","F:\Google Drive - Monash\Bibliography\Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf; C:\Users\smczx\Zotero\storage\3LCNJLQ5\Long-Short-Term-Memory.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3J83CP7J","journalArticle","2014","Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua","Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation","arXiv:1406.1078 [cs, stat]","","","","http://arxiv.org/abs/1406.1078","In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","2014-09-02","2021-05-01 07:09:24","2021-05-01 10:29:46","","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1406.1078","<div data-schema-version=""2""><p>Annotation</p> <p>GRU</p> </div>; Comment: EMNLP 2014","C:\Users\smczx\Zotero\storage\R4UWJIIU\1406.html; F:\Google Drive - Monash\Bibliography\Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf","","Unread","Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7IT5IMV","journalArticle","2016","Ba, Jimmy Lei; Kiros, Jamie Ryan; Hinton, Geoffrey E.","Layer Normalization","arXiv:1607.06450 [cs, stat]","","","","http://arxiv.org/abs/1607.06450","Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.","2016-07-21","2021-05-01 08:18:07","2021-05-01 10:30:43","","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1607.06450","","C:\Users\smczx\Zotero\storage\39KTZ95D\1607.html; F:\Google Drive - Monash\Bibliography\Ba et al_2016_Layer Normalization.pdf","","Unread","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDRH2S5A","journalArticle","2021","He, Zhenliang; Kan, Meina; Shan, Shiguang","EigenGAN: Layer-Wise Eigen-Learning for GANs","arXiv:2104.12476 [cs, stat]","","","","http://arxiv.org/abs/2104.12476","Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via the adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of ""eigen-dimensions"" at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, under the linear circumstance, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow.","2021-04-26","2021-05-01 16:51:17","2021-05-03 08:15:37","","","","","","","","EigenGAN","","","","","","","","","","","","arXiv.org","","arXiv: 2104.12476","<div data-schema-version=""2""><p>Annotation</p> <p>EigenGAN</p> <p>Previous methods are all post-processing algorithms for well-trained GAN generators. So can a generator itself automatically/unsupervisedly learn explicit control of the semantic attributes represented in different layers?</p> <p>Embedding a linear subspace model with orthogonal basis can build the semantic variations with each layer. Also, the generator can capture the principal variations with different layers/scales and they are orthogonal. All these features are controllable and interpretable.</p> <p>They proposed EigenGAN, the generator of which is a chain of 2-strided deconv layers, and each layer add to a subspace linear model with orthonormal basis. The output of subspace linear model is calculated by the U, L, z and mu, which is equivalent to PCA, so it is named EigenGAN.</p> <p>They used CelebA for quantitative evaluation, and FFHQ and Danbooru 2019 Portraits for qualitative evaluation.</p> <p>From the result, the features discovered has more strong correlation than SeFa.</p> <p>+ve: Can make interpretable dimensions for different layers of generator unsupervisedly. Good performance compared to other model.</p> <p>-ve: Some features are still entangled. Some features are ignored because appear less frequently in the dataset.</p> <p>In my opinion, this methods provide a very powerful way to use unsupervised learning with GAN to make the input variables controllable, interpretable and distangled. It's purely unsupervised learning and GAN.</p> </div>; Comment: Code: https://github.com/LynnHo/EigenGAN-Tensorflow","C:\Users\smczx\Zotero\storage\ZHZ55574\2104.html; F:\Google Drive - Monash\Bibliography\He et al_2021_EigenGAN.pdf","","","Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P723GDXA","journalArticle","2020","Wang, Zhihao; Chen, Jian; Hoi, Steven C.H.","Deep Learning for Image Super-resolution: A Survey","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/TPAMI.2020.2982166","","Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.","2020","2021-05-03 09:12:35","2021-05-03 09:13:09","","1-1","","","","","","Deep Learning for Image Super-resolution","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","<div data-schema-version=""2""><p>Related blog</p> <p>https://zhuanlan.zhihu.com/p/143380729</p> </div>","C:\Users\smczx\Zotero\storage\88H69X25\9044873.html; F:\Google Drive - Monash\Bibliography\Wang et al_2020_Deep Learning for Image Super-resolution.pdf","","Unread","Deep learning; Deep Learning; Animals; Benchmark testing; Convolutional Neural Networks (CNN); Degradation; Generative Adversarial Nets (GAN); Image Super-resolution; Measurement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7ITAU5B","conferencePaper","2014","Dong, Chao; Loy, Chen Change; He, Kaiming; Tang, Xiaoou","Learning a Deep Convolutional Network for Image Super-Resolution","Computer Vision – ECCV 2014","978-3-319-10593-2","","10.1007/978-3-319-10593-2_13","","We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.","2014","2021-05-03 10:05:19","2021-05-03 10:05:33","","184-199","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>SRCNN</p> </div>","F:\Google Drive - Monash\Bibliography\Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf","","Unread","Super-resolution; deep convolutional neural networks","Fleet, David; Pajdla, Tomas; Schiele, Bernt; Tuytelaars, Tinne","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JBA8RMR","conferencePaper","2017","Lim, Bee; Son, Sanghyun; Kim, Heewon; Nah, Seungjun; Mu Lee, Kyoung","Enhanced Deep Residual Networks for Single Image Super-Resolution","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops","","","","https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html","Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.","2017","2021-05-03 15:01:34","2021-05-03 15:08:54","2021-05-03 15:01:34","136-144","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Batch Normalization is bad in SR.</p> </div>","F:\Google Drive - Monash\Bibliography\Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf; C:\Users\smczx\Zotero\storage\RN6MA3UV\Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops","","","","","","","","","","","","","","",""
"46P454WN","conferencePaper","2018","Wang, Xintao; Yu, Ke; Wu, Shixiang; Gu, Jinjin; Liu, Yihao; Dong, Chao; Qiao, Yu; Change Loy, Chen","ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks","Proceedings of the European Conference on Computer Vision (ECCV) Workshops","","","","https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html","The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.","2018","2021-05-03 15:08:43","2021-05-03 15:09:11","2021-05-03 15:08:43","0-0","","","","","","ESRGAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>ESRGAN</p> </div>","C:\Users\smczx\Zotero\storage\GQHG2PUB\Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html; F:\Google Drive - Monash\Bibliography\Wang et al_2018_ESRGAN.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV) Workshops","","","","","","","","","","","","","","",""
"6M6AS5LC","conferencePaper","2017","Ledig, Christian; Theis, Lucas; Huszar, Ferenc; Caballero, Jose; Cunningham, Andrew; Acosta, Alejandro; Aitken, Andrew; Tejani, Alykhan; Totz, Johannes; Wang, Zehan; Shi, Wenzhe","Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html","Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.","2017","2021-05-03 15:09:33","2021-05-03 15:09:55","2021-05-03 15:09:33","4681-4690","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>SRGAN</p> </div>","F:\Google Drive - Monash\Bibliography\Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf; C:\Users\smczx\Zotero\storage\IN3MZN46\Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"7GCK76XV","journalArticle","2021","Jiang, Junjun; Wang, Chenyang; Liu, Xianming; Ma, Jiayi","Deep Learning-based Face Super-resolution: A Survey","arXiv:2101.03749 [cs]","","","","http://arxiv.org/abs/2101.03749","Face super-resolution, also known as face hallucination, which is aimed at enhancing the resolution of low-resolution (LR) one or a sequence of face images to generate the corresponding high-resolution (HR) face images, is a domain-specific image super-resolution problem. Recently, face super-resolution has received considerable attention, and witnessed dazzling advances with deep learning techniques. To date, few summaries of the studies on the deep learning-based face super-resolution are available. In this survey, we present a comprehensive review of deep learning techniques in face super-resolution in a systematic manner. First, we summarize the problem formulation of face super-resolution. Second, we compare the differences between generic image super-resolution and face super-resolution. Third, datasets and performance metrics commonly used in facial hallucination are presented. Fourth, we roughly categorize existing methods according to the utilization of face-specific information. In each category, we start with a general description of design principles, present an overview of representative approaches, and compare the similarities and differences among various methods. Finally, we envision prospects for further technical advancement in this field.","2021-01-11","2021-05-04 07:20:15","2021-05-04 07:47:50","2021-05-04 07:20:15","","","","","","","Deep Learning-based Face Super-resolution","","","","","","","","","","","","arXiv.org","","arXiv: 2101.03749","Comment: 40 pages, 13 figures","C:\Users\smczx\Zotero\storage\NQA699NJ\2101.html; F:\Google Drive - Monash\Bibliography\Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"97G592DN","conferencePaper","2020","Meishvili, Givi; Jenni, Simon; Favaro, Paolo","Learning to Have an Ear for Face Super-Resolution","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html","We propose a novel method to use both audio and a low-resolution image to perform extreme face super-resolution (a 16x increase of the input size). When the resolution of the input image is very low (e.g., 8x8 pixels), the loss of information is so dire that important details of the original identity have been lost and audio can aid the recovery of a plausible high-resolution image. In fact, audio carries information about facial attributes, such as gender and age. To combine the aural and visual modalities, we propose a method to first build the latent representations of a face from the lone audio track and then from the lone low-resolution image. We then train a network to fuse these two representations. We show experimentally that audio can assist in recovering attributes such as the gender, the age and the identity, and thus improve the correctness of the high-resolution image reconstruction process. Our procedure does not make use of human annotation and thus can be easily trained with existing video datasets. Moreover, we show that our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations.","2020","2021-05-04 07:28:39","2021-05-04 15:38:26","2021-05-04 07:28:39","1364-1374","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Add audio for image super resolution</p> <p>The important semantic information lossed whtn the resolution is extremely low. Do SR in this LR images might cause wrong gender and ages.</p> <p>The audio information might be helpful to extract gender and age features. Model can learn the correlation between the voice to the personal identity.</p> <p>The method they proposed has several steps to train. Step 1: Train a HR encoder to reverse the pretrained StyleGAN (generator) as auto-encoder with L1 and VGG-percepture loss. Step 2: Train both the encoder and StyleGAN to fine tune in the dataset with loss of step 1 and generator-weights regularization. Step 3: Train the LR encoder to regress the same latent representations from HR encoder with L1 loss for latent representations and the pairs of LR and downsampling of SR. Then, use the HR encoder and filpped face to get latent representations of neutral frontal facing poses. Train the A (audio) encoder to predict the neutral front latent representations with L1 loss. Step 4: Train the fusing layer to regress the latents of HR encoder with L1 loss of latents and the pairs of LR and downsampling of SR.</p> <p>The dataset is VoxCeleba2. </p> <p>The metrics is PSNR, SSIM, the classification error of identity classifier, gender classifier and age classifier. The proposed method is better than others for recovery of identity, gender and age.</p> <p>+ve: It can recover the age and gender attributes. Utilize the information from audio, which is easily accessed in video.</p> <p>-ve: The model is not robust to some cases when the gender can be easily guessed from the LR image. Naive end-to-end training is not applied.</p> <p>This paper introduced the first method for image face super resolution combined the LR image and audio. The information of audio contains some kind of informations which is not easily gussed from LR.</p> </div>","F:\Google Drive - Monash\Bibliography\Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf; C:\Users\smczx\Zotero\storage\3I29U2DH\Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"DXKQLZP7","journalArticle","2020","Liu, Hongying; Ruan, Zhubo; Zhao, Peng; Dong, Chao; Shang, Fanhua; Liu, Yuanyuan; Yang, Linlin","Video Super Resolution Based on Deep Learning: A Comprehensive Survey","arXiv:2007.12928 [cs, eess]","","","","http://arxiv.org/abs/2007.12928","In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning.","2020-12-20","2021-05-04 07:47:31","2021-05-04 07:48:05","2021-05-04 07:47:31","","","","","","","Video Super Resolution Based on Deep Learning","","","","","","","","","","","","arXiv.org","","arXiv: 2007.12928","","C:\Users\smczx\Zotero\storage\YFSWND62\2007.html; F:\Google Drive - Monash\Bibliography\Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S47FKPTC","conferencePaper","2018","Chen, Yu; Tai, Ying; Liu, Xiaoming; Shen, Chunhua; Yang, Jian","FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html","Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to superresolve very low-resolution (LR) face images without wellaligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.","2018","2021-05-04 08:15:10","2021-05-04 08:15:29","2021-05-04 08:15:10","2492-2501","","","","","","FSRNet","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Chen et al_2018_FSRNet.pdf; C:\Users\smczx\Zotero\storage\U6JM6ABA\Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"B7RALPT2","journalArticle","2021","Chan, Kelvin C. K.; Wang, Xintao; Yu, Ke; Dong, Chao; Loy, Chen Change","BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond","arXiv:2012.02181 [cs]","","","","http://arxiv.org/abs/2012.02181","Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.","2021-04-07","2021-05-04 08:31:51","2021-05-04 08:33:41","2021-05-04 08:31:51","","","","","","","BasicVSR","","","","","","","","","","","","arXiv.org","","arXiv: 2012.02181","Comment: CVPR 2021 camera-ready","C:\Users\smczx\Zotero\storage\6ZFN8LSS\2012.html; F:\Google Drive - Monash\Bibliography\Chan et al_2021_BasicVSR.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""