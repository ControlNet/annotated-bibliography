"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"2JBPGV5S","journalArticle","1991","Kramer, Mark A.","Nonlinear principal component analysis using autoassociative neural networks","AIChE Journal","","1547-5905","https://doi.org/10.1002/aic.690370209","https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690370209","Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.","1991","2021-03-16 13:41:25","2021-03-16 13:41:25","2021-03-16 13:41:25","233-243","","2","37","","","","","","","","","","en","Copyright © 1991 American Institute of Chemical Engineers","","","","Wiley Online Library","","_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690370209","<div data-schema-version=""2""><p>Annotation</p> <p>Auto Encoder (Nonlinear Principal Component Analysis NLPCA)</p> <p>Engineers are often confronted with the problem of extracting information about poorly-known process of data</p> <p>The dimensionality reduction is closely related to feature extraction and it can capture the information contained in the original data.</p> <p>The proposed method is the using multi-layer neural networks with same input and output and a bottleneck. The training target is to minimize the reconstruction error. Compared to PCA, the nonlinearity improve the performance of feature extraction.</p> <p>Datasets are generated by some simple functions.</p> <p>The results of experiments shows the reconstruction error is less than PCA and ANN without mapping layer.</p> <p>+ve: Can find and eliminates nonlinear correlations in the data. Can remove redundant information. More effective than PCA.</p> <p>-ve: Limited by the practicalities of computing functional approximations from limited data.</p> <p>The origin of auto encoder. A great milestone.</p> <p></p></div>","C:\Users\smczx\Zotero\storage\TA7ULUCD\aic.html; F:\Google Drive - Monash\Bibliography\Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMJ59NJI","journalArticle","1998","Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P.","Gradient-based learning applied to document recognition","Proceedings of the IEEE","","1558-2256","10.1109/5.726791","","Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.","1998-11","2021-03-16 13:50:37","2021-03-16 15:33:19","","2278-2324","","11","86","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Proceedings of the IEEE","<div data-schema-version=""1""><p>Annotation</p> <p>Convolutional Neural Networks (CNN) and LeNet</p> <p>To find a better pattern recognition systems can be built by relying more on automatic learning and less on hand-designed heuristics.</p> <p>The new method is made possible by recent progress in machine learning and computer technology. The hand-crafted feature extraction can be advantageously replaced by carefully designed learning machines that operate directly on pixel images.</p> <p>The architecture of CNN is based on convolutional layers, pooling layers, and some fully-connected layers. Convolutional layers and pooling layers are effective for extracting features from 2D images, and they can also share the parameters with nearby numbers which can dramatically reduce the parameters, and it is beneficial for model performance.</p> <p>The dataset is the called MNIST, with 60000 training images and 10000 test images, combined by SD-1 and SD-2 datasets.</p> <p>The classification error of LeNet is lower compared to other traditional methods and fully-connected NN.</p> <p>+ve: Better classification performance than traditional methods and fully-connected NN. Suitable for hardware implementations with low memory requirements. More robust for shape variance and noise.</p> <p>-ve: Longer training time.</p> <p>The origin of CNN, huge contribution to computer vision area.</p></div>","C:\Users\smczx\Zotero\storage\8RPNJDWS\726791.html; F:\Google Drive - Monash\Bibliography\Lecun et al_1998_Gradient-based learning applied to document recognition.pdf","","","2D shape variability; back-propagation; backpropagation; Character recognition; cheque reading; complex decision surface synthesis; convolution; convolutional neural network character recognizers; document recognition; document recognition systems; Feature extraction; field extraction; gradient based learning technique; gradient-based learning; graph transformer networks; GTN; handwritten character recognition; handwritten digit recognition task; Hidden Markov models; high-dimensional patterns; language modeling; Machine learning; Multi-layer neural network; multilayer neural networks; multilayer perceptrons; multimodule systems; Neural networks; optical character recognition; Optical character recognition software; Optical computing; Pattern recognition; performance measure minimization; Principal component analysis; segmentation recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJPJAB46","journalArticle","2012","Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R.","Improving neural networks by preventing co-adaptation of feature detectors","arXiv:1207.0580 [cs]","","","","http://arxiv.org/abs/1207.0580","When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.","2012-07-03","2021-03-16 13:52:06","2021-04-07 06:44:01","2021-03-16 13:52:06","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1207.0580","<div data-schema-version=""2""><p>Annotation</p> <p>Dropout</p> <p>The overfitting, do worse on the test data than on the training data, happens.</p> <p>Finding a similar method with ""mean network"", which is formed by many separate networks, to reduce the test set error.</p> <p>The dropout is used as disable some proportion of hidden units in training process to prevent the hidden units replies to others. Normally, the drop probability is set to 0.5.</p> <p>Datasets are MNIST, TIMIT benchmark, CIFAR-10, ImageNet, and Reuters.</p> <p>The main metrics for comparison is the classification error for test set. The performance of dropout network is similar to ""mean network"", and better than the network without dropout.</p> <p>+ve: Almost all dropout probabilities can improve the generalization performance of the models. Dropout is simpler to implement.</p> <p>-ve: Some extreme probabilities will cause the performance worse.</p> </div>","F:\Google Drive - Monash\Bibliography\Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf; C:\Users\smczx\Zotero\storage\PCYSDJFS\1207.html","","","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLKIMCG8","journalArticle","2009","Bengio, Y.","Learning Deep Architectures for AI","Foundations and Trends® in Machine Learning","","1935-8237, 1935-8245","10.1561/2200000006","http://www.nowpublishers.com/article/Details/MAL-006","Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.","2009","2021-03-16 13:56:40","2021-04-06 15:02:00","2021-03-16 13:56:40","1-127","","1","2","","FNT in Machine Learning","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""2""><p>Annotation </p> <p>Survey</p> <p>This paper introduced many architectures for deep learning. In chapter 2, the researchers explain that a deeper structure can extract the feature more accuracy. Besides, in chapter 5, the paper introduced the convolutional neural network and auto encoder, and they also introduced an optimization principle to initialize parameters for each layer by unsupervised learning, which the auto encoder is included in.</p></div>","F:\Google Drive - Monash\Bibliography\Bengio_2009_Learning Deep Architectures for AI.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WMKXI6M","journalArticle","2006","Hinton, G. E.; Salakhutdinov, R. R.","Reducing the Dimensionality of Data with Neural Networks","Science","","0036-8075, 1095-9203","10.1126/science.1127647","https://science.sciencemag.org/content/313/5786/504","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.","2006-07-28","2021-03-16 14:00:23","2021-03-16 14:00:23","2021-03-16 14:00:23","504-507","","5786","313","","","","","","","","","","en","American Association for the Advancement of Science","","","","science.sciencemag.org","","Publisher: American Association for the Advancement of Science Section: Report PMID: 16873662","<div data-schema-version=""1""><p>Annotation</p> <p>Using RBM to pretrain the Auto encoder for dimensionality reduction.</p> <p>Multi-layer auto encoder is hard to train.</p> <p>Use a very different type of algorithm to pretrain the model and prove it can be generalized to other datasets.</p> <p>The method introduced in this paper uses RBM as the algorithm to pretrain the weights of an autoencoder. For each layer of the autoencoder, the weights will be pretrained as RBM, and pretrain the next layer. With the RBM pretrained weights, the autoencoder has better performance for reconstruction.</p> <p>MNIST hand-written digits and Olivetti face dataset</p> <p>No quantitative comparison.</p> <p>+ve: Better reconstruction results than PCA. This pretraining can also be used for classification and regression. Pretraining helps generalization. The autoencoders can map in bi-direction between data and code spaces. Can apply to very large datasets.</p> <p>-ve:</p> <p>Provide a new pretrain method for auto encoder to improve the performance. Combine the RBM and the neural networks.</p> <p></p> <p></p></div>","; C:\Users\smczx\Zotero\storage\MVMYC94D\504.html; F:\Google Drive - Monash\Bibliography\Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf","http://www.ncbi.nlm.nih.gov/pubmed/16873662","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CA2QE2FS","journalArticle","2010","Vincent, Pascal; Larochelle, Hugo; Lajoie, Isabelle; Bengio, Yoshua; Manzagol, Pierre-Antoine","Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion","The Journal of Machine Learning Research","","1532-4435","","","We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.","2010-12-01","2021-03-16 14:03:28","2021-03-16 14:03:28","","3371–3408","","","11","","J. Mach. Learn. Res.","Stacked Denoising Autoencoders","","","","","","","","","","","","3/1/2010","","","<div data-schema-version=""1""><p>Annotation</p> <p>Denoising autoencoder (DAE)</p> <p>To overcome the gap between the stacking RBMs and stacking autoencoders. And find out what can shape a good, useful representation.</p> <p>The authors were looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns. And expected the latent representation is robust and stable, also can perform denoising task.</p> <p>The method proposed is the DAE which adds the noise to input data, and train the autoencoder to reconstruct the original data without noise. The noise added can be Gaussian noise or masking noise. With the noise inside, DAE can learn the features more stable and robust. So, an autoencoder with noisy data input can encode as higher level representations which have better performance than without noise added. The stacked DAE is the pretrained method to improve the performance of autoencoder.</p> <p>The dataset used is the 12 x 12 patches from Olshausen for DAE. The MNIST, and tzanetakis audio genre classification data set for stacked DAE.</p> <p>They compared the test error rate for classification task with other methods, and the SDAE (stacked DAE) performs the best.</p> <p>+ve: Better performance than DBNs. Establish the value of using the denoising criterion as an unsupervised objective for useful higher level presentations.</p> <p>-ve: The DAEs used in the paper are shallow.</p> <p>This paper introduced a simple way (add noise to input data) to improve the performance of encoder, which is easy to implement and prove the value for denoising task in the AE training. </p> <p></p></div>","F:\Google Drive - Monash\Bibliography\Vincent et al_2010_Stacked Denoising Autoencoders.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"454BKCFN","journalArticle","2021","Doersch, Carl","Tutorial on Variational Autoencoders","arXiv:1606.05908 [cs, stat]","","","","http://arxiv.org/abs/1606.05908","In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.","2021-01-03","2021-03-16 14:05:22","2021-04-06 15:06:51","2021-03-16 14:05:22","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1606.05908","<div data-schema-version=""1""><p>Annotation</p> <p>This paper introduces a variant of autoencoder, which is the variational autoencoder. For the ordinary autoencoder, the code is a n-dimensional vector. As for VAE, the code trained is a Gaussian distribution. In the network structure, the mean and the variance of the distribution will be trained. And then, the generator resample the code by the Gaussian distribution encoded. Compared with the vanilla autoencoder, VAE has better performance.</p></div>","F:\Google Drive - Monash\Bibliography\Doersch_2021_Tutorial on Variational Autoencoders.pdf; C:\Users\smczx\Zotero\storage\GBJ99QGT\1606.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8PJBJU3","journalArticle","2016","Makhzani, Alireza; Shlens, Jonathon; Jaitly, Navdeep; Goodfellow, Ian; Frey, Brendan","Adversarial Autoencoders","arXiv:1511.05644 [cs]","","","","http://arxiv.org/abs/1511.05644","In this paper, we propose the ""adversarial autoencoder"" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.","2016-05-24","2021-03-16 14:06:13","2021-04-08 05:20:40","2021-03-16 14:06:13","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1511.05644","<div data-schema-version=""1""><p>Annotation</p> <p>Adversarial Autoencoder (AAE)</p> <p>The MCMC methods computing the gradient becomes more imprecise in training progress.</p> <p>Some generative models can avoid the difficulties of the training by being trained via direct back-propagation.</p> <p>The AAE can convert an autoencoder as a generative model. Both the reconstruction loss and adversarial loss are used in training, fitting the distribution of latent representation to any prior distribution (for example, normal distribution). The authors proposed several structures for unsupervised, semi-supervised and supervised lerning. In other word, the AAE has a discriminator classifing the latent representation if it is from encoder output or prior distribution. Therefore, with the discriminator loss, the encoder should confuse the discriminator to think all latent representations are from predefined distribution, which means these two distribution are identical. So the distribution of latent representation can be controlled.</p> <p>MNIST, Toronto face dataset (TFD) and SVHN.</p> <p>Use Log-likelihood of test data to compare with other generative networks. The results are the best. Use classification error for comparing the semi-supervised AAE structure with other generative networks. The results are the best compared to the others. Use clustering error rate to compare the clustering performance.</p> <p>+ve: Can use the prior distribution by its samples rather than the explicit functional form. Can be used in different situations (unsupervised, semi-unsupervised, clustering, ...). </p> <p>-ve:</p> <p>AAE is a great example for the improvement of AE, as it combined the concept of AE and GAN to regularize the distribution of latent representations.</p></div>","F:\Google Drive - Monash\Bibliography\Makhzani et al_2016_Adversarial Autoencoders.pdf; C:\Users\smczx\Zotero\storage\HFBFFJF8\1511.html","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQKA9BPK","journalArticle","2014","Shlens, Jonathon","A Tutorial on Principal Component Analysis","arXiv:1404.1100 [cs, stat]","","","","http://arxiv.org/abs/1404.1100","Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.","2014-04-03","2021-03-16 14:08:59","2021-04-06 15:04:42","2021-03-16 14:08:59","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1404.1100","<div data-schema-version=""1""><p>Annotation</p> <p>This paper introduces how the PCA works and how to apply the PCA into datasets. The PCA was invented in 1901, and is a transformation to a new space with smaller dimension by scalar projection. Take a low dimension (2D) dataset as an example, the PCA means transforming the dataset to a line (1D) which has the projection of the dataset with the highest variance. If the transformed dimension is larger than 1, the new axis is located with the highest variance of dataset projection in the orthogonal plane of other axises.</p> <p><br> </p></div>","F:\Google Drive - Monash\Bibliography\Shlens_2014_A Tutorial on Principal Component Analysis.pdf; C:\Users\smczx\Zotero\storage\CR8D93IJ\1404.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPDBRJGH","journalArticle","1995","Cootes, T. F.; Taylor, C. J.; Cooper, D. H.; Graham, J.","Active Shape Models-Their Training and Application","Computer Vision and Image Understanding","","1077-3142","10.1006/cviu.1995.1004","https://www.sciencedirect.com/science/article/pii/S1077314285710041","Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our Active Shape Models can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images.","1995-01-01","2021-03-16 14:10:07","2021-03-16 14:10:07","2021-03-16 14:10:07","38-59","","1","61","","Computer Vision and Image Understanding","","","","","","","","en","","","","","ScienceDirect","","","<div data-schema-version=""1""><p>Annotation</p> <p>Active Shape Model (ASM)</p> <p>The problem of locating examples of known objects in images.</p> <p>The shape of the objects of same class may vary. Using flexible models or deformable templates can be used to allow for some degree of variability in the shape of the imaged objects.</p> <p>The method is to use mean template and a deformable transformation applied to point distribution model to fit the target shape. And the shape is constrained by PCA.</p> <p>The shapes of registers.</p> <p>No quantitative comparison with other methods.</p> <p>+ve: Low time and space complexity. Can be applied to a wide range of image interpretation tasks.</p> <p>-ve: Require annotations for training images. Not robust to noise, clutter and occlusion.</p> <p>A traditional method for shape fitting with the constrain of PCA and the training with transformation.</p></div>","F:\Google Drive - Monash\Bibliography\Cootes et al_1995_Active Shape Models-Their Training and Application.pdf; C:\Users\smczx\Zotero\storage\A9CBSJ67\S1077314285710041.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NUJHIL5","journalArticle","2001","Cootes, T. F.; Edwards, G. J.; Taylor, C. J.","Active appearance models","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/34.927467","","We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.","2001-06","2021-03-16 14:11:16","2021-04-08 05:27:36","","681-685","","6","23","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","<div data-schema-version=""1""><p>Annotation</p> <p>Active Appearance Models (AAM)</p> <p>Previous methods are very slow or easily become trapped in local minima to fit photo-realistic appearance. Some previous methods cannot fit because the shape-normalized texture map cannot be reconstructed.</p> <p>The authors thought the hypothesis of the optimization problem is similar to each time so can be similarities offline, which can find the directions of rapid convergence.</p> <p>The AAM is a further improvement of ASM, which combining both the shape and texture for fitting the model.</p> <p>Datasets are 100 hand-labeled face images as training set and another 100 as test set.</p> <p>No quantitative comparison to other methods.</p> <p>+ve: Good reliability and robustness of image appearance search.</p> <p>-ve: The training speed of AAM is slower than ASM.</p> <p>The improvement of ASM combining the shape and texture.</p></div>","C:\Users\smczx\Zotero\storage\2Y22Q7QV\927467.html; F:\Google Drive - Monash\Bibliography\Cootes et al_2001_Active appearance models.pdf","","","active appearance models; Active shape model; Deformable models; deformable template; gray-level variation; Image generation; image matching; Image reconstruction; Image segmentation; image texture; Iterative algorithms; iterative method; iterative methods; learning; learning (artificial intelligence); model matching; optimisation; Optimization methods; Robustness; Shape control; shape matching; statistical analysis; statistical models; Surface fitting; texture matching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TFE7ABP5","report","2018","Rhue, Lauren","Racial Influence on Automated Perceptions of Emotions","","","","","https://papers.ssrn.com/abstract=3281765","The practical applications of artificial intelligence are expanding into various elements of society, leading to a growing interest in the potential biases of such algorithms. Facial analysis, one application of artificial intelligence, is increasingly used in real-word situations. For example, some organizations tell candidates to answer predefined questions in a recorded video and use facial recognition to analyze the potential applicant faces. In addition, some companies are developing facial recognition software to scan the faces in crowds and assess threats, specifically mentioning doubt and anger as emotions that indicate threats.  This study provides evidence that facial recognition software interprets emotions differently based on the person’s race. Using a publically available data set of professional basketball players’ pictures, I compare the emotional analysis from two different facial recognition services, Face   and Microsoft's Face API. Both services interpret black players as having more negative emotions than white players; however, there are two different mechanisms. Face   consistently interprets black players as angrier than white players, even controlling for their degree of smiling. Microsoft registers contempt instead of anger, and it interprets black players as more contemptuous when their facial expressions are ambiguous. As the players’ smile widens, the disparity disappears. This finding has implications for individuals, organizations, and society, and it contributes to the growing literature of bias and/or disparate impact in AI.","2018-11-09","2021-03-16 14:12:55","2021-04-06 15:05:45","2021-03-16 14:12:55","","","","","","","","","","","","Social Science Research Network","Rochester, NY","en","","SSRN Scholarly Paper","","","papers.ssrn.com","","DOI: 10.2139/ssrn.3281765","<div data-schema-version=""1""><p>Annotation</p> <p>Racial influence on expression recognition</p></div>","C:\Users\smczx\Zotero\storage\LJCMBRU6\papers.html; F:\Google Drive - Monash\Bibliography\Rhue_2018_Racial Influence on Automated Perceptions of Emotions.pdf","","Unreviewed","artificial intelligence; bias; coarsened exact matching; econometrics; facial recognition; race","","","","","","","","","","","","","","","","","","","ID 3281765","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZHFBZTZ","conferencePaper","2011","Rudovic, O.; Pantic, M.","Shape-constrained Gaussian process regression for facial-point-based head-pose normalization","2011 International Conference on Computer Vision","","","10.1109/ICCV.2011.6126407","","Given the facial points extracted from an image of a face in an arbitrary pose, the goal of facial-point-based head-pose normalization is to obtain the corresponding facial points in a predefined pose (e.g., frontal). This involves inference of complex and high-dimensional mappings due to the large number of the facial points employed, and due to differences in head-pose and facial expression. Most regression-based approaches for learning such mappings focus on modeling correlations only between the inputs (i.e., the facial points in a non-frontal pose) and the outputs (i.e., the facial points in the frontal pose), but not within the inputs and the outputs of the model. This makes these models prone to errors due to noise and outliers in test data, often resulting in anatomically impossible facial configurations formed by their predictions. To address this, we propose Shape-constrained Gaussian Process (SC-GP) regression for facial-point-based head-pose normalization. Specifically, a deformable face-shape model is used to learn a face-shape prior, which is placed on both the input and the output of GP regression in order to constrain the model predictions to anatomically feasible facial configurations. Our extensive experiments on both synthetic and real image data show that the proposed approach generalizes well across poses and handles successfully noise and outliers in test data. In addition, the proposed model outperforms previously proposed approaches to facial-point-based head-pose normalization.","2011-11","2021-03-16 14:14:23","2021-04-06 15:01:43","","1495-1502","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2380-7504","<div data-schema-version=""1""><p>Annotation</p> <p>Gaussian process regression and apply PCA to manipulate</p></div>","C:\Users\smczx\Zotero\storage\T82G2MK8\6126407.html; F:\Google Drive - Monash\Bibliography\Rudovic_Pantic_2011_Shape-constrained Gaussian process regression for facial-point-based head-pose.pdf","","Unreviewed","Principal component analysis; Deformable models; arbitrary pose; Computational modeling; Data models; deformable face-shape model; face recognition; facial point extraction; facial-point-based head-pose normalization; feature extraction; Gaussian processes; high-dimensional mappings; pose estimation; regression analysis; Shape; shape-constrained Gaussian process regression; Three dimensional displays; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 International Conference on Computer Vision","","","","","","","","","","","","","","",""
"9LTVQVP7","journalArticle","2019","Li, S.; Deng, W.","Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition","IEEE Transactions on Image Processing","","1941-0042","10.1109/TIP.2018.2868382","","Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30 000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI, and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning-based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community.","2019-01","2021-03-16 14:16:39","2021-04-06 15:06:02","","356-370","","1","28","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Image Processing","<div data-schema-version=""1""><p>Annotation</p> <p>RAF-DB</p></div>","F:\Google Drive - Monash\Bibliography\Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf","","Unreviewed","convolution; Machine learning; learning (artificial intelligence); face recognition; basic emotion; CK+ database; compound emotion; Compounds; cross-database study; crowdsourcing annotation; Databases; deep learning; deep learning-based methods; deep locality-preserving convolutional neural network method; descriptor encodings; DLP-CNN; emotion labels; emotion recognition; expectation-maximisation algorithm; expectation-maximization algorithm; express compound; Expression recognition; Face; Face recognition; facial behavior posed; facial expression database; facial images; feedforward neural nets; inter-class scatter; laboratory-controlled emotions; locality closeness; mixture emotions; multimodal expressions recognition; RAF database; RAF-DB; Reactive power; real-world affective face database; real-world emotions; Reliability; unconstrained facial expression recognition; visual databases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7ZNACBK","journalArticle","2015","Simonyan, Karen; Zisserman, Andrew","Very Deep Convolutional Networks for Large-Scale Image Recognition","arXiv:1409.1556 [cs]","","","","http://arxiv.org/abs/1409.1556","In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.","2015-04-10","2021-03-16 14:24:33","2021-04-08 07:19:57","2021-03-16 14:24:33","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1409.1556","<div data-schema-version=""1""><p>Annotation</p> <p>VGG</p> <p>Improve the previous CNN structure for better accuracy for ILSVRC.</p> <p>The depth of the structure of CNN can be discussed for better performance of the model.</p> <p>They proposed two best VGG-16 and VGG-19 structures. Instead of using big kernel size, the VGG only use 3x3 kernel sizes and 16 layers or 19 layers.</p> <p>ILSVRC-2014</p> <p>The metric is test set classification error. Their score is better than the best of ILSVRC-2013.</p> <p>+ve: More depth is beneficial for the performance. VGG structures has widely usage for other tasks.</p> <p>-ve:</p> <p>Provide a famous VGG structure for CNN area, which indicates the depth contributes more than width of CNN.</p></div>","F:\Google Drive - Monash\Bibliography\Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf; C:\Users\smczx\Zotero\storage\9VVQ6FR8\1409.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGZ5ML22","conferencePaper","2016","He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian","Deep Residual Learning for Image Recognition","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html","Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","2016","2021-03-16 14:25:33","2021-04-06 15:05:00","2021-03-16 14:25:33","770-778","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>ResNet</p></div>","F:\Google Drive - Monash\Bibliography\He et al_2016_Deep Residual Learning for Image Recognition.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"5DX7TJDY","journalArticle","2014","Lin, Min; Chen, Qiang; Yan, Shuicheng","Network In Network","arXiv:1312.4400 [cs]","","","","http://arxiv.org/abs/1312.4400","We propose a novel deep network structure called ""Network In Network"" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","2014-03-04","2021-03-16 14:27:20","2021-04-08 06:08:21","2021-03-16 14:27:20","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1312.4400","<div data-schema-version=""1""><p>Annotation</p> <p>NIN</p> <p>The traditional convolutional layer as generalized linear model (GLM) has low level of abstraction.</p> <p>By replacing the GLM in convolutional layer with a non-linear function approximator can improve the &nbsp;abstraction ability of local model.</p> <p>The authors proposed two concepts, using MLP to replace the GLM in convolutional layer, and the usage of global average pooling layer. They use convolution layers with 1x1 kernel to simulate the MLP inside. And the global average pooling layer is used to replace the fully-connected layers in traditional CNN.</p> <p>CIFAR-10, CIFAR-100, SVHN, MNIST.</p> <p>The metric is test set classification error. In these 4 datasets, the NIN has an outstanding performance compared to other structures. Also the global average pooling as a regularizer is also better than fully connected + dropout set.</p> <p>+ve: The Mlpconv allows more complex and learnable interaction of cross channel information. Global average pooling is more meaningful and interpretable. Global average pooling is more robust to spatial translations of the input.</p> <p>-ve:</p> <p>The ideas in this paper is amazing, and many famous structures such as ResNet and GoogLeNet use these ideas.</p> <p></p></div>; Comment: 10 pages, 4 figures, for iclr2014","F:\Google Drive - Monash\Bibliography\Lin et al_2014_Network In Network.pdf; C:\Users\smczx\Zotero\storage\GBJXNLGT\1312.html","","","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQEVWNAP","conferencePaper","2015","Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew","Going Deeper With Convolutions","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html","We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification.","2015","2021-03-16 14:27:59","2021-04-08 09:13:54","2021-03-16 14:27:59","1-9","","","","","","","","","","","","","","","","","","www.cv-foundation.org","","","<div data-schema-version=""1""><p>Annotation</p> <p>Inception Module and GoogLeNet (InceptionV1)</p> <p>The mobile and embedded computting has limited power for models. A high-efficiency method is required.</p> <p>Decide to develop a high-efficiency method keeping a computational budget of 1.5 billion operations.</p> <p>The Inception module has multiple routes for input features. Each route has different kernel size of convolutions and max-pooling. And then, concatenate the outputs as the result of this module. Otherwise, the 1x1 convolutions can be used to reduce the number of channels. GoogLeNet is the model formed by Inception modules.</p> <p>ILSVRC 2014</p> <p>The metric is Top-5 error in ILSVRC 2014. The GoogLeNet ranked 1st in this competition.</p> <p>+ve: A significant quality improvement with slightly increase of computational requirements. Also available for object detection.</p> <p>-ve:</p> <p>The Inception Module is the new direction that the CNN can be not sequence. And, the high-efficiency also is an important metrics to judge the usability of a neural network structure.</p></div>","C:\Users\smczx\Zotero\storage\PTFRKBPB\Szegedy_Going_Deeper_With_2015_CVPR_paper.html; F:\Google Drive - Monash\Bibliography\Szegedy et al_2015_Going Deeper With Convolutions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"Y99BNDVT","journalArticle","2012","Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey","ImageNet Classification with Deep Convolutional Neural Networks","Advances in neural information processing systems","","","10.1145/3065386","","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.","2012-01-01","2021-03-16 14:39:49","2021-04-06 15:04:35","","1097–1105","","","25","","Neural Information Processing Systems","","","","","","","","","","","","","ResearchGate","","","<div data-schema-version=""2""><p>Annotation</p> <p>AlexNet / ReLU</p></div>","; F:\Google Drive - Monash\Bibliography\Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf","https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7T2LWIMS","conferencePaper","2020","Pidhorskyi, Stanislav; Adjeroh, Donald A.; Doretto, Gianfranco","Adversarial Latent Autoencoders","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html","Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.","2020","2021-03-16 14:44:03","2021-04-06 15:06:27","2021-03-16 14:44:03","14104-14113","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\smczx\Zotero\storage\8TBAEC8L\Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html; F:\Google Drive - Monash\Bibliography\Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"ZS3LF4TU","journalArticle","2017","Arjovsky, Martin; Chintala, Soumith; Bottou, Léon","Wasserstein GAN","arXiv:1701.07875 [cs, stat]","","","","http://arxiv.org/abs/1701.07875","We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.","2017-12-06","2021-03-16 14:44:26","2021-04-06 15:05:33","2021-03-16 14:44:26","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1701.07875","<div data-schema-version=""1""><p>Annotation</p> <p>WGAN</p></div>","F:\Google Drive - Monash\Bibliography\Arjovsky et al_2017_Wasserstein GAN.pdf; C:\Users\smczx\Zotero\storage\PWZ8UTT5\1701.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMPBRIPK","journalArticle","2020","Zheng, Xin; Guo, Yanqing; Huang, Huaibo; Li, Yi; He, Ran","A Survey of Deep Facial Attribute Analysis","International Journal of Computer Vision","","1573-1405","10.1007/s11263-020-01308-z","https://doi.org/10.1007/s11263-020-01308-z","Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.","2020-09-01","2021-03-16 14:46:55","2021-04-06 15:06:45","2021-03-16 14:46:55","2002-2034","","8","128","","Int J Comput Vis","","","","","","","","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Introduction to all facial analysis</p></div>","F:\Google Drive - Monash\Bibliography\Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNGWI5TW","journalArticle","2016","Radford, Alec; Metz, Luke; Chintala, Soumith","Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","arXiv:1511.06434 [cs]","","","","http://arxiv.org/abs/1511.06434","In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","2016-01-07","2021-03-16 14:49:34","2021-04-06 15:05:28","2021-03-16 14:49:34","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1511.06434","<div data-schema-version=""1""><p>Annotation</p> <p>DCGAN</p></div>; Comment: Under review as a conference paper at ICLR 2016","F:\Google Drive - Monash\Bibliography\Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf; C:\Users\smczx\Zotero\storage\DQWLAZHB\1511.html","","Unreviewed","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTCMR42I","conferencePaper","2015","Ioffe, Sergey; Szegedy, Christian","Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v37/ioffe15.html","Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the t...","2015-06-01","2021-03-16 14:50:26","2021-04-06 15:04:50","2021-03-16 14:50:26","448-456","","","","","","Batch Normalization","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","<div data-schema-version=""1""><p>Annotation</p> <p>Batch normalization and InceptionV2</p> <p></p></div>","C:\Users\smczx\Zotero\storage\T2BGYQWB\ioffe15.html; F:\Google Drive - Monash\Bibliography\Ioffe_Szegedy_2015_Batch Normalization.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"VP88VTAW","conferencePaper","2020","Mildenhall, Ben; Srinivasan, Pratul; Tancik, Matthew; Barron, Jonathan; Ramamoorthi, Ravi; Ng, Ren","NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","European Conference on Computer Vision","978-3-030-58451-1","","10.1007","","We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","2020","2021-03-16 14:54:40","2021-03-16 15:32:14","","405-421","","","","","","","","","","","","Springer, Cham","","","","","","","","DOI: 10.1007/978-3-030-58452-8_24","","F:\Google Drive - Monash\Bibliography\Mildenhall et al_2020_NeRF.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","European Conference on Computer Vision","","","","","","","","","","","","","","",""
"C3G62U2M","journalArticle","2020","Pumarola, Albert; Corona, Enric; Pons-Moll, Gerard; Moreno-Noguer, Francesc","D-NeRF: Neural Radiance Fields for Dynamic Scenes","arXiv:2011.13961 [cs]","","","","http://arxiv.org/abs/2011.13961","Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.","2020-11-27","2021-03-16 14:59:24","2021-03-16 15:32:20","2021-03-16 14:59:24","","","","","","","D-NeRF","","","","","","","","","","","","arXiv.org","","arXiv: 2011.13961","","F:\Google Drive - Monash\Bibliography\Pumarola et al_2020_D-NeRF.pdf; C:\Users\smczx\Zotero\storage\4M84DPVT\2011.html","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLLHIHTD","conferencePaper","2016","Johnson, Justin; Alahi, Alexandre; Fei-Fei, Li","Perceptual Losses for Real-Time Style Transfer and Super-Resolution","Computer Vision – ECCV 2016","978-3-319-46475-6","","10.1007/978-3-319-46475-6_43","","We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.","2016","2021-03-18 05:21:38","2021-04-06 15:05:03","","694-711","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""1""><p>Annotation</p> <p>Perceptual loss is the difference (MSE) between features of hidden layers in loss network from y_true and y_pred.</p></div>","F:\Google Drive - Monash\Bibliography\Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf","","Unreviewed","Deep learning; Style transfer; Super-resolution","Leibe, Bastian; Matas, Jiri; Sebe, Nicu; Welling, Max","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCKMHJ8W","conferencePaper","2020","Hong, Ming; Xie, Yuan; Li, Cuihua; Qu, Yanyun","Distilling Image Dehazing With Heterogeneous Task Imitation","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html","","2020","2021-03-18 05:07:28","2021-04-06 15:06:16","2021-03-18 05:07:28","3462-3471","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation and an autoencoder teacher to train a dehazing network with representation mimicking loss.</p></div>","F:\Google Drive - Monash\Bibliography\Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf; C:\Users\smczx\Zotero\storage\ABU8RUMW\Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"YEH4KREU","conferencePaper","2019","Gao, Qinquan; Zhao, Yan; Li, Gen; Tong, Tong","Image Super-Resolution Using Knowledge Distillation","Computer Vision – ACCV 2018","978-3-030-20890-5","","10.1007/978-3-030-20890-5_34","","The significant improvements in image super-resolution (SR) in recent years is majorly resulted from the use of deeper and deeper convolutional neural networks (CNN). However, both computational time and memory consumption simultaneously increase with the utilization of very deep CNN models, posing challenges to deploy SR models in realtime on computationally limited devices. In this work, we propose a novel strategy that uses a teacher-student network to improve the image SR performance. The training of a small but efficient student network is guided by a deep and powerful teacher network. We have evaluated the performance using different ways of knowledge distillation. Through the validations on four datasets, the proposed method significantly improves the SR performance of a student network without changing its structure. This means that the computational time and the memory consumption do not increase during the testing stage while the SR performance is significantly improved.","2019","2021-03-18 05:04:35","2021-04-06 15:05:55","","527-541","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""1""><p>Annotation</p> <p>Perform super resolution with knowledge distillation.</p></div>","F:\Google Drive - Monash\Bibliography\Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf","","Unreviewed","Super-resolution; Convolutional neural networks; Knowledge distillation; Teacher-student network","Jawahar, C. V.; Li, Hongdong; Mori, Greg; Schindler, Konrad","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94Q7XD2W","conferencePaper","2018","Woo, Sanghyun; Park, Jongchan; Lee, Joon-Young; Kweon, In So","CBAM: Convolutional Block Attention Module","Proceedings of the European Conference on Computer Vision (ECCV)","","","","https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html","","2018","2021-03-18 03:50:54","2021-04-06 15:05:47","2021-03-18 03:50:54","3-19","","","","","","CBAM","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>With Channel Attention Module and Spatial Attention Module, the CBAM can be inserted into conv blocks to improve the CNN performance.</p></div>","F:\Google Drive - Monash\Bibliography\Woo et al_2018_CBAM.pdf; C:\Users\smczx\Zotero\storage\JEERU7SI\Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV)","","","","","","","","","","","","","","",""
"SC6MFJRV","journalArticle","2020","Pihlgren, Gustav Grund; Sandin, Fredrik; Liwicki, Marcus","Pretraining Image Encoders without Reconstruction via Feature Prediction Loss","arXiv:2003.07441 [cs]","","","","http://arxiv.org/abs/2003.07441","This work investigates three methods for calculating loss for autoencoder-based pretraining of image encoders: The commonly used reconstruction loss, the more recently introduced deep perceptual similarity loss, and a feature prediction loss proposed here; the latter turning out to be the most efficient choice. Standard auto-encoder pretraining for deep learning tasks is done by comparing the input image and the reconstructed image. Recent work shows that predictions based on embeddings generated by image autoencoders can be improved by training with perceptual loss, i.e., by adding a loss network after the decoding step. So far the autoencoders trained with loss networks implemented an explicit comparison of the original and reconstructed images using the loss network. However, given such a loss network we show that there is no need for the time-consuming task of decoding the entire image. Instead, we propose to decode the features of the loss network, hence the name ""feature prediction loss"". To evaluate this method we perform experiments on three standard publicly available datasets (LunarLander-v2, STL-10, and SVHN) and compare six different procedures for training image encoders (pixel-wise, perceptual similarity, and feature prediction losses; combined with two variations of image and feature encoding/decoding). The embedding-based prediction results show that encoders trained with feature prediction loss is as good or better than those trained with the other two losses. Additionally, the encoder is significantly faster to train using feature prediction loss in comparison to the other losses. The method implementation used in this work is available online: https://github.com/guspih/Perceptual-Autoencoders","2020-07-15","2021-03-18 03:03:43","2021-04-06 15:06:29","2021-03-18 03:03:43","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2003.07441","<div data-schema-version=""1""><p>Annotation</p> <p>Another way to train the Autoencoder with other pretrained feature extractor models.</p></div>","C:\Users\smczx\Zotero\storage\TYYGD6EZ\2003.html; F:\Google Drive - Monash\Bibliography\Pihlgren et al_2020_Pretraining Image Encoders without Reconstruction via Feature Prediction Loss.pdf","","Unreviewed","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLTXINZX","conferencePaper","2020","Viazovetskyi, Yuri; Ivashkin, Vladimir; Kashin, Evgeny","StyleGAN2 Distillation for Feed-Forward Image Manipulation","Computer Vision – ECCV 2020","978-3-030-58542-6","","10.1007/978-3-030-58542-6_11","","StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces’ transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.","2020","2021-03-18 02:37:32","2021-04-06 15:06:32","","170-186","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""1""><p>Annotation</p> <p>Using StyleGAN to generate pairs for pix2pixHD to be trained as image translation task.</p></div>","F:\Google Drive - Monash\Bibliography\Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf","","Unreviewed","Computer vision; Distillation; StyleGAN2; Synthetic data","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Vision – ECCV 2020","","","","","","","","","","","","","","",""
"8YP6UX9S","journalArticle","2019","Wang, J.; Gou, L.; Zhang, W.; Yang, H.; Shen, H.","DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation","IEEE Transactions on Visualization and Computer Graphics","","1941-0506","10.1109/TVCG.2019.2903943","","Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.","2019-06","2021-03-18 01:51:56","2021-04-06 15:06:09","","2168-2180","","6","25","","","DeepVID","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Visualization and Computer Graphics","<div data-schema-version=""1""><p>Annotation</p> <p>Using VAE and knowledge distillation to generate neighbours of interesting points.</p></div>","F:\Google Drive - Monash\Bibliography\Wang et al_2019_DeepVID.pdf; C:\Users\smczx\Zotero\storage\ZF8Z5MGH\8667661.html","","Unreviewed","Neural networks; learning (artificial intelligence); Data models; Training; Deep learning; Analytical models; deep generative model; deep learning experts; Deep neural networks; Deep Neural Networks; deep visual diagnosis; deep visual interpretation; DeepVID; DNN; generative model; image classification; image classifiers; knowledge distillation; model interpretation; neural nets; safety-critical applications; Semantics; visual analytics; Visual analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTVLVRAI","journalArticle","2015","Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff","Distilling the Knowledge in a Neural Network","arXiv:1503.02531 [cs, stat]","","","","http://arxiv.org/abs/1503.02531","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","2015-03-09","2021-03-17 12:52:08","2021-04-06 15:04:46","2021-03-17 12:52:08","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1503.02531","<div data-schema-version=""1""><p>Annotation</p> <p>Knowledge Distillation</p></div>; Comment: NIPS 2014 Deep Learning Workshop","C:\Users\smczx\Zotero\storage\8KILIT88\1503.html; F:\Google Drive - Monash\Bibliography\Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EWLEGZ2W","conferencePaper","2020","Li, Zeqi; Jiang, Ruowei; Aarabi, Parham","Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation","Computer Vision – ECCV 2020","978-3-030-58574-7","","10.1007/978-3-030-58574-7_39","","Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher’s feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively.","2020","2021-03-22 05:27:10","2021-04-06 15:06:23","","648-663","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation to perform image-image translation with proposed semantic relation preserving matrix and semantic preserving distillation loss.</p></div>","F:\Google Drive - Monash\Bibliography\Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf","","Unreviewed","Knowledge distillation; Generative adversarial networks; Image-to-image translation; Model compression","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Vision – ECCV 2020","","","","","","","","","","","","","","",""
"QYH9LAF7","conferencePaper","2019","Zhai, Mengyao; Chen, Lei; Tung, Frederick; He, Jiawei; Nawhal, Megha; Mori, Greg","Lifelong GAN: Continual Learning for Conditional Image Generation","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html","","2019","2021-03-22 05:51:57","2021-04-06 15:06:13","2021-03-22 05:51:57","2759-2768","","","","","","Lifelong GAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>Lifelong GAN. Using knowledge distillation to perform continous training BicycleGAN for image translation avoiding forgeting.</p></div>","F:\Google Drive - Monash\Bibliography\Zhai et al_2019_Lifelong GAN.pdf; C:\Users\smczx\Zotero\storage\MCSJBUDY\Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"HRBB3NBJ","journalArticle","2018","Zhu, Jun-Yan; Zhang, Richard; Pathak, Deepak; Darrell, Trevor; Efros, Alexei A.; Wang, Oliver; Shechtman, Eli","Toward Multimodal Image-to-Image Translation","arXiv:1711.11586 [cs, stat]","","","","http://arxiv.org/abs/1711.11586","Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.","2018-10-23","2021-03-22 07:05:57","2021-04-06 15:05:49","2021-03-22 07:05:57","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1711.11586","<div data-schema-version=""1""><p>Annotation</p> <p>BicycleGAN</p></div>; Comment: NIPS 2017 Final paper. v4 updated acknowledgment. Website: https://junyanz.github.io/BicycleGAN/","F:\Google Drive - Monash\Bibliography\Zhu et al_2018_Toward Multimodal Image-to-Image Translation.pdf; C:\Users\smczx\Zotero\storage\4JC6HZMQ\1711.html","","Unreviewed","Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRK93TD9","conferencePaper","2016","Larsen, Anders Boesen Lindbo; Sønderby, Søren Kaae; Larochelle, Hugo; Winther, Ole","Autoencoding beyond pixels using a learned similarity metric","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v48/larsen16.html","We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GA...","2016-06-11","2021-03-22 07:10:53","2021-04-06 15:05:07","2021-03-22 07:10:53","1558-1566","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","<div data-schema-version=""1""><p>Annotation</p> <p>VAE-GAN is introduced to combine VAE and GAN to improve the GAN training.</p></div>","C:\Users\smczx\Zotero\storage\4IHWY4CY\larsen16.html; F:\Google Drive - Monash\Bibliography\Larsen et al_2016_Autoencoding beyond pixels using a learned similarity metric.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"5PPVC36J","conferencePaper","2017","Bao, Jianmin; Chen, Dong; Wen, Fang; Li, Houqiang; Hua, Gang","CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html","","2017","2021-03-22 07:12:39","2021-04-06 15:05:36","2021-03-22 07:12:39","2745-2754","","","","","","CVAE-GAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>Intoducing CVAE-GAN (Conditional VAE-GAN) by using label to improve the performance of VAE-GAN.</p></div>","C:\Users\smczx\Zotero\storage\HJSQ369H\Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html; F:\Google Drive - Monash\Bibliography\Bao et al_2017_CVAE-GAN.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"4FLMXLVV","journalArticle","2020","Yuan, M.; Peng, Y.","CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis","IEEE Transactions on Multimedia","","1941-0077","10.1109/TMM.2019.2951463","","Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.","2020-08","2021-03-22 08:05:43","2021-04-06 15:06:40","","1955-1968","","8","22","","","CKD","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Multimedia","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation to perform text to image synthesis task.</p></div>","C:\Users\smczx\Zotero\storage\DTFEDK7M\8890866.html; F:\Google Drive - Monash\Bibliography\Yuan_Peng_2020_CKD.pdf","","Unreviewed","Neural networks; learning (artificial intelligence); knowledge distillation; Semantics; Generative adversarial networks; approximate the real data distribution; CKD; complex RGB pixel space; consistent semantic contents; cross-task knowledge distillation approach; highly challenging task; huge pixel space; Image color analysis; image colour analysis; image contents; image semantic understanding; Image synthesis; multiple image semantic understanding tasks; multistage knowledge distillation paradigm; pattern recognition; photo-realistic images; realistic images; semantic consistency issue; semantic representation; semantic space; synthetic images; T2IS; Task analysis; text analysis; text description; text-to-image synthesis; Text-to-image synthesis; transfer learning; visual quality issue; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ES8WZY5Q","conferencePaper","2019","Tung, Frederick; Mori, Greg","Similarity-Preserving Knowledge Distillation","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html","","2019","2021-03-22 08:16:09","2021-04-06 15:06:07","2021-03-22 08:16:09","1365-1374","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>Using corsine similarity matrix to represent the relation in each batch, to replace the feature map to be learned in knowledge distillation.</p></div>","C:\Users\smczx\Zotero\storage\NXTWWDCS\Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html; F:\Google Drive - Monash\Bibliography\Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"J956Z7GT","journalArticle","2020","Jing, L.; Tian, Y.","Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/TPAMI.2020.2992393","","Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.","2020","2021-03-29 04:54:01","2021-03-29 04:55:56","","1-1","","","","","","Self-supervised Visual Feature Learning with Deep Neural Networks","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","","F:\Google Drive - Monash\Bibliography\Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf; C:\Users\smczx\Zotero\storage\X8PV7RTB\9086055.html","","Unread","Feature extraction; Training; Task analysis; Visualization; Annotations; Convolutional Neural Network; Deep Learning; Learning systems; Self-supervised Learning; Transfer Learning; Unsupervised Learning; Videos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YN6MUWWP","conferencePaper","2017","Huang, Xun; Belongie, Serge","Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html","","2017","2021-03-29 04:43:34","2021-04-09 01:53:06","2021-03-29 04:43:34","1501-1510","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Huang_Belongie_2017_Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization.pdf; C:\Users\smczx\Zotero\storage\ESXAEPLT\Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"AVFLWRB6","conferencePaper","2019","Karras, Tero; Laine, Samuli; Aila, Timo","A Style-Based Generator Architecture for Generative Adversarial Networks","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html","","2019","2021-03-29 04:41:33","2021-04-01 05:28:10","2021-03-29 04:41:33","4401-4410","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf; C:\Users\smczx\Zotero\storage\8LKELHY9\Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"L8IB9M3W","conferencePaper","2020","Wang, Huan; Li, Yijun; Wang, Yuehai; Hu, Haoji; Yang, Ming-Hsuan","Collaborative Distillation for Ultra-Resolution Universal Style Transfer","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html","Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.","2020","2021-03-29 04:33:48","2021-03-29 04:35:29","2021-03-29 04:33:48","1860-1869","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf; C:\Users\smczx\Zotero\storage\VZ2DNRMF\Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"FG3L3HGN","journalArticle","2019","Brock, Andrew; Donahue, Jeff; Simonyan, Karen","Large Scale GAN Training for High Fidelity Natural Image Synthesis","arXiv:1809.11096 [cs, stat]","","","","http://arxiv.org/abs/1809.11096","Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.","2019-02-25","2021-03-29 04:21:24","2021-03-29 04:24:47","2021-03-29 04:21:24","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1809.11096","","F:\Google Drive - Monash\Bibliography\Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis2.pdf; C:\Users\smczx\Zotero\storage\KYRR8X9E\1809.html","","Unread","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TD5237DC","journalArticle","2017","Zhang, Xiangyu; Zhou, Xinyu; Lin, Mengxiao; Sun, Jian","ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices","arXiv:1707.01083 [cs]","","","","http://arxiv.org/abs/1707.01083","We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.","2017-12-07","2021-03-29 04:17:08","2021-03-29 04:17:23","2021-03-29 04:17:08","","","","","","","ShuffleNet","","","","","","","","","","","","arXiv.org","","arXiv: 1707.01083","","C:\Users\smczx\Zotero\storage\NJXA8YRI\1707.html; F:\Google Drive - Monash\Bibliography\Zhang et al_2017_ShuffleNet.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SZH46QB","conferencePaper","2020","Yin, Hongxu; Molchanov, Pavlo; Alvarez, Jose M.; Li, Zhizhong; Mallya, Arun; Hoiem, Derek; Jha, Niraj K.; Kautz, Jan","Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html","We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ""invert"" a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.","2020","2021-03-29 04:15:29","2021-03-29 04:16:21","2021-03-29 04:15:29","8715-8724","","","","","","Dreaming to Distill","","","","","","","","","","","","openaccess.thecvf.com","","","","F:\Google Drive - Monash\Bibliography\Yin et al_2020_Dreaming to Distill.pdf","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"UWGZZ5T8","journalArticle","2020","Li, Kang; Yu, Lequan; Wang, Shujun; Heng, Pheng-Ann","Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","10.1609/aaai.v34i01.5421","https://ojs.aaai.org/index.php/AAAI/article/view/5421","","2020-04-03","2021-03-29 00:40:26","2021-04-06 15:06:21","2021-03-29 00:40:26","775-783","","01","34","","AAAI","","","","","","","","en","Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","Number: 01","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation to improve the performance of image segmentation.</p></div>","C:\Users\smczx\Zotero\storage\K5LHY756\5421.html; F:\Google Drive - Monash\Bibliography\Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBQELRWF","journalArticle","2020","Zhu, M.; Li, J.; Wang, N.; Gao, X.","Knowledge Distillation for Face Photo-Sketch Synthesis","IEEE Transactions on Neural Networks and Learning Systems","","2162-2388","10.1109/TNNLS.2020.3030536","","Significant progress has been made with face photo-sketch synthesis in recent years due to the development of deep convolutional neural networks, particularly generative adversarial networks (GANs). However, the performance of existing methods is still limited because of the lack of training data (photo-sketch pairs). To address this challenge, we investigate the effect of knowledge distillation (KD) on training neural networks for the face photo-sketch synthesis task and propose an effective KD model to improve the performance of synthetic images. In particular, we utilize a teacher network trained on a large amount of data in a related task to separately learn knowledge of the face photo and knowledge of the face sketch and simultaneously transfer this knowledge to two student networks designed for the face photo-sketch synthesis task. In addition to assimilating the knowledge from the teacher network, the two student networks can mutually transfer their own knowledge to further enhance their learning. To further enhance the perception quality of the synthetic image, we propose a KD+ model that combines GANs with KD. The generator can produce images with more realistic textures and less noise under the guide of knowledge. Extensive experiments and a user study demonstrate the superiority of our models over the state-of-the-art methods.","2020","2021-03-29 00:18:36","2021-04-06 15:06:48","","1-14","","","","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Neural Networks and Learning Systems","<div data-schema-version=""1""><p>Annotation</p> <p>Using Knowledge Distillation and GAN to translate images between face photo and sketch.</p></div>","F:\Google Drive - Monash\Bibliography\Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf; C:\Users\smczx\Zotero\storage\FZIX6WUE\9240973.html","","Unreviewed","Data models; Face recognition; Task analysis; Face photo-sketch synthesis; Faces; Gallium nitride; generative adversarial networks (GANs); knowledge distillation (KD); Knowledge engineering; teacher-student model.; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7UITLDP","journalArticle","2019","Aguinaldo, Angeline; Chiang, Ping-Yeh; Gain, Alex; Patil, Ameya; Pearson, Kolten; Feizi, Soheil","Compressing GANs using Knowledge Distillation","arXiv:1902.00159 [cs, stat]","","","","http://arxiv.org/abs/1902.00159","Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super resolution, and synthetic data generation. State-of-the-art GANs often use tens of millions of parameters, making them expensive to deploy for applications in low SWAP (size, weight, and power) hardware, such as mobile devices, and for applications with real time capabilities. There has been no work found to reduce the number of parameters used in GANs. Therefore, we propose a method to compress GANs using knowledge distillation techniques, in which a smaller ""student"" GAN learns to mimic a larger ""teacher"" GAN. We show that the distillation methods used on MNIST, CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1, 58:1, and 87:1, respectively, while retaining the quality of the generated image. From our experiments, we observe a qualitative limit for GAN's compression. Moreover, we observe that, with a fixed parameter budget, compressed GANs outperform GANs trained using standard training methods. We conjecture that this is partially owing to the optimization landscape of over-parameterized GANs which allows efficient training using alternating gradient descent. Thus, training an over-parameterized GAN followed by our proposed compression scheme provides a high quality generative model with a small number of parameters.","2019-01-31","2021-03-26 07:04:30","2021-04-06 15:05:51","2021-03-26 07:04:30","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1902.00159","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation to compress the generator of GAN.</p></div>","C:\Users\smczx\Zotero\storage\QQ42XCWA\1902.html; F:\Google Drive - Monash\Bibliography\Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDJH6KVN","journalArticle","1901","F.R.S, Karl Pearson","LIII. On lines and planes of closest fit to systems of points in space","The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science","","1941-5982","10.1080/14786440109462720","https://doi.org/10.1080/14786440109462720","","1901-11-01","2021-04-08 05:23:21","2021-04-08 05:24:13","2021-04-08 05:23:21","559-572","","11","2","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/14786440109462720","<div data-schema-version=""1""><p>Annotation</p> <p>PCA original paper</p></div>","F:\Google Drive - Monash\Bibliography\F.R.S_1901_LIII.pdf; C:\Users\smczx\Zotero\storage\NTCZUUMU\14786440109462720.html","","Unread","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"698AHGVY","journalArticle","2014","Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua","Generative Adversarial Networks","arXiv:1406.2661 [cs, stat]","","","","http://arxiv.org/abs/1406.2661","We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","2014-06-10","2021-04-08 09:17:51","2021-04-08 09:17:51","2021-04-08 09:17:51","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1406.2661","<div data-schema-version=""2""><p>Annotation</p> <p>Generative Adversarial Networks (GAN)</p> <p>It is difficult to approximate because the difficulty of utilizing the benefits of picewise linear units in the generative context.</p> <p>It is possible to use a adversarial structures to overcome the difficulties.</p> <p>The GAN framework has two models, generative model and discriminative model. The generator input a noise vector try to confuse the discriminator, and the discriminator can classify the image is from real dataset or generator.</p> <p>MNIST, Toronto Face Database (TFD), CIFAR-10</p> <p>The metric is window-based log-likelihood etimates. The adversarial nets is better than other generative networks.</p> <p>+ve: Has potential in the future. No inference is needed during learning. A wide variety of functions can be incorporated into the model. The adversarial models does not directly receive any information from dataset. GAN can represent very sharp distributions.</p> <p>-ve: The discriminator must be synchronized well with generator during training (Hard to train). </p> <p>The development of GAN broaden the area of image synthesis and translation area. GAN has the advantages of generating very realistic data, so will be widely used in the future generation tasks. However, GAN is hard to train, but there will be future works to improve that.</p></div>","F:\Google Drive - Monash\Bibliography\Goodfellow et al_2014_Generative Adversarial Networks.pdf; C:\Users\smczx\Zotero\storage\VPGCMDSE\1406.html","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YWSL99D4","journalArticle","2020","Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil","An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","arXiv:2010.11929 [cs]","","","","http://arxiv.org/abs/2010.11929","While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","2020-10-22","2021-04-09 06:01:35","2021-04-09 11:22:25","2021-04-09 06:01:35","","","","","","","An Image is Worth 16x16 Words","","","","","","","","","","","","arXiv.org","","arXiv: 2010.11929","Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer","C:\Users\smczx\Zotero\storage\JWTXXQUI\2010.html; F:\Google Drive - Monash\Bibliography\Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf","","Unread","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTBA5S6Z","journalArticle","2021","Vaswani, Ashish; Ramachandran, Prajit; Srinivas, Aravind; Parmar, Niki; Hechtman, Blake; Shlens, Jonathon","Scaling Local Self-Attention for Parameter Efficient Visual Backbones","arXiv:2103.12731 [cs]","","","","http://arxiv.org/abs/2103.12731","Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.","2021-03-30","2021-04-09 05:56:57","2021-04-09 11:22:10","2021-04-09 05:56:57","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2103.12731","Comment: CVPR 2021 Oral","C:\Users\smczx\Zotero\storage\Y8LLUYWL\2103.html; F:\Google Drive - Monash\Bibliography\Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NL79FS5","journalArticle","2019","Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon","Stand-Alone Self-Attention in Vision Models","arXiv:1906.05909 [cs]","","","","http://arxiv.org/abs/1906.05909","Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.","2019-06-13","2021-04-09 05:56:03","2021-04-09 11:22:36","2021-04-09 05:56:03","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1906.05909","","C:\Users\smczx\Zotero\storage\ZHYHSP49\1906.html; F:\Google Drive - Monash\Bibliography\Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf","","Unread","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""