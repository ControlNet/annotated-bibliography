"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"RLTXINZX","conferencePaper","2020","Viazovetskyi, Yuri; Ivashkin, Vladimir; Kashin, Evgeny","StyleGAN2 Distillation for Feed-Forward Image Manipulation","Computer Vision – ECCV 2020","978-3-030-58542-6","","10.1007/978-3-030-58542-6_11","","StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces’ transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.","2020","2021-03-18 02:37:32","2021-04-06 15:06:32","","170-186","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""1""><p>Annotation</p> <p>Using StyleGAN to generate pairs for pix2pixHD to be trained as image translation task.</p></div>","G:\My Drive\Bibliography\Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf","","Unreviewed","Computer vision; Distillation; StyleGAN2; Synthetic data","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Vision – ECCV 2020","","","","","","","","","","","","","","",""
"SC6MFJRV","journalArticle","2020","Pihlgren, Gustav Grund; Sandin, Fredrik; Liwicki, Marcus","Pretraining Image Encoders without Reconstruction via Feature Prediction Loss","arXiv:2003.07441 [cs]","","","","http://arxiv.org/abs/2003.07441","This work investigates three methods for calculating loss for autoencoder-based pretraining of image encoders: The commonly used reconstruction loss, the more recently introduced deep perceptual similarity loss, and a feature prediction loss proposed here; the latter turning out to be the most efficient choice. Standard auto-encoder pretraining for deep learning tasks is done by comparing the input image and the reconstructed image. Recent work shows that predictions based on embeddings generated by image autoencoders can be improved by training with perceptual loss, i.e., by adding a loss network after the decoding step. So far the autoencoders trained with loss networks implemented an explicit comparison of the original and reconstructed images using the loss network. However, given such a loss network we show that there is no need for the time-consuming task of decoding the entire image. Instead, we propose to decode the features of the loss network, hence the name ""feature prediction loss"". To evaluate this method we perform experiments on three standard publicly available datasets (LunarLander-v2, STL-10, and SVHN) and compare six different procedures for training image encoders (pixel-wise, perceptual similarity, and feature prediction losses; combined with two variations of image and feature encoding/decoding). The embedding-based prediction results show that encoders trained with feature prediction loss is as good or better than those trained with the other two losses. Additionally, the encoder is significantly faster to train using feature prediction loss in comparison to the other losses. The method implementation used in this work is available online: https://github.com/guspih/Perceptual-Autoencoders","2020-07-15","2021-03-18 03:03:43","2021-04-06 15:06:29","2021-03-18 03:03:43","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2003.07441","<div data-schema-version=""1""><p>Annotation</p> <p>Another way to train the Autoencoder with other pretrained feature extractor models.</p></div>","C:\Users\controlnet\Zotero\storage\TYYGD6EZ\2003.html; G:\My Drive\Bibliography\Pihlgren et al_2020_Pretraining Image Encoders without Reconstruction via Feature Prediction Loss.pdf","","Unreviewed","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7T2LWIMS","conferencePaper","2020","Pidhorskyi, Stanislav; Adjeroh, Donald A.; Doretto, Gianfranco","Adversarial Latent Autoencoders","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html","Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.","2020","2021-03-16 14:44:03","2021-04-06 15:06:27","2021-03-16 14:44:03","14104-14113","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf; C:\Users\controlnet\Zotero\storage\8TBAEC8L\Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"EWLEGZ2W","conferencePaper","2020","Li, Zeqi; Jiang, Ruowei; Aarabi, Parham","Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation","Computer Vision – ECCV 2020","978-3-030-58574-7","","10.1007/978-3-030-58574-7_39","","Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher’s feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively.","2020","2021-03-22 05:27:10","2021-04-06 15:06:23","","648-663","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation to perform image-image translation with proposed semantic relation preserving matrix and semantic preserving distillation loss.</p></div>","G:\My Drive\Bibliography\Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf","","Unreviewed","Generative adversarial networks; Image-to-image translation; Knowledge distillation; Model compression","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Vision – ECCV 2020","","","","","","","","","","","","","","",""
"UWGZZ5T8","journalArticle","2020","Li, Kang; Yu, Lequan; Wang, Shujun; Heng, Pheng-Ann","Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","10.1609/aaai.v34i01.5421","https://ojs.aaai.org/index.php/AAAI/article/view/5421","","2020-04-03","2021-03-29 00:40:26","2021-04-06 15:06:21","2021-03-29 00:40:26","775-783","","01","34","","AAAI","","","","","","","","en","Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","Number: 01","<div data-schema-version=""1""><p>Annotation</p> <p>Using knowledge distillation to improve the performance of image segmentation.</p></div>","G:\My Drive\Bibliography\Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf; C:\Users\controlnet\Zotero\storage\K5LHY756\5421.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCKMHJ8W","conferencePaper","2020","Hong, Ming; Xie, Yuan; Li, Cuihua; Qu, Yanyun","Distilling Image Dehazing With Heterogeneous Task Imitation","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html","","2020","2021-03-18 05:07:28","2021-04-06 15:06:16","2021-03-18 05:07:28","3462-3471","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation and an autoencoder teacher to train a dehazing network with representation mimicking loss.</p></div>","G:\My Drive\Bibliography\Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf; C:\Users\controlnet\Zotero\storage\ABU8RUMW\Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"QYH9LAF7","conferencePaper","2019","Zhai, Mengyao; Chen, Lei; Tung, Frederick; He, Jiawei; Nawhal, Megha; Mori, Greg","Lifelong GAN: Continual Learning for Conditional Image Generation","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html","","2019","2021-03-22 05:51:57","2021-04-06 15:06:13","2021-03-22 05:51:57","2759-2768","","","","","","Lifelong GAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>Lifelong GAN. Using knowledge distillation to perform continous training BicycleGAN for image translation avoiding forgeting.</p></div>","C:\Users\controlnet\Zotero\storage\MCSJBUDY\Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html; G:\My Drive\Bibliography\Zhai et al_2019_Lifelong GAN.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"8YP6UX9S","journalArticle","2019","Wang, J.; Gou, L.; Zhang, W.; Yang, H.; Shen, H.","DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation","IEEE Transactions on Visualization and Computer Graphics","","1941-0506","10.1109/TVCG.2019.2903943","","Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.","2019-06","2021-03-18 01:51:56","2021-04-06 15:06:09","","2168-2180","","6","25","","","DeepVID","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Visualization and Computer Graphics","<div data-schema-version=""1""><p>Annotation</p> <p>Using VAE and knowledge distillation to generate neighbours of interesting points.</p></div>","C:\Users\controlnet\Zotero\storage\ZF8Z5MGH\8667661.html; G:\My Drive\Bibliography\Wang et al_2019_DeepVID.pdf","","Unreviewed","Analytical models; Data models; deep generative model; Deep learning; deep learning experts; Deep neural networks; Deep Neural Networks; deep visual diagnosis; deep visual interpretation; DeepVID; DNN; generative model; image classification; image classifiers; knowledge distillation; learning (artificial intelligence); model interpretation; neural nets; Neural networks; safety-critical applications; Semantics; Training; visual analytics; Visual analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ES8WZY5Q","conferencePaper","2019","Tung, Frederick; Mori, Greg","Similarity-Preserving Knowledge Distillation","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html","","2019","2021-03-22 08:16:09","2021-04-06 15:06:07","2021-03-22 08:16:09","1365-1374","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Using corsine similarity matrix to represent the relation in each batch, to replace the feature map to be learned in knowledge distillation.</p></div>","C:\Users\controlnet\Zotero\storage\NXTWWDCS\Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html; G:\My Drive\Bibliography\Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"9LTVQVP7","journalArticle","2019","Li, S.; Deng, W.","Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition","IEEE Transactions on Image Processing","","1941-0042","10.1109/TIP.2018.2868382","","Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30 000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI, and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning-based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community.","2019-01","2021-03-16 14:16:39","2021-04-06 15:06:02","","356-370","","1","28","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Image Processing","<div data-schema-version=""1""><p>Annotation</p> <p>RAF-DB</p></div>","G:\My Drive\Bibliography\Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf","","Unreviewed","learning (artificial intelligence); basic emotion; CK+ database; compound emotion; Compounds; convolution; cross-database study; crowdsourcing annotation; Databases; deep learning; deep learning-based methods; deep locality-preserving convolutional neural network method; descriptor encodings; DLP-CNN; emotion labels; emotion recognition; expectation-maximisation algorithm; expectation-maximization algorithm; express compound; Expression recognition; Face; face recognition; Face recognition; facial behavior posed; facial expression database; facial images; feedforward neural nets; inter-class scatter; laboratory-controlled emotions; locality closeness; Machine learning; mixture emotions; multimodal expressions recognition; RAF database; RAF-DB; Reactive power; real-world affective face database; real-world emotions; Reliability; unconstrained facial expression recognition; visual databases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YEH4KREU","conferencePaper","2019","Gao, Qinquan; Zhao, Yan; Li, Gen; Tong, Tong","Image Super-Resolution Using Knowledge Distillation","Computer Vision – ACCV 2018","978-3-030-20890-5","","10.1007/978-3-030-20890-5_34","","The significant improvements in image super-resolution (SR) in recent years is majorly resulted from the use of deeper and deeper convolutional neural networks (CNN). However, both computational time and memory consumption simultaneously increase with the utilization of very deep CNN models, posing challenges to deploy SR models in realtime on computationally limited devices. In this work, we propose a novel strategy that uses a teacher-student network to improve the image SR performance. The training of a small but efficient student network is guided by a deep and powerful teacher network. We have evaluated the performance using different ways of knowledge distillation. Through the validations on four datasets, the proposed method significantly improves the SR performance of a student network without changing its structure. This means that the computational time and the memory consumption do not increase during the testing stage while the SR performance is significantly improved.","2019","2021-03-18 05:04:35","2021-04-06 15:05:55","","527-541","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Perform super resolution with knowledge distillation.</p></div>","G:\My Drive\Bibliography\Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf","","Unreviewed","Knowledge distillation; Convolutional neural networks; Super-resolution; Teacher-student network","Jawahar, C. V.; Li, Hongdong; Mori, Greg; Schindler, Konrad","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7UITLDP","journalArticle","2019","Aguinaldo, Angeline; Chiang, Ping-Yeh; Gain, Alex; Patil, Ameya; Pearson, Kolten; Feizi, Soheil","Compressing GANs using Knowledge Distillation","arXiv:1902.00159 [cs, stat]","","","","http://arxiv.org/abs/1902.00159","Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super resolution, and synthetic data generation. State-of-the-art GANs often use tens of millions of parameters, making them expensive to deploy for applications in low SWAP (size, weight, and power) hardware, such as mobile devices, and for applications with real time capabilities. There has been no work found to reduce the number of parameters used in GANs. Therefore, we propose a method to compress GANs using knowledge distillation techniques, in which a smaller ""student"" GAN learns to mimic a larger ""teacher"" GAN. We show that the distillation methods used on MNIST, CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1, 58:1, and 87:1, respectively, while retaining the quality of the generated image. From our experiments, we observe a qualitative limit for GAN's compression. Moreover, we observe that, with a fixed parameter budget, compressed GANs outperform GANs trained using standard training methods. We conjecture that this is partially owing to the optimization landscape of over-parameterized GANs which allows efficient training using alternating gradient descent. Thus, training an over-parameterized GAN followed by our proposed compression scheme provides a high quality generative model with a small number of parameters.","2019-01-31","2021-03-26 07:04:30","2021-04-06 15:05:51","2021-03-26 07:04:30","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1902.00159","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation to compress the generator of GAN.</p></div>","G:\My Drive\Bibliography\Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf; C:\Users\controlnet\Zotero\storage\QQ42XCWA\1902.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRBB3NBJ","journalArticle","2018","Zhu, Jun-Yan; Zhang, Richard; Pathak, Deepak; Darrell, Trevor; Efros, Alexei A.; Wang, Oliver; Shechtman, Eli","Toward Multimodal Image-to-Image Translation","arXiv:1711.11586 [cs, stat]","","","","http://arxiv.org/abs/1711.11586","Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.","2018-10-23","2021-03-22 07:05:57","2021-04-06 15:05:49","2021-03-22 07:05:57","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1711.11586","<div data-schema-version=""1""><p>Annotation</p> <p>BicycleGAN</p></div>; Comment: NIPS 2017 Final paper. v4 updated acknowledgment. Website: https://junyanz.github.io/BicycleGAN/","C:\Users\controlnet\Zotero\storage\4JC6HZMQ\1711.html; G:\My Drive\Bibliography\Zhu et al_2018_Toward Multimodal Image-to-Image Translation.pdf","","Unreviewed","Computer Science - Computer Vision and Pattern Recognition; Statistics - Machine Learning; Computer Science - Graphics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94Q7XD2W","conferencePaper","2018","Woo, Sanghyun; Park, Jongchan; Lee, Joon-Young; Kweon, In So","CBAM: Convolutional Block Attention Module","Proceedings of the European Conference on Computer Vision (ECCV)","","","","https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html","","2018","2021-03-18 03:50:54","2021-04-06 15:05:47","2021-03-18 03:50:54","3-19","","","","","","CBAM","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""1""><p>Annotation</p> <p>With Channel Attention Module and Spatial Attention Module, the CBAM can be inserted into conv blocks to improve the CNN performance.</p></div>","C:\Users\controlnet\Zotero\storage\JEERU7SI\Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html; G:\My Drive\Bibliography\Woo et al_2018_CBAM.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV)","","","","","","","","","","","","","","",""
"TFE7ABP5","report","2018","Rhue, Lauren","Racial Influence on Automated Perceptions of Emotions","","","","","https://papers.ssrn.com/abstract=3281765","The practical applications of artificial intelligence are expanding into various elements of society, leading to a growing interest in the potential biases of such algorithms. Facial analysis, one application of artificial intelligence, is increasingly used in real-word situations. For example, some organizations tell candidates to answer predefined questions in a recorded video and use facial recognition to analyze the potential applicant faces. In addition, some companies are developing facial recognition software to scan the faces in crowds and assess threats, specifically mentioning doubt and anger as emotions that indicate threats.  This study provides evidence that facial recognition software interprets emotions differently based on the person’s race. Using a publically available data set of professional basketball players’ pictures, I compare the emotional analysis from two different facial recognition services, Face   and Microsoft's Face API. Both services interpret black players as having more negative emotions than white players; however, there are two different mechanisms. Face   consistently interprets black players as angrier than white players, even controlling for their degree of smiling. Microsoft registers contempt instead of anger, and it interprets black players as more contemptuous when their facial expressions are ambiguous. As the players’ smile widens, the disparity disappears. This finding has implications for individuals, organizations, and society, and it contributes to the growing literature of bias and/or disparate impact in AI.","2018-11-09","2021-03-16 14:12:55","2021-04-06 15:05:45","2021-03-16 14:12:55","","","","","","","","","","","","Social Science Research Network","Rochester, NY","en","","SSRN Scholarly Paper","","","papers.ssrn.com","","DOI: 10.2139/ssrn.3281765","<div data-schema-version=""1""><p>Annotation</p> <p>Racial influence on expression recognition</p></div>","G:\My Drive\Bibliography\Rhue_2018_Racial Influence on Automated Perceptions of Emotions.pdf; C:\Users\controlnet\Zotero\storage\LJCMBRU6\papers.html","","Unreviewed","artificial intelligence; bias; coarsened exact matching; econometrics; facial recognition; race","","","","","","","","","","","","","","","","","","","ID 3281765","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5PPVC36J","conferencePaper","2017","Bao, Jianmin; Chen, Dong; Wen, Fang; Li, Houqiang; Hua, Gang","CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html","","2017","2021-03-22 07:12:39","2021-04-06 15:05:36","2021-03-22 07:12:39","2745-2754","","","","","","CVAE-GAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Intoducing CVAE-GAN (Conditional VAE-GAN) by using label to improve the performance of VAE-GAN.</p></div>","G:\My Drive\Bibliography\Bao et al_2017_CVAE-GAN.pdf; C:\Users\controlnet\Zotero\storage\HJSQ369H\Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"ZS3LF4TU","journalArticle","2017","Arjovsky, Martin; Chintala, Soumith; Bottou, Léon","Wasserstein GAN","arXiv:1701.07875 [cs, stat]","","","","http://arxiv.org/abs/1701.07875","We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.","2017-12-06","2021-03-16 14:44:26","2021-04-06 15:05:33","2021-03-16 14:44:26","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1701.07875","<div data-schema-version=""1""><p>Annotation</p> <p>WGAN</p></div>","G:\My Drive\Bibliography\Arjovsky et al_2017_Wasserstein GAN.pdf; C:\Users\controlnet\Zotero\storage\PWZ8UTT5\1701.html","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNGWI5TW","journalArticle","2016","Radford, Alec; Metz, Luke; Chintala, Soumith","Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","arXiv:1511.06434 [cs]","","","","http://arxiv.org/abs/1511.06434","In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","2016-01-07","2021-03-16 14:49:34","2021-04-06 15:05:28","2021-03-16 14:49:34","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1511.06434","<div data-schema-version=""2""><p>Annotation</p> <p>DCGAN</p></div>; Comment: Under review as a conference paper at ICLR 2016","C:\Users\controlnet\Zotero\storage\DQWLAZHB\1511.html; G:\My Drive\Bibliography\Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf","","Unreviewed","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRK93TD9","conferencePaper","2016","Larsen, Anders Boesen Lindbo; Sønderby, Søren Kaae; Larochelle, Hugo; Winther, Ole","Autoencoding beyond pixels using a learned similarity metric","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v48/larsen16.html","We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GA...","2016-06-11","2021-03-22 07:10:53","2021-04-06 15:05:07","2021-03-22 07:10:53","1558-1566","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","<div data-schema-version=""1""><p>Annotation</p> <p>VAE-GAN is introduced to combine VAE and GAN to improve the GAN training.</p></div>","G:\My Drive\Bibliography\Larsen et al_2016_Autoencoding beyond pixels using a learned similarity metric.pdf; C:\Users\controlnet\Zotero\storage\4IHWY4CY\larsen16.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"HLLHIHTD","conferencePaper","2016","Johnson, Justin; Alahi, Alexandre; Fei-Fei, Li","Perceptual Losses for Real-Time Style Transfer and Super-Resolution","Computer Vision – ECCV 2016","978-3-319-46475-6","","10.1007/978-3-319-46475-6_43","","We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.","2016","2021-03-18 05:21:38","2021-04-06 15:05:03","","694-711","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Perceptual loss is the difference (MSE) between features of hidden layers in loss network from y_true and y_pred.</p></div>","G:\My Drive\Bibliography\Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf","","Unreviewed","Deep learning; Super-resolution; Style transfer","Leibe, Bastian; Matas, Jiri; Sebe, Nicu; Welling, Max","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTVLVRAI","journalArticle","2015","Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff","Distilling the Knowledge in a Neural Network","arXiv:1503.02531 [cs, stat]","","","","http://arxiv.org/abs/1503.02531","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","2015-03-09","2021-03-17 12:52:08","2021-04-06 15:04:46","2021-03-17 12:52:08","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1503.02531","<div data-schema-version=""1""><p>Annotation</p> <p>Knowledge Distillation</p></div>; Comment: NIPS 2014 Deep Learning Workshop","C:\Users\controlnet\Zotero\storage\8KILIT88\1503.html; G:\My Drive\Bibliography\Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQKA9BPK","journalArticle","2014","Shlens, Jonathon","A Tutorial on Principal Component Analysis","arXiv:1404.1100 [cs, stat]","","","","http://arxiv.org/abs/1404.1100","Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.","2014-04-03","2021-03-16 14:08:59","2021-04-06 15:04:42","2021-03-16 14:08:59","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1404.1100","<div data-schema-version=""1""><p>Annotation</p> <p>This paper introduces how the PCA works and how to apply the PCA into datasets. The PCA was invented in 1901, and is a transformation to a new space with smaller dimension by scalar projection. Take a low dimension (2D) dataset as an example, the PCA means transforming the dataset to a line (1D) which has the projection of the dataset with the highest variance. If the transformed dimension is larger than 1, the new axis is located with the highest variance of dataset projection in the orthogonal plane of other axises.</p> <p><br> </p></div>","C:\Users\controlnet\Zotero\storage\CR8D93IJ\1404.html; G:\My Drive\Bibliography\Shlens_2014_A Tutorial on Principal Component Analysis.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y99BNDVT","journalArticle","2012","Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey","ImageNet Classification with Deep Convolutional Neural Networks","Advances in neural information processing systems","","","10.1145/3065386","","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.","2012-01-01","2021-03-16 14:39:49","2021-04-06 15:04:35","","1097–1105","","","25","","Neural Information Processing Systems","","","","","","","","","","","","","ResearchGate","","","<div data-schema-version=""2""><p>Annotation</p> <p>AlexNet / ReLU</p></div>","G:\My Drive\Bibliography\Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf; ","https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLKIMCG8","journalArticle","2009","Bengio, Y.","Learning Deep Architectures for AI","Foundations and Trends® in Machine Learning","","1935-8237, 1935-8245","10.1561/2200000006","http://www.nowpublishers.com/article/Details/MAL-006","Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.","2009","2021-03-16 13:56:40","2021-04-06 15:02:00","2021-03-16 13:56:40","1-127","","1","2","","FNT in Machine Learning","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""2""><p>Annotation </p> <p>Survey</p> <p>This paper introduced many architectures for deep learning. In chapter 2, the researchers explain that a deeper structure can extract the feature more accuracy. Besides, in chapter 5, the paper introduced the convolutional neural network and auto encoder, and they also introduced an optimization principle to initialize parameters for each layer by unsupervised learning, which the auto encoder is included in.</p></div>","G:\My Drive\Bibliography\Bengio_2009_Learning Deep Architectures for AI.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZHFBZTZ","conferencePaper","2011","Rudovic, O.; Pantic, M.","Shape-constrained Gaussian process regression for facial-point-based head-pose normalization","2011 International Conference on Computer Vision","","","10.1109/ICCV.2011.6126407","","Given the facial points extracted from an image of a face in an arbitrary pose, the goal of facial-point-based head-pose normalization is to obtain the corresponding facial points in a predefined pose (e.g., frontal). This involves inference of complex and high-dimensional mappings due to the large number of the facial points employed, and due to differences in head-pose and facial expression. Most regression-based approaches for learning such mappings focus on modeling correlations only between the inputs (i.e., the facial points in a non-frontal pose) and the outputs (i.e., the facial points in the frontal pose), but not within the inputs and the outputs of the model. This makes these models prone to errors due to noise and outliers in test data, often resulting in anatomically impossible facial configurations formed by their predictions. To address this, we propose Shape-constrained Gaussian Process (SC-GP) regression for facial-point-based head-pose normalization. Specifically, a deformable face-shape model is used to learn a face-shape prior, which is placed on both the input and the output of GP regression in order to constrain the model predictions to anatomically feasible facial configurations. Our extensive experiments on both synthetic and real image data show that the proposed approach generalizes well across poses and handles successfully noise and outliers in test data. In addition, the proposed model outperforms previously proposed approaches to facial-point-based head-pose normalization.","2011-11","2021-03-16 14:14:23","2021-04-06 15:01:43","","1495-1502","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2380-7504","<div data-schema-version=""1""><p>Annotation</p> <p>Gaussian process regression and apply PCA to manipulate</p></div>","C:\Users\controlnet\Zotero\storage\T82G2MK8\6126407.html; G:\My Drive\Bibliography\Rudovic_Pantic_2011_Shape-constrained Gaussian process regression for facial-point-based head-pose.pdf","","Unreviewed","Data models; Training; face recognition; arbitrary pose; Computational modeling; deformable face-shape model; Deformable models; facial point extraction; facial-point-based head-pose normalization; feature extraction; Gaussian processes; high-dimensional mappings; pose estimation; Principal component analysis; regression analysis; Shape; shape-constrained Gaussian process regression; Three dimensional displays","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 International Conference on Computer Vision","","","","","","","","","","","","","","",""
"TJQ2XG7P","conferencePaper","2019","Rezatofighi, Hamid; Tsoi, Nathan; Gwak, JunYoung; Sadeghian, Amir; Reid, Ian; Savarese, Silvio","Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.html","Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.","2019","2021-07-26 08:50:31","2021-07-26 08:50:45","2021-07-26 08:50:31","658-666","","","","","","Generalized Intersection Over Union","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Rezatofighi et al_2019_Generalized Intersection Over Union.pdf; C:\Users\controlnet\Zotero\storage\95BMGDRC\Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"BSDAK6I6","conferencePaper","2019","Bello, Irwan; Zoph, Barret; Vaswani, Ashish; Shlens, Jonathon; Le, Quoc V.","Attention Augmented Convolutional Networks","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html","","2019","2021-04-23 10:22:17","2021-07-25 03:14:20","","3286-3295","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""3""><p>Annotations</p> <p>Attention-Augmented Convolution (AA).</p> <p>The convolution layers has limited local receptive fields and translation equivalence via weight sharing. Both are the bias for modelling images.</p> <p>Self-attention can capture long range interactions but not applied to computer vision tasks yet. Combine the self-attention and convolution can generate the best result.</p> <p>First, flatten the input feature map through spacial dimensions with a positional encoding added, then sent it to multi-head self-attention layer. The positional encoding used is a relative position encodings. It is a trainable encoding based on relative position. Two encodings, for Width and Height, will be added to the K vectors. The results from both self-attention and convolutions will be concatenated.</p> <p>CIFAR-100 and ImageNet for classification. COCO for object detection.</p> <p>The results for classification and object detection are both better than previous methods. The positional encoding also contributes a lot.</p> <p>+ve: Better performance compared to other purely convolutional neural networks in classification and object detection.</p> <p>-ve: </p> <p>This paper uses the self-attention layers to enhance the traditional convolutional layers. They used a simple concat to fusion two features but provides a innovative relative positional encoding, which is very important in the future.</p> </div>","G:\My Drive\Bibliography\Bello et al_2019_Attention Augmented Convolutional Networks.pdf; C:\Users\controlnet\Zotero\storage\4VKKC22S\Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"TS26MZSF","journalArticle","2018","Shaw, Peter; Uszkoreit, Jakob; Vaswani, Ashish","Self-Attention with Relative Position Representations","arXiv:1803.02155 [cs]","","","","http://arxiv.org/abs/1803.02155","Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.","2018-04-12","2021-07-24 23:48:13","2021-07-24 23:48:13","2021-07-24 23:48:13","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1803.02155","Comment: NAACL 2018","C:\Users\controlnet\Zotero\storage\9NECYPWK\1803.html; G:\My Drive\Bibliography\Shaw et al_2018_Self-Attention with Relative Position Representations.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SGY62L5","conferencePaper","2021","Touvron, Hugo; Cord, Matthieu; Douze, Matthijs; Massa, Francisco; Sablayrolles, Alexandre; Jegou, Herve","Training data-efficient image transformers & distillation through attention","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v139/touvron21a.html","Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.","2021-07-01","2021-07-21 16:18:49","2021-07-21 16:19:09","2021-07-21 16:18:49","10347-10357","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\controlnet\Zotero\storage\6FCVZIZP\Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf; G:\My Drive\Bibliography\Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"NM2ESQXZ","conferencePaper","2020","Chugh, Komal; Gupta, Parul; Dhall, Abhinav; Subramanian, Ramanathan","Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization","Proceedings of the 28th ACM International Conference on Multimedia","978-1-4503-7988-5","","10.1145/3394171.3413700","http://doi.org/10.1145/3394171.3413700","We propose detection of deepfake videos based on the dissimilarity between the audio and visual modalities, termed as the Modality Dissonance Score (MDS). We hypothesize that manipulation of either modality will lead to dis-harmony between the two modalities, e.g., loss of lip-sync, unnatural facial and lip movements, etc. MDS is computed as the mean aggregate of dissimilarity scores between audio and visual segments in a video. Discriminative features are learnt for the audio and visual channels in a chunk-wise manner, employing the cross-entropy loss for individual modalities, and a contrastive loss that models inter-modality similarity. Extensive experiments on the DFDC and DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art by up to 7%. We also demonstrate temporal forgery localization, and show how our technique identifies the manipulated video segments.","2020-10-12","2021-07-19 03:54:14","2021-07-19 03:54:14","2021-07-18","439–447","","","","","","","MM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","G:\My Drive\Bibliography\Chugh et al_2020_Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and.pdf","","","contrastive loss; deepfake detection and localization; modality dissonance; neural networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKQ7INVK","journalArticle","2019","Rössler, Andreas; Cozzolino, Davide; Verdoliva, Luisa; Riess, Christian; Thies, Justus; Nießner, Matthias","FaceForensics++: Learning to Detect Manipulated Facial Images","arXiv:1901.08971 [cs]","","","","http://arxiv.org/abs/1901.08971","The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domainspecific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.","2019-08-26","2021-07-19 03:52:51","2021-07-19 03:52:51","2021-07-19 03:52:51","","","","","","","FaceForensics++","","","","","","","","","","","","arXiv.org","","arXiv: 1901.08971","Comment: Video: https://youtu.be/x2g48Q2I2ZQ","C:\Users\controlnet\Zotero\storage\V5YQHJ6R\1901.html; G:\My Drive\Bibliography\Rössler et al_2019_FaceForensics++.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMEJWXIP","journalArticle","2020","Dolhansky, Brian; Bitton, Joanna; Pflaum, Ben; Lu, Jikuo; Howes, Russ; Wang, Menglin; Ferrer, Cristian Canton","The DeepFake Detection Challenge (DFDC) Dataset","arXiv:2006.07397 [cs]","","","","http://arxiv.org/abs/2006.07397","Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real ""in-the-wild"" Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https://ai.facebook.com/datasets/dfdc.","2020-10-27","2021-07-19 03:52:43","2021-07-19 03:52:43","2021-07-19 03:52:43","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2006.07397","","C:\Users\controlnet\Zotero\storage\2BB7ZX3G\2006.html; G:\My Drive\Bibliography\Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXGHKZSB","journalArticle","2021","Kwon, Patrick; You, Jaeseong; Nam, Gyuhyeon; Park, Sungwoo; Chae, Gyeongsu","KoDF: A Large-scale Korean DeepFake Detection Dataset","arXiv:2103.10094 [cs]","","","","http://arxiv.org/abs/2103.10094","A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratizing the face synthesis technology to a great extent. Videos generated as such have come to be collectively called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we provide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detection datasets, and underline the importance of using multiple datasets for real-world generalization. KoDF is publicly available at https://moneybrain-research.github.io/kodf in its entirety (i.e. real clips, synthesized clips, clips with additive noise, and their corresponding metadata).","2021-03-18","2021-07-19 03:52:28","2021-07-19 03:52:28","2021-07-19 03:52:28","","","","","","","KoDF","","","","","","","","","","","","arXiv.org","","arXiv: 2103.10094","","C:\Users\controlnet\Zotero\storage\RKAFDS2F\2103.html; G:\My Drive\Bibliography\Kwon et al_2021_KoDF.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSJNQ6JJ","journalArticle","2021","Khan, Shehroz S.; Khoshbakhtian, Faraz; Ashraf, Ahmed Bilal","Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest X-rays","arXiv:2010.02814 [cs, eess]","","","","http://arxiv.org/abs/2010.02814","The current COVID-19 pandemic is now getting contained, albeit at the cost of morethan2.3million human lives. A critical phase in any pandemic is the early detection of cases to develop preventive treatments and strategies. In the case of COVID-19,several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such asCOVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of data from infected persons could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate the problem of identifying early cases in a pandemic as an anomaly detection problem, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present several unsupervised deep learning approaches, including convolutional and adversarially trained autoencoder. We tested two settings on a publicly available dataset (COVIDx)by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. Afterperforming3-fold cross validation, we obtain a ROC-AUC of0.765. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays","2021-04-13","2021-07-12 05:33:18","2021-07-12 05:33:18","2021-07-12 05:33:18","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2010.02814","Comment: 9 pages, 3 tables, 3 figures","C:\Users\controlnet\Zotero\storage\56RRUJEC\2010.html; G:\My Drive\Bibliography\Khan et al_2021_Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2PG9LEGT","journalArticle","2020","Mehta, Vineet; Dhall, Abhinav; Pal, Sujata; Khan, Shehroz S.","Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging","arXiv:2004.08352 [cs]","","","","http://arxiv.org/abs/2004.08352","Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline.","2020-10-24","2021-07-12 05:33:01","2021-07-12 05:33:01","2021-07-12 05:33:01","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2004.08352","Comment: 8 pages,7 figures","C:\Users\controlnet\Zotero\storage\IT6K9NIG\2004.html; G:\My Drive\Bibliography\Mehta et al_2020_Motion and Region Aware Adversarial Learning for Fall Detection with Thermal.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N28PBR8N","journalArticle","2016","Ahmed, Mohiuddin; Naser Mahmood, Abdun; Hu, Jiankun","A survey of network anomaly detection techniques","Journal of Network and Computer Applications","","1084-8045","10.1016/j.jnca.2015.11.016","https://www.sciencedirect.com/science/article/pii/S1084804515002891","Information and Communication Technology (ICT) has a great impact on social wellbeing, economic growth and national security in todays world. Generally, ICT includes computers, mobile communication devices and networks. ICT is also embraced by a group of people with malicious intent, also known as network intruders, cyber criminals, etc. Confronting these detrimental cyber activities is one of the international priorities and important research area. Anomaly detection is an important data analysis task which is useful for identifying the network intrusions. This paper presents an in-depth analysis of four major categories of anomaly detection techniques which include classification, statistical, information theory and clustering. The paper also discusses research challenges with the datasets used for network intrusion detection.","2016-01-01","2021-07-12 05:30:56","2021-07-12 05:30:56","2021-07-12 05:30:56","19-31","","","60","","Journal of Network and Computer Applications","","","","","","","","en","","","","","ScienceDirect","","","","G:\My Drive\Bibliography\Ahmed et al_2016_A survey of network anomaly detection techniques.pdf; C:\Users\controlnet\Zotero\storage\DMU8HV86\S1084804515002891.html","","","Anomaly detection; Classification; Clustering; Computer security; Information theory; Intrusion detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMQBMR7P","journalArticle","2021","Lindemann, Benjamin; Maschler, Benjamin; Sahlab, Nada; Weyrich, Michael","A Survey on Anomaly Detection for Technical Systems using LSTM Networks","arXiv:2105.13810 [cs, stat]","","","","http://arxiv.org/abs/2105.13810","Anomalies represent deviations from the intended system operation and can lead to decreased efficiency as well as partial or complete system failure. As the causes of anomalies are often unknown due to complex system dynamics, efficient anomaly detection is necessary. Conventional detection approaches rely on statistical and time-invariant methods that fail to address the complex and dynamic nature of anomalies. With advances in artificial intelligence and increasing importance for anomaly detection and prevention in various domains, artificial neural network approaches enable the detection of more complex anomaly types while considering temporal and contextual characteristics. In this article, a survey on state-of-the-art anomaly detection using deep neural and especially long short-term memory networks is conducted. The investigated approaches are evaluated based on the application scenario, data and anomaly types as well as further metrics. To highlight the potential of upcoming anomaly detection techniques, graph-based and transfer learning approaches are also included in the survey, enabling the analysis of heterogeneous data as well as compensating for its shortage and improving the handling of dynamic processes.","2021-05-28","2021-07-12 05:29:42","2021-07-12 05:29:42","2021-07-12 05:29:42","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2105.13810","Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry","C:\Users\controlnet\Zotero\storage\623SNNTF\2105.html; G:\My Drive\Bibliography\Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVBBJ9NX","conferencePaper","2017","Huang, Gao; Liu, Zhuang; van der Maaten, Laurens; Weinberger, Kilian Q.","Densely Connected Convolutional Networks","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html","Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR- 10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.","2017","2021-06-30 13:43:42","2021-06-30 13:43:55","2021-06-30 13:43:42","4700-4708","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Huang et al_2017_Densely Connected Convolutional Networks.pdf; C:\Users\controlnet\Zotero\storage\859NCWVS\Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"QQSVS73Y","journalArticle","2019","Zhou, Hang; Liu, Ziwei; Xu, Xudong; Luo, Ping; Wang, Xiaogang","Vision-Infused Deep Audio Inpainting","arXiv:1910.10997 [cs]","","","","http://arxiv.org/abs/1910.10997","Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, \ie synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://hangz-nju-cuhk.github.io/projects/AudioInpainting","2019-10-24","2021-06-08 03:59:55","2021-06-08 03:59:55","2021-06-08 03:59:55","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1910.10997","Comment: To appear in ICCV 2019. Code, models, dataset and video results are available at the project page: https://hangz-nju-cuhk.github.io/projects/AudioInpainting","C:\Users\controlnet\Zotero\storage\PLC8QGEF\1910.html; G:\My Drive\Bibliography\Zhou et al_2019_Vision-Infused Deep Audio Inpainting.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S75V4C5R","journalArticle","2020","Shor, Joel; Jansen, Aren; Maor, Ronnie; Lang, Oran; Tuval, Omry; Quitry, Felix de Chaumont; Tagliasacchi, Marco; Shavitt, Ira; Emanuel, Dotan; Haviv, Yinnon","Towards Learning a Universal Non-Semantic Representation of Speech","Interspeech 2020","","","10.21437/Interspeech.2020-1242","http://arxiv.org/abs/2002.12764","The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark, models, and evaluation code are publicly released.","2020-10-25","2021-06-05 18:23:32","2021-06-05 18:23:34","2021-06-05 18:23:32","140-144","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2002.12764","","C:\Users\controlnet\Zotero\storage\5BDT64YF\2002.html; G:\My Drive\Bibliography\Shor et al_2020_Towards Learning a Universal Non-Semantic Representation of Speech.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ZI23PQB","journalArticle","2020","Liu, Xin; Fromm, Josh; Patel, Shwetak; McDuff, Daniel","Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc//paper/2020/hash/e1228be46de6a0234ac22ded31417bc7-Abstract.html","Telehealth and remote health monitoring have become increasingly important during the SARS-CoV-2 pandemic and it is widely expected that this will have a lasting impact on healthcare practices. These tools can help reduce the risk of exposing patients and medical staff to infection, make healthcare services more accessible, and allow providers to see more patients. However, objective measurement of vital signs is challenging without direct contact with a patient. We present a video-based and on-device optical cardiopulmonary vital sign measurement approach. It leverages a novel multi-task temporal shift convolutional attention network (MTTS-CAN) and enables real-time cardiovascular and respiratory measurements on mobile platforms. We evaluate our system on an Advanced RISC Machine (ARM) CPU and achieve state-of-the-art accuracy while running at over 150 frames per second which enables real-time applications. Systematic experimentation on large benchmark datasets reveals that our approach leads to substantial (20%-50%) reductions in error and generalizes well across datasets.","2020","2021-06-02 14:25:18","2021-06-02 14:25:32","2021-06-02 14:25:18","19400-19411","","","33","","","","","","","","","","en","","","","","proceedings.neurips.cc","","","","G:\My Drive\Bibliography\Liu et al_2020_Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals.pdf; C:\Users\controlnet\Zotero\storage\BPCRD48F\e1228be46de6a0234ac22ded31417bc7-Abstract.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZPAPNUF","conferencePaper","2020","Xiong, Ruibin; Yang, Yunchang; He, Di; Zheng, Kai; Zheng, Shuxin; Xing, Chen; Zhang, Huishuai; Lan, Yanyan; Wang, Liwei; Liu, Tieyan","On Layer Normalization in the Transformer Architecture","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v119/xiong20b.html","The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial...","2020-11-21","2021-06-02 07:30:34","2021-06-02 07:30:34","2021-06-02 07:30:34","10524-10533","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\controlnet\Zotero\storage\Z4VA752I\xiong20b.html; G:\My Drive\Bibliography\Xiong et al_2020_On Layer Normalization in the Transformer Architecture.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"JFZM7JVX","journalArticle","2021","Strudel, Robin; Garcia, Ricardo; Laptev, Ivan; Schmid, Cordelia","Segmenter: Transformer for Semantic Segmentation","arXiv:2105.05633 [cs]","","","","http://arxiv.org/abs/2105.05633","Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution based approaches, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.","2021-05-12","2021-05-28 05:34:09","2021-05-28 05:34:09","2021-05-28 05:34:09","","","","","","","Segmenter","","","","","","","","","","","","arXiv.org","","arXiv: 2105.05633","Comment: Code available at https://github.com/rstrudel/segmenter","C:\Users\controlnet\Zotero\storage\HG2QG8K4\2105.html; G:\My Drive\Bibliography\Strudel et al_2021_Segmenter.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMJ6X9EB","conferencePaper","2020","Chen, Ting; Kornblith, Simon; Norouzi, Mohammad; Hinton, Geoffrey","A Simple Framework for Contrastive Learning of Visual Representations","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v119/chen20j.html","This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring sp...","2020-11-21","2021-05-18 06:43:19","2021-05-18 06:43:23","2021-05-18 06:43:19","1597-1607","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","G:\My Drive\Bibliography\Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf; C:\Users\controlnet\Zotero\storage\D3MGZFGC\chen20j.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"UNG3XC6Q","journalArticle","2021","Esser, Patrick; Rombach, Robin; Ommer, Björn","Taming Transformers for High-Resolution Image Synthesis","arXiv:2012.09841 [cs]","","","","http://arxiv.org/abs/2012.09841","Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://compvis.github.io/taming-transformers/ .","2021-02-11","2021-05-17 16:56:52","2021-05-17 16:56:52","2021-05-17 16:56:52","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2012.09841","<div data-schema-version=""2""><p>Annotation</p> <p>VQGAN</p> </div>","C:\Users\controlnet\Zotero\storage\7NCDD3LK\2012.html; G:\My Drive\Bibliography\Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQU48C2B","journalArticle","2021","Li, Duo; Hu, Jie; Wang, Changhu; Li, Xiangtai; She, Qi; Zhu, Lei; Zhang, Tong; Chen, Qifeng","Involution: Inverting the Inherence of Convolution for Visual Recognition","arXiv:2103.06255 [cs]","","","","http://arxiv.org/abs/2103.06255","Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at https://github.com/d-li14/involution.","2021-04-11","2021-05-17 04:37:19","2021-05-17 04:43:40","2021-05-17 04:37:19","","","","","","","Involution","","","","","","","","","","","","arXiv.org","","arXiv: 2103.06255","Comment: Accepted to CVPR 2021. Code and models are available at https://github.com/d-li14/involution","C:\Users\controlnet\Zotero\storage\MVAKS3MS\2103.html; G:\My Drive\Bibliography\Li et al_2021_Involution.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4WEEL3V4","journalArticle","2021","Jahn, Manuel; Rombach, Robin; Ommer, Björn","High-Resolution Complex Scene Synthesis with Transformers","arXiv:2105.06458 [cs]","","","","http://arxiv.org/abs/2105.06458","The use of coarse-grained layouts for controllable synthesis of complex scene images via deep generative models has recently gained popularity. However, results of current approaches still fall short of their promise of high-resolution synthesis. We hypothesize that this is mostly due to the highly engineered nature of these approaches which often rely on auxiliary losses and intermediate steps such as mask generators. In this note, we present an orthogonal approach to this task, where the generative model is based on pure likelihood training without additional objectives. To do so, we first optimize a powerful compression model with adversarial training which learns to reconstruct its inputs via a discrete latent bottleneck and thereby effectively strips the latent representation of high-frequency details such as texture. Subsequently, we train an autoregressive transformer model to learn the distribution of the discrete image representations conditioned on a tokenized version of the layouts. Our experiments show that the resulting system is able to synthesize high-quality images consistent with the given layouts. In particular, we improve the state-of-the-art FID score on COCO-Stuff and on Visual Genome by up to 19% and 53% and demonstrate the synthesis of images up to 512 x 512 px on COCO and Open Images.","2021-05-13","2021-05-16 16:25:22","2021-05-17 04:42:35","2021-05-16 16:25:22","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2105.06458","<div data-schema-version=""2""><p>Annotation</p> <p>Method of layout-to-image generation.</p> </div>; Comment: AI for Content Creation Workshop, CVPR 2021","C:\Users\controlnet\Zotero\storage\C3ZTADTD\2105.html; G:\My Drive\Bibliography\Jahn et al_2021_High-Resolution Complex Scene Synthesis with Transformers.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMIN2EBE","journalArticle","2021","Lee-Thorp, James; Ainslie, Joshua; Eckstein, Ilya; Ontanon, Santiago","FNet: Mixing Tokens with Fourier Transforms","arXiv:2105.03824 [cs]","","","","http://arxiv.org/abs/2105.03824","We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that ""mix"" input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate ""efficient"" Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.","2021-05-08","2021-05-16 16:25:19","2021-05-17 04:42:32","2021-05-16 16:25:19","","","","","","","FNet","","","","","","","","","","","","arXiv.org","","arXiv: 2105.03824","","C:\Users\controlnet\Zotero\storage\9Q7GQAVI\2105.html; G:\My Drive\Bibliography\Lee-Thorp et al_2021_FNet.pdf","","","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGZ6QX5G","conferencePaper","2019","Oh, Tae-Hyun; Dekel, Tali; Kim, Changil; Mosseri, Inbar; Freeman, William T.; Rubinstein, Michael; Matusik, Wojciech","Speech2Face: Learning the Face Behind a Voice","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Oh_Speech2Face_Learning_the_Face_Behind_a_Voice_CVPR_2019_paper.html","How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/Youtube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers.","2019","2021-05-16 07:08:32","2021-05-17 04:42:30","2021-05-16 07:08:32","7539-7548","","","","","","Speech2Face","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Oh et al_2019_Speech2Face.pdf; C:\Users\controlnet\Zotero\storage\C3SLLYKV\Oh_Speech2Face_Learning_the_Face_Behind_a_Voice_CVPR_2019_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"8J3Y4SM4","conferencePaper","2018","Karras, Tero; Aila, Timo; Laine, Samuli; Lehtinen, Jaakko","Progressive Growing of GANs for Improved Quality, Stability, and Variation","International Conference on Learning Representations","","","","https://openreview.net/forum?id=Hk99zCeAb","We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality.","2018-02-15","2021-05-14 03:02:29","2021-05-17 04:42:28","2021-05-14 03:02:29","","","","","","","","","","","","","","en","","","","","openreview.net","","","<div data-schema-version=""2""><p>Annotation</p> <p>ProGAN, Progressive Growing GAN</p> </div>","G:\My Drive\Bibliography\Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf; C:\Users\controlnet\Zotero\storage\RXZQC545\forum.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Learning Representations","","","","","","","","","","","","","","",""
"4IY6UTQT","journalArticle","2021","Zhu, Hao; Luo, Man-Di; Wang, Rui; Zheng, Ai-Hua; He, Ran","Deep Audio-visual Learning: A Survey","International Journal of Automation and Computing","","1751-8520","10.1007/s11633-021-1293-0","https://doi.org/10.1007/s11633-021-1293-0","Audio-visual learning, aimed at exploiting the relationship between audio and visual modalities, has drawn considerable attention since deep learning started to be used successfully. Researchers tend to leverage these two modalities to improve the performance of previously considered single-modality tasks or address new challenging problems. In this paper, we provide a comprehensive survey of recent audio-visual learning development. We divide the current audio-visual learning tasks into four different subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. State-of-the-art methods, as well as the remaining challenges of each subfield, are further discussed. Finally, we summarize the commonly used datasets and challenges.","2021-06-01","2021-05-14 01:26:24","2021-05-17 04:42:27","2021-05-14 01:26:24","351-376","","3","18","","Int. J. Autom. Comput.","Deep Audio-visual Learning","","","","","","","en","","","","","Springer Link","","","","G:\My Drive\Bibliography\Zhu et al_2021_Deep Audio-visual Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SI85BI9","conferencePaper","2016","Shi, Wenzhe; Caballero, Jose; Huszar, Ferenc; Totz, Johannes; Aitken, Andrew P.; Bishop, Rob; Rueckert, Daniel; Wang, Zehan","Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html","Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.","2016","2021-05-13 08:23:56","2021-05-17 04:42:25","2021-05-13 08:23:56","1874-1883","","","","","","","","","","","","","","","","","","www.cv-foundation.org","","","<div data-schema-version=""2""><p>Annotation</p> <p>Sub-Pixel Convolution</p> </div>","G:\My Drive\Bibliography\Shi et al_2016_Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel.pdf; C:\Users\controlnet\Zotero\storage\6R3JMNB7\Shi_Real-Time_Single_Image_CVPR_2016_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"T88BEC56","journalArticle","2021","Zheng, Sixiao; Lu, Jiachen; Zhao, Hengshuang; Zhu, Xiatian; Luo, Zekun; Wang, Yabiao; Fu, Yanwei; Feng, Jianfeng; Xiang, Tao; Torr, Philip H. S.; Zhang, Li","Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers","arXiv:2012.15840 [cs]","","","","http://arxiv.org/abs/2012.15840","Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.","2021-03-30","2021-05-12 13:40:38","2021-05-17 04:42:23","2021-05-12 13:40:38","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2012.15840","<div data-schema-version=""2""><p>Annotation</p> <p>SETR, Segmentation Transformer</p> <p>Hard to learn the long-range dependency information because of the limited receptive fields of CNN in segmentation task.</p> <p>The encoder-decoder structure </p> </div>; Comment: CVPR 2021. Project page at https://fudan-zvg.github.io/SETR/","C:\Users\controlnet\Zotero\storage\FM3HRLK8\2012.html; G:\My Drive\Bibliography\Zheng et al_2021_Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQWYFZQ3","conferencePaper","2020","Karras, Tero; Laine, Samuli; Aittala, Miika; Hellsten, Janne; Lehtinen, Jaakko; Aila, Timo","Analyzing and Improving the Image Quality of StyleGAN","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html","The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.","2020","2021-05-10 09:08:54","2021-05-17 04:42:22","2021-05-10 09:08:54","8110-8119","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>StyleGAN2</p> </div>","G:\My Drive\Bibliography\Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf; C:\Users\controlnet\Zotero\storage\PF3FYJ5N\Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"QFABC76D","journalArticle","2020","Yang, Fuzhi; Yang, Huan; Fu, Jianlong; Lu, Hongtao; Guo, Baining","Learning Texture Transformer Network for Image Super-Resolution","arXiv:2006.04139 [cs]","","","","http://arxiv.org/abs/2006.04139","We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a soft-attention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1x to 4x magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.","2020-06-22","2021-05-04 16:24:24","2021-05-17 04:42:20","2021-05-04 16:24:24","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2006.04139","Comment: Accepted by CVPR 2020","G:\My Drive\Bibliography\Yang et al_2020_Learning Texture Transformer Network for Image Super-Resolution.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7RALPT2","journalArticle","2021","Chan, Kelvin C. K.; Wang, Xintao; Yu, Ke; Dong, Chao; Loy, Chen Change","BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond","arXiv:2012.02181 [cs]","","","","http://arxiv.org/abs/2012.02181","Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.","2021-04-07","2021-05-04 08:31:51","2021-05-17 04:42:18","2021-05-04 08:31:51","","","","","","","BasicVSR","","","","","","","","","","","","arXiv.org","","arXiv: 2012.02181","Comment: CVPR 2021 camera-ready","C:\Users\controlnet\Zotero\storage\6ZFN8LSS\2012.html; G:\My Drive\Bibliography\Chan et al_2021_BasicVSR.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S47FKPTC","conferencePaper","2018","Chen, Yu; Tai, Ying; Liu, Xiaoming; Shen, Chunhua; Yang, Jian","FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html","Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to superresolve very low-resolution (LR) face images without wellaligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.","2018","2021-05-04 08:15:10","2021-05-17 04:42:16","2021-05-04 08:15:10","2492-2501","","","","","","FSRNet","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Chen et al_2018_FSRNet.pdf; C:\Users\controlnet\Zotero\storage\U6JM6ABA\Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"DXKQLZP7","journalArticle","2020","Liu, Hongying; Ruan, Zhubo; Zhao, Peng; Dong, Chao; Shang, Fanhua; Liu, Yuanyuan; Yang, Linlin","Video Super Resolution Based on Deep Learning: A Comprehensive Survey","arXiv:2007.12928 [cs, eess]","","","","http://arxiv.org/abs/2007.12928","In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning.","2020-12-20","2021-05-04 07:47:31","2021-05-17 04:42:15","2021-05-04 07:47:31","","","","","","","Video Super Resolution Based on Deep Learning","","","","","","","","","","","","arXiv.org","","arXiv: 2007.12928","","C:\Users\controlnet\Zotero\storage\YFSWND62\2007.html; G:\My Drive\Bibliography\Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GCK76XV","journalArticle","2021","Jiang, Junjun; Wang, Chenyang; Liu, Xianming; Ma, Jiayi","Deep Learning-based Face Super-resolution: A Survey","arXiv:2101.03749 [cs]","","","","http://arxiv.org/abs/2101.03749","Face super-resolution, also known as face hallucination, which is aimed at enhancing the resolution of low-resolution (LR) one or a sequence of face images to generate the corresponding high-resolution (HR) face images, is a domain-specific image super-resolution problem. Recently, face super-resolution has received considerable attention, and witnessed dazzling advances with deep learning techniques. To date, few summaries of the studies on the deep learning-based face super-resolution are available. In this survey, we present a comprehensive review of deep learning techniques in face super-resolution in a systematic manner. First, we summarize the problem formulation of face super-resolution. Second, we compare the differences between generic image super-resolution and face super-resolution. Third, datasets and performance metrics commonly used in facial hallucination are presented. Fourth, we roughly categorize existing methods according to the utilization of face-specific information. In each category, we start with a general description of design principles, present an overview of representative approaches, and compare the similarities and differences among various methods. Finally, we envision prospects for further technical advancement in this field.","2021-01-11","2021-05-04 07:20:15","2021-05-17 04:42:13","2021-05-04 07:20:15","","","","","","","Deep Learning-based Face Super-resolution","","","","","","","","","","","","arXiv.org","","arXiv: 2101.03749","Comment: 40 pages, 13 figures","C:\Users\controlnet\Zotero\storage\NQA699NJ\2101.html; G:\My Drive\Bibliography\Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6M6AS5LC","conferencePaper","2017","Ledig, Christian; Theis, Lucas; Huszar, Ferenc; Caballero, Jose; Cunningham, Andrew; Acosta, Alejandro; Aitken, Andrew; Tejani, Alykhan; Totz, Johannes; Wang, Zehan; Shi, Wenzhe","Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html","Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.","2017","2021-05-03 15:09:33","2021-05-17 04:42:11","2021-05-03 15:09:33","4681-4690","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>SRGAN</p> </div>","G:\My Drive\Bibliography\Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf; C:\Users\controlnet\Zotero\storage\IN3MZN46\Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"46P454WN","conferencePaper","2018","Wang, Xintao; Yu, Ke; Wu, Shixiang; Gu, Jinjin; Liu, Yihao; Dong, Chao; Qiao, Yu; Change Loy, Chen","ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks","Proceedings of the European Conference on Computer Vision (ECCV) Workshops","","","","https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html","The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.","2018","2021-05-03 15:08:43","2021-05-17 04:42:09","2021-05-03 15:08:43","0-0","","","","","","ESRGAN","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>ESRGAN</p> </div>","C:\Users\controlnet\Zotero\storage\GQHG2PUB\Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html; G:\My Drive\Bibliography\Wang et al_2018_ESRGAN.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV) Workshops","","","","","","","","","","","","","","",""
"8JBA8RMR","conferencePaper","2017","Lim, Bee; Son, Sanghyun; Kim, Heewon; Nah, Seungjun; Mu Lee, Kyoung","Enhanced Deep Residual Networks for Single Image Super-Resolution","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops","","","","https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html","Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.","2017","2021-05-03 15:01:34","2021-05-17 04:42:08","2021-05-03 15:01:34","136-144","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Batch Normalization is bad in SR.</p> </div>","G:\My Drive\Bibliography\Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf; C:\Users\controlnet\Zotero\storage\RN6MA3UV\Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops","","","","","","","","","","","","","","",""
"Y7ITAU5B","conferencePaper","2014","Dong, Chao; Loy, Chen Change; He, Kaiming; Tang, Xiaoou","Learning a Deep Convolutional Network for Image Super-Resolution","Computer Vision – ECCV 2014","978-3-319-10593-2","","10.1007/978-3-319-10593-2_13","","We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.","2014","2021-05-03 10:05:19","2021-05-17 04:42:07","","184-199","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>SRCNN</p> </div>","G:\My Drive\Bibliography\Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf","","","Super-resolution; deep convolutional neural networks","Fleet, David; Pajdla, Tomas; Schiele, Bernt; Tuytelaars, Tinne","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P723GDXA","journalArticle","2020","Wang, Zhihao; Chen, Jian; Hoi, Steven C.H.","Deep Learning for Image Super-resolution: A Survey","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/TPAMI.2020.2982166","","Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.","2020","2021-05-03 09:12:35","2021-05-17 04:42:03","","1-1","","","","","","Deep Learning for Image Super-resolution","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","<div data-schema-version=""2""><p>Related blog</p> <p>https://zhuanlan.zhihu.com/p/143380729</p> </div>","C:\Users\controlnet\Zotero\storage\88H69X25\9044873.html; G:\My Drive\Bibliography\Wang et al_2020_Deep Learning for Image Super-resolution.pdf","","","Deep learning; Animals; Benchmark testing; Convolutional Neural Networks (CNN); Deep Learning; Degradation; Generative Adversarial Nets (GAN); Image Super-resolution; Measurement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7IT5IMV","journalArticle","2016","Ba, Jimmy Lei; Kiros, Jamie Ryan; Hinton, Geoffrey E.","Layer Normalization","arXiv:1607.06450 [cs, stat]","","","","http://arxiv.org/abs/1607.06450","Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.","2016-07-21","2021-05-01 08:18:07","2021-05-17 04:42:01","","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1607.06450","","C:\Users\controlnet\Zotero\storage\39KTZ95D\1607.html; G:\My Drive\Bibliography\Ba et al_2016_Layer Normalization.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3J83CP7J","journalArticle","2014","Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua","Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation","arXiv:1406.1078 [cs, stat]","","","","http://arxiv.org/abs/1406.1078","In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","2014-09-02","2021-05-01 07:09:24","2021-05-17 04:41:59","","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1406.1078","<div data-schema-version=""2""><p>Annotation</p> <p>GRU</p> </div>; Comment: EMNLP 2014","C:\Users\controlnet\Zotero\storage\R4UWJIIU\1406.html; G:\My Drive\Bibliography\Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FI3WQZCW","journalArticle","1997","Hochreiter, Sepp; Schmidhuber, Jürgen","Long Short-Term Memory","Neural Computation","","0899-7667","10.1162/neco.1997.9.8.1735","https://doi.org/10.1162/neco.1997.9.8.1735","Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.","1997-11-15","2021-05-01 07:07:59","2021-05-17 04:41:57","2021-05-01 07:07:59","1735-1780","","8","9","","Neural Computation","","","","","","","","","","","","","Silverchair","","","<div data-schema-version=""2""><p>Annotation</p> <p>LSTM</p> </div>","G:\My Drive\Bibliography\Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf; C:\Users\controlnet\Zotero\storage\3LCNJLQ5\Long-Short-Term-Memory.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUPV2IWH","conferencePaper","2019","Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","","","10.18653/v1/N19-1423","https://www.aclweb.org/anthology/N19-1423","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","2019-06","2021-04-27 14:50:52","2021-05-17 04:41:55","2021-04-27 14:50:52","4171–4186","","","","","","BERT","","","","","Association for Computational Linguistics","Minneapolis, Minnesota","","","","","","ACLWeb","","","","G:\My Drive\Bibliography\Devlin et al_2019_BERT.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2019","","","","","","","","","","","","","","",""
"973HNL3T","conferencePaper","2018","Parmar, Niki; Vaswani, Ashish; Uszkoreit, Jakob; Kaiser, Lukasz; Shazeer, Noam; Ku, Alexander; Tran, Dustin","Image Transformer","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v80/parmar18a.html","Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual seq...","2018-07-03","2021-04-23 10:52:07","2021-05-17 04:41:54","2021-04-23 10:52:07","4055-4064","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","G:\My Drive\Bibliography\Parmar et al_2018_Image Transformer2.pdf; C:\Users\controlnet\Zotero\storage\5JCVU8L8\parmar18a.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"36MVED78","journalArticle","2021","Khan, Salman; Naseer, Muzammal; Hayat, Munawar; Zamir, Syed Waqas; Khan, Fahad Shahbaz; Shah, Mubarak","Transformers in Vision: A Survey","arXiv:2101.01169 [cs]","","","","http://arxiv.org/abs/2101.01169","Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.","2021-02-22","2021-04-23 10:41:54","2021-05-17 04:41:52","2021-04-23 10:41:54","","","","","","","Transformers in Vision","","","","","","","","","","","","arXiv.org","","arXiv: 2101.01169","Comment: 28 pages","C:\Users\controlnet\Zotero\storage\GWYVPP4U\2101.html; G:\My Drive\Bibliography\Khan et al_2021_Transformers in Vision.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8BUHT44I","bookSection","2021","Sharma, Garima; Dhall, Abhinav","A Survey on Automatic Multimodal Emotion Recognition in the Wild","Advances in Data Science: Methodologies and Applications","978-3-030-51870-7","","","https://doi.org/10.1007/978-3-030-51870-7_3","Affective computing has been an active area of research for the past two decades. One of the major component of affective computing is automatic emotion recognition. This chapter gives a detailed overview of different emotion recognition techniques and the predominantly used signal modalities. The discussion starts with the different emotion representations and their limitations. Given that affective computing is a data-driven research area, a thorough comparison of standard emotion labelled databases is presented. Based on the source of the data, feature extraction and analysis techniques are presented for emotion recognition. Further, applications of automatic emotion recognition are discussed along with current and important issues such as privacy and fairness.","2021","2021-04-23 07:21:27","2021-05-17 04:41:49","2021-04-23 07:21:27","35-64","","","","","","","Intelligent Systems Reference Library","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","DOI: 10.1007/978-3-030-51870-7_3","","G:\My Drive\Bibliography\Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf","","","","Phillips-Wren, Gloria; Esposito, Anna; Jain, Lakhmi C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFC7X2G5","conferencePaper","2019","Chung, Soo-Whan; Chung, Joon Son; Kang, Hong-Goo","Perfect Match: Improved Cross-modal Embeddings for Audio-visual Synchronisation","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","10.1109/ICASSP.2019.8682524","","This paper proposes a new strategy for learning powerful cross-modal embeddings for audio-to-video synchronisation. Here, we set up the problem as one of cross-modal retrieval, where the objective is to find the most relevant audio segment given a short video clip. The method builds on the recent advances in learning representations from cross-modal self-supervision. The main contributions of this paper are as follows: (1) we propose a new learning strategy where the embeddings are learnt via a multi-way matching problem, as opposed to a binary classification (matching or non-matching) problem as proposed by recent papers; (2) we demonstrate that performance of this method far exceeds the existing baselines on the synchronisation task; (3) we use the learnt embeddings for visual speech recognition in self-supervision, and show that the performance matches the representations learnt end-to-end in a fully-supervised manner.","2019-05","2021-04-23 07:11:46","2021-05-17 04:41:47","","3965-3969","","","","","","Perfect Match","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","G:\My Drive\Bibliography\Chung et al_2019_Perfect Match.pdf; C:\Users\controlnet\Zotero\storage\CCLS68BR\8682524.html","","","Training; audio-visual synchronisation; cross-modal embedding; Cross-modal supervision; Lips; self-supervised learning; Speech recognition; Streaming media; Synchronization; Task analysis; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","","","","","","","","","","","","",""
"FHCWTIXV","conferencePaper","2017","Chung, Joon Son; Zisserman, Andrew","Out of Time: Automated Lip Sync in the Wild","Computer Vision – ACCV 2016 Workshops","978-3-319-54427-4","","10.1007/978-3-319-54427-4_19","","The goal of this work is to determine the audio-video synchronisation between mouth motion and speech in a video.We propose a two-stream ConvNet architecture that enables the mapping between the sound and the mouth images to be trained end-to-end from unlabelled data. The trained network is used to determine the lip-sync error in a video.We apply the network to two further tasks: active speaker detection and lip reading. On both tasks we set a new state-of-the-art on standard benchmark datasets.","2017","2021-04-23 07:10:44","2021-05-17 04:41:45","","251-263","","","","","","Out of Time","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","G:\My Drive\Bibliography\Chung_Zisserman_2017_Out of Time.pdf","","","Audio Stream; Convolutional Neural Network; Equal Error Rate; Mouth Shape; Phoneme Recognition","Chen, Chu-Song; Lu, Jiwen; Ma, Kai-Kuang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQX3QM86","conferencePaper","2020","Carion, Nicolas; Massa, Francisco; Synnaeve, Gabriel; Usunier, Nicolas; Kirillov, Alexander; Zagoruyko, Sergey","End-to-End Object Detection with Transformers","Computer Vision – ECCV 2020","978-3-030-58452-8","","10.1007/978-3-030-58452-8_13","","We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.","2020","2021-04-23 04:39:37","2021-05-17 04:41:43","","213-229","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","<div data-schema-version=""3""><p>Annotation</p> <p>Detection Transformer (DETR)</p> <p>The previous modern object detection algorithms relies on proposals, anchors, and windows to predict object boxes, which is easily influenced by preprocessing.</p> <p>As the E2E solutions are developped well in other area, it is necessary to use E2E in object detection.</p> <p>First step is a CNN backbone to extract features from images. Then is a transformer encoder takes the flattened feature maps as input sequence and generate the sequence output. Then the output will be feed into the middle part decoder for prediction. The object queries are trainable parameter for predicting the result boxes. Notice in each self attention layer, the positional encoding is applied. After the decoder, there is FFN to predict the bounding box and class.</p> <p>COCO</p> <p>The results is better than Faster-RCNN in object detection, also better than other dataset for segmentation.</p> <p>+ve: Better performance than Faster R-CNN. Easy to implement and flexible. Also can be used for segmentation. </p> <p>-ve: Not good for small objects. Training too difficult.</p> <p>This method used transformer to implement to E2E object detection and segmentation tasks, and perform great results. This is an elegant way to achieve E2E, which is very innovative.</p> </div>","G:\My Drive\Bibliography\Carion et al_2020_End-to-End Object Detection with Transformers.pdf","","","","Vedaldi, Andrea; Bischof, Horst; Brox, Thomas; Frahm, Jan-Michael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMXHCLUD","conferencePaper","2017","Arandjelovic, Relja; Zisserman, Andrew","Look, Listen and Learn","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html","We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel ""Audio-Visual Correspondence"" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.","2017","2021-04-22 16:07:57","2021-05-17 04:41:42","2021-04-22 16:07:57","609-617","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Arandjelovic_Zisserman_2017_Look, Listen and Learn.pdf; C:\Users\controlnet\Zotero\storage\AU3848SY\Arandjelovic_Look_Listen_and_ICCV_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"KCEFSMA2","conferencePaper","2018","Wang, Ting-Chun; Liu, Ming-Yu; Zhu, Jun-Yan; Tao, Andrew; Kautz, Jan; Catanzaro, Bryan","High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html","We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.","2018","2021-04-22 13:32:51","2021-05-17 04:41:40","2021-04-22 13:32:51","8798-8807","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\3FUDQRMP\Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html; G:\My Drive\Bibliography\Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"9VG323DT","conferencePaper","2016","Amodei, Dario; Ananthanarayanan, Sundaram; Anubhai, Rishita; Bai, Jingliang; Battenberg, Eric; Case, Carl; Casper, Jared; Catanzaro, Bryan; Cheng, Qiang; Chen, Guoliang; Chen, Jie; Chen, Jingdong; Chen, Zhijie; Chrzanowski, Mike; Coates, Adam; Diamos, Greg; Ding, Ke; Du, Niandong; Elsen, Erich; Engel, Jesse; Fang, Weiwei; Fan, Linxi; Fougner, Christopher; Gao, Liang; Gong, Caixia; Hannun, Awni; Han, Tony; Johannes, Lappi; Jiang, Bing; Ju, Cai; Jun, Billy; LeGresley, Patrick; Lin, Libby; Liu, Junjie; Liu, Yang; Li, Weigao; Li, Xiangang; Ma, Dongpeng; Narang, Sharan; Ng, Andrew; Ozair, Sherjil; Peng, Yiping; Prenger, Ryan; Qian, Sheng; Quan, Zongfeng; Raiman, Jonathan; Rao, Vinay; Satheesh, Sanjeev; Seetapun, David; Sengupta, Shubho; Srinet, Kavya; Sriram, Anuroop; Tang, Haiyuan; Tang, Liliang; Wang, Chong; Wang, Jidong; Wang, Kaifu; Wang, Yi; Wang, Zhijian; Wang, Zhiqian; Wu, Shuang; Wei, Likai; Xiao, Bo; Xie, Wen; Xie, Yan; Yogatama, Dani; Yuan, Bin; Zhan, Jun; Zhu, Zhenyao","Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v48/amodei16.html","We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-eng...","2016-06-11","2021-04-22 12:39:27","2021-05-17 04:41:39","2021-04-22 12:39:27","173-182","","","","","","Deep Speech 2","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","","G:\My Drive\Bibliography\Amodei et al_2016_Deep Speech 2.pdf; C:\Users\controlnet\Zotero\storage\XUINVS2H\amodei16.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"WX2RB87G","conferencePaper","2019","Park, Taesung; Liu, Ming-Yu; Wang, Ting-Chun; Zhu, Jun-Yan","Semantic Image Synthesis With Spatially-Adaptive Normalization","We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html","","2019","2021-04-22 12:35:13","2021-05-17 04:41:36","2021-04-22 12:35:13","2337-2346","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Park et al_2019_Semantic Image Synthesis With Spatially-Adaptive Normalization.pdf; C:\Users\controlnet\Zotero\storage\G8CYMQAZ\Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"NVF5W5CD","conferencePaper","2016","Vondrick, Carl; Pirsiavash, Hamed; Torralba, Antonio","Generating Videos with Scene Dynamics","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf","We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.","2016","2021-04-22 12:06:12","2021-05-17 04:41:31","","613-621","","","29","","","","","","","","Curran Associates, Inc.","","","","","","","","","","","C:\Users\controlnet\Zotero\storage\LUFQTG57\04025959b191f8f9de3f924f0940515f-Abstract.html; G:\My Drive\Bibliography\Vondrick et al_2016_Generating Videos with Scene Dynamics.pdf","","","","Lee, D.; Sugiyama, M.; Luxburg, U.; Guyon, I.; Garnett, R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GP9G4I4X","conferencePaper","2017","Saito, Masaki; Matsumoto, Eiichi; Saito, Shunta","Temporal Generative Adversarial Nets With Singular Value Clipping","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html","In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.","2017","2021-04-22 11:57:13","2021-05-17 04:41:30","2021-04-22 11:57:13","2830-2839","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf; C:\Users\controlnet\Zotero\storage\IZS7A9V9\Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"MPZXSPBG","conferencePaper","2018","Wang, Ting-Chun; Liu, Ming-Yu; Zhu, Jun-Yan; Liu, Guilin; Tao, Andrew; Kautz, Jan; Catanzaro, Bryan","Video-to-Video Synthesis","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2018/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf","We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)","2018","2021-04-22 11:47:08","2021-05-17 04:41:28","","1144-1156","","","31","","","","","","","","Curran Associates, Inc.","","","","","","","","","","","C:\Users\controlnet\Zotero\storage\TS53RTJY\d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html; G:\My Drive\Bibliography\Wang et al_2018_Video-to-Video Synthesis.pdf","","","","Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VC4TDGY7","journalArticle","2017","Suwajanakorn, Supasorn; Seitz, Steven M.; Kemelmacher-Shlizerman, Ira","Synthesizing Obama: learning lip sync from audio","ACM Transactions on Graphics","","0730-0301","10.1145/3072959.3073640","http://doi.org/10.1145/3072959.3073640","Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.","2017-07-20","2021-04-22 11:33:13","2021-05-17 04:41:27","2021-04-22 11:33:13","95:1–95:13","","4","36","","ACM Trans. Graph.","Synthesizing Obama","","","","","","","","","","","","July 2017","","","","G:\My Drive\Bibliography\Suwajanakorn et al_2017_Synthesizing Obama.pdf","","","audio; audiovisual speech; big data; face synthesis; lip sync; LSTM; RNN; uncanny valley; videos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFY9NNH8","journalArticle","2019","Aneja, Deepali; Li, Wilmot","Real-Time Lip Sync for Live 2D Animation","arXiv:1910.08685 [cs]","","","","http://arxiv.org/abs/1910.08685","The emergence of commercial tools for real-time performance-based 2D animation has enabled 2D characters to appear on live broadcasts and streaming platforms. A key requirement for live animation is fast and accurate lip sync that allows characters to respond naturally to other actors or the audience through the voice of a human performer. In this work, we present a deep learning based interactive system that automatically generates live lip sync for layered 2D characters using a Long Short Term Memory (LSTM) model. Our system takes streaming audio as input and produces viseme sequences with less than 200ms of latency (including processing time). Our contributions include specific design decisions for our feature definition and LSTM configuration that provide a small but useful amount of lookahead to produce accurate lip sync. We also describe a data augmentation procedure that allows us to achieve good results with a very small amount of hand-animated training data (13-20 minutes). Extensive human judgement experiments show that our results are preferred over several competing methods, including those that only support offline (non-live) processing. Video summary and supplementary results at GitHub link: https://github.com/deepalianeja/CharacterLipSync2D","2019-10-18","2021-04-22 10:34:19","2021-05-17 04:41:25","2021-04-22 10:34:19","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1910.08685","","G:\My Drive\Bibliography\Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf; C:\Users\controlnet\Zotero\storage\7I4RYARH\1910.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Graphics; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8WSAZGX","conferencePaper","2019","Tian, Guanzhong; Yuan, Yi; Liu, Yong","Audio2Face: Generating Speech/Face Animation from Single Audio with Attention-Based Bidirectional LSTM Networks","2019 IEEE International Conference on Multimedia Expo Workshops (ICMEW)","","","10.1109/ICMEW.2019.00069","","We propose an end to end deep learning approach for generating real-time facial animation from just audio. Specifically, our deep architecture employs deep bidirectional long short-term memory network and attention mechanism to discover the latent representations of time-varying contextual information within the speech and recognize the significance of different information contributed to certain face status. Therefore, our model is able to drive different levels of facial movements at inference and automatically keep up with the corresponding pitch and latent speaking style in the input audio, with no assumption or further human intervention. Evaluation results show that our method could not only generate accurate lip movements from audio, but also successfully regress the speaker's time-varying facial movements.","2019-07","2021-04-22 10:24:08","2021-05-17 04:41:24","","366-371","","","","","","Audio2Face","","","","","","","","","","","","IEEE Xplore","","","","C:\Users\controlnet\Zotero\storage\YQE2BYHQ\8795082.html; G:\My Drive\Bibliography\Tian et al_2019_Audio2Face.pdf","","","Animation, Long short-term memory network, Attention mechanism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE International Conference on Multimedia Expo Workshops (ICMEW)","","","","","","","","","","","","","","",""
"BTBA5S6Z","journalArticle","2021","Vaswani, Ashish; Ramachandran, Prajit; Srinivas, Aravind; Parmar, Niki; Hechtman, Blake; Shlens, Jonathon","Scaling Local Self-Attention for Parameter Efficient Visual Backbones","arXiv:2103.12731 [cs]","","","","http://arxiv.org/abs/2103.12731","Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.","2021-03-30","2021-04-09 05:56:57","2021-05-17 04:41:23","2021-04-09 05:56:57","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2103.12731","Comment: CVPR 2021 Oral","C:\Users\controlnet\Zotero\storage\Y8LLUYWL\2103.html; G:\My Drive\Bibliography\Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NL79FS5","journalArticle","2019","Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon","Stand-Alone Self-Attention in Vision Models","arXiv:1906.05909 [cs]","","","","http://arxiv.org/abs/1906.05909","Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.","2019-06-13","2021-04-09 05:56:03","2021-05-17 04:41:21","2021-04-09 05:56:03","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1906.05909","","C:\Users\controlnet\Zotero\storage\ZHYHSP49\1906.html; G:\My Drive\Bibliography\Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDJH6KVN","journalArticle","1901","F.R.S, Karl Pearson","LIII. On lines and planes of closest fit to systems of points in space","The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science","","1941-5982","10.1080/14786440109462720","https://doi.org/10.1080/14786440109462720","","1901-11-01","2021-04-08 05:23:21","2021-05-17 04:41:19","2021-04-08 05:23:21","559-572","","11","2","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/14786440109462720","<div data-schema-version=""1""><p>Annotation</p> <p>PCA original paper</p></div>","G:\My Drive\Bibliography\F.R.S_1901_LIII.pdf; C:\Users\controlnet\Zotero\storage\NTCZUUMU\14786440109462720.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J956Z7GT","journalArticle","2020","Jing, L.; Tian, Y.","Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/TPAMI.2020.2992393","","Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.","2020","2021-03-29 04:54:01","2021-05-17 04:41:17","","1-1","","","","","","Self-supervised Visual Feature Learning with Deep Neural Networks","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","","C:\Users\controlnet\Zotero\storage\X8PV7RTB\9086055.html; G:\My Drive\Bibliography\Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf","","","Training; Deep Learning; Task analysis; Visualization; Convolutional Neural Network; Annotations; Feature extraction; Learning systems; Self-supervised Learning; Transfer Learning; Unsupervised Learning; Videos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVFLWRB6","conferencePaper","2019","Karras, Tero; Laine, Samuli; Aila, Timo","A Style-Based Generator Architecture for Generative Adversarial Networks","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html","","2019","2021-03-29 04:41:33","2021-05-17 04:41:15","2021-03-29 04:41:33","4401-4410","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>StyleGAN</p> <p>The understanding of the process of image synthesis lacks and there is no quantitative way to measure the latent space interpolations.</p> <p>The architecture inspired from style transfer can control the image synthesis process. With the noise and style info injected to the networks, it can separate the high-level attributes and stochastic variations unsupervisedly. &nbsp;Also, using a mapping network to map the noise input to intermediate latent space can free the restriction and allow to be disentangled.</p> <p></p> </div>","G:\My Drive\Bibliography\Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf; C:\Users\controlnet\Zotero\storage\8LKELHY9\Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"L8IB9M3W","conferencePaper","2020","Wang, Huan; Li, Yijun; Wang, Yuehai; Hu, Haoji; Yang, Ming-Hsuan","Collaborative Distillation for Ultra-Resolution Universal Style Transfer","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html","Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.","2020","2021-03-29 04:33:48","2021-05-17 04:41:14","2021-03-29 04:33:48","1860-1869","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\VZ2DNRMF\Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html; G:\My Drive\Bibliography\Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"FG3L3HGN","journalArticle","2019","Brock, Andrew; Donahue, Jeff; Simonyan, Karen","Large Scale GAN Training for High Fidelity Natural Image Synthesis","arXiv:1809.11096 [cs, stat]","","","","http://arxiv.org/abs/1809.11096","Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.","2019-02-25","2021-03-29 04:21:24","2021-05-17 04:41:13","2021-03-29 04:21:24","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1809.11096","","C:\Users\controlnet\Zotero\storage\KYRR8X9E\1809.html; G:\My Drive\Bibliography\Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TD5237DC","journalArticle","2017","Zhang, Xiangyu; Zhou, Xinyu; Lin, Mengxiao; Sun, Jian","ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices","arXiv:1707.01083 [cs]","","","","http://arxiv.org/abs/1707.01083","We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.","2017-12-07","2021-03-29 04:17:08","2021-05-17 04:41:11","2021-03-29 04:17:08","","","","","","","ShuffleNet","","","","","","","","","","","","arXiv.org","","arXiv: 1707.01083","","C:\Users\controlnet\Zotero\storage\NJXA8YRI\1707.html; G:\My Drive\Bibliography\Zhang et al_2017_ShuffleNet.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SZH46QB","conferencePaper","2020","Yin, Hongxu; Molchanov, Pavlo; Alvarez, Jose M.; Li, Zhizhong; Mallya, Arun; Hoiem, Derek; Jha, Niraj K.; Kautz, Jan","Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html","We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ""invert"" a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.","2020","2021-03-29 04:15:29","2021-05-17 04:41:10","2021-03-29 04:15:29","8715-8724","","","","","","Dreaming to Distill","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Yin et al_2020_Dreaming to Distill.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"C3G62U2M","journalArticle","2020","Pumarola, Albert; Corona, Enric; Pons-Moll, Gerard; Moreno-Noguer, Francesc","D-NeRF: Neural Radiance Fields for Dynamic Scenes","arXiv:2011.13961 [cs]","","","","http://arxiv.org/abs/2011.13961","Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.","2020-11-27","2021-03-16 14:59:24","2021-05-17 04:41:08","2021-03-16 14:59:24","","","","","","","D-NeRF","","","","","","","","","","","","arXiv.org","","arXiv: 2011.13961","","C:\Users\controlnet\Zotero\storage\4M84DPVT\2011.html; G:\My Drive\Bibliography\Pumarola et al_2020_D-NeRF.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VP88VTAW","conferencePaper","2020","Mildenhall, Ben; Srinivasan, Pratul; Tancik, Matthew; Barron, Jonathan; Ramamoorthi, Ravi; Ng, Ren","NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","European Conference on Computer Vision","978-3-030-58451-1","","10.1007","","We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","2020","2021-03-16 14:54:40","2021-05-17 04:40:18","","405-421","","","","","","","","","","","","Springer, Cham","","","","","","","","DOI: 10.1007/978-3-030-58452-8_24","","G:\My Drive\Bibliography\Mildenhall et al_2020_NeRF.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","European Conference on Computer Vision","","","","","","","","","","","","","","",""
"PKB7WT8N","journalArticle","2021","Jiang, Yifan; Chang, Shiyu; Wang, Zhangyang","TransGAN: Two Transformers Can Make One Strong GAN","arXiv:2102.07074 [cs]","","","","http://arxiv.org/abs/2102.07074","The recent explosive interest on transformers has suggested their potential to become powerful ""universal"" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA $64\times64$, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at \url{https://github.com/VITA-Group/TransGAN}.","2021-02-16","2021-05-07 04:11:45","2021-05-17 04:39:13","2021-05-07 04:11:45","","","","","","","TransGAN","","","","","","","","","","","","arXiv.org","","arXiv: 2102.07074","<div data-schema-version=""2""><p>Annotation</p> <p>TransGAN. Pure transformer GAN.</p> <p>The convolution-based models cannot capture the features in global statistics. Can we build a strong GAN with completely self-attention layers/transformers? </p> <p>The transformer has strong representation capability for global view. Transformer is universal for multiple tasks.</p> <p>The Transformer encoder is used as basic block in TransGAN, which contains MHA(multi-head attention) and MLP with GELU, layer normalization and residual connections. In G, it is formed by transformer encoders and upsampling layers with increasing resolutions. The D is a ViT. For training, data augmentation, co-training with self-supervised auxiliary task, locality-aware initialization for self-attention are used to improve the performance. Also, scaling up the model benefits the performance.</p> <p>CIFAR-10. Also STL-10 and CelebA for higher resolution task.</p> <p>In CIFAR-10, it is better than other models except StyleGAN2. In STL-10 and CelebA, this model is better than other models.</p> <p>+ve: The performance is achieving the same level of previous CNN-based models. Unify the task pipeline as transformer structure is generic. </p> <p>-ve: Need more sophisticated tokenizing. Need pre-training transformers using pretext tasks. Need stronger attention forms. Need stronger and efficient self-attention forms. Need conditional image generation.</p> <p>In my opinion, the TransGAN provides a basic transformer-based GAN structure and several training tricks for better performance. This paper build the fundamental knowledge for transformer-based GAN, leading to more possibilities of generative networks on transformer.</p> </div>","C:\Users\controlnet\Zotero\storage\J7FQDBCT\2102.html; G:\My Drive\Bibliography\Jiang et al_2021_TransGAN.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"97G592DN","conferencePaper","2020","Meishvili, Givi; Jenni, Simon; Favaro, Paolo","Learning to Have an Ear for Face Super-Resolution","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html","We propose a novel method to use both audio and a low-resolution image to perform extreme face super-resolution (a 16x increase of the input size). When the resolution of the input image is very low (e.g., 8x8 pixels), the loss of information is so dire that important details of the original identity have been lost and audio can aid the recovery of a plausible high-resolution image. In fact, audio carries information about facial attributes, such as gender and age. To combine the aural and visual modalities, we propose a method to first build the latent representations of a face from the lone audio track and then from the lone low-resolution image. We then train a network to fuse these two representations. We show experimentally that audio can assist in recovering attributes such as the gender, the age and the identity, and thus improve the correctness of the high-resolution image reconstruction process. Our procedure does not make use of human annotation and thus can be easily trained with existing video datasets. Moreover, we show that our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations.","2020","2021-05-04 07:28:39","2021-05-17 04:39:11","2021-05-04 07:28:39","1364-1374","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>Add audio for image super resolution</p> <p>The important semantic information lossed whtn the resolution is extremely low. Do SR in this LR images might cause wrong gender and ages.</p> <p>The audio information might be helpful to extract gender and age features. Model can learn the correlation between the voice to the personal identity.</p> <p>The method they proposed has several steps to train. Step 1: Train a HR encoder to reverse the pretrained StyleGAN (generator) as auto-encoder with L1 and VGG-percepture loss. Step 2: Train both the encoder and StyleGAN to fine tune in the dataset with loss of step 1 and generator-weights regularization. Step 3: Train the LR encoder to regress the same latent representations from HR encoder with L1 loss for latent representations and the pairs of LR and downsampling of SR. Then, use the HR encoder and filpped face to get latent representations of neutral frontal facing poses. Train the A (audio) encoder to predict the neutral front latent representations with L1 loss. Step 4: Train the fusing layer to regress the latents of HR encoder with L1 loss of latents and the pairs of LR and downsampling of SR.</p> <p>The dataset is VoxCeleba2. </p> <p>The metrics is PSNR, SSIM, the classification error of identity classifier, gender classifier and age classifier. The proposed method is better than others for recovery of identity, gender and age.</p> <p>+ve: It can recover the age and gender attributes. Utilize the information from audio, which is easily accessed in video.</p> <p>-ve: The model is not robust to some cases when the gender can be easily guessed from the LR image. Naive end-to-end training is not applied.</p> <p>This paper introduced the first method for image face super resolution combined the LR image and audio. The information of audio contains some kind of informations which is not easily gussed from LR.</p> </div>","G:\My Drive\Bibliography\Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf; C:\Users\controlnet\Zotero\storage\3I29U2DH\Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"CDRH2S5A","journalArticle","2021","He, Zhenliang; Kan, Meina; Shan, Shiguang","EigenGAN: Layer-Wise Eigen-Learning for GANs","arXiv:2104.12476 [cs, stat]","","","","http://arxiv.org/abs/2104.12476","Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via the adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of ""eigen-dimensions"" at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, under the linear circumstance, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow.","2021-04-26","2021-05-01 16:51:17","2021-05-17 04:39:08","","","","","","","","EigenGAN","","","","","","","","","","","","arXiv.org","","arXiv: 2104.12476","<div data-schema-version=""2""><p>Annotation</p> <p>EigenGAN</p> <p>Previous methods are all post-processing algorithms for well-trained GAN generators. So can a generator itself automatically/unsupervisedly learn explicit control of the semantic attributes represented in different layers?</p> <p>Embedding a linear subspace model with orthogonal basis can build the semantic variations with each layer. Also, the generator can capture the principal variations with different layers/scales and they are orthogonal. All these features are controllable and interpretable.</p> <p>They proposed EigenGAN, the generator of which is a chain of 2-strided deconv layers, and each layer add to a subspace linear model with orthonormal basis. The output of subspace linear model is calculated by the U, L, z and mu, which is equivalent to PCA, so it is named EigenGAN.</p> <p>They used CelebA for quantitative evaluation, and FFHQ and Danbooru 2019 Portraits for qualitative evaluation.</p> <p>From the result, the features discovered has more strong correlation than SeFa.</p> <p>+ve: Can make interpretable dimensions for different layers of generator unsupervisedly. Good performance compared to other model.</p> <p>-ve: Some features are still entangled. Some features are ignored because appear less frequently in the dataset.</p> <p>In my opinion, this methods provide a very powerful way to use unsupervised learning with GAN to make the input variables controllable, interpretable and distangled. It's purely unsupervised learning and GAN.</p> </div>; Comment: Code: https://github.com/LynnHo/EigenGAN-Tensorflow","C:\Users\controlnet\Zotero\storage\ZHZ55574\2104.html; G:\My Drive\Bibliography\He et al_2021_EigenGAN.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JBYFZKE","journalArticle","2021","Fan, Haoqi; Xiong, Bo; Mangalam, Karttikeya; Li, Yanghao; Yan, Zhicheng; Malik, Jitendra; Feichtenhofer, Christoph","Multiscale Vision Transformers","arXiv:2104.11227 [cs]","","","","http://arxiv.org/abs/2104.11227","We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast","2021-04-22","2021-04-23 05:56:08","2021-05-17 04:39:05","","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2104.11227","<div data-schema-version=""2""><p>Annotation</p> <p>Multi-scale ViT (MViT)</p> <p>The previous vision transformer (ViT) only used one scale to analyze the images, lack of the thoughts of ""pyramid"" strategies, which need to go to different scales to get variety of features from different scales.</p> <p>The ""pyramid"" strategies is a multi-scale processing, which means working in a lower resolution can cover the features with better ""sense"" of context. The ""depth"" of modern deep neural networks can help. Authors hope to connect the multi-scale feature hierarchies concept with the transformer. And this concept is beneficial to the transformer structures.</p> <p>They proposed Multiscale Vision Transformer (MViT) and it is built by Multi-Head Pooling Attention (MHPA) layers. Compared to the Muti-Head Attention layer, the MHPA has pooling layers to each route (identity, Q, K and V). In the MViT, the dimension of feature maps will decrease through the MHPA layers to detect multi-scale features.</p> <p>The dataset used is Kinetics-400 and Kinetics-600 for image classification. And Something-Something-v2, Charades and AVA for transfer learning.+</p> <p>The results shows the MViT has better accuracy for video classification than others. Also it has less parameters and less computational complexity than ViT. In image recognition with transfer learning, the MViT also performs the best comparing to other models.</p> <p>+ve: Expand the feature complexity. Big advantage than single-scale vision transformer.</p> <p>-ve:</p> <p>In my opinion, this MViT brings the pyramid concepts to transformer models, also provide a method to work with video. With higher performance and lower computational complexity and parameters. This MViT is very useful for video analysis.</p> </div>; Comment: Technical report","C:\Users\controlnet\Zotero\storage\3CH8Q456\2104.html; G:\My Drive\Bibliography\Fan et al_2021_Multiscale Vision Transformers.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BR6PGMGD","conferencePaper","2020","Kumar, Neeraj; Goel, Srishti; Narang, Ankur; Hasan, Mujtaba","Robust One Shot Audio to Video Generation","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops","","","","https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html","","2020","2021-04-22 07:05:29","2021-05-17 04:39:02","2021-04-22 07:05:29","770-771","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>OneShotA2V, using audio and a single unseen image to generate video.</p> <p>The previous methods cannot generate smooth transition or need high-compute time to generate video for a new unseen speaker.</p> <p>Using multiple multi-level discriminators can generate synchronized and realistic video. And using multiple loss can learn better.</p> <p>The OneShotA2V structure is an adversarial networks with a generator and 3 discriminators. the generator is similar to SPADE architecture and do transfer learning from deepspeech2 network. The multi-scale frame discriminator classify the real and fake frame images in 3 scales. The multi-scale temporal discriminator can classify the real or fake image based on previous frame in time-series. The synchronization discriminator uses SyncNet ensure the lip in the face matching the audio. In the training, adversarial loss, reconstruction loss, feature-matching loss, percepture loss (VGG19), contrastive loss (for SyncNet), and blink loss.</p> <p>GRID dataset and LOMBARD GRID.</p> <p>The metrics used is PSNR, SSIM, CPBD, WER and ACD. The results are better than RSDGAN and Speech2Vid.</p> <p>+ve: Use speech features for lower word error rate. Generation performance is better. Robust without other datasets.</p> <p>-ve: No emotion included to train. Not very sophisticated. </p> <p>This paper provide a clear and efficient techinuq for ""image + audio -&gt; video"" generation. With using multi-scale multiple discriminators and multiple losses to improve the quarlity of output.</p> </div>","G:\My Drive\Bibliography\Kumar et al_2020_Robust One Shot Audio to Video Generation.pdf; C:\Users\controlnet\Zotero\storage\Q9YPKPYG\Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops","","","","","","","","","","","","","","",""
"T6VFNVXY","conferencePaper","2017","Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Łukasz; Polosukhin, Illia","Attention is all you need","Proceedings of the 31st International Conference on Neural Information Processing Systems","978-1-5108-6096-4","","","","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.","2017-12-04","2021-04-12 07:30:28","2021-05-17 04:38:59","","6000–6010","","","","","","","NIPS'17","","","","Curran Associates Inc.","Red Hook, NY, USA","","","","","","ACM Digital Library","","","<div data-schema-version=""2""><p>Annotation</p> <p>Self-attention and transformer</p> <p>The previous recurrent model structures cannot compute parallelly.</p> <p>The attention mechanisms can allow the dependencies without regard to their distances in the sequences. The transformer structure can parallel the computation, and reach a better quarlity.</p> <p>They proposed the Transformer with Multi-Head Attention blocks. The tansformer is a encoder-decoder structure, and the attention layers are the main part of this structure. The Multi-head attention layer has 3 imput, Query, Key and Value. The Q, K and V will be used to calculate the output of attention layer.</p> <p>The standard WMT 2014 English-German dataset and WMT 2014 English-French dataset.</p> <p>The results shows the performance is better than other models and the training cost is less than others.</p> <p>+ve: Faster train. Better performance.</p> <p>-ve: Only used for text data. No locality focus.</p> <p>This paper is the origin of self-attention and transformer, which replaces the traditional RNN based structures for NLP tasks, with faster training speed and better perforance. Based on this method, lots of work also focus on the transformer for computer vision.</p> </div>","G:\My Drive\Bibliography\Vaswani et al_2017_Attention is all you need.pdf","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 31st International Conference on Neural Information Processing Systems","","","","","","","","","","","","","","",""
"YWSL99D4","journalArticle","2020","Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil","An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","arXiv:2010.11929 [cs]","","","","http://arxiv.org/abs/2010.11929","While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","2020-10-22","2021-04-09 06:01:35","2021-05-17 04:38:56","","","","","","","","An Image is Worth 16x16 Words","","","","","","","","","","","","arXiv.org","","arXiv: 2010.11929","<div data-schema-version=""2""><p>Annotation</p> <p>Vision Transformer (ViT)</p> <p>In previous, there is few methods using transformer to replace CNN using in the computer vision image analysis tasks. Or it is still hard to be accelerated.</p> <p>The authors assume to use image patches to replace NLP tokens in transformer.</p> <p>The structure of ViT is very similar to the transformer encoder part in NLP area. For the input image, it will be divided as patches, and the patches are like tokens added by trainable positional encoding to be sent into transformer. There is another trainable token in the first place to be sent into the transformer. In the output of the transformer encoder, the output for that trainable token is used for classification.</p> <p>ILSVRC-2012 ImageNet dataset, ImageNet-21k and JFT are used.</p> <p>In the large dataset, the transformers with more parameters have advantages comparing to the less one and CNN.</p> <p>+ve: Better performance with pretraining in large dataset compared to CNN.</p> <p>-ve: Perform bad when data is not enough. Didn't try to implement ViT to other computer vision tasks. Need to continue exploring self-supervised pre-training methods.</p> <p>In my opinion, this paper provides a great method for using the original structure of transformer. The tokens in NLP is replaced with image patches in computer vision. From the result of image classification, we can expect this structure works in other CV tasks.</p> </div>; Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer","C:\Users\controlnet\Zotero\storage\JWTXXQUI\2010.html; G:\My Drive\Bibliography\Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"698AHGVY","journalArticle","2014","Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua","Generative Adversarial Networks","arXiv:1406.2661 [cs, stat]","","","","http://arxiv.org/abs/1406.2661","We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","2014-06-10","2021-04-08 09:17:51","2021-05-17 04:38:53","2021-04-08 09:17:51","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1406.2661","<div data-schema-version=""2""><p>Annotation</p> <p>Generative Adversarial Networks (GAN)</p> <p>It is difficult to approximate because the difficulty of utilizing the benefits of picewise linear units in the generative context.</p> <p>It is possible to use a adversarial structures to overcome the difficulties.</p> <p>The GAN framework has two models, generative model and discriminative model. The generator input a noise vector try to confuse the discriminator, and the discriminator can classify the image is from real dataset or generator.</p> <p>MNIST, Toronto Face Database (TFD), CIFAR-10</p> <p>The metric is window-based log-likelihood etimates. The adversarial nets is better than other generative networks.</p> <p>+ve: Has potential in the future. No inference is needed during learning. A wide variety of functions can be incorporated into the model. The adversarial models does not directly receive any information from dataset. GAN can represent very sharp distributions.</p> <p>-ve: The discriminator must be synchronized well with generator during training (Hard to train). </p> <p>The development of GAN broaden the area of image synthesis and translation area. GAN has the advantages of generating very realistic data, so will be widely used in the future generation tasks. However, GAN is hard to train, but there will be future works to improve that.</p></div>","C:\Users\controlnet\Zotero\storage\VPGCMDSE\1406.html; G:\My Drive\Bibliography\Goodfellow et al_2014_Generative Adversarial Networks.pdf","","Viewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTCMR42I","conferencePaper","2015","Ioffe, Sergey; Szegedy, Christian","Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","International Conference on Machine Learning","","","","http://proceedings.mlr.press/v37/ioffe15.html","Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the t...","2015-06-01","2021-03-16 14:50:26","2021-05-17 04:38:49","","448-456","","","","","","Batch Normalization","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","<div data-schema-version=""2""><p>Annotation</p> <p>Batch normalization and InceptionV2</p> <p>The hidden layers in neural networks need to continuously adapt to the new distribution or will cause the covariate shift (Shimodaira, 2000).</p> <p>The optimizer will train fast when the distribution of input remains more stable. </p> <p>They proposed a method called Batch Normalization. The BN transform applied to the mini-batch of the input. The BN layer will calculate the mini-batch mean and variance to normalize the input batch to similar to N(0, 1). And also will learn a scale and shift function to map the output better.</p> <p>MNIST</p> <p>The metric is classification error. The training is faster and the result is more accurate than the model without BN.</p> <p>+ve: The training speed is faster. The performance of model is better.</p> <p>-ve: Not explored the full range of other possibilities of BN.</p> <p>The BN contributes a lot for modern deep models to improve the speed in training and the performance. In the many models today, the BN layers will apply after each convolutional layers to improve the performance.</p> </div>","G:\My Drive\Bibliography\Ioffe_Szegedy_2015_Batch Normalization.pdf; C:\Users\controlnet\Zotero\storage\T2BGYQWB\ioffe15.html","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"CQEVWNAP","conferencePaper","2015","Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew","Going Deeper With Convolutions","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html","We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification.","2015","2021-03-16 14:27:59","2021-05-17 04:38:44","2021-03-16 14:27:59","1-9","","","","","","","","","","","","","","","","","","www.cv-foundation.org","","","<div data-schema-version=""2""><p>Annotation</p> <p>Inception Module and GoogLeNet (InceptionV1)</p> <p>The mobile and embedded computting has limited power for models. A high-efficiency method is required.</p> <p>Decide to develop a high-efficiency method keeping a computational budget of 1.5 billion operations.</p> <p>The Inception module has multiple routes for input features. Each route has different kernel size of convolutions and max-pooling. And then, concatenate the outputs as the result of this module. Otherwise, the 1x1 convolutions can be used to reduce the number of channels. GoogLeNet is the model formed by Inception modules.</p> <p>ILSVRC 2014</p> <p>The metric is Top-5 error in ILSVRC 2014. The GoogLeNet ranked 1st in this competition.</p> <p>+ve: A significant quality improvement with slightly increase of computational requirements. Also available for object detection.</p> <p>-ve:</p> <p>The Inception Module is the new direction that the CNN can be not sequence. And, the high-efficiency also is an important metrics to judge the usability of a neural network structure.</p></div>","C:\Users\controlnet\Zotero\storage\PTFRKBPB\Szegedy_Going_Deeper_With_2015_CVPR_paper.html; G:\My Drive\Bibliography\Szegedy et al_2015_Going Deeper With Convolutions.pdf","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"5DX7TJDY","journalArticle","2014","Lin, Min; Chen, Qiang; Yan, Shuicheng","Network In Network","arXiv:1312.4400 [cs]","","","","http://arxiv.org/abs/1312.4400","We propose a novel deep network structure called ""Network In Network"" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","2014-03-04","2021-03-16 14:27:20","2021-05-17 04:38:39","2021-03-16 14:27:20","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1312.4400","<div data-schema-version=""2""><p>Annotation</p> <p>NIN</p> <p>The traditional convolutional layer as generalized linear model (GLM) has low level of abstraction.</p> <p>By replacing the GLM in convolutional layer with a non-linear function approximator can improve the &nbsp;abstraction ability of local model.</p> <p>The authors proposed two concepts, using MLP to replace the GLM in convolutional layer, and the usage of global average pooling layer. They use convolution layers with 1x1 kernel to simulate the MLP inside. And the global average pooling layer is used to replace the fully-connected layers in traditional CNN.</p> <p>CIFAR-10, CIFAR-100, SVHN, MNIST.</p> <p>The metric is test set classification error. In these 4 datasets, the NIN has an outstanding performance compared to other structures. Also the global average pooling as a regularizer is also better than fully connected + dropout set.</p> <p>+ve: The Mlpconv allows more complex and learnable interaction of cross channel information. Global average pooling is more meaningful and interpretable. Global average pooling is more robust to spatial translations of the input.</p> <p>-ve:</p> <p>The ideas in this paper is amazing, and many famous structures such as ResNet and GoogLeNet use these ideas.</p> <p></p></div>; Comment: 10 pages, 4 figures, for iclr2014","C:\Users\controlnet\Zotero\storage\GBJXNLGT\1312.html; G:\My Drive\Bibliography\Lin et al_2014_Network In Network.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGZ5ML22","conferencePaper","2016","He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian","Deep Residual Learning for Image Recognition","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html","Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","2016","2021-03-16 14:25:33","2021-05-17 04:38:36","","770-778","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","<div data-schema-version=""2""><p>Annotation</p> <p>ResNet and Residual block</p> <p>Will the model be better with more layers? How to solve the problem of gradients vanishment and explosion.</p> <p>The hypothesis is the model constructed with some identity mappings can produce a better training error than shallower models.</p> <p>In the residual block, there is an identity mapping to the output of the block and sum them. In the residual networks (ResNet), &nbsp;they are formed by multiple residual blocks with different number of layers. </p> <p>ImageNet 2012 classification dataset. CIFAR-10. PASCAL VOC 2007 and 2012 and COCO.</p> <p>The comparison between the models of 18 layers and 34 layers indicates the ResNet can produce better performance when the number of layers is deeper. Then the ResNets are compared with other networks, leading to a better result. The compared applied to CIFAR-10 between different layer versions of ResNet shows extremely deep network does not produce a better result (ResNet-1202 vs ResNet-110). Also, the experiments for object detection on PASCAL VOC 2007, 2012 and COCO shows the ResNet has good generalization performance.</p> <p>+ve: Produce better performance with deeper ResNet. Has good generalization performance.</p> <p>-ve:</p> <p>A great method that release the limitation of the depth of model design to let the researchers available for any depth they want, which powered the community has better flexbility of networks architecture.</p> </div>","G:\My Drive\Bibliography\He et al_2016_Deep Residual Learning for Image Recognition.pdf","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"Q7ZNACBK","journalArticle","2015","Simonyan, Karen; Zisserman, Andrew","Very Deep Convolutional Networks for Large-Scale Image Recognition","arXiv:1409.1556 [cs]","","","","http://arxiv.org/abs/1409.1556","In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.","2015-04-10","2021-03-16 14:24:33","2021-05-17 04:38:32","2021-03-16 14:24:33","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1409.1556","<div data-schema-version=""2""><p>Annotation</p> <p>VGG</p> <p>Improve the previous CNN structure for better accuracy for ILSVRC.</p> <p>The depth of the structure of CNN can be discussed for better performance of the model.</p> <p>They proposed two best VGG-16 and VGG-19 structures. Instead of using big kernel size, the VGG only use 3x3 kernel sizes and 16 layers or 19 layers.</p> <p>ILSVRC-2014</p> <p>The metric is test set classification error. Their score is better than the best of ILSVRC-2013.</p> <p>+ve: More depth is beneficial for the performance. VGG structures has widely usage for other tasks.</p> <p>-ve:</p> <p>Provide a famous VGG structure for CNN area, which indicates the depth contributes more than width of CNN.</p></div>","C:\Users\controlnet\Zotero\storage\9VVQ6FR8\1409.html; G:\My Drive\Bibliography\Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NUJHIL5","journalArticle","2001","Cootes, T. F.; Edwards, G. J.; Taylor, C. J.","Active appearance models","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/34.927467","","We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.","2001-06","2021-03-16 14:11:16","2021-05-17 04:38:29","","681-685","","6","23","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence","<div data-schema-version=""2""><p>Annotation</p> <p>Active Appearance Models (AAM)</p> <p>Previous methods are very slow or easily become trapped in local minima to fit photo-realistic appearance. Some previous methods cannot fit because the shape-normalized texture map cannot be reconstructed.</p> <p>The authors thought the hypothesis of the optimization problem is similar to each time so can be similarities offline, which can find the directions of rapid convergence.</p> <p>The AAM is a further improvement of ASM, which combining both the shape and texture for fitting the model.</p> <p>Datasets are 100 hand-labeled face images as training set and another 100 as test set.</p> <p>No quantitative comparison to other methods.</p> <p>+ve: Good reliability and robustness of image appearance search.</p> <p>-ve: The training speed of AAM is slower than ASM.</p> <p>The improvement of ASM combining the shape and texture.</p></div>","G:\My Drive\Bibliography\Cootes et al_2001_Active appearance models.pdf; C:\Users\controlnet\Zotero\storage\2Y22Q7QV\927467.html","","Viewed","learning (artificial intelligence); Deformable models; active appearance models; Active shape model; deformable template; gray-level variation; Image generation; image matching; Image reconstruction; Image segmentation; image texture; Iterative algorithms; iterative method; iterative methods; learning; model matching; optimisation; Optimization methods; Robustness; Shape control; shape matching; statistical analysis; statistical models; Surface fitting; texture matching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPDBRJGH","journalArticle","1995","Cootes, T. F.; Taylor, C. J.; Cooper, D. H.; Graham, J.","Active Shape Models-Their Training and Application","Computer Vision and Image Understanding","","1077-3142","10.1006/cviu.1995.1004","https://www.sciencedirect.com/science/article/pii/S1077314285710041","Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our Active Shape Models can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images.","1995-01-01","2021-03-16 14:10:07","2021-05-17 04:38:25","2021-03-16 14:10:07","38-59","","1","61","","Computer Vision and Image Understanding","","","","","","","","en","","","","","ScienceDirect","","","<div data-schema-version=""2""><p>Annotation</p> <p>Active Shape Model (ASM)</p> <p>The problem of locating examples of known objects in images.</p> <p>The shape of the objects of same class may vary. Using flexible models or deformable templates can be used to allow for some degree of variability in the shape of the imaged objects.</p> <p>The method is to use mean template and a deformable transformation applied to point distribution model to fit the target shape. And the shape is constrained by PCA.</p> <p>The shapes of registers.</p> <p>No quantitative comparison with other methods.</p> <p>+ve: Low time and space complexity. Can be applied to a wide range of image interpretation tasks.</p> <p>-ve: Require annotations for training images. Not robust to noise, clutter and occlusion.</p> <p>A traditional method for shape fitting with the constrain of PCA and the training with transformation.</p></div>","G:\My Drive\Bibliography\Cootes et al_1995_Active Shape Models-Their Training and Application.pdf; C:\Users\controlnet\Zotero\storage\A9CBSJ67\S1077314285710041.html","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8PJBJU3","journalArticle","2016","Makhzani, Alireza; Shlens, Jonathon; Jaitly, Navdeep; Goodfellow, Ian; Frey, Brendan","Adversarial Autoencoders","arXiv:1511.05644 [cs]","","","","http://arxiv.org/abs/1511.05644","In this paper, we propose the ""adversarial autoencoder"" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.","2016-05-24","2021-03-16 14:06:13","2021-05-17 04:38:21","2021-03-16 14:06:13","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1511.05644","<div data-schema-version=""2""><p>Annotation</p> <p>Adversarial Autoencoder (AAE)</p> <p>The MCMC methods computing the gradient becomes more imprecise in training progress.</p> <p>Some generative models can avoid the difficulties of the training by being trained via direct back-propagation.</p> <p>The AAE can convert an autoencoder as a generative model. Both the reconstruction loss and adversarial loss are used in training, fitting the distribution of latent representation to any prior distribution (for example, normal distribution). The authors proposed several structures for unsupervised, semi-supervised and supervised lerning. In other word, the AAE has a discriminator classifing the latent representation if it is from encoder output or prior distribution. Therefore, with the discriminator loss, the encoder should confuse the discriminator to think all latent representations are from predefined distribution, which means these two distribution are identical. So the distribution of latent representation can be controlled.</p> <p>MNIST, Toronto face dataset (TFD) and SVHN.</p> <p>Use Log-likelihood of test data to compare with other generative networks. The results are the best. Use classification error for comparing the semi-supervised AAE structure with other generative networks. The results are the best compared to the others. Use clustering error rate to compare the clustering performance.</p> <p>+ve: Can use the prior distribution by its samples rather than the explicit functional form. Can be used in different situations (unsupervised, semi-unsupervised, clustering, ...). </p> <p>-ve:</p> <p>AAE is a great example for the improvement of AE, as it combined the concept of AE and GAN to regularize the distribution of latent representations.</p></div>","C:\Users\controlnet\Zotero\storage\HFBFFJF8\1511.html; G:\My Drive\Bibliography\Makhzani et al_2016_Adversarial Autoencoders.pdf","","Viewed","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CA2QE2FS","journalArticle","2010","Vincent, Pascal; Larochelle, Hugo; Lajoie, Isabelle; Bengio, Yoshua; Manzagol, Pierre-Antoine","Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion","The Journal of Machine Learning Research","","1532-4435","","","We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.","2010-12-01","2021-03-16 14:03:28","2021-05-17 04:38:13","","3371–3408","","","11","","J. Mach. Learn. Res.","Stacked Denoising Autoencoders","","","","","","","","","","","","3/1/2010","","","<div data-schema-version=""2""><p>Annotation</p> <p>Denoising autoencoder (DAE)</p> <p>To overcome the gap between the stacking RBMs and stacking autoencoders. And find out what can shape a good, useful representation.</p> <p>The authors were looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns. And expected the latent representation is robust and stable, also can perform denoising task.</p> <p>The method proposed is the DAE which adds the noise to input data, and train the autoencoder to reconstruct the original data without noise. The noise added can be Gaussian noise or masking noise. With the noise inside, DAE can learn the features more stable and robust. So, an autoencoder with noisy data input can encode as higher level representations which have better performance than without noise added. The stacked DAE is the pretrained method to improve the performance of autoencoder.</p> <p>The dataset used is the 12 x 12 patches from Olshausen for DAE. The MNIST, and tzanetakis audio genre classification data set for stacked DAE.</p> <p>They compared the test error rate for classification task with other methods, and the SDAE (stacked DAE) performs the best.</p> <p>+ve: Better performance than DBNs. Establish the value of using the denoising criterion as an unsupervised objective for useful higher level presentations.</p> <p>-ve: The DAEs used in the paper are shallow.</p> <p>This paper introduced a simple way (add noise to input data) to improve the performance of encoder, which is easy to implement and prove the value for denoising task in the AE training. </p> <p></p></div>","G:\My Drive\Bibliography\Vincent et al_2010_Stacked Denoising Autoencoders.pdf","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WMKXI6M","journalArticle","2006","Hinton, G. E.; Salakhutdinov, R. R.","Reducing the Dimensionality of Data with Neural Networks","Science","","0036-8075, 1095-9203","10.1126/science.1127647","https://science.sciencemag.org/content/313/5786/504","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.","2006-07-28","2021-03-16 14:00:23","2021-05-17 04:38:10","2021-03-16 14:00:23","504-507","","5786","313","","","","","","","","","","en","American Association for the Advancement of Science","","","","science.sciencemag.org","","Publisher: American Association for the Advancement of Science Section: Report PMID: 16873662","<div data-schema-version=""2""><p>Annotation</p> <p>Using RBM to pretrain the Auto encoder for dimensionality reduction.</p> <p>Multi-layer auto encoder is hard to train.</p> <p>Use a very different type of algorithm to pretrain the model and prove it can be generalized to other datasets.</p> <p>The method introduced in this paper uses RBM as the algorithm to pretrain the weights of an autoencoder. For each layer of the autoencoder, the weights will be pretrained as RBM, and pretrain the next layer. With the RBM pretrained weights, the autoencoder has better performance for reconstruction.</p> <p>MNIST hand-written digits and Olivetti face dataset</p> <p>No quantitative comparison.</p> <p>+ve: Better reconstruction results than PCA. This pretraining can also be used for classification and regression. Pretraining helps generalization. The autoencoders can map in bi-direction between data and code spaces. Can apply to very large datasets.</p> <p>-ve:</p> <p>Provide a new pretrain method for auto encoder to improve the performance. Combine the RBM and the neural networks.</p> <p></p> <p></p></div>","G:\My Drive\Bibliography\Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf; ; C:\Users\controlnet\Zotero\storage\MVMYC94D\504.html","http://www.ncbi.nlm.nih.gov/pubmed/16873662","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJPJAB46","journalArticle","2012","Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R.","Improving neural networks by preventing co-adaptation of feature detectors","arXiv:1207.0580 [cs]","","","","http://arxiv.org/abs/1207.0580","When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.","2012-07-03","2021-03-16 13:52:06","2021-05-17 04:38:07","2021-03-16 13:52:06","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1207.0580","<div data-schema-version=""2""><p>Annotation</p> <p>Dropout</p> <p>The overfitting, do worse on the test data than on the training data, happens.</p> <p>Finding a similar method with ""mean network"", which is formed by many separate networks, to reduce the test set error.</p> <p>The dropout is used as disable some proportion of hidden units in training process to prevent the hidden units replies to others. Normally, the drop probability is set to 0.5.</p> <p>Datasets are MNIST, TIMIT benchmark, CIFAR-10, ImageNet, and Reuters.</p> <p>The main metrics for comparison is the classification error for test set. The performance of dropout network is similar to ""mean network"", and better than the network without dropout.</p> <p>+ve: Almost all dropout probabilities can improve the generalization performance of the models. Dropout is simpler to implement.</p> <p>-ve: Some extreme probabilities will cause the performance worse.</p> </div>","C:\Users\controlnet\Zotero\storage\PCYSDJFS\1207.html; G:\My Drive\Bibliography\Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf","","Viewed","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMJ59NJI","journalArticle","1998","Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P.","Gradient-based learning applied to document recognition","Proceedings of the IEEE","","1558-2256","10.1109/5.726791","","Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.","1998-11","2021-03-16 13:50:37","2021-05-17 04:38:03","","2278-2324","","11","86","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Proceedings of the IEEE","<div data-schema-version=""2""><p>Annotation</p> <p>Convolutional Neural Networks (CNN) and LeNet</p> <p>To find a better pattern recognition systems can be built by relying more on automatic learning and less on hand-designed heuristics.</p> <p>The new method is made possible by recent progress in machine learning and computer technology. The hand-crafted feature extraction can be advantageously replaced by carefully designed learning machines that operate directly on pixel images.</p> <p>The architecture of CNN is based on convolutional layers, pooling layers, and some fully-connected layers. Convolutional layers and pooling layers are effective for extracting features from 2D images, and they can also share the parameters with nearby numbers which can dramatically reduce the parameters, and it is beneficial for model performance.</p> <p>The dataset is the called MNIST, with 60000 training images and 10000 test images, combined by SD-1 and SD-2 datasets.</p> <p>The classification error of LeNet is lower compared to other traditional methods and fully-connected NN.</p> <p>+ve: Better classification performance than traditional methods and fully-connected NN. Suitable for hardware implementations with low memory requirements. More robust for shape variance and noise.</p> <p>-ve: Longer training time.</p> <p>The origin of CNN, huge contribution to computer vision area.</p></div>","C:\Users\controlnet\Zotero\storage\8RPNJDWS\726791.html; G:\My Drive\Bibliography\Lecun et al_1998_Gradient-based learning applied to document recognition.pdf","","Viewed","Neural networks; convolution; Machine learning; Principal component analysis; Feature extraction; 2D shape variability; back-propagation; backpropagation; Character recognition; cheque reading; complex decision surface synthesis; convolutional neural network character recognizers; document recognition; document recognition systems; field extraction; gradient based learning technique; gradient-based learning; graph transformer networks; GTN; handwritten character recognition; handwritten digit recognition task; Hidden Markov models; high-dimensional patterns; language modeling; Multi-layer neural network; multilayer neural networks; multilayer perceptrons; multimodule systems; optical character recognition; Optical character recognition software; Optical computing; Pattern recognition; performance measure minimization; segmentation recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JBPGV5S","journalArticle","1991","Kramer, Mark A.","Nonlinear principal component analysis using autoassociative neural networks","AIChE Journal","","1547-5905","https://doi.org/10.1002/aic.690370209","https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690370209","Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.","1991","2021-03-16 13:41:25","2021-05-17 04:37:59","2021-03-16 13:41:25","233-243","","2","37","","","","","","","","","","en","Copyright © 1991 American Institute of Chemical Engineers","","","","Wiley Online Library","","_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690370209","<div data-schema-version=""2""><p>Annotation</p> <p>Auto Encoder (Nonlinear Principal Component Analysis NLPCA)</p> <p>Engineers are often confronted with the problem of extracting information about poorly-known process of data</p> <p>The dimensionality reduction is closely related to feature extraction and it can capture the information contained in the original data.</p> <p>The proposed method is the using multi-layer neural networks with same input and output and a bottleneck. The training target is to minimize the reconstruction error. Compared to PCA, the nonlinearity improve the performance of feature extraction.</p> <p>Datasets are generated by some simple functions.</p> <p>The results of experiments shows the reconstruction error is less than PCA and ANN without mapping layer.</p> <p>+ve: Can find and eliminates nonlinear correlations in the data. Can remove redundant information. More effective than PCA.</p> <p>-ve: Limited by the practicalities of computing functional approximations from limited data.</p> <p>The origin of auto encoder. A great milestone.</p> <p></p></div>","G:\My Drive\Bibliography\Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf; C:\Users\controlnet\Zotero\storage\TA7ULUCD\aic.html","","Viewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YN6MUWWP","conferencePaper","2017","Huang, Xun; Belongie, Serge","Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html","","2017","2021-03-29 04:43:34","2021-04-09 01:53:06","2021-03-29 04:43:34","1501-1510","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Huang_Belongie_2017_Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization.pdf; C:\Users\controlnet\Zotero\storage\ESXAEPLT\Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"454BKCFN","journalArticle","2021","Doersch, Carl","Tutorial on Variational Autoencoders","arXiv:1606.05908 [cs, stat]","","","","http://arxiv.org/abs/1606.05908","In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.","2021-01-03","2021-03-16 14:05:22","2021-04-06 15:06:51","2021-03-16 14:05:22","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1606.05908","<div data-schema-version=""1""><p>Annotation</p> <p>This paper introduces a variant of autoencoder, which is the variational autoencoder. For the ordinary autoencoder, the code is a n-dimensional vector. As for VAE, the code trained is a Gaussian distribution. In the network structure, the mean and the variance of the distribution will be trained. And then, the generator resample the code by the Gaussian distribution encoded. Compared with the vanilla autoencoder, VAE has better performance.</p></div>","C:\Users\controlnet\Zotero\storage\GBJ99QGT\1606.html; G:\My Drive\Bibliography\Doersch_2021_Tutorial on Variational Autoencoders.pdf","","Unreviewed","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBQELRWF","journalArticle","2020","Zhu, M.; Li, J.; Wang, N.; Gao, X.","Knowledge Distillation for Face Photo-Sketch Synthesis","IEEE Transactions on Neural Networks and Learning Systems","","2162-2388","10.1109/TNNLS.2020.3030536","","Significant progress has been made with face photo-sketch synthesis in recent years due to the development of deep convolutional neural networks, particularly generative adversarial networks (GANs). However, the performance of existing methods is still limited because of the lack of training data (photo-sketch pairs). To address this challenge, we investigate the effect of knowledge distillation (KD) on training neural networks for the face photo-sketch synthesis task and propose an effective KD model to improve the performance of synthetic images. In particular, we utilize a teacher network trained on a large amount of data in a related task to separately learn knowledge of the face photo and knowledge of the face sketch and simultaneously transfer this knowledge to two student networks designed for the face photo-sketch synthesis task. In addition to assimilating the knowledge from the teacher network, the two student networks can mutually transfer their own knowledge to further enhance their learning. To further enhance the perception quality of the synthetic image, we propose a KD+ model that combines GANs with KD. The generator can produce images with more realistic textures and less noise under the guide of knowledge. Extensive experiments and a user study demonstrate the superiority of our models over the state-of-the-art methods.","2020","2021-03-29 00:18:36","2021-04-06 15:06:48","","1-14","","","","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Neural Networks and Learning Systems","<div data-schema-version=""1""><p>Annotation</p> <p>Using Knowledge Distillation and GAN to translate images between face photo and sketch.</p></div>","C:\Users\controlnet\Zotero\storage\FZIX6WUE\9240973.html; G:\My Drive\Bibliography\Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf","","Unreviewed","Data models; Face recognition; Task analysis; Face photo-sketch synthesis; Faces; Gallium nitride; generative adversarial networks (GANs); knowledge distillation (KD); Knowledge engineering; teacher-student model.; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMPBRIPK","journalArticle","2020","Zheng, Xin; Guo, Yanqing; Huang, Huaibo; Li, Yi; He, Ran","A Survey of Deep Facial Attribute Analysis","International Journal of Computer Vision","","1573-1405","10.1007/s11263-020-01308-z","https://doi.org/10.1007/s11263-020-01308-z","Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.","2020-09-01","2021-03-16 14:46:55","2021-04-06 15:06:45","2021-03-16 14:46:55","2002-2034","","8","128","","Int J Comput Vis","","","","","","","","en","","","","","Springer Link","","","<div data-schema-version=""2""><p>Annotation</p> <p>Introduction to all facial analysis</p></div>","G:\My Drive\Bibliography\Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf","","Unreviewed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FLMXLVV","journalArticle","2020","Yuan, M.; Peng, Y.","CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis","IEEE Transactions on Multimedia","","1941-0077","10.1109/TMM.2019.2951463","","Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.","2020-08","2021-03-22 08:05:43","2021-04-06 15:06:40","","1955-1968","","8","22","","","CKD","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Multimedia","<div data-schema-version=""2""><p>Annotation</p> <p>Using knowledge distillation to perform text to image synthesis task.</p></div>","C:\Users\controlnet\Zotero\storage\DTFEDK7M\8890866.html; G:\My Drive\Bibliography\Yuan_Peng_2020_CKD.pdf","","Unreviewed","Generative adversarial networks; knowledge distillation; learning (artificial intelligence); Neural networks; Semantics; Task analysis; Visualization; approximate the real data distribution; CKD; complex RGB pixel space; consistent semantic contents; cross-task knowledge distillation approach; highly challenging task; huge pixel space; Image color analysis; image colour analysis; image contents; image semantic understanding; Image synthesis; multiple image semantic understanding tasks; multistage knowledge distillation paradigm; pattern recognition; photo-realistic images; realistic images; semantic consistency issue; semantic representation; semantic space; synthetic images; T2IS; text analysis; text description; text-to-image synthesis; Text-to-image synthesis; transfer learning; visual quality issue","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B3LJTGTN","journalArticle","2018","Carbonneau, Marc-André; Cheplygina, Veronika; Granger, Eric; Gagnon, Ghyslain","Multiple instance learning: A survey of problem characteristics and applications","Pattern Recognition","","0031-3203","10.1016/j.patcog.2017.10.009","https://www.sciencedirect.com/science/article/pii/S0031320317304065","Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research. Code is available on-line at https://github.com/macarbonneau/MILSurvey.","2018-05-01","2021-08-02 04:40:21","2021-08-02 04:40:21","2021-08-02 04:40:21","329-353","","","77","","Pattern Recognition","Multiple instance learning","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\controlnet\Zotero\storage\KRCUABCA\Carbonneau et al. - 2018 - Multiple instance learning A survey of problem ch.pdf; C:\Users\controlnet\Zotero\storage\SXF3JQUP\S0031320317304065.html","","","Computer vision; Classification; Computer aided diagnosis; Document classification; Drug activity prediction; Multi-instance learning; Multiple instance learning; Weakly supervised learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVUD8FU3","journalArticle","2021","Sun, Che; Song, Hao; Wu, Xinxiao; Jia, Yunde; Luo, Jiebo","Exploiting Informative Video Segments for Temporal Action Localization","IEEE Transactions on Multimedia","","1941-0077","10.1109/TMM.2021.3050067","","We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognizing actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segment-level attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks: THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with the state-of-the-art methods. Moreover, compared with the baseline method that equally treats video segments, STAN achieves significant improvements with the mAP increased from 30.4% to 39.8% on the THUMOS2014 dataset and from 31.4% to 35.9% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization.","2021","2021-08-03 04:01:02","2021-08-03 04:01:02","","1-1","","","","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Multimedia","","C:\Users\controlnet\Zotero\storage\9BWABMEP\9316901.html; G:\My Drive\Bibliography\Sun et al_2021_Exploiting Informative Video Segments for Temporal Action Localization.pdf","","","Feature extraction; Image segmentation; Location awareness; Temporal Action Localization; Aggregates; Attention Mechanism; Generators; Informative Video Segments; Motion segmentation; Proposals; Supervised Temporal Attention Network","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RJFNFG9","journalArticle","2021","Ding, Xinpeng; Wang, Nannan; Gao, Xinbo; Li, Jie; Wang, Xiaoyu; Liu, Tongliang","KFC: An Efficient Framework for Semi-supervised Temporal Action Localization","IEEE Transactions on Image Processing","","1941-0042","10.1109/TIP.2021.3099407","","In temporal action localization (TAL), semi-supervised learning is a promising technique to mitigate the cost of precise boundary annotations. Semi-supervised approaches employing consistency regularization (CR), encouraging models to be robust to the perturbed inputs, have achieved great success in image classification problems. The success of CR is largely depended on the perturbations, where instances are perturbed to train a robust model without altering their semantic information. However, the perturbations for image or video classification tasks are not fit to apply to TAL. Since videos in TAL are too long to train the model with raw videos in an end-to-end manner. In this paper, we devise a method named K-farthest crossover to construct perturbations based on video features and apply it to TAL. Motivated by the observation that features in the same action instance become more and more similar during the training process while those in different action instances or backgrounds become more and more divergent, we add perturbations to each feature along temporal axis and adopt CR to encourage the model to retain this observation. Specifically, for a feature, we first find the top-k dissimilar features and average them to form a perturbation. Then, similar to chromosomal crossover, we select a large part of the feature and a small part of the perturbation to recombine a perturbed feature, which preserves the feature semantics yet enough discrepancy.","2021","2021-08-03 04:01:41","2021-08-03 04:01:41","","1-1","","","","","","KFC","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Image Processing","","G:\My Drive\Bibliography\Ding et al_2021_KFC.pdf; C:\Users\controlnet\Zotero\storage\36WANIUK\9500051.html","","","Semantics; Training; Annotations; Feature extraction; Location awareness; Perturbation methods; semi-supervised learning; Semisupervised learning; Temporal Action Localization; video understanding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KITJLFYD","journalArticle","2021","Yang, Xiangli; Song, Zixing; King, Irwin; Xu, Zenglin","A Survey on Deep Semi-supervised Learning","arXiv:2103.00550 [cs]","","","","http://arxiv.org/abs/2103.00550","Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from model design perspectives and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we offer a detailed comparison of these methods in terms of the type of losses, contributions, and architecture differences. In addition to the past few years' progress, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.","2021-02-28","2021-08-03 17:02:21","2021-08-03 17:02:24","2021-08-03 17:02:21","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2103.00550","Comment: 24 pages, 6 figures","C:\Users\controlnet\Zotero\storage\C9UNYFVA\2103.html; G:\My Drive\Bibliography\Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43MC99CP","journalArticle","2017","Laine, Samuli; Aila, Timo","Temporal Ensembling for Semi-Supervised Learning","arXiv:1610.02242 [cs]","","","","http://arxiv.org/abs/1610.02242","In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.","2017-03-15","2021-08-04 08:01:03","2021-08-04 08:01:03","2021-08-04 08:01:03","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1610.02242","Comment: Final ICLR 2017 version. Includes new results for CIFAR-100 with additional unlabeled data from Tiny Images dataset","C:\Users\controlnet\Zotero\storage\N3BE367I\1610.html; G:\My Drive\Bibliography\Laine_Aila_2017_Temporal Ensembling for Semi-Supervised Learning.pdf","","","Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXNSJIKD","journalArticle","2018","Tarvainen, Antti; Valpola, Harri","Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results","arXiv:1703.01780 [cs, stat]","","","","http://arxiv.org/abs/1703.01780","The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.","2018-04-16","2021-08-04 08:03:32","2021-08-04 08:03:32","2021-08-04 08:03:32","","","","","","","Mean teachers are better role models","","","","","","","","","","","","arXiv.org","","arXiv: 1703.01780","Comment: In this version: Corrected hyperparameters of the 4000-label CIFAR-10 ResNet experiment. Changed Antti's contact info, Advances in Neural Information Processing Systems 30 (NIPS 2017) pre-proceedings","C:\Users\controlnet\Zotero\storage\W7GAKXQ5\1703.html; G:\My Drive\Bibliography\Tarvainen_Valpola_2018_Mean teachers are better role models.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJR4NL4B","journalArticle","2020","Xie, Qizhe; Dai, Zihang; Hovy, Eduard; Luong, Minh-Thang; Le, Quoc V.","Unsupervised Data Augmentation for Consistency Training","arXiv:1904.12848 [cs, stat]","","","","http://arxiv.org/abs/1904.12848","Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.","2020-11-05","2021-08-04 08:14:05","2021-08-04 08:14:05","2021-08-04 08:14:05","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1904.12848","Comment: NeurIPS 2020","C:\Users\controlnet\Zotero\storage\IPQ6IQFI\1904.html; G:\My Drive\Bibliography\Xie et al_2020_Unsupervised Data Augmentation for Consistency Training.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABVI8JIH","journalArticle","2020","Sohn, Kihyuk; Berthelot, David; Li, Chun-Liang; Zhang, Zizhao; Carlini, Nicholas; Cubuk, Ekin D.; Kurakin, Alex; Zhang, Han; Raffel, Colin","FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence","arXiv:2001.07685 [cs, stat]","","","","http://arxiv.org/abs/2001.07685","Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.","2020-11-25","2021-08-04 12:30:34","2021-08-04 12:30:34","2021-08-04 12:30:34","","","","","","","FixMatch","","","","","","","","","","","","arXiv.org","","arXiv: 2001.07685","Comment: Published at NeurIPS 2020 as a conference paper","C:\Users\controlnet\Zotero\storage\H6BGNMWR\2001.html; G:\My Drive\Bibliography\Sohn et al_2020_FixMatch.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"REJDYJKP","conferencePaper","2016","Salimans, Tim; Goodfellow, Ian; Zaremba, Wojciech; Cheung, Vicki; Radford, Alec; Chen, Xi","Improved techniques for training GANs","Proceedings of the 30th International Conference on Neural Information Processing Systems","978-1-5108-3881-9","","","","We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.","2016-12-05","2021-08-04 12:38:40","2021-08-04 12:38:40","2021-08-04","2234–2242","","","","","","","NIPS'16","","","","Curran Associates Inc.","Red Hook, NY, USA","","","","","","ACM Digital Library","","","","G:\My Drive\Bibliography\Salimans et al_2016_Improved techniques for training GANs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8M4LE3WD","conferencePaper","2019","Kim, Youngdong; Yim, Junho; Yun, Juseung; Kim, Junmo","NLNL: Negative Learning for Noisy Labels","","","","","https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.html","","2019","2021-08-04 13:11:11","2021-08-04 13:11:11","2021-08-04 13:11:11","101-110","","","","","","NLNL","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Kim et al_2019_NLNL.pdf; C:\Users\controlnet\Zotero\storage\G3KPZ8CR\Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF International Conference on Computer Vision","","","","","","","","","","","","","","",""
"KYNGBBW9","conferencePaper","2016","Yeung, Serena; Russakovsky, Olga; Mori, Greg; Fei-Fei, Li","End-To-End Learning of Action Detection From Frame Glimpses in Videos","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2016/html/Yeung_End-To-End_Learning_of_CVPR_2016_paper.html","In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and whether to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's task-specific decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% or less) of the video frames.","2016","2021-08-04 13:49:04","2021-08-04 13:49:24","2021-08-04 13:49:04","2678-2687","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\QUFKHF9E\Yeung_End-To-End_Learning_of_CVPR_2016_paper.html; G:\My Drive\Bibliography\Yeung et al_2016_End-To-End Learning of Action Detection From Frame Glimpses in Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"97UVSK9V","conferencePaper","2016","Yuan, Jun; Ni, Bingbing; Yang, Xiaokang; Kassim, Ashraf A.","Temporal Action Localization With Pyramid of Score Distribution Features","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2016/html/Yuan_Temporal_Action_Localization_CVPR_2016_paper.html","We investigate the feature design and classification architectures in temporal action localization. This application focuses on detecting and labeling actions in untrimmed videos, which brings more challenge than classifying pre-segmented videos. The major difficulty for action localization is the uncertainty of action occurrence and utilization of information from different scales. Two innovations are proposed to address this issue. First, we propose a Pyramid of Score Distribution Feature (PSDF) to capture the motion information at multiple resolutions centered at each detection window. This novel feature mitigates the influence of unknown action position and duration, and shows significant performance gain over previous detection approaches. Second, inter-frame consistency is further explored by incorporating PSDF into the state-of-the-art Recurrent Neural Networks, which gives additional performance gain in detecting actions in temporally untrimmed videos. We tested our action localization framework on the THUMOS'15 and MPII Cooking Activities Dataset, both of which show a large performance improvement over previous attempts.","2016","2021-08-04 13:50:03","2021-08-04 13:50:19","2021-08-04 13:50:03","3093-3102","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\2YX7GE92\Yuan_Temporal_Action_Localization_CVPR_2016_paper.html; G:\My Drive\Bibliography\Yuan et al_2016_Temporal Action Localization With Pyramid of Score Distribution Features.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"IWL24P7N","conferencePaper","2016","Shou, Zheng; Wang, Dongang; Chang, Shih-Fu","Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2016/html/Shou_Temporal_Action_Localization_CVPR_2016_paper.html","We address temporal action localization in untrimmed long videos. This is important because videos in real applications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal action localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as initialization for the localization network; and (3) a localization network fine-tunes the learned classification network to localize each action instance. We propose a novel loss function for the localization network to explicitly consider temporal overlap and achieve high temporal localization accuracy. In the end, only the proposal network and the localization network are used during prediction. On two large-scale benchmarks, our approach achieves significantly superior performances compared with other state-of-the-art systems: mAP increases from 1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014.","2016","2021-08-04 13:53:08","2021-08-04 13:53:18","2021-08-04 13:53:08","1049-1058","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Shou et al_2016_Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs.pdf; C:\Users\controlnet\Zotero\storage\H9IJ85TD\Shou_Temporal_Action_Localization_CVPR_2016_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"XZ5TU439","conferencePaper","2017","Zhu, Yi; Newsam, Shawn","Efficient Action Detection in Untrimmed Videos via Multi-task Learning","2017 IEEE Winter Conference on Applications of Computer Vision (WACV)","","","10.1109/WACV.2017.29","","This paper studies the joint learning of action recognition and temporal localization in long, untrimmed videos. We employ a multi-task learning framework that performs the three highly related steps of action proposal, action recognition, and action localization refinement in parallel instead of the standard sequential pipeline that performs the steps in order. We develop a novel temporal actionness regression module that estimates what proportion of a clip contains action. We use it for temporal localization but it could have other applications like video retrieval, surveillance, summarization, etc. We also introduce random shear augmentation during training to simulate viewpoint change. We evaluate our framework on three popular video benchmarks. Results demonstrate that our joint model is efficient in terms of storage and computation in that we do not need to compute and cache dense trajectory features, and that it is several times faster than its sequential ConvNets counterpart. Yet, despite being more efficient, it outperforms stateof-the-art methods with respect to accuracy.","2017-03","2021-08-04 13:54:28","2021-08-04 13:54:28","","197-206","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","C:\Users\controlnet\Zotero\storage\HYAUW7KQ\7926612.html; G:\My Drive\Bibliography\Zhu_Newsam_2017_Efficient Action Detection in Untrimmed Videos via Multi-task Learning.pdf","","","Training; Computational modeling; Videos; Training data; Proposals; Three-dimensional displays","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2017 IEEE Winter Conference on Applications of Computer Vision (WACV)","","","","","","","","","","","","","","",""
"J7GQGGXU","conferencePaper","2017","Shou, Zheng; Chan, Jonathan; Zareian, Alireza; Miyazawa, Kazuyuki; Chang, Shih-Fu","CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2017/html/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html","Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket.org/columbiadvmm/cdc.","2017","2021-08-04 17:11:41","2021-08-04 17:11:51","2021-08-04 17:11:41","5734-5743","","","","","","CDC","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Shou et al_2017_CDC.pdf; C:\Users\controlnet\Zotero\storage\U9BC2LLV\Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"EKSITX9M","conferencePaper","2017","Zhao, Yue; Xiong, Yuanjun; Wang, Limin; Wu, Zhirong; Tang, Xiaoou; Lin, Dahua","Temporal Action Detection With Structured Segment Networks","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Zhao_Temporal_Action_Detection_ICCV_2017_paper.html","Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS'14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.","2017","2021-08-04 17:12:17","2021-08-04 17:12:22","2021-08-04 17:12:17","2914-2923","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\I2YM839G\Zhao_Temporal_Action_Detection_ICCV_2017_paper.html; G:\My Drive\Bibliography\Zhao et al_2017_Temporal Action Detection With Structured Segment Networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"8TBA3Q7C","conferencePaper","2017","Dai, Xiyang; Singh, Bharat; Zhang, Guyue; Davis, Larry S.; Qiu Chen, Yan","Temporal Context Network for Activity Localization in Videos","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Temporal_Context_Network_ICCV_2017_paper.html","We present a Temporal Context Network (TCN) for precise temporal localization of human activities. Similar to the Faster-RCNN architecture, proposals are placed at equal intervals in a video which span multiple temporal scales. We propose a novel representation for ranking these proposals. Since pooling features only inside a segment is not sufficient to predict activity boundaries, we construct a representation which explicitly captures context around a proposal for ranking it. For each temporal segment inside a proposal, features are uniformly sampled at a pair of scales and are input to a temporal convolutional neural network for classification. After ranking proposals, non-maximum suppression is applied and classification is performed to obtain final detections. TCN outperforms state-of-the-art methods on the ActivityNet dataset and the THUMOS14 dataset.","2017","2021-08-04 17:12:40","2021-08-04 17:12:48","2021-08-04 17:12:40","5793-5802","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Dai et al_2017_Temporal Context Network for Activity Localization in Videos.pdf; C:\Users\controlnet\Zotero\storage\C2H25BAL\Dai_Temporal_Context_Network_ICCV_2017_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"FFR3P8JX","journalArticle","2017","Gao, Jiyang; Yang, Zhenheng; Nevatia, Ram","Cascaded Boundary Regression for Temporal Action Detection","arXiv:1705.01180 [cs]","","","","http://arxiv.org/abs/1705.01180","Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classifiers on sliding windows. Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and specific actions are detected respectively in the first and the second stage. CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows. The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.","2017-05-02","2021-08-04 17:13:34","2021-08-04 17:13:34","2021-08-04 17:13:34","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 1705.01180","","C:\Users\controlnet\Zotero\storage\N6897AEH\1705.html; G:\My Drive\Bibliography\Gao et al_2017_Cascaded Boundary Regression for Temporal Action Detection.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GN8KTIQD","conferencePaper","2017","Xu, Huijuan; Das, Abir; Saenko, Kate","R-C3D: Region Convolutional 3D Network for Temporal Activity Detection","Proceedings of the IEEE International Conference on Computer Vision","","","","https://openaccess.thecvf.com/content_iccv_2017/html/Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.html","We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at http://ai.bu.edu/r-c3d/.","2017","2021-08-04 17:13:51","2021-08-04 17:13:56","2021-08-04 17:13:51","5783-5792","","","","","","R-C3D","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\II5QYZN3\Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.html; G:\My Drive\Bibliography\Xu et al_2017_R-C3D.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"P5QYLNEQ","webpage","","","Single Shot Temporal Action Detection | Proceedings of the 25th ACM international conference on Multimedia","","","","","https://dl.acm.org/doi/abs/10.1145/3123266.3123343","","","2021-08-04 17:14:16","2021-08-04 17:14:16","2021-08-04 17:14:16","","","","","","","","","","","","","","","","","","","","","","","C:\Users\controlnet\Zotero\storage\SGT7UVS3\3123266.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIAPTBFD","conferencePaper","2017","Lin, Tianwei; Zhao, Xu; Shou, Zheng","Single Shot Temporal Action Detection","Proceedings of the 25th ACM international conference on Multimedia","978-1-4503-4906-2","","10.1145/3123266.3123343","http://doi.org/10.1145/3123266.3123343","Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the ""detection by classification"" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from $19.0%$ to $24.6%$ on THUMOS 2014 and from 7.4% to $11.0%$ on MEXaction2.","2017-10-19","2021-08-04 17:14:36","2021-08-04 17:14:45","2021-08-04","988–996","","","","","","","MM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","G:\My Drive\Bibliography\Lin et al_2017_Single Shot Temporal Action Detection.pdf","","","ssad network; temporal action detection; untrimmed video","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 25th ACM international conference on Multimedia","","","","","","","","","","","","","","",""
"ZZ2RFWKJ","conferencePaper","2018","Lin, Tianwei; Zhao, Xu; Su, Haisheng; Wang, Chongjing; Yang, Ming","BSN: Boundary Sensitive Network for Temporal Action Proposal Generation","Proceedings of the European Conference on Computer Vision (ECCV)","","","","https://openaccess.thecvf.com/content_ECCV_2018/html/Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper.html","Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts ""local to global"" fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.","2018","2021-08-04 17:15:09","2021-08-04 17:15:23","2021-08-04 17:15:09","3-19","","","","","","BSN","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Lin et al_2018_BSN.pdf; C:\Users\controlnet\Zotero\storage\VH4WAC5E\Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV)","","","","","","","","","","","","","","",""
"VT5ZD4IX","journalArticle","2019","Buch, Shyamal; Escorcia, Victor; Ghanem, Bernard; Fei-Fei, Li; Niebles, Juan Carlos","End-to-end, single-stream temporal action detection in untrimmed videos","Procedings of the British Machine Vision Conference 2017","","","10.5244/c.31.93","https://research.kaust.edu.sa/en/publications/end-to-end-single-stream-temporal-action-detection-in-untrimmed-v","In this work, we present a new intuitive, end-to-end approach for temporal action detection in untrimmed videos. We introduce our new architecture for Single-Stream Temporal Action Detection (SS-TAD), which effectively integrates joint action detection with its semantic sub-tasks in a single unifying end-to-end framework. We develop a method for training our deep recurrent architecture based on enforcing semantic constraints on intermediate modules that are gradually relaxed as learning progresses. We find that such a dynamic learning scheme enables SS-TAD to achieve higher overall detection performance, with fewer training epochs. By design, our single-pass network is very efficient and can operate at 701 frames per second, while simultaneously outperforming the state-of-the-art methods for temporal action detection on THUMOS’14.","2019-05-01","2021-08-04 17:15:54","2021-08-04 17:16:06","2021-08-04 17:15:54","","","","","","","","","","","","","","English (US)","","","","","research.kaust.edu.sa","","Publisher: British Machine Vision Association","","G:\My Drive\Bibliography\Buch et al_2019_End-to-end, single-stream temporal action detection in untrimmed videos.pdf; C:\Users\controlnet\Zotero\storage\YUVIFUW9\end-to-end-single-stream-temporal-action-detection-in-untrimmed-v.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KQNK4I8G","journalArticle","2018","Yang, Ke; Qiao, Peng; Li, Dongsheng; Lv, Shaohe; Dou, Yong","Exploring Temporal Preservation Networks for Precise Temporal Action Localization","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","","https://ojs.aaai.org/index.php/AAAI/article/view/12234","Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments pre-determined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization with minimal loss of time information. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement in both per-frame action prediction and segment-level temporal action localization.","2018-04-27","2021-08-04 17:16:40","2021-08-04 17:16:47","2021-08-04 17:16:40","","","1","32","","","","","","","","","","en","Copyright (c)","","","","ojs.aaai.org","","Number: 1","","G:\My Drive\Bibliography\Yang et al_2018_Exploring Temporal Preservation Networks for Precise Temporal Action.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRBFVA2E","conferencePaper","2018","Huang, Jingjia; Li, Nannan; Zhang, Tao; Li, Ge; Huang, Tiejun; Gao, Wen","SAP: Self-Adaptive Proposal Model for Temporal Action Detection Based on Reinforcement Learning","Thirty-Second AAAI Conference on Artificial Intelligence","","","","https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16109","Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose a Self-Adaptive Proposal (SAP) model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at the beginning of the video and traverse the whole video by adopting a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent’s decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS’14 validate the effectiveness of SAP, which can achieve competitive performance with current action detection algorithms via much fewer proposals.","2018-04-27","2021-08-04 17:17:13","2021-08-04 17:17:13","2021-08-04 17:17:13","","","","","","","SAP","","","","","","","en","Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.","","","","www.aaai.org","","","","G:\My Drive\Bibliography\Huang et al_2018_SAP.pdf; C:\Users\controlnet\Zotero\storage\AP454MZS\16109.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Thirty-Second AAAI Conference on Artificial Intelligence","","","","","","","","","","","","","","",""
"IHGZ35U2","conferencePaper","2018","Qiu, Haonan; Zheng, Yingbin; Ye, Hao; Lu, Yao; Wang, Feng; He, Liang","Precise Temporal Action Localization by Evolving Temporal Proposals","Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval","978-1-4503-5046-4","","10.1145/3206025.3206029","http://doi.org/10.1145/3206025.3206029","Locating actions in long untrimmed videos has been a challenging problem in video content analysis. The performances of existing action localization approaches remain unsatisfactory in precisely determining the beginning and the end of an action. Imitating the human perception procedure with observations and refinements, we propose a novel three-phase action localization framework. Our framework is embedded with an Actionness Network to generate initial proposals through frame-wise similarity grouping, and then a Refinement Network to conduct boundary adjustment on these proposals. Finally, the refined proposals are sent to a Localization Network for further fine-grained location regression. The whole process can be deemed as multi-stage refinement using a novel non-local pyramid feature under various temporal granularities. We evaluate our framework on THUMOS14 benchmark and obtain a significant improvement over the state-of-the-arts approaches. Specifically, the performance gain is remarkable under precise localization with high IoU thresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2%.","2018-06-05","2021-08-04 17:17:29","2021-08-04 17:17:32","2021-08-04","388–396","","","","","","","ICMR '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","G:\My Drive\Bibliography\Qiu et al_2018_Precise Temporal Action Localization by Evolving Temporal Proposals.pdf","","","action localization; deep neural network; temporal proposal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval","","","","","","","","","","","","","","",""
"UHGDAUE8","conferencePaper","2018","Chao, Yu-Wei; Vijayanarasimhan, Sudheendra; Seybold, Bryan; Ross, David A.; Deng, Jia; Sukthankar, Rahul","Rethinking the Faster R-CNN Architecture for Temporal Action Localization","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html","We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.","2018","2021-08-04 17:17:48","2021-08-06 03:24:41","2021-08-04 17:17:48","1130-1139","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Chao et al_2018_Rethinking the Faster R-CNN Architecture for Temporal Action Localization.pdf; C:\Users\controlnet\Zotero\storage\9J99MCZT\Chao_Rethinking_the_Faster_CVPR_2018_paper.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"YUSX8JPM","conferencePaper","2021","Qing, Zhiwu; Su, Haisheng; Gan, Weihao; Wang, Dongliang; Wu, Wei; Wang, Xiang; Qiao, Yu; Yan, Junjie; Gao, Changxin; Sang, Nong","Temporal Context Aggregation Network for Temporal Action Proposal Refinement","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content/CVPR2021/html/Qing_Temporal_Context_Aggregation_Network_for_Temporal_Action_Proposal_Refinement_CVPR_2021_paper.html","Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through local and global temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both local and global temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task.","2021","2021-08-06 03:17:35","2021-08-06 03:17:43","2021-08-06 03:17:35","485-494","","","","","","","","","","","","","en","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Qing et al_2021_Temporal Context Aggregation Network for Temporal Action Proposal Refinement.pdf; C:\Users\controlnet\Zotero\storage\DXUYT3DW\Qing_Temporal_Context_Aggregation_Network_for_Temporal_Action_Proposal_Refinement_CVPR_2021_pap.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"QAAHW6KL","journalArticle","2021","Tan, Jing; Tang, Jiaqi; Wang, Limin; Wu, Gangshan","Relaxed Transformer Decoders for Direct Action Proposal Generation","arXiv:2102.01894 [cs]","","","","http://arxiv.org/abs/2102.01894","Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action instances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and end-to-end learnable framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer encoder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models will be made available at https://github.com/MCG-NJU/RTD-Action.","2021-04-07","2021-08-06 03:23:56","2021-08-06 03:23:56","2021-08-06 03:23:56","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2102.01894","Comment: Technical report","C:\Users\controlnet\Zotero\storage\HV3MYFBC\2102.html; G:\My Drive\Bibliography\Tan et al_2021_Relaxed Transformer Decoders for Direct Action Proposal Generation.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NPF8L9NN","journalArticle","2021","Su, Haisheng; Gan, Weihao; Wu, Wei; Qiao, Yu; Yan, Junjie","BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation","arXiv:2009.07641 [cs]","","","","http://arxiv.org/abs/2009.07641","Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.","2021-03-01","2021-08-06 03:24:20","2021-08-06 03:24:20","2021-08-06 03:24:20","","","","","","","BSN++","","","","","","","","","","","","arXiv.org","","arXiv: 2009.07641","Comment: Accepted by AAAI 2021. Ranked 1st place in the CVPR19 - ActivityNet Challenge leaderboard on Temporal Action Localization task. arXiv admin note: substantial text overlap with arXiv:2007.09883","C:\Users\controlnet\Zotero\storage\KDXASUC4\2009.html; G:\My Drive\Bibliography\Su et al_2021_BSN++.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47G5ZBPC","conferencePaper","2020","Prajwal, K R; Mukhopadhyay, Rudrabha; Namboodiri, Vinay P.; Jawahar, C.V.","A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild","Proceedings of the 28th ACM International Conference on Multimedia","978-1-4503-7988-5","","10.1145/3394171.3413532","http://doi.org/10.1145/3394171.3413532","In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model, and also publicly release the code, models, and evaluation benchmarks on our website.","2020-10-12","2021-08-06 07:21:17","2021-08-06 07:21:23","2021-08-06","484–492","","","","","","","MM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","G:\My Drive\Bibliography\Prajwal et al_2020_A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild.pdf","","","lip sync; talking face generation; video generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QD5L8XMY","journalArticle","2017","Wang, Yuxuan; Skerry-Ryan, R. J.; Stanton, Daisy; Wu, Yonghui; Weiss, Ron J.; Jaitly, Navdeep; Yang, Zongheng; Xiao, Ying; Chen, Zhifeng; Bengio, Samy; Le, Quoc; Agiomyrgiannakis, Yannis; Clark, Rob; Saurous, Rif A.","Tacotron: Towards End-to-End Speech Synthesis","arXiv:1703.10135 [cs]","","","","http://arxiv.org/abs/1703.10135","A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.","2017-04-06","2021-08-06 07:25:28","2021-08-06 07:25:28","2021-08-06 07:25:28","","","","","","","Tacotron","","","","","","","","","","","","arXiv.org","","arXiv: 1703.10135","Comment: Submitted to Interspeech 2017. v2 changed paper title to be consistent with our conference submission (no content change other than typo fixes)","C:\Users\controlnet\Zotero\storage\8HYCZNNI\1703.html; G:\My Drive\Bibliography\Wang et al_2017_Tacotron.pdf","","","Computer Science - Machine Learning; Computer Science - Computation and Language; Computer Science - Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NAQUJ2M","conferencePaper","2018","Shen, Jonathan; Pang, Ruoming; Weiss, Ron J.; Schuster, Mike; Jaitly, Navdeep; Yang, Zongheng; Chen, Zhifeng; Zhang, Yu; Wang, Yuxuan; Skerrv-Ryan, Rj; Saurous, Rif A.; Agiomvrgiannakis, Yannis; Wu, Yonghui","Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","10.1109/ICASSP.2018.8461368","","This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.","2018-04","2021-08-06 07:26:21","2021-08-06 07:26:27","","4779-4783","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","C:\Users\controlnet\Zotero\storage\E5WQYRWA\8461368.html; G:\My Drive\Bibliography\Shen et al_2018_Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions.pdf","","","Training; Acoustics; Decoding; Linguistics; Spectrogram; Tacotron 2; text-to-speech; Time-domain analysis; Vocoders; WaveNet","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","","","","","","","","","","","","",""
"9NT4QMEQ","conferencePaper","2018","Jia, Ye; Zhang, Yu; Weiss, Ron J.; Wang, Quan; Shen, Jonathan; Ren, Fei; Chen, Zhifeng; Nguyen, Patrick; Pang, Ruoming; Moreno, Ignacio Lopez; Wu, Yonghui","Transfer learning from speaker verification to multispeaker text-to-speech synthesis","Proceedings of the 32nd International Conference on Neural Information Processing Systems","","","","","We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.","2018-12-03","2021-08-06 12:04:07","2021-08-06 12:04:11","2021-08-06","4485–4495","","","","","","","NIPS'18","","","","Curran Associates Inc.","Red Hook, NY, USA","","","","","","ACM Digital Library","","","","G:\My Drive\Bibliography\Jia et al_2018_Transfer learning from speaker verification to multispeaker text-to-speech.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQNY665Q","journalArticle","2021","Tan, Xu; Qin, Tao; Soong, Frank; Liu, Tie-Yan","A Survey on Neural Speech Synthesis","arXiv:2106.15561 [cs, eess]","","","","http://arxiv.org/abs/2106.15561","Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.","2021-07-23","2021-08-11 08:05:27","2021-08-11 08:05:28","2021-08-11 08:05:27","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2106.15561","Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references","C:\Users\controlnet\Zotero\storage\7X5H8MFN\2106.html; G:\My Drive\Bibliography\Tan et al_2021_A Survey on Neural Speech Synthesis.pdf","","","Computer Science - Machine Learning; Computer Science - Computation and Language; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Multimedia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9W7R9ZX8","journalArticle","2021","Hao, Zekun; Mallya, Arun; Belongie, Serge; Liu, Ming-Yu","GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds","arXiv:2104.07659 [cs]","","","","http://arxiv.org/abs/2104.07659","We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .","2021-04-15","2021-08-11 08:06:22","2021-08-11 08:06:22","2021-08-11 08:06:22","","","","","","","GANcraft","","","","","","","","","","","","arXiv.org","","arXiv: 2104.07659","","C:\Users\controlnet\Zotero\storage\BT3FKSJT\2104.html; G:\My Drive\Bibliography\Hao et al_2021_GANcraft.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R5L3EYJL","journalArticle","2021","Chen, Mingjian; Tan, Xu; Li, Bohan; Liu, Yanqing; Qin, Tao; Zhao, Sheng; Liu, Tie-Yan","AdaSpeech: Adaptive Text to Speech for Custom Voice","arXiv:2103.00993 [cs, eess]","","","","http://arxiv.org/abs/2103.00993","Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech data. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions that could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we use two acoustic encoders to extract an utterance-level vector and a sequence of phoneme-level vectors from the target speech during training; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. Audio samples are available at https://speechresearch.github.io/adaspeech/.","2021-03-01","2021-08-11 10:12:20","2021-08-11 10:12:20","2021-08-11 10:12:20","","","","","","","AdaSpeech","","","","","","","","","","","","arXiv.org","","arXiv: 2103.00993","Comment: Accepted by ICLR 2021","C:\Users\controlnet\Zotero\storage\6F86Z8ZC\2103.html; G:\My Drive\Bibliography\Chen et al_2021_AdaSpeech.pdf","","","Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUBFZD9B","journalArticle","2021","Casanova, Edresson; Shulby, Christopher; Gölge, Eren; Müller, Nicolas Michael; de Oliveira, Frederico Santos; Junior, Arnaldo Candido; Soares, Anderson da Silva; Aluisio, Sandra Maria; Ponti, Moacir Antonelli","SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model","arXiv:2104.05557 [cs, eess]","","","","http://arxiv.org/abs/2104.05557","In this paper, we propose SC-GlowTTS: an efficient zero-shot multi-speaker text-to-speech model that improves similarity for speakers unseen during training. We propose a speaker-conditional architecture that explores a flow-based decoder that works in a zero-shot scenario. As text encoders, we explore a dilated residual convolutional-based encoder, gated convolutional-based encoder, and transformer-based encoder. Additionally, we have shown that adjusting a GAN-based vocoder for the spectrograms predicted by the TTS model on the training dataset can significantly improve the similarity and speech quality for new speakers. Our model converges using only 11 speakers, reaching state-of-the-art results for similarity with new speakers, as well as high speech quality.","2021-06-15","2021-08-11 10:13:07","2021-08-11 10:13:07","2021-08-11 10:13:07","","","","","","","SC-GlowTTS","","","","","","","","","","","","arXiv.org","","arXiv: 2104.05557","Comment: Accepted on Interspeech 2021","C:\Users\controlnet\Zotero\storage\WTFRCGKT\2104.html; G:\My Drive\Bibliography\Casanova et al_2021_SC-GlowTTS.pdf","","","Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7SZ7T3G","journalArticle","2020","Chen, Renwang; Chen, Xuanhong; Ni, Bingbing; Ge, Yanhao","SimSwap: An Efficient Framework For High Fidelity Face Swapping","Proceedings of the 28th ACM International Conference on Multimedia","","","10.1145/3394171.3413630","http://arxiv.org/abs/2106.06340","We propose an efficient framework, called Simple Swap (SimSwap), aiming for generalized and high fidelity face swapping. In contrast to previous approaches that either lack the ability to generalize to arbitrary identity or fail to preserve attributes like facial expression and gaze direction, our framework is capable of transferring the identity of an arbitrary source face into an arbitrary target face while preserving the attributes of the target face. We overcome the above defects in the following two ways. First, we present the ID Injection Module (IIM) which transfers the identity information of the source face into the target face at feature level. By using this module, we extend the architecture of an identity-specific face swapping algorithm to a framework for arbitrary face swapping. Second, we propose the Weak Feature Matching Loss which efficiently helps our framework to preserve the facial attributes in an implicit way. Extensive experiments on wild faces demonstrate that our SimSwap is able to achieve competitive identity performance while preserving attributes better than previous state-of-the-art methods. The code is already available on github: https://github.com/neuralchen/SimSwap.","2020-10-12","2021-08-12 14:59:07","2021-08-12 14:59:09","2021-08-12 14:59:07","2003-2011","","","","","","SimSwap","","","","","","","","","","","","arXiv.org","","arXiv: 2106.06340","Comment: Accepted by ACMMM 2020","C:\Users\controlnet\Zotero\storage\KXMX6LR6\2106.html; G:\My Drive\Bibliography\Chen et al_2020_SimSwap.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"67GQTPBY","journalArticle","2021","Khalid, Hasam; Tariq, Shahroz; Woo, Simon S.","FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset","arXiv:2108.05080 [cs]","","","","http://arxiv.org/abs/2108.05080","With the significant advancements made in generation of forged video and audio, commonly known as deepfakes, using deep learning technologies, the problem of its misuse is a well-known issue now. Recently, a new problem of generating cloned or synthesized human voice of a person is emerging. AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake videos and audios, new deepfake detectors are need that focuses on both, video and audio. Detecting deepfakes is a challenging task and researchers have made numerous attempts and proposed several deepfake detection methods. To develop a good deepfake detector, a handsome amount of good quality dataset is needed that captures the real world scenarios. Many researchers have contributed in this cause and provided several deepfake dataset, self generated and in-the-wild. However, almost all of these datasets either contains deepfake videos or audio. Moreover, the recent deepfake datasets proposed by researchers have racial bias issues. Hence, there is a crucial need of a good deepfake video and audio deepfake dataset. To fill this gap, we propose a novel Audio-Video Deepfake dataset (FakeAVCeleb) that not only contains deepfake videos but respective synthesized cloned audios as well. We generated our dataset using recent most popular deepfake generation methods and the videos and audios are perfectly lip-synced with each other. To generate a more realistic dataset, we selected real YouTube videos of celebrities having four racial backgrounds (Caucasian, Black, East Asian and South Asian) to counter the racial bias issue. Lastly, we propose a novel multimodal detection method that detects deepfake videos and audios based on our multimodal Audio-Video deepfake dataset.","2021-08-11","2021-08-17 05:47:40","2021-08-17 05:47:42","2021-08-17 05:47:40","","","","","","","FakeAVCeleb","","","","","","","","","","","","arXiv.org","","arXiv: 2108.05080","","C:\Users\controlnet\Zotero\storage\DI4HARK9\2108.html; G:\My Drive\Bibliography\Khalid et al_2021_FakeAVCeleb.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Sound; I.4.9; I.5.4","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WHSXQME","conferencePaper","2021","Lahiri, Avisek; Kwatra, Vivek; Frueh, Christian; Lewis, John; Bregler, Chris","LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces From Video Using Pose and Lighting Normalization","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content/CVPR2021/html/Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_paper.html","In this paper, we present a video-based learning framework for animating personalized 3D talking faces from audio. We introduce two training-time data normalizations that significantly improve data sample efficiency. First, we isolate and represent faces in a normalized space that decouples 3D geometry, head pose, and texture. This decomposes the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo constancy of skin to isolate and remove spatiotemporal lighting variations. Together, these normalizations allow simple networks to generate high fidelity lip-sync videos under novel ambient illumination while training with just a single video (of usually < 5 minutes). Further, to stabilize temporal dynamics, we introduce an auto-regressive approach that conditions the model on its previous visual state. Human ratings and objective metrics demonstrate that our method outperforms contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications enabled by our framework.","2021","2021-08-24 04:31:06","2021-08-24 04:31:27","2021-08-24 04:31:06","2755-2764","","","","","","LipSync3D","","","","","","","en","","","","","openaccess.thecvf.com","","","","G:\My Drive\Bibliography\Lahiri et al_2021_LipSync3D.pdf; C:\Users\controlnet\Zotero\storage\KNN54D4I\Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"JB9FL6R9","journalArticle","2021","Liu, Jinglin; Zhu, Zhiying; Ren, Yi; Zhao, Zhou","High-Speed and High-Quality Text-to-Lip Generation","arXiv:2107.06831 [cs]","","","","http://arxiv.org/abs/2107.06831","As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video. Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner. However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation. This encourages the research of parallel T2L generation. In this work, we propose a novel parallel decoding model for high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner. Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem. Extensive experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L generates lip movements with competitive quality compared with the state-of-the-art AR T2L model DualLip and exceeds the baseline AR model TransformerT2L by a notable margin benefiting from the mitigation of the error propagation problem; and 2) exhibits distinct superiority in inference speed (an average speedup of 19$\times$ than DualLip on TCD-TIMIT).","2021-07-14","2021-08-24 04:32:43","2021-08-24 04:32:43","2021-08-24 04:32:43","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2107.06831","Comment: Author draft","C:\Users\controlnet\Zotero\storage\VR9ITL8S\2107.html; G:\My Drive\Bibliography\Liu et al_2021_High-Speed and High-Quality Text-to-Lip Generation.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Multimedia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VSW4Y32Z","journalArticle","2021","Zhang, Chenxu; Zhao, Yifan; Huang, Yifei; Zeng, Ming; Ni, Saifeng; Budagavi, Madhukar; Guo, Xiaohu","FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning","arXiv:2108.07938 [cs]","","","","http://arxiv.org/abs/2108.07938","In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the attention map of eye blinks as input to generate the photo-realistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods.","2021-08-17","2021-08-24 04:33:14","2021-08-24 04:33:14","2021-08-24 04:33:14","","","","","","","FACIAL","","","","","","","","","","","","arXiv.org","","arXiv: 2108.07938","Comment: 10 pages, 9 figures. Accepted by ICCV 2021","C:\Users\controlnet\Zotero\storage\2SX6VRU8\2108.html; G:\My Drive\Bibliography\Zhang et al_2021_FACIAL.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JNTKSI9","conferencePaper","2021","Zhou, Hang; Sun, Yasheng; Wu, Wayne; Loy, Chen Change; Wang, Xiaogang; Liu, Ziwei","Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Representation_CVPR_2021_paper.html","While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on non-aligned raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework. Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are controllable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization.","2021","2021-08-24 04:36:57","2021-08-24 04:37:04","2021-08-24 04:36:57","4176-4186","","","","","","","","","","","","","en","","","","","openaccess.thecvf.com","","","","C:\Users\controlnet\Zotero\storage\FJNUDUHN\Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Represent.html; G:\My Drive\Bibliography\Zhou et al_2021_Pose-Controllable Talking Face Generation by Implicitly Modularized.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"8ADATKGA","journalArticle","2021","Qian, Shenhan; Tu, Zhi; Zhi, YiHao; Liu, Wen; Gao, Shenghua","Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates","arXiv:2108.08020 [cs]","","","","http://arxiv.org/abs/2108.08020","Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we design a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity. For our method, the template vector determines the general appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both indispensable for synthesizing a realistic gesture sequence. Due to the intractability of an objective metric for gesture-speech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization ability of our model. Extensive experiments show the superiority of our method in both objective and subjective evaluations on fidelity and synchronization.","2021-08-18","2021-08-24 04:37:18","2021-08-24 04:37:18","2021-08-24 04:37:18","","","","","","","Speech Drives Templates","","","","","","","","","","","","arXiv.org","","arXiv: 2108.08020","Comment: Accepted by ICCV 2021","C:\Users\controlnet\Zotero\storage\A7B4P3A8\2108.html; G:\My Drive\Bibliography\Qian et al_2021_Speech Drives Templates.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNL7HD6X","journalArticle","2021","Coccomini, Davide; Messina, Nicola; Gennaro, Claudio; Falchi, Fabrizio","Combining EfficientNet and Vision Transformers for Video Deepfake Detection","arXiv:2107.02612 [cs]","","","","http://arxiv.org/abs/2107.02612","Deepfakes are the result of digital manipulation to obtain credible videos in order to deceive the viewer. This is done through deep learning techniques based on autoencoders or GANs that become more accessible and accurate year after year, resulting in fake videos that are very difficult to distinguish from real ones. Traditionally, CNN networks have been used to perform deepfake detection, with the best results obtained using methods based on EfficientNet B7. In this study, we combine various types of Vision Transformers with a convolutional EfficientNet B0 used as a feature extractor, obtaining comparable results with some very recent methods that use Vision Transformers. Differently from the state-of-the-art approaches, we use neither distillation nor ensemble methods. The best model achieved an AUC of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the DeepFake Detection Challenge (DFDC).","2021-07-06","2021-09-15 22:00:31","2021-09-15 22:00:31","2021-09-15 22:00:31","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2107.02612 version: 1","","C:\Users\controlnet\Zotero\storage\BVI89ZBE\2107.html; G:\My Drive\Bibliography\Coccomini et al_2021_Combining EfficientNet and Vision Transformers for Video Deepfake Detection.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XEK4PZTR","journalArticle","2021","Heo, Young-Jin; Choi, Young-Ju; Lee, Young-Woon; Kim, Byung-Gyu","Deepfake Detection Scheme Based on Vision Transformer and Distillation","arXiv:2104.01353 [cs]","","","","http://arxiv.org/abs/2104.01353","Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition.","2021-04-03","2021-09-15 22:04:42","2021-09-15 22:04:42","2021-09-15 22:04:42","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2104.01353","Comment: 7 pages, 5 figures","C:\Users\controlnet\Zotero\storage\JYYL6RQ7\2104.html; G:\My Drive\Bibliography\Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHBZYBHY","journalArticle","2021","Bagchi, Anurag; Mahmood, Jazib; Fernandes, Dolton; Sarvadevabhatla, Ravi Kiran","Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization","arXiv:2106.14118 [cs]","","","","http://arxiv.org/abs/2106.14118","State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available at https://github.com/skelemoa/tal-hmo.","2021-08-19","2021-09-15 22:29:01","2021-09-15 22:29:01","2021-09-15 22:29:01","","","","","","","Hear Me Out","","","","","","","","","","","","arXiv.org","","arXiv: 2106.14118 version: 3","","C:\Users\controlnet\Zotero\storage\K87WNXUL\2106.html; G:\My Drive\Bibliography\Bagchi et al_2021_Hear Me Out.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Multimedia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""