var tree =
[
  {
    "text": "3D Models",
    "item-id": "c15,i1243",
    "nodes": [
      {
        "text": "3D Face Reconstruction by Learning from Synthetic Data",
        "item-id": "i1243",
        "nodes": [
          {
            "text": "Richardson et al_2016_3D Face Reconstruction by Learning from Synthetic Data.pdf",
            "item-id": "i1245",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Richardson et al_2016_3D Face Reconstruction by Learning from Synthetic Data.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Richardson et al_2016_3D Face Reconstruction by Learning from Synthetic Data.pdf"
              ]
            ],
            "resource": "storage/i1245.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "3D Face Reconstruction by Learning from Synthetic Data",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Fast and robust three-dimensional reconstruction of facial geometric structure from a single image is a challenging task with numerous applications. Here, we introduce a learning-based approach for reconstructing a three-dimensional face from a single image. Recent face recovery methods rely on accurate localization of key characteristic points. In contrast, the proposed approach is based on a Convolutional-Neural-Network (CNN) which extracts the face geometry directly from its image. Although such deep architectures outperform other models in complex computer vision problems, training them properly requires a large dataset of annotated examples. In the case of three-dimensional faces, currently, there are no large volume data sets, while acquiring such big-data is a tedious task. As an alternative, we propose to generate random, yet nearly photo-realistic, facial images for which the geometric form is known. The suggested model successfully recovers facial shapes from real images, even for faces with extreme expressions and under various lighting conditions."
          ],
          [
            "Conference Name",
            "2016 Fourth International Conference on 3D Vision (3DV)"
          ],
          [
            "Creators",
            "Elad Richardson, Matan Sela, Ron Kimmel"
          ],
          [
            "DOI",
            "10.1109/3DV.2016.56"
          ],
          [
            "Date",
            "2016-10-00 2016-10"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "460-469"
          ],
          [
            "Proceedings Title",
            "2016 Fourth International Conference on 3D Vision (3DV)"
          ],
          [
            "Title",
            "3D Face Reconstruction by Learning from Synthetic Data"
          ]
        ],
        "resource": "storage/i1245.pdf",
        "selectable": false
      },
      {
        "text": "GANFIT",
        "item-id": "i1240",
        "nodes": [
          {
            "text": "Gecer et al_2019_GANFIT.pdf",
            "item-id": "i1241",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gecer et al_2019_GANFIT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gecer et al_2019_GANFIT.pdf"
              ]
            ],
            "resource": "storage/i1241.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GANFIT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In the past few years, a lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the most recent works, differentiable renderers were employed in order to learn the relationship between the facial identity features and the parameters of a 3D morphable model for shape and texture. The texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction of the state-of-the-art methods is still not capable of modeling textures in high fidelity. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful generator of facial texture in UV space. Then, we revisit the original 3D Morphable Models (3DMMs) fitting approaches making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. We optimize the parameters with the supervision of pretrained deep identity features through our end-to-end differentiable framework. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details."
          ],
          [
            "Access Date",
            "2021-12-06 13:41:34"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1155-1164"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "GANFIT"
          ],
          [
            "Title",
            "GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Gecer_GANFIT_Generative_Adversarial_Network_Fitting_for_High_Fidelity_3D_Face_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i1241.pdf",
        "selectable": false
      }
    ],
    "item_title": "3D Models",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Anomaly Detection",
    "item-id": "c6,i1862",
    "nodes": [
      {
        "text": "A Survey on Anomaly Detection for Technical Systems using LSTM Networks",
        "item-id": "i699",
        "nodes": [
          {
            "text": "Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry",
            "item-id": "n700",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry</div>",
            "node_type": "note"
          },
          {
            "text": "Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf",
            "item-id": "i702",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf"
              ]
            ],
            "resource": "storage/i702.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Anomaly Detection for Technical Systems using LSTM Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Anomalies represent deviations from the intended system operation and can lead to decreased efficiency as well as partial or complete system failure. As the causes of anomalies are often unknown due to complex system dynamics, efficient anomaly detection is necessary. Conventional detection approaches rely on statistical and time-invariant methods that fail to address the complex and dynamic nature of anomalies. With advances in artificial intelligence and increasing importance for anomaly detection and prevention in various domains, artificial neural network approaches enable the detection of more complex anomaly types while considering temporal and contextual characteristics. In this article, a survey on state-of-the-art anomaly detection using deep neural and especially long short-term memory networks is conducted. The investigated approaches are evaluated based on the application scenario, data and anomaly types as well as further metrics. To highlight the potential of upcoming anomaly detection techniques, graph-based and transfer learning approaches are also included in the survey, enabling the analysis of heterogeneous data as well as compensating for its shortage and improving the handling of dynamic processes."
          ],
          [
            "Access Date",
            "2021-07-12 05:29:42"
          ],
          [
            "Creators",
            "Benjamin Lindemann, Benjamin Maschler, Nada Sahlab, Michael Weyrich"
          ],
          [
            "Date",
            "2021-05-28 2021-05-28"
          ],
          [
            "Extra",
            "arXiv: 2105.13810"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2105.13810 [cs, stat]"
          ],
          [
            "Title",
            "A Survey on Anomaly Detection for Technical Systems using LSTM Networks"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2105.13810"
          ]
        ],
        "resource": "storage/i702.pdf",
        "selectable": false
      },
      {
        "text": "A survey of network anomaly detection techniques",
        "item-id": "i704",
        "nodes": [
          {
            "text": "Ahmed et al_2016_A survey of network anomaly detection techniques.pdf",
            "item-id": "i706",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ahmed et al_2016_A survey of network anomaly detection techniques.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JGRGT7VZ/2\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/3\">Roadmap of the paper</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/3\">Preliminary discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/3\">Types of anomalies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Output of anomaly detection techniques</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Types of network attacks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Mapping of network attacks with anomalies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Classification based network anomaly detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/5\">Support vector machine</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/5\">Bayesian network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Neural network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Rule-based</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Statistical anomaly detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Mixture model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/7\">Signal processing technique</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/7\">Principal component analysis (PCA)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/7\">Information theory</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/8\">Correlation analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/8\">Clustering-based</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/8\">Regular clustering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/9\">Co-clustering</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/9\">Intrusion detection datasets and issues</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/9\">Limitations of DARPA/KDD datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/10\">Contemporary network attacks evaluation dataset: ADFA-LD12</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/10\">Current network data repositories</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/10\">Evaluation of network anomaly detection techniques</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/11\">Conclusions and future research directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/11\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ahmed et al_2016_A survey of network anomaly detection techniques.pdf"
              ]
            ],
            "resource": "storage/i706.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A survey of network anomaly detection techniques",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Information and Communication Technology (ICT) has a great impact on social wellbeing, economic growth and national security in todays world. Generally, ICT includes computers, mobile communication devices and networks. ICT is also embraced by a group of people with malicious intent, also known as network intruders, cyber criminals, etc. Confronting these detrimental cyber activities is one of the international priorities and important research area. Anomaly detection is an important data analysis task which is useful for identifying the network intrusions. This paper presents an in-depth analysis of four major categories of anomaly detection techniques which include classification, statistical, information theory and clustering. The paper also discusses research challenges with the datasets used for network intrusion detection."
          ],
          [
            "Access Date",
            "2021-07-12 05:30:56"
          ],
          [
            "Creators",
            "Mohiuddin Ahmed, Abdun Naser Mahmood, Jiankun Hu"
          ],
          [
            "DOI",
            "10.1016/j.jnca.2015.11.016"
          ],
          [
            "Date",
            "2016-01-01 January 1, 2016"
          ],
          [
            "ISSN",
            "1084-8045"
          ],
          [
            "Journal Abbreviation",
            "Journal of Network and Computer Applications"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "19-31"
          ],
          [
            "Publication Title",
            "Journal of Network and Computer Applications"
          ],
          [
            "Title",
            "A survey of network anomaly detection techniques"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1084804515002891"
          ],
          [
            "Volume",
            "60"
          ]
        ],
        "resource": "storage/i706.pdf",
        "selectable": false
      },
      {
        "text": "Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest X-rays",
        "item-id": "i723",
        "nodes": [
          {
            "text": "Comment: 9 pages, 3 tables, 3 figures",
            "item-id": "n724",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 9 pages, 3 tables, 3 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 9 pages, 3 tables, 3 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Khan et al_2021_Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest.pdf",
            "item-id": "i726",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khan et al_2021_Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JJJNQCK4/1\">1. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/2\">2. Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/3\">3. Unsupervised Deep Learning based Anomaly Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/4\">4. Dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/4\">5. Experimental Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/4\">5.1. Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/5\">6. Conclusions and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JJJNQCK4/6\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khan et al_2021_Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest.pdf"
              ]
            ],
            "resource": "storage/i726.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest X-rays",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The current COVID-19 pandemic is now getting contained, albeit at the cost of morethan2.3million human lives. A critical phase in any pandemic is the early detection of cases to develop preventive treatments and strategies. In the case of COVID-19,several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such asCOVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of data from infected persons could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate the problem of identifying early cases in a pandemic as an anomaly detection problem, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present several unsupervised deep learning approaches, including convolutional and adversarially trained autoencoder. We tested two settings on a publicly available dataset (COVIDx)by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. Afterperforming3-fold cross validation, we obtain a ROC-AUC of0.765. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays"
          ],
          [
            "Access Date",
            "2021-07-12 05:33:18"
          ],
          [
            "Creators",
            "Shehroz S. Khan, Faraz Khoshbakhtian, Ahmed Bilal Ashraf"
          ],
          [
            "Date",
            "2021-04-13 2021-04-13"
          ],
          [
            "Extra",
            "arXiv: 2010.02814"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2010.02814 [cs, eess]"
          ],
          [
            "Title",
            "Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest X-rays"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2010.02814"
          ]
        ],
        "resource": "storage/i726.pdf",
        "selectable": false
      },
      {
        "text": "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning",
        "item-id": "i1862",
        "nodes": [
          {
            "text": "Georgescu et al_2021_Anomaly Detection in Video via Self-Supervised and Multi-Task Learning.pdf",
            "item-id": "i1961",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Georgescu et al_2021_Anomaly Detection in Video via Self-Supervised and Multi-Task Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Georgescu et al_2021_Anomaly Detection in Video via Self-Supervised and Multi-Task Learning.pdf"
              ]
            ],
            "resource": "storage/i1961.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Anomaly detection in video is a challenging computer vision problem. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without full supervision. In this paper, we approach anomalous event detection in video through self-supervised and multi-task learning at the object level. We first utilize a pre-trained detector to detect objects. Then, we train a 3D convolutional neural network to produce discriminative anomaly-specific information by jointly learning multiple proxy tasks: three self-supervised and one based on knowledge distillation. The self-supervised tasks are: (i) discrimination of forward/backward moving objects (arrow of time), (ii) discrimination of objects in consecutive/intermittent frames (motion irregularity) and (iii) reconstruction of object-specific appearance information. The knowledge distillation task takes into account both classification and detection information, generating large prediction discrepancies between teacher and student models when anomalies occur. To the best of our knowledge, we are the first to approach anomalous event detection in video as a multi-task learning problem, integrating multiple self-supervised and knowledge distillation proxy tasks in a single architecture. Our lightweight architecture outperforms the state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. Additionally, we perform an ablation study demonstrating the importance of integrating self-supervised learning and normality-specific distillation in a multi-task learning setting."
          ],
          [
            "Access Date",
            "2022-11-01 15:55:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, Mubarak Shah"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12742-12752"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Georgescu_Anomaly_Detection_in_Video_via_Self-Supervised_and_Multi-Task_Learning_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1961.pdf",
        "selectable": false
      },
      {
        "text": "Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging",
        "item-id": "i1122",
        "nodes": [
          {
            "text": "Mehta et al_2021_Motion and Region Aware Adversarial Learning for Fall Detection with Thermal.pdf",
            "item-id": "i1151",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mehta et al_2021_Motion and Region Aware Adversarial Learning for Fall Detection with Thermal.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mehta et al_2021_Motion and Region Aware Adversarial Learning for Fall Detection with Thermal.pdf"
              ]
            ],
            "resource": "storage/i1151.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline."
          ],
          [
            "Conference Name",
            "2020 25th International Conference on Pattern Recognition (ICPR)"
          ],
          [
            "Creators",
            "Vineet Mehta, Abhinav Dhall, Sujata Pal, Shehroz S. Khan"
          ],
          [
            "DOI",
            "10.1109/ICPR48806.2021.9412632"
          ],
          [
            "Date",
            "2021-01-00 2021-01"
          ],
          [
            "Extra",
            "ISSN: 1051-4651"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6321-6328"
          ],
          [
            "Proceedings Title",
            "2020 25th International Conference on Pattern Recognition (ICPR)"
          ],
          [
            "Title",
            "Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging"
          ]
        ],
        "resource": "storage/i1151.pdf",
        "selectable": false
      }
    ],
    "item_title": "Anomaly Detection",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Attribute Recognition",
    "item-id": "c27,i2803",
    "nodes": [
      {
        "text": "A Survey of Deep Facial Attribute Analysis",
        "item-id": "i70",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n294",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Introduction to all facial analysis</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf",
            "item-id": "i295",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf"
              ]
            ],
            "resource": "storage/i295.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey of Deep Facial Attribute Analysis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions."
          ],
          [
            "Access Date",
            "2021-03-16 14:46:55"
          ],
          [
            "Creators",
            "Xin Zheng, Yanqing Guo, Huaibo Huang, Yi Li, Ran He"
          ],
          [
            "DOI",
            "10.1007/s11263-020-01308-z"
          ],
          [
            "Date",
            "2020-09-01 2020-09-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "2002-2034"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Title",
            "A Survey of Deep Facial Attribute Analysis"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-020-01308-z"
          ],
          [
            "Volume",
            "128"
          ]
        ],
        "resource": "storage/i295.pdf",
        "selectable": false
      },
      {
        "text": "Adaptively Weighted Multi-task Deep Network for Person Attribute Classification",
        "item-id": "i1872",
        "nodes": [
          {
            "text": "He et al_2017_Adaptively Weighted Multi-task Deep Network for Person Attribute Classification.pdf",
            "item-id": "i2025",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2017_Adaptively Weighted Multi-task Deep Network for Person Attribute Classification.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2017_Adaptively Weighted Multi-task Deep Network for Person Attribute Classification.pdf"
              ]
            ],
            "resource": "storage/i2025.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Adaptively Weighted Multi-task Deep Network for Person Attribute Classification",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multi-task learning aims to boost the performance of multiple prediction tasks by appropriately sharing relevant information among them. However, it always suffers from the negative transfer problem. And due to the diverse learning difficulties and convergence rates of different tasks, jointly optimizing multiple tasks is very challenging. To solve these problems, we present a weighted multi-task deep convolutional neural network for person attribute analysis. A novel validation loss trend algorithm is, for the first time proposed to dynamically and adaptively update the weight for learning each task in the training process. Extensive experiments on CelebA, Market-1501 attribute and Duke attribute datasets clearly show that state-of-the-art performance is obtained; and this validates the effectiveness of our proposed framework."
          ],
          [
            "Access Date",
            "2022-10-28"
          ],
          [
            "Creators",
            "Keke He, Zhanxiong Wang, Yanwei Fu, Rui Feng, Yu-Gang Jiang, Xiangyang Xue"
          ],
          [
            "DOI",
            "10.1145/3123266.3123424"
          ],
          [
            "Date",
            "2017-10-23 2017-10-23"
          ],
          [
            "Extra",
            "44 citations (Semantic Scholar/DOI) [2022-10-29]"
          ],
          [
            "ISBN",
            "978-1-4503-4906-2"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1636\u20131644"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 25th ACM international conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '17"
          ],
          [
            "Title",
            "Adaptively Weighted Multi-task Deep Network for Person Attribute Classification"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3123266.3123424"
          ]
        ],
        "resource": "storage/i2025.pdf",
        "selectable": false
      },
      {
        "text": "Attributes for Improved Attributes",
        "item-id": "i1895",
        "nodes": [
          {
            "text": "Hand_Chellappa_2017_Attributes for Improved Attributes.pdf",
            "item-id": "i2026",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hand_Chellappa_2017_Attributes for Improved Attributes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hand_Chellappa_2017_Attributes for Improved Attributes.pdf"
              ]
            ],
            "resource": "storage/i2026.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attributes for Improved Attributes",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Attributes, or mid-level semantic features, have gained popularity in the past few years in domains ranging from activity recognition to face verification. Improving the accuracy of attribute classifiers is an important first step in any application which uses these attributes. In most works to date, attributes have been considered independent of each other. However, attributes can be strongly related, such as heavy makeup and wearing lipstick as well as male and goatee and many others. We propose a multi-task deep convolutional neural network (MCNN) with an auxiliary network at the top (AUX) which takes advantage of attribute relationships for improved classification. We call our final network MCNN-AUX. MCNN-AUX uses attribute relationships in three ways: by sharing the lowest layers for all attributes, by sharing the higher layers for spatially-related attributes, and by feeding the attribute scores from MCNN into the AUX network to find score-level relationships. Using MCNN-AUX rather than individual attribute classifiers, we are able to reduce the number of parameters in the network from 64 million to fewer than 16 million and reduce the training time by a factor of 16. We demonstrate the effectiveness of our method by producing results on two challenging publicly available datasets achieving state-of-the-art performance on many attributes."
          ],
          [
            "Access Date",
            "2022-10-29 02:25:34"
          ],
          [
            "Creators",
            "Emily Hand, Rama Chellappa"
          ],
          [
            "DOI",
            "10.1609/aaai.v31i1.11229"
          ],
          [
            "Date",
            "2017-02-12 2017-02-12"
          ],
          [
            "Extra",
            "117 citations (Semantic Scholar/DOI) [2022-10-29]\nNumber: 1"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c)"
          ],
          [
            "Short Title",
            "Attributes for Improved Attributes"
          ],
          [
            "Title",
            "Attributes for Improved Attributes: A Multi-Task Network Utilizing Implicit and Explicit Relationships for Facial Attribute Classification"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/11229"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i2026.pdf",
        "selectable": false
      },
      {
        "text": "CelebV-HQ",
        "item-id": "i2501",
        "nodes": [
          {
            "text": "Zhu et al_2022_CelebV-HQ.pdf",
            "item-id": "i2695",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2022_CelebV-HQ.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_83PTNWB6/1\">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2022_CelebV-HQ.pdf"
              ]
            ],
            "resource": "storage/i2695.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CelebV-HQ",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35,\u00a0666 video clips with the resolution of $$512\\times 512$$512\u00d7512at least, involving 15,\u00a0653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. We finally envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available (Project page: https://celebv-hq.github.io/Code and models: https://github.com/CelebV-HQ/CelebV-HQ)."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, Chen Change Loy, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-20071-7_38"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-20071-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "650-667"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "CelebV-HQ"
          ],
          [
            "Title",
            "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset"
          ]
        ],
        "resource": "storage/i2695.pdf",
        "selectable": false
      },
      {
        "text": "Deep Learning Face Attributes in the Wild",
        "item-id": "i1912",
        "nodes": [
          {
            "text": "Liu et al_2015_Deep Learning Face Attributes in the Wild.pdf",
            "item-id": "i2051",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2015_Deep Learning Face Attributes in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2015_Deep Learning Face Attributes in the Wild.pdf"
              ]
            ],
            "resource": "storage/i2051.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Learning Face Attributes in the Wild",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts."
          ],
          [
            "Access Date",
            "2022-10-22 08:52:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3730-3738"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Deep Learning Face Attributes in the Wild"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2015/html/Liu_Deep_Learning_Face_ICCV_2015_paper.html"
          ]
        ],
        "resource": "storage/i2051.pdf",
        "selectable": false
      },
      {
        "text": "Deep Multi-Task Multi-Label CNN for Effective Facial Attribute Classification",
        "item-id": "i1899",
        "nodes": [
          {
            "text": "Mao et al_2022_Deep Multi-Task Multi-Label CNN for Effective Facial Attribute Classification.pdf",
            "item-id": "i2020",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mao et al_2022_Deep Multi-Task Multi-Label CNN for Effective Facial Attribute Classification.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mao et al_2022_Deep Multi-Task Multi-Label CNN for Effective Facial Attribute Classification.pdf"
              ]
            ],
            "resource": "storage/i2020.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Multi-Task Multi-Label CNN for Effective Facial Attribute Classification",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Facial Attribute Classification (FAC) has attracted increasing attention in computer vision and pattern recognition. However, state-of-the-art FAC methods perform face detection/alignment and FAC independently. The inherent dependencies between these tasks are not fully exploited. In addition, most methods predict all facial attributes using the same CNN network architecture, which ignores the different learning complexities of facial attributes. To address the above problems, we propose a novel deep multi-task multi-label CNN, termed DMM-CNN, for effective FAC. Specifically, DMM-CNN jointly optimizes two closely-related tasks (i.e., facial landmark detection and FAC) to improve the performance of FAC by taking advantage of multi-task learning. To deal with the diverse learning complexities of facial attributes, we divide the attributes into two groups: objective attributes and subjective attributes. Two different network architectures are respectively designed to extract features for two groups of attributes, and a novel dynamic weighting scheme is proposed to automatically assign the loss weight to each facial attribute during training. Furthermore, an adaptive thresholding strategy is developed to effectively alleviate the problem of class imbalance for multi-label learning. Experimental results on the challenging CelebA and LFWA datasets show the superiority of the proposed DMM-CNN method compared with several state-of-the-art FAC methods."
          ],
          [
            "Creators",
            "Longbiao Mao, Yan Yan, Jing-Hao Xue, Hanzi Wang"
          ],
          [
            "DOI",
            "10.1109/TAFFC.2020.2969189"
          ],
          [
            "Date",
            "2022-04-00 2022-04"
          ],
          [
            "Extra",
            "16 citations (Semantic Scholar/DOI) [2022-10-29]\nConference Name: IEEE Transactions on Affective Computing"
          ],
          [
            "ISSN",
            "1949-3045"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "818-828"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Affective Computing"
          ],
          [
            "Title",
            "Deep Multi-Task Multi-Label CNN for Effective Facial Attribute Classification"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "resource": "storage/i2020.pdf",
        "selectable": false
      },
      {
        "text": "FaceTracer",
        "item-id": "i1894",
        "nodes": [
          {
            "text": "Kumar et al_2008_FaceTracer.pdf",
            "item-id": "i2024",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kumar et al_2008_FaceTracer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EVS4EG9L/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/4\">Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/4\">Creating the Face Database</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/5\">Automatic Attribute Classification for Face Images</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/7\">Classifier Architecture</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/11\">Comparison to Prior Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/11\">The FaceTracer Engine</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EVS4EG9L/13\">Discussion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kumar et al_2008_FaceTracer.pdf"
              ]
            ],
            "resource": "storage/i2024.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FaceTracer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We have created the first image search engine based entirely on faces. Using simple text queries such as \u201csmiling men with blond hair and mustaches,\u201d users can search through over 3.1 million faces which have been automatically labeled on the basis of several facial attributes. Faces in our database have been extracted and aligned from images downloaded from the internet using a commercial face detector, and the number of images and attributes continues to grow daily. Our classification approach uses a novel combination of Support Vector Machines and Adaboost which exploits the strong structure of faces to select and train on the optimal set of features for each attribute. We show state-of-the-art classification results compared to previous works, and demonstrate the power of our architecture through a functional, large-scale face search engine. Our framework is fully automatic, easy to scale, and computes all labels off-line, leading to fast on-line search performance. In addition, we describe how our system can be used for a number of applications, including law enforcement, social networks, and personal photo management. Our search engine will soon be made publicly available."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Neeraj Kumar, Peter Belhumeur, Shree Nayar, David Forsyth, Philip Torr, Andrew Zisserman"
          ],
          [
            "DOI",
            "10.1007/978-3-540-88693-8_25"
          ],
          [
            "Date",
            "2008-00-00 2008"
          ],
          [
            "Extra",
            "399 citations (Semantic Scholar/DOI) [2022-10-29]"
          ],
          [
            "ISBN",
            "978-3-540-88693-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "340-353"
          ],
          [
            "Place",
            "Berlin, Heidelberg"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "FaceTracer"
          ],
          [
            "Title",
            "FaceTracer: A Search Engine for Large Collections of Images with Faces"
          ]
        ],
        "resource": "storage/i2024.pdf",
        "selectable": false
      },
      {
        "text": "FairFace",
        "item-id": "i1861",
        "nodes": [
          {
            "text": "K\u00e4rkk\u00e4inen_Joo_2019_FairFace.pdf",
            "item-id": "i1959",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "K\u00e4rkk\u00e4inen_Joo_2019_FairFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "K\u00e4rkk\u00e4inen_Joo_2019_FairFace.pdf"
              ]
            ],
            "resource": "storage/i1959.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FairFace",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups."
          ],
          [
            "Access Date",
            "2022-11-01 15:56:33"
          ],
          [
            "Archiveid",
            "arXiv:1908.04913"
          ],
          [
            "Creators",
            "Kimmo K\u00e4rkk\u00e4inen, Jungseock Joo"
          ],
          [
            "DOI",
            "10.48550/arXiv.1908.04913"
          ],
          [
            "Date",
            "2019-08-13 2019-08-13"
          ],
          [
            "Extra",
            "arXiv:1908.04913 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FairFace"
          ],
          [
            "Title",
            "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1908.04913"
          ]
        ],
        "resource": "storage/i1959.pdf",
        "selectable": false
      },
      {
        "text": "Heterogeneous Face Attribute Estimation",
        "item-id": "i1896",
        "nodes": [
          {
            "text": "Han et al_2018_Heterogeneous Face Attribute Estimation.pdf",
            "item-id": "i2027",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Han et al_2018_Heterogeneous Face Attribute Estimation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Han et al_2018_Heterogeneous Face Attribute Estimation.pdf"
              ]
            ],
            "resource": "storage/i2027.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Heterogeneous Face Attribute Estimation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face attribute estimation has many potential applications in video surveillance, face retrieval, and social media. While a number of methods have been proposed for face attribute estimation, most of them did not explicitly consider the attribute correlation and heterogeneity (e.g., ordinal versus nominal and holistic versus local) during feature representation learning. In this paper, we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate multiple heterogeneous attributes from a single face image. In DMTL, we tackle attribute correlation and heterogeneity with convolutional neural networks (CNNs) consisting of shared feature learning for all the attributes, and category-specific feature learning for heterogeneous attributes. We also introduce an unconstrained face database (LFW+), an extension of public-domain LFW, with heterogeneous demographic attributes (age, gender, and race) obtained via crowdsourcing. Experimental results on benchmarks with multiple face attributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed approach has superior performance compared to state of the art. Finally, evaluations on a public-domain face database (LAP) with a single attribute show that the proposed approach has excellent generalization ability."
          ],
          [
            "Creators",
            "Hu Han, Anil K. Jain, Fang Wang, Shiguang Shan, Xilin Chen"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2017.2738004"
          ],
          [
            "Date",
            "2018-11-00 2018-11"
          ],
          [
            "Extra",
            "208 citations (Semantic Scholar/DOI) [2022-10-29]\nConference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2597-2609"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "Heterogeneous Face Attribute Estimation"
          ],
          [
            "Title",
            "Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning Approach"
          ],
          [
            "Volume",
            "40"
          ]
        ],
        "resource": "storage/i2027.pdf",
        "selectable": false
      },
      {
        "text": "HyperExtended LightFace",
        "item-id": "i2103",
        "nodes": [
          {
            "text": "Serengil_Ozpinar_2021_HyperExtended LightFace.pdf",
            "item-id": "i2141",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Serengil_Ozpinar_2021_HyperExtended LightFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Serengil_Ozpinar_2021_HyperExtended LightFace.pdf"
              ]
            ],
            "resource": "storage/i2141.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HyperExtended LightFace",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Facial attribute analysis from facial images has always been a challenging task. Its practical use cases are very different. This paper mentioned how to build machine learning models with facial image data for age, gender, facial expression, and race/ethnicity prediction tasks. A fully coded open-source software framework was also developed and published with those functionalities."
          ],
          [
            "Conference Name",
            "2021 International Conference on Engineering and Emerging Technologies (ICEET)"
          ],
          [
            "Creators",
            "Sefik Ilkin Serengil, Alper Ozpinar"
          ],
          [
            "DOI",
            "10.1109/ICEET53442.2021.9659697"
          ],
          [
            "Date",
            "2021-10-00 2021-10"
          ],
          [
            "Extra",
            "ISSN: 2409-2983"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-4"
          ],
          [
            "Proceedings Title",
            "2021 International Conference on Engineering and Emerging Technologies (ICEET)"
          ],
          [
            "Short Title",
            "HyperExtended LightFace"
          ],
          [
            "Title",
            "HyperExtended LightFace: A Facial Attribute Analysis Framework"
          ]
        ],
        "resource": "storage/i2141.pdf",
        "selectable": false
      },
      {
        "text": "Labeled faces in the wild: Updates and new reporting procedures",
        "item-id": "i2803",
        "nodes": [
          {
            "text": "Learned-Miller_2014_Labeled faces in the wild.pdf",
            "item-id": "i2857",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Learned-Miller_2014_Labeled faces in the wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Learned-Miller_2014_Labeled faces in the wild.pdf"
              ]
            ],
            "resource": "storage/i2857.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-font",
        "item_title": "Labeled faces in the wild: Updates and new reporting procedures",
        "item_type": "report",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Creators",
            "Gary B. Huang Erik Learned-Miller"
          ],
          [
            "Date",
            "2014-05-00 2014-05"
          ],
          [
            "Extra",
            "Issue: UM-CS-2014-003"
          ],
          [
            "Institution",
            "University of Massachusetts, Amherst"
          ],
          [
            "Title",
            "Labeled faces in the wild: Updates and new reporting procedures"
          ]
        ],
        "resource": "storage/i2857.pdf",
        "selectable": false
      },
      {
        "text": "LightFace",
        "item-id": "i2102",
        "nodes": [
          {
            "text": "Serengil_Ozpinar_2020_LightFace.pdf",
            "item-id": "i2139",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Serengil_Ozpinar_2020_LightFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Serengil_Ozpinar_2020_LightFace.pdf"
              ]
            ],
            "resource": "storage/i2139.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "LightFace",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face recognition constitutes a relatively a popular area which has emerged from the rulers of the social media to top universities in the world. Those frontiers and rule makers recently designed deep learning based custom face recognition models. A modern face recognition pipeline consists of four common stages: detecting, alignment, representation and verification. However, face recognition studies mainly mention the representation stage of a pipeline. In this paper, first of all a review face recognition has been done and then the description of the developed lightweight hybrid high performance face recognition framework has been made. Its hybrid feature enables to switch face recognition models among state-of-the-art ones."
          ],
          [
            "Conference Name",
            "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
          ],
          [
            "Creators",
            "Sefik Ilkin Serengil, Alper Ozpinar"
          ],
          [
            "DOI",
            "10.1109/ASYU50717.2020.9259802"
          ],
          [
            "Date",
            "2020-10-00 2020-10"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-5"
          ],
          [
            "Proceedings Title",
            "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
          ],
          [
            "Short Title",
            "LightFace"
          ],
          [
            "Title",
            "LightFace: A Hybrid Deep Face Recognition Framework"
          ]
        ],
        "resource": "storage/i2139.pdf",
        "selectable": false
      },
      {
        "text": "PANDA",
        "item-id": "i1900",
        "nodes": [
          {
            "text": "Zhang et al_2014_PANDA.pdf",
            "item-id": "i2018",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2014_PANDA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2014_PANDA.pdf"
              ]
            ],
            "resource": "storage/i2018.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PANDA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets and DPM have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person."
          ],
          [
            "Access Date",
            "2022-10-29 02:29:04"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ning Zhang, Manohar Paluri, Marc'Aurelio Ranzato, Trevor Darrell, Lubomir Bourdev"
          ],
          [
            "Date",
            "2014-00-00 2014"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1637-1644"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "PANDA"
          ],
          [
            "Title",
            "PANDA: Pose Aligned Networks for Deep Attribute Modeling"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2014/html/Zhang_PANDA_Pose_Aligned_2014_CVPR_paper.html"
          ]
        ],
        "resource": "storage/i2018.pdf",
        "selectable": false
      },
      {
        "text": "Photo search by face positions and facial attributes on touch devices",
        "item-id": "i1873",
        "nodes": [
          {
            "text": "Lei et al_2011_Photo search by face positions and facial attributes on touch devices.pdf",
            "item-id": "i2023",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lei et al_2011_Photo search by face positions and facial attributes on touch devices.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8MQ96MRF/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/2\">Observations and System  Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/2\">Detecting Face Attributes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/2\">Estimating Face Similarities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/3\">Assessing Photo Aesthetics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/3\">Photo Search on Touch Devices</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/3\">User Interface</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/3\">Ranking Function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/3\">Block-based Indexing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/4\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/4\">Dataset and Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/4\">Performance Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/4\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8MQ96MRF/4\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lei et al_2011_Photo search by face positions and facial attributes on touch devices.pdf"
              ]
            ],
            "resource": "storage/i2023.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Photo search by face positions and facial attributes on touch devices",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the explosive growth of camera devices, people can freely take photos to capture moments of life, especially the ones accompanied with friends and family. Therefore, a better solution to organize the increasing number of personal or group photos is highly required. In this paper, we propose a novel way to search for face images according facial attributes and face similarity of the target persons. To better match the face layout in mind, our system allows the user to graphically specify the face positions and sizes on a query \"canvas,\" where each attribute or identity is defined as an \"icon\" for easier representation. Moreover, we provide aesthetics filtering to enhance visual experience by removing candidates of poor photographic qualities. The scenario has been realized on a touch device with an intuitive user interface. With the proposed block-based indexing approach, we can achieve near real-time retrieval (0.1 second on average) in a large-scale dataset (more than 200k faces in Flickr images)."
          ],
          [
            "Access Date",
            "2022-10-28"
          ],
          [
            "Creators",
            "Yu-Heng Lei, Yan-Ying Chen, Lime Iida, Bor-Chun Chen, Hsiao-Hang Su, Winston H. Hsu"
          ],
          [
            "DOI",
            "10.1145/2072298.2072410"
          ],
          [
            "Date",
            "2011-11-28 2011-11-28"
          ],
          [
            "Extra",
            "38 citations (Semantic Scholar/DOI) [2022-10-29]"
          ],
          [
            "ISBN",
            "978-1-4503-0616-4"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "651\u2013654"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 19th ACM international conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '11"
          ],
          [
            "Title",
            "Photo search by face positions and facial attributes on touch devices"
          ],
          [
            "URL",
            "https://doi.org/10.1145/2072298.2072410"
          ]
        ],
        "resource": "storage/i2023.pdf",
        "selectable": false
      },
      {
        "text": "Segment-Based Methods for Facial Attribute Detection from Partial Faces",
        "item-id": "i1898",
        "nodes": [
          {
            "text": "Mahbub et al_2020_Segment-Based Methods for Facial Attribute Detection from Partial Faces.pdf",
            "item-id": "i2021",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mahbub et al_2020_Segment-Based Methods for Facial Attribute Detection from Partial Faces.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mahbub et al_2020_Segment-Based Methods for Facial Attribute Detection from Partial Faces.pdf"
              ]
            ],
            "resource": "storage/i2021.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Segment-Based Methods for Facial Attribute Detection from Partial Faces",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "State-of-the-art methods of attribute detection from faces almost always assume the presence of a full, unoccluded face. Hence, their performance degrades for partially visible and occluded faces. In this paper, we introduce SPLITFACE, a deep convolutional neural network-based method that is explicitly designed to perform attribute detection in partially occluded faces. Taking several facial segments and the full face as input, the proposed method takes a data driven approach to determine which attributes are localized in which facial segments. The unique architecture of the network allows each attribute to be predicted by multiple segments, which permits the implementation of committee machine techniques for combining local and global decisions to boost performance. With access to segment-based predictions, SPLITFACE can predict well those attributes which are localized in the visible parts of the face, without having to rely on the presence of the whole face. We use the CelebA and LFWA facial attribute datasets for standard evaluations. We also modify both datasets, to occlude the faces, so that we can evaluate the performance of attribute detection algorithms on partial faces. Our evaluation shows that SPLITFACE significantly outperforms other recent methods especially for partial faces."
          ],
          [
            "Creators",
            "Upal Mahbub, Sayantan Sarkar, Rama Chellappa"
          ],
          [
            "DOI",
            "10.1109/TAFFC.2018.2820048"
          ],
          [
            "Date",
            "2020-10-00 2020-10"
          ],
          [
            "Extra",
            "24 citations (Semantic Scholar/DOI) [2022-10-29]\nConference Name: IEEE Transactions on Affective Computing"
          ],
          [
            "ISSN",
            "1949-3045"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "601-613"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Affective Computing"
          ],
          [
            "Title",
            "Segment-Based Methods for Facial Attribute Detection from Partial Faces"
          ],
          [
            "Volume",
            "11"
          ]
        ],
        "resource": "storage/i2021.pdf",
        "selectable": false
      }
    ],
    "item_title": "Attribute Recognition",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Audio Analysis",
    "item-id": "c4,i3660",
    "nodes": [
      {
        "text": "A review of deep learning techniques for speech processing",
        "item-id": "i2551",
        "nodes": [
          {
            "text": "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf",
            "item-id": "i2620",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf"
              ]
            ],
            "resource": "storage/i2620.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A review of deep learning techniques for speech processing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field\u2019s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field."
          ],
          [
            "Access Date",
            "2023-06-07 15:17:12"
          ],
          [
            "Creators",
            "Ambuj Mehrish, Navonil Majumder, Rishabh Bharadwaj, Rada Mihalcea, Soujanya Poria"
          ],
          [
            "DOI",
            "10.1016/j.inffus.2023.101869"
          ],
          [
            "Date",
            "2023-06-03 2023-06-03"
          ],
          [
            "ISSN",
            "1566-2535"
          ],
          [
            "Journal Abbreviation",
            "Information Fusion"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "101869"
          ],
          [
            "Publication Title",
            "Information Fusion"
          ],
          [
            "Title",
            "A review of deep learning techniques for speech processing"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1566253523001859"
          ]
        ],
        "resource": "storage/i2620.pdf",
        "selectable": false
      },
      {
        "text": "AST",
        "item-id": "i2208",
        "nodes": [
          {
            "text": "Gong et al_2021_AST.pdf",
            "item-id": "i2241",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gong et al_2021_AST.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gong et al_2021_AST.pdf"
              ]
            ],
            "resource": "storage/i2241.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AST",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-02-25 06:28:16"
          ],
          [
            "Conference Name",
            "Interspeech 2021"
          ],
          [
            "Creators",
            "Yuan Gong, Yu-An Chung, James Glass"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-698"
          ],
          [
            "Date",
            "2021-08-30 2021-8-30"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "571-575"
          ],
          [
            "Proceedings Title",
            "Interspeech 2021"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "AST"
          ],
          [
            "Title",
            "AST: Audio Spectrogram Transformer"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html"
          ]
        ],
        "resource": "storage/i2241.pdf",
        "selectable": false
      },
      {
        "text": "Audio Albert",
        "item-id": "i947",
        "nodes": [
          {
            "text": "Chi et al_2021_Audio Albert.pdf",
            "item-id": "i972",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chi et al_2021_Audio Albert.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chi et al_2021_Audio Albert.pdf"
              ]
            ],
            "resource": "storage/i972.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio Albert",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-supervised speech models are powerful speech representation extractors for downstream applications. Recently, larger models have been utilized in acoustic model training to achieve better performance. We propose Audio ALBERT, a lite version of the self-supervised speech representation model. We apply the lightweight representation extractor to two downstream tasks, speaker classification and phoneme classification. We show that Audio ALBERT achieves performance comparable with massive pre-trained networks in the downstream tasks while having 91% fewer parameters. Moreover, we design probing models to measure how much the latent representations can encode the speaker's and phoneme's information. We find that the representations encoded in internal layers of Audio ALBERT contain more information for both phoneme and speaker than the last layer, which is generally used for downstream tasks. Our findings provide a new avenue for using self-supervised networks to achieve better performance and efficiency."
          ],
          [
            "Conference Name",
            "2021 IEEE Spoken Language Technology Workshop (SLT)"
          ],
          [
            "Creators",
            "Po-Han Chi, Pei-Hung Chung, Tsung-Han Wu, Chun-Cheng Hsieh, Yen-Hao Chen, Shang-Wen Li, Hung-yi Lee"
          ],
          [
            "DOI",
            "10.1109/SLT48900.2021.9383575"
          ],
          [
            "Date",
            "2021-01-00 2021-01"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "344-350"
          ],
          [
            "Proceedings Title",
            "2021 IEEE Spoken Language Technology Workshop (SLT)"
          ],
          [
            "Short Title",
            "Audio Albert"
          ],
          [
            "Title",
            "Audio Albert: A Lite Bert for Self-Supervised Learning of Audio Representation"
          ]
        ],
        "resource": "storage/i972.pdf",
        "selectable": false
      },
      {
        "text": "Audio Set",
        "item-id": "i2522",
        "nodes": [
          {
            "text": "Gemmeke et al_2017_Audio Set.pdf",
            "item-id": "i2726",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gemmeke et al_2017_Audio Set.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gemmeke et al_2017_Audio Set.pdf"
              ]
            ],
            "resource": "storage/i2726.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio Set",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers."
          ],
          [
            "Conference Name",
            "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2017.7952261"
          ],
          [
            "Date",
            "2017-03-00 2017-03"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "776-780"
          ],
          [
            "Proceedings Title",
            "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Audio Set"
          ],
          [
            "Title",
            "Audio Set: An ontology and human-labeled dataset for audio events"
          ]
        ],
        "resource": "storage/i2726.pdf",
        "selectable": false
      },
      {
        "text": "Generalized End-to-End Loss for Speaker Verification",
        "item-id": "i1028",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n3289",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>Resemblyzer</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf",
            "item-id": "i1039",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf"
              ]
            ],
            "resource": "storage/i1039.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generalized End-to-End Loss for Speaker Verification",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., \u201cOK Google\u201d and \u201cHey Google\u201d) as well as multiple dialects."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8462665"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4879-4883"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Generalized End-to-End Loss for Speaker Verification"
          ]
        ],
        "resource": "storage/i1039.pdf",
        "selectable": false
      },
      {
        "text": "High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder",
        "item-id": "i2529",
        "nodes": [
          {
            "text": "Haque et al_2020_High-Fidelity Audio Generation and Representation Learning With Guided.pdf",
            "item-id": "i2738",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Haque et al_2020_High-Fidelity Audio Generation and Representation Learning With Guided.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_P782DFVB/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/2\">II Background and Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/2\">II-A Audio Representation Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">II-B Audio Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">II-C Closely Related Architectures</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III Proposed Research Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III-A Architecture of the GAAE</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III-A1 Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III-A2 Classifier</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/4\">III-A3 Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-A4 Discriminators</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-B Losses</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-B1 Encoder, Classifier and Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-B2 Discriminators' loss</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV Data and Evaluation Metrics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV-A Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV-B Data Preprocessing</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV-C Measurement Metrics</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">IV-C1 Inception Score (IS)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">IV-C2 Fr\u00e9chet Inception Distance (FID)</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V Experimental Setup, Results and Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V-A Impact of Labelled Data for Conditional Sample Generation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V-A1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V-A2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B Evaluation of Conditional Sample Generation based on Guidance</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B2 Result and Discussion: Manual test</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B3 Results and Discussions: CNN based Classification accuracy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-C Conditional Sample Generation using guidance from a different dataset</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/9\">V-C1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-C2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-D Guided representation Learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-D1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-D2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-E General Representation/Style Representation Learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-E1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-E2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-F Coherence of the General Representation/Latent Space</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-F1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-F2 Results</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/12\">VI Hyperparameter Tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/12\">VII Classifier of the GAAE model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/12\">VIII Conclusion and Lesson Learnt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/1\">REFERENCES</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A ARCHITECTURAL DETAILS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A-A Supervised BigGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A-B Unsupervised BigGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A-C BiGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/17\">A-D GAAE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">A-E Simple Classifier</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">Kazi Nazmul Haque</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">Rajib Rana</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">Bj\u00f6rn W. Schuller (M'05-SM'15-F'18)</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Haque et al_2020_High-Fidelity Audio Generation and Representation Learning With Guided.pdf"
              ]
            ],
            "resource": "storage/i2738.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating high-fidelity conditional audio samples and learning representation from unlabelled audio data are two challenging problems in machine learning research. Recent advances in the Generative Adversarial Neural Networks (GAN) architectures show great promise in addressing these challenges. To learn powerful representation using GAN architecture, it requires superior sample generation quality, which requires an enormous amount of labelled data. In this paper, we address this issue by proposing Guided Adversarial Autoencoder (GAAE), which can generate superior conditional audio samples from unlabelled audio data using a small percentage of labelled data as guidance. Representation learned from unlabelled data without any supervision does not guarantee its' usability for any downstream task. On the other hand, during the representation learning, if the model is highly biased towards the downstream task, it losses its generalisation capability. This makes the learned representation hardly useful for any other tasks that are not related to that downstream task. The proposed GAAE model also address these issues. Using this superior conditional generation, GAAE can learn representation specific to the downstream task. Furthermore, GAAE learns another type of representation capturing the general attributes of the data, which is independent of the downstream task at hand. Experimental results involving the S09 and the NSynth dataset attest the superior performance of GAAE compared to the state-of-the-art alternatives."
          ],
          [
            "Creators",
            "Kazi Nazmul Haque, Rajib Rana, Bj\u00f6rn W. Schuller"
          ],
          [
            "DOI",
            "10.1109/ACCESS.2020.3040797"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Access"
          ],
          [
            "ISSN",
            "2169-3536"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "223509-223528"
          ],
          [
            "Publication Title",
            "IEEE Access"
          ],
          [
            "Title",
            "High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i2738.pdf",
        "selectable": false
      },
      {
        "text": "Hybrid Transformers for Music Source Separation",
        "item-id": "i2827",
        "nodes": [
          {
            "text": "Rouard et al_2023_Hybrid Transformers for Music Source Separation.pdf",
            "item-id": "i2893",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rouard et al_2023_Hybrid Transformers for Music Source Separation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rouard et al_2023_Hybrid Transformers for Music Source Separation.pdf"
              ]
            ],
            "resource": "storage/i2893.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Hybrid Transformers for Music Source Separation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A natural question arising in Music Source Separation (MSS) is whether long range contextual information is useful, or whether local acoustic features are sufficient. In other fields, attention based Transformers [1] have shown their ability to integrate information over long sequences. In this work, we introduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral bi-U-Net based on Hybrid Demucs [2], where the innermost layers are replaced by a cross-domain Transformer Encoder, using self-attention within one domain, and cross-attention across domains. While it performs poorly when trained only on MUSDB [3], we show that it outperforms Hybrid Demucs (trained on the same data) by 0.45 dB of SDR when using 800 extra training songs. Using sparse attention kernels to extend its receptive field, and per source fine-tuning, we achieve state-of-the-art results on MUSDB with extra training data, with 9.20 dB of SDR."
          ],
          [
            "Conference Name",
            "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Simon Rouard, Francisco Massa, Alexandre D\u00e9fossez"
          ],
          [
            "DOI",
            "10.1109/ICASSP49357.2023.10096956"
          ],
          [
            "Date",
            "2023-06-00 2023-06"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-5"
          ],
          [
            "Proceedings Title",
            "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Hybrid Transformers for Music Source Separation"
          ]
        ],
        "resource": "storage/i2893.pdf",
        "selectable": false
      },
      {
        "text": "Libri-Light",
        "item-id": "i3660",
        "nodes": [
          {
            "text": "IEEE Xplore Abstract Record",
            "item-id": "i3687",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "IEEE Xplore Abstract Record",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 12:03:31"
              ],
              [
                "Title",
                "IEEE Xplore Abstract Record"
              ],
              [
                "URL",
                "https://ieeexplore.ieee.org/abstract/document/9052942"
              ]
            ],
            "resource": "storage/i3687.html"
          },
          {
            "text": "Kahn et al_2020_Libri-Light.pdf",
            "item-id": "i3688",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kahn et al_2020_Libri-Light.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kahn et al_2020_Libri-Light.pdf"
              ]
            ],
            "resource": "storage/i3688.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Libri-Light",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio, which is, to our knowledge, the largest freely-available corpus of speech. The audio has been segmented using voice activity detection and is tagged with SNR, speaker ID and genre descriptions. Additionally, we provide baseline systems and evaluation metrics working under three settings: (1) the zero resource/unsupervised setting (ABX), (2) the semi- supervised setting (PER, CER) and (3) the distant supervision setting (WER). Settings (2) and (3) use limited textual resources (10 minutes to 10 hours) aligned with the speech. Setting (3) uses large amounts of unaligned text. They are evaluated on the standard LibriSpeech dev and test sets for comparison with the supervised state-of-the-art."
          ],
          [
            "Access Date",
            "2024-01-14 12:03:19"
          ],
          [
            "Conference Name",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "J. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazar\u00e9, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, E. Dupoux"
          ],
          [
            "DOI",
            "10.1109/ICASSP40776.2020.9052942"
          ],
          [
            "Date",
            "2020-05-00 2020-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "7669-7673"
          ],
          [
            "Proceedings Title",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Libri-Light"
          ],
          [
            "Title",
            "Libri-Light: A Benchmark for ASR with Limited or No Supervision"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/9052942"
          ]
        ],
        "resource": "storage/i3688.pdf",
        "selectable": false
      },
      {
        "text": "Librispeech",
        "item-id": "i3653",
        "nodes": [
          {
            "text": "IEEE Xplore Abstract Record",
            "item-id": "i3689",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "IEEE Xplore Abstract Record",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 12:00:17"
              ],
              [
                "Title",
                "IEEE Xplore Abstract Record"
              ],
              [
                "URL",
                "https://ieeexplore.ieee.org/abstract/document/7178964"
              ]
            ],
            "resource": "storage/i3689.html"
          },
          {
            "text": "Panayotov et al_2015_Librispeech.pdf",
            "item-id": "i3690",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Panayotov et al_2015_Librispeech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Panayotov et al_2015_Librispeech.pdf"
              ]
            ],
            "resource": "storage/i3690.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Librispeech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems."
          ],
          [
            "Access Date",
            "2024-01-14 12:00:11"
          ],
          [
            "Conference Name",
            "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2015.7178964"
          ],
          [
            "Date",
            "2015-04-00 2015-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "5206-5210"
          ],
          [
            "Proceedings Title",
            "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Librispeech"
          ],
          [
            "Title",
            "Librispeech: An ASR corpus based on public domain audio books"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/7178964"
          ]
        ],
        "resource": "storage/i3690.pdf",
        "selectable": false
      },
      {
        "text": "Looking to Listen at the Cocktail Party",
        "item-id": "i1247",
        "nodes": [
          {
            "text": "Comment: Accepted to SIGGRAPH 2018. Project webpage: https://looking-to-listen.github.io",
            "item-id": "n1251",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted to SIGGRAPH 2018. Project webpage: https://looking-to-listen.github.io",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted to SIGGRAPH 2018. Project webpage: https://looking-to-listen.github.io</div>",
            "node_type": "note"
          },
          {
            "text": "Ephrat et al_2018_Looking to Listen at the Cocktail Party.pdf",
            "item-id": "i1250",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ephrat et al_2018_Looking to Listen at the Cocktail Party.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_FKDJT4R2/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/2\">2 Related work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/3\">3 AVSpeech Dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/4\">4 Audio-Visual Speech Separation Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/4\">4.1 Video and Audio Representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/5\">4.2 Network architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/6\">4.3 Implementation details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/6\">5 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/6\">5.1 Quantitative Analysis on Synthetic Mixtures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/8\">5.2 Real-World Speech Separation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/8\">5.3 Comparison with Previous Work in Audio-Visual Speech Separation and Enhancement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/8\">5.4 Application to Video Transcription</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/9\">5.5 Additional Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/10\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/10\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/11\">A Objective metrics used for evaluating separation quality</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/11\">A.1 SDR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FKDJT4R2/11\">A.2 ViSQOL</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ephrat et al_2018_Looking to Listen at the Cocktail Party.pdf"
              ]
            ],
            "resource": "storage/i1250.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Looking to Listen at the Cocktail Party",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest)."
          ],
          [
            "Access Date",
            "2021-12-07 14:08:55"
          ],
          [
            "Creators",
            "Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, Michael Rubinstein"
          ],
          [
            "DOI",
            "10.1145/3197517.3201357"
          ],
          [
            "Date",
            "2018-08-10 2018-08-10"
          ],
          [
            "Extra",
            "arXiv: 1804.03619"
          ],
          [
            "ISSN",
            "0730-0301, 1557-7368"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "ACM Trans. Graph."
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "1-11"
          ],
          [
            "Publication Title",
            "ACM Transactions on Graphics"
          ],
          [
            "Short Title",
            "Looking to Listen at the Cocktail Party"
          ],
          [
            "Title",
            "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1804.03619"
          ],
          [
            "Volume",
            "37"
          ]
        ],
        "resource": "storage/i1250.pdf",
        "selectable": false
      },
      {
        "text": "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation",
        "item-id": "i1705",
        "nodes": [
          {
            "text": "Comment: 22 pages, 8 figures. Under the review process",
            "item-id": "n1757",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 22 pages, 8 figures. Under the review process",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 22 pages, 8 figures. Under the review process</div>",
            "node_type": "note"
          },
          {
            "text": "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf",
            "item-id": "i1756",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G7GYACED/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/4\">2.1 Masked Autoencoders (MAE)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3 Masked Spectrogram Modeling using Masked Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3.1 Input Audio Duration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3.2 Patch Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">3.3 Feature Calculation for Downstream Tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4.1 Experimental Details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4.1.1 Pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/7\">4.1.2 Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/7\">4.2 Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/8\">4.3 Experimental Results: Comparison with the HEAR 2021 Results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4 Experimental Results: Impact of Design Choices on Performance</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4.1 Impact of Input Audio Duration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4.2 Impact of Patch Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/11\">4.4.3 Impact of Input Splitting: Patches vs. Strips</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/12\">4.5 Qualitative Analysis with Visualizations</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/12\">4.5.1 Reconstructions with Random Masks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/13\">4.5.2 Reconstructions with Patterned Masks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/13\">4.5.3 Reconstructions with Various Mask Ratios</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/15\">4.5.4 Self-Attention Map Visualizations</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/17\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/21\">A Reconstruction Examples of Various MSM-MAE Models</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf"
              ]
            ],
            "resource": "storage/i1756.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent general-purpose audio representations show state-of-the-art performance on various audio tasks. These representations are pre-trained by self-supervised learning methods that create training signals from the input. For example, typical audio contrastive learning uses temporal relationships among input sounds to create training signals, whereas some methods use a difference among input views created by data augmentations. However, these training signals do not provide information derived from the intact input sound, which we think is suboptimal for learning representation that describes the input as it is. In this paper, we seek to learn audio representations from the input itself as supervision using a pretext task of auto-encoding of masked spectrogram patches, Masked Spectrogram Modeling (MSM, a variant of Masked Image Modeling applied to audio spectrogram). To implement MSM, we use Masked Autoencoders (MAE), an image self-supervised learning method. MAE learns to efficiently encode the small number of visible patches into latent representations to carry essential information for reconstructing a large number of masked patches. While training, MAE minimizes the reconstruction error, which uses the input as training signal, consequently achieving our goal. We conducted experiments on our MSM using MAE (MSM-MAE) models under the evaluation benchmark of the HEAR 2021 NeurIPS Challenge. Our MSM-MAE models outperformed the HEAR 2021 Challenge results on seven out of 15 tasks (e.g., accuracies of 73.4% on CREMA-D and 85.8% on LibriCount), while showing top performance on other tasks where specialized models perform better. We also investigate how the design choices of MSM-MAE impact the performance and conduct qualitative analysis of visualization outcomes to gain an understanding of learned representations. We make our code available online."
          ],
          [
            "Access Date",
            "2022-07-17 14:53:45"
          ],
          [
            "Archiveid",
            "arXiv:2204.12260"
          ],
          [
            "Creators",
            "Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.12260"
          ],
          [
            "Date",
            "2022-04-26 2022-04-26"
          ],
          [
            "Extra",
            "arXiv:2204.12260 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.12260"
          ]
        ],
        "resource": "storage/i1756.pdf",
        "selectable": false
      },
      {
        "text": "RMVPE",
        "item-id": "i3657",
        "nodes": [
          {
            "text": "Comment: This paper has been accepted by INTERSPEECH 2023",
            "item-id": "n3685",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: This paper has been accepted by INTERSPEECH 2023",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: This paper has been accepted by INTERSPEECH 2023</div>",
            "node_type": "note"
          },
          {
            "text": "Wei et al_2023_RMVPE.pdf",
            "item-id": "i3684",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wei et al_2023_RMVPE.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Model Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> The Loss Function</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Datasets and Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Experimental setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Results on Polyphonic Music</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Results with Different Types of Noise</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Noise Robustness</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Results on Clean Vocals</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ISKTK55G/1\"> References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wei et al_2023_RMVPE.pdf"
              ]
            ],
            "resource": "storage/i3684.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3683",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 15:18:25"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2306.15412"
              ]
            ],
            "resource": "storage/i3683.html"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "RMVPE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Vocal pitch is an important high-level feature in music audio processing. However, extracting vocal pitch in polyphonic music is more challenging due to the presence of accompaniment. To eliminate the influence of the accompaniment, most previous methods adopt music source separation models to obtain clean vocals from polyphonic music before predicting vocal pitches. As a result, the performance of vocal pitch estimation is affected by the music source separation models. To address this issue and directly extract vocal pitches from polyphonic music, we propose a robust model named RMVPE. This model can extract effective hidden features and accurately predict vocal pitches from polyphonic music. The experimental results demonstrate the superiority of RMVPE in terms of raw pitch accuracy (RPA) and raw chroma accuracy (RCA). Additionally, experiments conducted with different types of noise show that RMVPE is robust across all signal-to-noise ratio (SNR) levels. The code of RMVPE is available at https://github.com/Dream-High/RMVPE."
          ],
          [
            "Access Date",
            "2024-01-14 15:18:17"
          ],
          [
            "Creators",
            "Haojie Wei, Xueke Cao, Tangpeng Dan, Yueguo Chen"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2023-528"
          ],
          [
            "Date",
            "2023-08-20 2023-8-20"
          ],
          [
            "Extra",
            "arXiv:2306.15412 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "5421-5425"
          ],
          [
            "Proceedings Title",
            "INTERSPEECH 2023"
          ],
          [
            "Short Title",
            "RMVPE"
          ],
          [
            "Title",
            "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.15412"
          ]
        ],
        "resource": "storage/i3684.pdf",
        "selectable": false
      },
      {
        "text": "Robust wav2vec 2.0",
        "item-id": "i2515",
        "nodes": [
          {
            "text": "Hsu et al. - 2021 - Robust wav2vec 2.0 Analyzing Domain Shift in Self.pdf",
            "item-id": "i2712",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hsu et al. - 2021 - Robust wav2vec 2.0 Analyzing Domain Shift in Self.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-06-08 06:02:35"
              ],
              [
                "Title",
                "Hsu et al. - 2021 - Robust wav2vec 2.0 Analyzing Domain Shift in Self.pdf"
              ],
              [
                "URL",
                "https://www.isca-speech.org/archive/pdfs/interspeech_2021/hsu21_interspeech.pdf"
              ]
            ],
            "resource": "storage/i2712.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Robust wav2vec 2.0",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-06-08 06:02:39"
          ],
          [
            "Conference Name",
            "Interspeech 2021"
          ],
          [
            "Creators",
            "Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, Michael Auli"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-236"
          ],
          [
            "Date",
            "2021-08-30 2021-8-30"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "721-725"
          ],
          [
            "Proceedings Title",
            "Interspeech 2021"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "Robust wav2vec 2.0"
          ],
          [
            "Title",
            "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2021/hsu21_interspeech.html"
          ]
        ],
        "resource": "storage/i2712.pdf",
        "selectable": false
      },
      {
        "text": "Speaker Representations for Speaker Adaptation in Multiple Speakers\u2019 BLSTM-RNN-Based Speech Synthesis",
        "item-id": "i1439",
        "nodes": [
          {
            "text": "Zhao et al_2016_Speaker Representations for Speaker Adaptation in Multiple Speakers\u2019.PDF",
            "item-id": "i1497",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao et al_2016_Speaker Representations for Speaker Adaptation in Multiple Speakers\u2019.PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao et al_2016_Speaker Representations for Speaker Adaptation in Multiple Speakers\u2019.PDF"
              ]
            ],
            "resource": "storage/i1497.PDF"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Speaker Representations for Speaker Adaptation in Multiple Speakers\u2019 BLSTM-RNN-Based Speech Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Training a high quality acoustic model with a limited database and synthesizing a new speaker\u2019s voice with a few utterances have been hot topics in deep neural network (DNN) based statistical parametric speech synthesis (SPSS). To solve these problems, we built a unified framework for speaker adaptive training as well as speaker adaptation on Bidirectional Long Short-Term Memory with Recurrent Neural Network (BLSTM-RNN) acoustic model. In this paper, we mainly focus on speaker identity control at the input layer of our framework. We have investigated i-vector and speaker code as different speaker representations when used in an augmented input vector, and also propose two approaches to estimate a new speaker\u2019s code. Experimental results show that the speaker representations input to the first layer of acoustic model can effectively control speaker identity during speaker adaptive training, thus improving the synthesized speech quality of speakers included in training phase. For speaker adaptation, speaker code estimated from MFCCs can achieve higher preference than other speaker representations."
          ],
          [
            "Access Date",
            "2022-03-29 09:50:21"
          ],
          [
            "Conference Name",
            "Interspeech 2016"
          ],
          [
            "Creators",
            "Yi Zhao, Daisuke Saito, Nobuaki Minematsu"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2016-506"
          ],
          [
            "Date",
            "2016-09-08 2016-9-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "2268-2272"
          ],
          [
            "Proceedings Title",
            "Interspeech 2016"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Title",
            "Speaker Representations for Speaker Adaptation in Multiple Speakers\u2019 BLSTM-RNN-Based Speech Synthesis"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2016/zhao16b_interspeech.html"
          ]
        ],
        "selectable": false
      },
      {
        "text": "Speech2Face",
        "item-id": "i630",
        "nodes": [
          {
            "text": "Oh et al_2019_Speech2Face.pdf",
            "item-id": "i631",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Oh et al_2019_Speech2Face.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Oh et al_2019_Speech2Face.pdf"
              ]
            ],
            "resource": "storage/i631.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Speech2Face",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/Youtube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers."
          ],
          [
            "Access Date",
            "2021-05-16 07:08:32"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Wojciech Matusik"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7539-7548"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Speech2Face"
          ],
          [
            "Title",
            "Speech2Face: Learning the Face Behind a Voice"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Oh_Speech2Face_Learning_the_Face_Behind_a_Voice_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i631.pdf",
        "selectable": false
      },
      {
        "text": "SpeechFormer",
        "item-id": "i2398",
        "nodes": [
          {
            "text": "Chen et al_2022_SpeechFormer.pdf",
            "item-id": "i2451",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2022_SpeechFormer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NZM4GI5A/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/2\">2  Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/2\">2.1  Vanilla Transformer</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/2\">2.1.1  MSA in vanilla Transformer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/2\">2.2  SpeechFormer framework</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/2\">2.2.1  Statistical characteristics of speech</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/3\">2.2.2  Speech-MSA in SpeechFormer block</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/3\">2.2.3  Merging block in SpeechFormer framework</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/3\">3  EXPERIMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/3\">3.1  Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/3\">3.2  Experimental setup</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/4\">3.3  Experimental results and analysis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/4\">3.3.1  Comparison to the baseline framework</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/4\">3.3.2  Comparison to previous state-of-the-art</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/4\">4  Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NZM4GI5A/5\">5  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2022_SpeechFormer.pdf"
              ]
            ],
            "resource": "storage/i2451.pdf"
          },
          {
            "text": "Comment: 5 pages, 4figures. This paper was submitted to Insterspeech 2022",
            "item-id": "n2452",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 5 pages, 4figures. This paper was submitted to Insterspeech 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 5 pages, 4figures. This paper was submitted to Insterspeech 2022</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "SpeechFormer",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Transformer has obtained promising results on cognitive speech signal processing field, which is of interest in various applications ranging from emotion to neurocognitive disorder analysis. However, most works treat speech signal as a whole, leading to the neglect of the pronunciation structure that is unique to speech and reflects the cognitive process. Meanwhile, Transformer has heavy computational burden due to its full attention operation. In this paper, a hierarchical efficient framework, called SpeechFormer, which considers the structural characteristics of speech, is proposed and can be served as a general-purpose backbone for cognitive speech signal processing. The proposed SpeechFormer consists of frame, phoneme, word and utterance stages in succession, each performing a neighboring attention according to the structural pattern of speech with high computational efficiency. SpeechFormer is evaluated on speech emotion recognition (IEMOCAP & MELD) and neurocognitive disorder detection (Pitt & DAIC-WOZ) tasks, and the results show that SpeechFormer outperforms the standard Transformer-based framework while greatly reducing the computational cost. Furthermore, our SpeechFormer achieves comparable results to the state-of-the-art approaches."
          ],
          [
            "Access Date",
            "2023-05-16 07:25:48"
          ],
          [
            "Archiveid",
            "arXiv:2203.03812"
          ],
          [
            "Creators",
            "Weidong Chen, Xiaofen Xing, Xiangmin Xu, Jianxin Pang, Lan Du"
          ],
          [
            "DOI",
            "10.48550/arXiv.2203.03812"
          ],
          [
            "Date",
            "2022-03-09 2022-03-09"
          ],
          [
            "Extra",
            "arXiv:2203.03812 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "SpeechFormer"
          ],
          [
            "Title",
            "SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2203.03812"
          ]
        ],
        "resource": "storage/i2451.pdf",
        "selectable": false
      },
      {
        "text": "Towards End-to-End Unsupervised Speech Recognition",
        "item-id": "i2200",
        "nodes": [
          {
            "text": "Comment: Preprint",
            "item-id": "n2232",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Preprint",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Preprint</div>",
            "node_type": "note"
          },
          {
            "text": "Liu et al_2022_Towards End-to-end Unsupervised Speech Recognition.pdf",
            "item-id": "i2231",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2022_Towards End-to-end Unsupervised Speech Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2022_Towards End-to-end Unsupervised Speech Recognition.pdf"
              ]
            ],
            "resource": "storage/i2231.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Towards End-to-End Unsupervised Speech Recognition",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Unsupervised speech recognition has shown great potential to make Automatic Speech Recognition (ASR) systems accessible to every language. However, existing methods still heavily rely on hand-crafted pre-processing. Similar to the trend of making supervised speech recognition end-to-end, we introduce wav2vec-U 2.0 which does away with all audio-side pre-processing and improves accuracy through better architecture. In addition, we introduce an auxiliary self-supervised objective that ties model predictions back to the input. Experiments show that wav2vec-U 2.0 improves unsupervised recognition results across different languages while being conceptually simpler."
          ],
          [
            "Access Date",
            "2023-03-12 14:22:41"
          ],
          [
            "Archiveid",
            "arXiv:2204.02492"
          ],
          [
            "Citation Key",
            "liuEndtoend2022a"
          ],
          [
            "Creators",
            "Alexander H. Liu, Wei-Ning Hsu, Michael Auli, Alexei Baevski"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.02492"
          ],
          [
            "Date",
            "2022-06-15 2022-06-15"
          ],
          [
            "Extra",
            "arXiv:2204.02492 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Towards End-to-End Unsupervised Speech Recognition"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.02492"
          ]
        ],
        "resource": "storage/i2231.pdf",
        "selectable": false
      },
      {
        "text": "Universal Paralinguistic Speech Representations Using self-Supervised Conformers",
        "item-id": "i2198",
        "nodes": [
          {
            "text": "Shor et al_2022_Universal Paralinguistic Speech Representations Using self-Supervised Conformers.pdf",
            "item-id": "i2225",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shor et al_2022_Universal Paralinguistic Speech Representations Using self-Supervised Conformers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shor et al_2022_Universal Paralinguistic Speech Representations Using self-Supervised Conformers.pdf"
              ]
            ],
            "resource": "storage/i2225.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Universal Paralinguistic Speech Representations Using self-Supervised Conformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Many speech applications require understanding aspects beyond the words being spoken, such as recognizing emotion, detecting whether the speaker is wearing a mask, or distinguishing real from synthetic speech. In this work, we introduce a new state-of-the-art paralinguistic representation derived from large-scale, fully self-supervised training of a 600M+ parameter Conformer-based architecture. We benchmark on a diverse set of speech tasks and demonstrate that simple linear classifiers trained on top of our time-averaged representation outperform nearly all previous results, in some cases by large margins. Our analyses of context-window size demonstrate that, surprisingly, 2 second context-windows achieve 96% the performance of the Conformers that use the full long-term context on 7 out of 9 tasks. Furthermore, while the best per-task representations are extracted internally in the network, stable performance across several layers allows a single universal representation to reach near optimal performance on all tasks."
          ],
          [
            "Conference Name",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Joel Shor, Aren Jansen, Wei Han, Daniel Park, Yu Zhang"
          ],
          [
            "DOI",
            "10.1109/ICASSP43922.2022.9747197"
          ],
          [
            "Date",
            "2022-05-00 2022-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "3169-3173"
          ],
          [
            "Proceedings Title",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Universal Paralinguistic Speech Representations Using self-Supervised Conformers"
          ]
        ],
        "resource": "storage/i2225.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Speech Recognition",
        "item-id": "i2516",
        "nodes": [
          {
            "text": "Baevski et al_2021_Unsupervised Speech Recognition.pdf",
            "item-id": "i2716",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Baevski et al_2021_Unsupervised Speech Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Baevski et al_2021_Unsupervised Speech Recognition.pdf"
              ]
            ],
            "resource": "storage/i2716.pdf"
          },
          {
            "text": "wav2vec-U",
            "item-id": "n2711",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "wav2vec-U",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>wav2vec-U</p>\n</div></div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Speech Recognition",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Despite rapid progress in the recent past, current speech recognition systems still require labeled training data which limits this technology to a small fraction of the languages spoken around the globe. This paper describes wav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition models without any labeled data. We leverage self-supervised speech representations to segment unlabeled audio and learn a mapping from these representations to phonemes via adversarial training. The right representations are key to the success of our method. Compared to the best previous unsupervised work, wav2vec-U reduces the phone error rate on the TIMIT benchmark from 26.1 to 11.3. On the larger English Librispeech benchmark, wav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the best published systems trained on 960 hours of labeled data from only two years ago. We also experiment on nine other languages, including low-resource languages such as Kyrgyz, Swahili and Tatar."
          ],
          [
            "Access Date",
            "2023-06-08 05:59:28"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Alexei Baevski, Wei-Ning Hsu, Alexis CONNEAU, Michael Auli"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "27826\u201327839"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Unsupervised Speech Recognition"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2021/hash/ea159dc9788ffac311592613b7f71fbb-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i2716.pdf",
        "selectable": false
      }
    ],
    "item_title": "Audio Analysis",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Audio Generation",
    "item-id": "c37,i3656",
    "nodes": [
      {
        "text": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion",
        "item-id": "i1786",
        "nodes": [
          {
            "text": "van Niekerk et al_2022_A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion.pdf",
            "item-id": "i1796",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "van Niekerk et al_2022_A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "van Niekerk et al_2022_A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion.pdf"
              ]
            ],
            "resource": "storage/i1796.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content \u2013 leading to mispronunciations. As a solution, we propose soft speech units learned by predicting a distribution over the discrete units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech.12"
          ],
          [
            "Conference Name",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Benjamin van Niekerk, Marc-Andr\u00e9 Carbonneau, Julian Za\u00efdi, Matthew Baas, Hugo Seut\u00e9, Herman Kamper"
          ],
          [
            "DOI",
            "10.1109/ICASSP43922.2022.9746484"
          ],
          [
            "Date",
            "2022-05-00 2022-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6562-6566"
          ],
          [
            "Proceedings Title",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion"
          ]
        ],
        "resource": "storage/i1796.pdf",
        "selectable": false
      },
      {
        "text": "A Review on Speech Synthesis Based on Machine Learning",
        "item-id": "i2557",
        "nodes": [
          {
            "text": "Kumari et al_2022_A Review on Speech Synthesis Based on Machine Learning.pdf",
            "item-id": "i2632",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kumari et al_2022_A Review on Speech Synthesis Based on Machine Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kumari et al_2022_A Review on Speech Synthesis Based on Machine Learning.pdf"
              ]
            ],
            "resource": "storage/i2632.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Review on Speech Synthesis Based on Machine Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, Speech synthesis is one of the growing techniques in the research domain that takes input as text and provides output as acoustical form. The speech synthesis system is more advantageous to physically impaired people. In execution process, there arise some complications by surrounding noises and communication style. To neglect such unnecessary noises various machine learning techniques are employed. In this paper, we described various techniques adopted to improve the naturalness and quality of synthesized speech. The main contribution of this paper is to elaborate and compare the characteristics of techniques utilized in speech synthesis for different languages. The techniques such as support vector machine, Artificial Neural Network, Gaussian mixture modeling, Generative adversarial network, Deep Neural Network and Hidden Markov Model are employed in this work to enhance the speech naturalness and quality of synthesized speech signals."
          ],
          [
            "Creators",
            "Ruchika Kumari, Amita Dev, Ashwni Kumar, Amita Dev, S. S. Agrawal, Arun Sharma"
          ],
          [
            "DOI",
            "10.1007/978-3-030-95711-7_3"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-030-95711-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "23-35"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Artificial Intelligence and Speech Technology"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Communications in Computer and Information Science"
          ],
          [
            "Title",
            "A Review on Speech Synthesis Based on Machine Learning"
          ]
        ],
        "resource": "storage/i2632.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Neural Speech Synthesis",
        "item-id": "i890",
        "nodes": [
          {
            "text": "Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references",
            "item-id": "n891",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references</div>",
            "node_type": "note"
          },
          {
            "text": "Tan et al_2021_A Survey on Neural Speech Synthesis.pdf",
            "item-id": "i893",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tan et al_2021_A Survey on Neural Speech Synthesis.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XPKM6J8W/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/2\">1.1 History of TTS Technology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/3\">1.2 Organization of This Survey</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/4\">2 Key Components in TTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/6\">2.1 Main Taxonomy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/6\">2.2 Text Analysis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/8\">2.3 Acoustic Models</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/8\">2.3.1 Acoustic Models in SPSS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/10\">2.3.2 Acoustic Models in End-to-End TTS</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/12\">2.4 Vocoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/16\">2.5 Towards Fully End-to-End TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/17\">2.6 Other Taxonomies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/18\">3 Advanced Topics in TTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/18\">3.1 Background and Taxonomy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/19\">3.2 Fast TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/22\">3.3 Low-Resource TTS</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/23\">3.4 Robust TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/24\">3.4.1 Enhancing Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/25\">3.4.2 Replacing Attention with Duration Prediction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/26\">3.4.3 Enhancing AR Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/27\">3.4.4 Replacing AR Generation with NAR Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/27\">3.5 Expressive TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/27\">3.5.1 Categorization of Variation Information</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/28\">3.5.2 Modeling Variation Information</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/29\">3.5.3 Disentangling, Controlling and Transferring</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/30\">3.6 Adaptive TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/31\">3.6.1 General Adaptation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/31\">3.6.2 Efficient Adaptation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/32\">4 Resources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/33\">5 Future Directions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tan et al_2021_A Survey on Neural Speech Synthesis.pdf"
              ]
            ],
            "resource": "storage/i893.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Neural Speech Synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS."
          ],
          [
            "Access Date",
            "2021-08-11 08:05:27"
          ],
          [
            "Creators",
            "Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu"
          ],
          [
            "Date",
            "2021-07-23 2021-07-23"
          ],
          [
            "Extra",
            "arXiv: 2106.15561"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2106.15561 [cs, eess]"
          ],
          [
            "Title",
            "A Survey on Neural Speech Synthesis"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2106.15561"
          ]
        ],
        "resource": "storage/i893.pdf",
        "selectable": false
      },
      {
        "text": "A deep learning approaches in text-to-speech system",
        "item-id": "i2559",
        "nodes": [
          {
            "text": "Kumar et al_2023_A deep learning approaches in text-to-speech system.pdf",
            "item-id": "i2637",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kumar et al_2023_A deep learning approaches in text-to-speech system.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kumar et al_2023_A deep learning approaches in text-to-speech system.pdf"
              ]
            ],
            "resource": "storage/i2637.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A deep learning approaches in text-to-speech system",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-to-speech systems (TTS) have come a long way in the last decade and are now a popular research topic for creating various human-computer interaction systems. Although, a range of speech synthesis models for various languages with several motive applications is available based on domain requirements. However, recent developments in speech synthesis have primarily attributed to deep learning-based techniques that have improved a variety of application scenarios, including intelligent speech interaction, chatbots, and conversational artificial intelligence (AI). Text-to-speech systems are discussed in this survey article as an active topic of study that has achieved significant progress in the recent decade, particularly for Indian and non-Indian languages. Furthermore, the study also covers the lifecycle of text-to-speech systems as well as developed platforms in it. We performed an efficient search for published survey articles up to May 2021 in the web of science, PubMed, Scopus, EBSCO(Elton B. Stephens CO (company)) and Google Scholar for Text-to-speech Systems (TTS) in various languages based on different approaches. This survey article offers a study of the contributions made by various researchers in Indian and non-Indian language text-to-speech systems and the techniques used to implement it with associated challenges in designing TTS systems. The work also compared different language text-to-speech systems based on the quality metrics such as recognition rate, accuracy, TTS score, precision, recall, and F1-score. Further, the study summarizes existing ideas and their shortcomings, emphasizing the scope of future research in Indian and non-Indian languages TTS, which may assist beginners in designing robust TTS systems."
          ],
          [
            "Access Date",
            "2023-06-07 13:12:40"
          ],
          [
            "Creators",
            "Yogesh Kumar, Apeksha Koul, Chamkaur Singh"
          ],
          [
            "DOI",
            "10.1007/s11042-022-13943-4"
          ],
          [
            "Date",
            "2023-04-01 2023-04-01"
          ],
          [
            "ISSN",
            "1573-7721"
          ],
          [
            "Issue",
            "10"
          ],
          [
            "Journal Abbreviation",
            "Multimed Tools Appl"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "15171-15197"
          ],
          [
            "Publication Title",
            "Multimedia Tools and Applications"
          ],
          [
            "Short Title",
            "A deep learning approaches in text-to-speech system"
          ],
          [
            "Title",
            "A deep learning approaches in text-to-speech system: a systematic review and recent research perspective"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11042-022-13943-4"
          ],
          [
            "Volume",
            "82"
          ]
        ],
        "resource": "storage/i2637.pdf",
        "selectable": false
      },
      {
        "text": "A review of deep learning techniques for speech processing",
        "item-id": "i2551",
        "nodes": [
          {
            "text": "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf",
            "item-id": "i2620",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf"
              ]
            ],
            "resource": "storage/i2620.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A review of deep learning techniques for speech processing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field\u2019s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field."
          ],
          [
            "Access Date",
            "2023-06-07 15:17:12"
          ],
          [
            "Creators",
            "Ambuj Mehrish, Navonil Majumder, Rishabh Bharadwaj, Rada Mihalcea, Soujanya Poria"
          ],
          [
            "DOI",
            "10.1016/j.inffus.2023.101869"
          ],
          [
            "Date",
            "2023-06-03 2023-06-03"
          ],
          [
            "ISSN",
            "1566-2535"
          ],
          [
            "Journal Abbreviation",
            "Information Fusion"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "101869"
          ],
          [
            "Publication Title",
            "Information Fusion"
          ],
          [
            "Title",
            "A review of deep learning techniques for speech processing"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1566253523001859"
          ]
        ],
        "resource": "storage/i2620.pdf",
        "selectable": false
      },
      {
        "text": "A study of speaker adaptation for DNN-based speech synthesis",
        "item-id": "i1440",
        "nodes": [
          {
            "text": "Wu et al_2015_A study of speaker adaptation for DNN-based speech synthesis.pdf",
            "item-id": "i1499",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2015_A study of speaker adaptation for DNN-based speech synthesis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2015_A study of speaker adaptation for DNN-based speech synthesis.pdf"
              ]
            ],
            "resource": "storage/i1499.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A study of speaker adaptation for DNN-based speech synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A major advantage of statistical parametric speech synthesis (SPSS) over unit-selection speech synthesis is its adaptability and controllability in changing speaker characteristics and speaking style. Recently, several studies using deep neural networks (DNNs) as acoustic models for SPSS have shown promising results. However, the adaptability of DNNs in SPSS has not been systematically studied. In this paper, we conduct an experimental analysis of speaker adaptation for DNN-based speech synthesis at different levels. In particular, we augment a low-dimensional speaker-specific vector with linguistic features as input to represent speaker identity, perform model adaptation to scale the hidden activation weights, and perform a feature space transformation at the output layer to modify generated acoustic features. We systematically analyse the performance of each individual adaptation technique and that of their combinations. Experimental results confirm the adaptability of the DNN, and listening tests demonstrate that the DNN can achieve significantly better adaptation performance than the hidden Markov model (HMM) baseline in terms of naturalness and speaker similarity."
          ],
          [
            "Conference Name",
            "Sixteenth Annual Conference of the International Speech Communication Association"
          ],
          [
            "Creators",
            "Zhizheng Wu, Pawel Swietojanski, Christophe Veaux, Steve Renals, Simon King"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2015-270"
          ],
          [
            "Date",
            "2015-09-00 2015-09"
          ],
          [
            "Library Catalog",
            "Google Scholar"
          ],
          [
            "Proceedings Title",
            "Sixteenth Annual Conference of the International Speech Communication Association"
          ],
          [
            "Title",
            "A study of speaker adaptation for DNN-based speech synthesis"
          ]
        ],
        "resource": "storage/i1499.pdf",
        "selectable": false
      },
      {
        "text": "AdaSpeech",
        "item-id": "i2541",
        "nodes": [
          {
            "text": "Chen et al_2021_AdaSpeech.pdf",
            "item-id": "i2600",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2021_AdaSpeech.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U8KNWGVA/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/3\">2 AdaSpeech</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/3\">2.1 Acoustic Condition Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/4\">2.2 Conditional Layer Normalization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/5\">2.3 Pipeline of AdaSpeech</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/5\">3 Experimental Setup</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/6\">4 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/6\">4.1 The Quality of Adaptation Voice</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/7\">4.2 Method Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U8KNWGVA/8\">5 Conclusions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2021_AdaSpeech.pdf"
              ]
            ],
            "resource": "storage/i2600.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AdaSpeech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech from her/him. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions which could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we model the acoustic information in both utterance and phoneme level. Specifically, we use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. The audio samples are available at https://speechresearch.github.io/adaspeech/."
          ],
          [
            "Access Date",
            "2023-06-07 16:00:20"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, Tie-Yan Liu"
          ],
          [
            "Date",
            "2021-01-12 2021/01/12"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "AdaSpeech"
          ],
          [
            "Title",
            "AdaSpeech: Adaptive Text to Speech for Custom Voice"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=Drynvt7gg4L"
          ]
        ],
        "resource": "storage/i2600.pdf",
        "selectable": false
      },
      {
        "text": "AdaSpeech 3",
        "item-id": "i1437",
        "nodes": [
          {
            "text": "Comment: Accepted by INTERSPEECH 2021",
            "item-id": "n1493",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted by INTERSPEECH 2021",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted by INTERSPEECH 2021</div>",
            "node_type": "note"
          },
          {
            "text": "Yan et al_2021_AdaSpeech 3.pdf",
            "item-id": "i1492",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yan et al_2021_AdaSpeech 3.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yan et al_2021_AdaSpeech 3.pdf"
              ]
            ],
            "resource": "storage/i1492.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AdaSpeech 3",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While recent text to speech (TTS) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained reading-style TTS model for spontaneous-style speech. Specifically, 1) to insert filled pauses (FP) in the text sequence appropriately, we introduce an FP predictor to the TTS model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts (MoE), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous TTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and rhythms in spontaneous styles, and achieves much better MOS and SMOS scores than previous adaptive TTS systems."
          ],
          [
            "Access Date",
            "2022-03-29 10:19:23"
          ],
          [
            "Creators",
            "Yuzi Yan, Xu Tan, Bohan Li, Guangyan Zhang, Tao Qin, Sheng Zhao, Yuan Shen, Wei-Qiang Zhang, Tie-Yan Liu"
          ],
          [
            "Date",
            "2021-07-06 2021-07-06"
          ],
          [
            "Extra",
            "arXiv: 2107.02530"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2107.02530 [cs, eess]"
          ],
          [
            "Short Title",
            "AdaSpeech 3"
          ],
          [
            "Title",
            "AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2107.02530"
          ]
        ],
        "resource": "storage/i1492.pdf",
        "selectable": false
      },
      {
        "text": "AdaSpeech 4",
        "item-id": "i2543",
        "icon": "glyphicon glyphicon-file",
        "item_title": "AdaSpeech 4",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Adaptive text to speech (TTS) can synthesize new voices in zero-shot scenarios efficiently, by using a well-trained source TTS model without adapting it on the speech data of new speakers. Considering seen and unseen speakers have diverse characteristics, zero-shot adaptive TTS requires strong generalization ability on speaker characteristics, which brings modeling challenges. In this paper, we develop AdaSpeech 4 1, a zeroshot adaptive TTS system for high-quality speech synthesis. We model the speaker characteristics systematically to improve the generalization on new speakers. Generally, the modeling of speaker characteristics can be categorized into three steps: extracting speaker representation, taking this speaker representation as condition, and synthesizing speech/mel-spectrogram given this speaker representation. Accordingly, we improve the modeling in three steps: 1) To extract speaker representation with better generalization, we factorize the speaker characteristics into basis vectors and extract speaker representation by weighted combining of these basis vectors through attention. 2) We leverage conditional layer normalization to integrate the extracted speaker representation to TTS model. 3) We propose a novel supervision loss based on the distribution of basis vectors to maintain the corresponding speaker characteristics in generated mel-spectrograms. Without any fine-tuning, AdaSpeech 4 achieves better voice quality and similarity than baselines in multiple datasets."
          ],
          [
            "Access Date",
            "2023-06-07 16:02:20"
          ],
          [
            "Conference Name",
            "Interspeech 2022"
          ],
          [
            "Creators",
            "Yihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, Tie-Yan Liu"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2022-901"
          ],
          [
            "Date",
            "2022-09-18 2022-9-18"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "2568-2572"
          ],
          [
            "Proceedings Title",
            "Interspeech 2022"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "AdaSpeech 4"
          ],
          [
            "Title",
            "AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2022/wu22f_interspeech.html"
          ]
        ]
      },
      {
        "text": "Adaspeech 2",
        "item-id": "i2534",
        "nodes": [
          {
            "text": "Yan et al_2021_Adaspeech 2.pdf",
            "item-id": "i2575",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yan et al_2021_Adaspeech 2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yan et al_2021_Adaspeech 2.pdf"
              ]
            ],
            "resource": "storage/i2575.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Adaspeech 2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text to speech (TTS) is widely used to synthesize personal voice for a target speaker, where a well-trained source TTS model is fine-tuned with few paired adaptation data (speech and its transcripts) on this target speaker. However, in many scenarios, only untranscribed speech data is available for adaptation, which brings challenges to the previous TTS adaptation pipelines (e.g., AdaSpeech). In this paper, we develop AdaSpeech 2, an adaptive TTS system that only leverages untranscribed speech data for adaptation. Specifically, we introduce a mel-spectrogram encoder to a well-trained TTS model to conduct speech reconstruction, and at the same time constrain the output sequence of the mel-spectrogram encoder to be close to that of the original phoneme encoder. In adaptation, we use untranscribed speech data for speech reconstruction and only fine-tune the TTS decoder. AdaSpeech 2 has two advantages: 1) Pluggable: our system can be easily applied to existing trained TTS models without re-training. 2) Effective: our system achieves on-par voice quality with the transcribed TTS adaptation (e.g., AdaSpeech) with the same amount of untranscribed data, and achieves better voice quality than previous untranscribed adaptation methods 1."
          ],
          [
            "Conference Name",
            "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Yuzi Yan, Xu Tan, Bohan Li, Tao Qin, Sheng Zhao, Yuan Shen, Tie-Yan Liu"
          ],
          [
            "DOI",
            "10.1109/ICASSP39728.2021.9414872"
          ],
          [
            "Date",
            "2021-06-00 2021-06"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6613-6617"
          ],
          [
            "Proceedings Title",
            "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Adaspeech 2"
          ],
          [
            "Title",
            "Adaspeech 2: Adaptive Text to Speech with Untranscribed Data"
          ]
        ],
        "resource": "storage/i2575.pdf",
        "selectable": false
      },
      {
        "text": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
        "item-id": "i2369",
        "nodes": [
          {
            "text": "Triantafyllopoulos et al_2023_An Overview of Affective Speech Synthesis and Conversion in the Deep Learning.pdf",
            "item-id": "i2402",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Triantafyllopoulos et al_2023_An Overview of Affective Speech Synthesis and Conversion in the Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Triantafyllopoulos et al_2023_An Overview of Affective Speech Synthesis and Conversion in the Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i2402.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Speech is the fundamental mode of human communication, and its synthesis has long been a core priority in human\u2013computer interaction research. In recent years, machines have managed to master the art of generating speech that is understandable by humans. However, the linguistic content of an utterance encompasses only a part of its meaning. Affect, or expressivity, has the capacity to turn speech into a medium capable of conveying intimate thoughts, feelings, and emotions\u2014aspects that are essential for engaging and naturalistic interpersonal communication. While the goal of imparting expressivity to synthesized utterances has so far remained elusive, following recent advances in text-to-speech synthesis, a paradigm shift is well under way in the fields of affective speech synthesis and conversion as well. Deep learning, as the technology that underlies most of the recent advances in artificial intelligence, is spearheading these efforts. In this overview, we outline ongoing trends and summarize state-of-the-art approaches in an attempt to provide a broad overview of this exciting field."
          ],
          [
            "Creators",
            "Andreas Triantafyllopoulos, Bj\u00f6rn W. Schuller, G\u00f6k\u00e7e \u0130ymen, Metin Sezgin, Xiangheng He, Zijiang Yang, Panagiotis Tzirakis, Shuo Liu, Silvan Mertes, Elisabeth Andr\u00e9, Ruibo Fu, Jianhua Tao"
          ],
          [
            "DOI",
            "10.1109/JPROC.2023.3250266"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Extra",
            "Conference Name: Proceedings of the IEEE"
          ],
          [
            "ISSN",
            "1558-2256"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-27"
          ],
          [
            "Publication Title",
            "Proceedings of the IEEE"
          ],
          [
            "Title",
            "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era"
          ]
        ],
        "resource": "storage/i2402.pdf",
        "selectable": false
      },
      {
        "text": "Attentron",
        "item-id": "i3215",
        "nodes": [
          {
            "text": "Choi et al_2020_Attentron.pdf",
            "item-id": "i3292",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Choi et al_2020_Attentron.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Choi et al_2020_Attentron.pdf"
              ]
            ],
            "resource": "storage/i3292.pdf"
          },
          {
            "text": "[TLDR] Attentron is proposed, a few-shot TTS model that clones voices of speakers unseen during training that significan",
            "item-id": "n3294",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "[TLDR] Attentron is proposed, a few-shot TTS model that clones voices of speakers unseen during training that significan",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">[TLDR] Attentron is proposed, a few-shot TTS model that clones voices of speakers unseen during training that significantly outperforms state-of-the-art models when generating speech for unseen speakers in terms of speaker similarity and quality.</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attentron",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "On account of growing demands for personalization, the need for a so-called few-shot TTS system that clones speakers with only a few data is emerging. To address this issue, we propose Attentron, a few-shot TTS model that clones voices of speakers unseen during training. It introduces two special encoders, each serving different purposes. A fine-grained encoder extracts variable-length style information via an attention mechanism, and a coarse-grained encoder greatly stabilizes the speech synthesis, circumventing unintelligible gibberish even for synthesizing speech of unseen speakers. In addition, the model can scale out to an arbitrary number of reference audios to improve the quality of the synthesized speech. According to our experiments, including a human evaluation, the proposed model significantly outperforms state-of-the-art models when generating speech for unseen speakers in terms of speaker similarity and quality."
          ],
          [
            "Access Date",
            "2023-11-10 16:50:41"
          ],
          [
            "Conference Name",
            "Interspeech 2020"
          ],
          [
            "Creators",
            "Seungwoo Choi, Seungju Han, Dongyoung Kim, Sungjoo Ha"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2020-2096"
          ],
          [
            "Date",
            "2020-10-25 2020-10-25"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Semantic Scholar"
          ],
          [
            "Pages",
            "2007-2011"
          ],
          [
            "Proceedings Title",
            "Interspeech 2020"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "Attentron"
          ],
          [
            "Title",
            "Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2020/choi20c_interspeech.html"
          ]
        ],
        "resource": "storage/i3292.pdf",
        "selectable": false
      },
      {
        "text": "Bi-Level Speaker Supervision for One-Shot Speech Synthesis",
        "item-id": "i2538",
        "nodes": [
          {
            "text": "Wang et al. - 2020 - Bi-Level Speaker Supervision for One-Shot Speech S.pdf",
            "item-id": "i2591",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al. - 2020 - Bi-Level Speaker Supervision for One-Shot Speech S.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-06-07 16:35:25"
              ],
              [
                "Title",
                "Wang et al. - 2020 - Bi-Level Speaker Supervision for One-Shot Speech S.pdf"
              ],
              [
                "URL",
                "https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/1737.pdf"
              ]
            ],
            "resource": "storage/i2591.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Bi-Level Speaker Supervision for One-Shot Speech Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The gap between speaker characteristics of reference speech and synthesized speech remains a challenging problem in oneshot speech synthesis. In this paper, we propose a bi-level speaker supervision framework to close the speaker characteristics gap via supervising the synthesized speech at speaker feature level and speaker identity level. The speaker feature extraction and speaker identity reconstruction are integrated in an end-to-end speech synthesis network, with the one on speaker feature level for closing speaker characteristics and the other on speaker identity level for preserving identity information. This framework guarantees that the synthesized speech has similar speaker characteristics to original speech, and it also ensures the distinguishability between different speakers. Additionally, to solve the in\ufb02uence of speech content on speaker feature extraction task, we propose a text-independent reference encoder (ti-reference encoder) module to extract speaker feature. Experiments on LibriTTS dataset show that our model is able to generate the speech similar to target speaker. Furthermore, we demonstrate that this model can learn meaningful speaker representations by bi-level speaker supervision and ti-reference encoder module."
          ],
          [
            "Access Date",
            "2023-06-07 16:35:28"
          ],
          [
            "Conference Name",
            "Interspeech 2020"
          ],
          [
            "Creators",
            "Tao Wang, Jianhua Tao, Ruibo Fu, Jiangyan Yi, Zhengqi Wen, Chunyu Qiang"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2020-1737"
          ],
          [
            "Date",
            "2020-10-25 2020-10-25"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "3989-3993"
          ],
          [
            "Proceedings Title",
            "Interspeech 2020"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Title",
            "Bi-Level Speaker Supervision for One-Shot Speech Synthesis"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2020/wang20ea_interspeech.html"
          ]
        ],
        "resource": "storage/i2591.pdf",
        "selectable": false
      },
      {
        "text": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
        "item-id": "i1766",
        "nodes": [
          {
            "text": "",
            "item-id": "n2894",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"></div>",
            "node_type": "note"
          },
          {
            "text": "Kim et al_2021_Conditional Variational Autoencoder with Adversarial Learning for End-to-End.pdf",
            "item-id": "i1774",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kim et al_2021_Conditional Variational Autoencoder with Adversarial Learning for End-to-End.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kim et al_2021_Conditional Variational Autoencoder with Adversarial Learning for End-to-End.pdf"
              ]
            ],
            "resource": "storage/i1774.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth."
          ],
          [
            "Access Date",
            "2022-08-23 18:07:56"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Jaehyeon Kim, Jungil Kong, Juhee Son"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "5530-5540"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 38th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v139/kim21f.html"
          ]
        ],
        "resource": "storage/i1774.pdf",
        "selectable": false
      },
      {
        "text": "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis",
        "item-id": "i2533",
        "nodes": [
          {
            "text": "Zhou et al_2022_Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker.pdf",
            "item-id": "i2572",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhou et al_2022_Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_L9A9H9FS/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/2\">2  Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/2\">2.1  Extracting local content and speaker embeddings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/2\">2.2  Content-dependent reference attention module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/2\">2.3  Preprocessing operations in the training stage</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/3\">3  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/3\">3.1  Training setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/3\">3.2  Compared methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/3\">3.3  Subjective evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/3\">3.4  Investigation and ablation study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/4\">3.5  Analysis and discussion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/4\">4  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L9A9H9FS/5\">5  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhou et al_2022_Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker.pdf"
              ]
            ],
            "resource": "storage/i2572.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-06-07 17:27:11"
          ],
          [
            "Conference Name",
            "Interspeech 2022"
          ],
          [
            "Creators",
            "Yixuan Zhou, Changhe Song, Xiang Li, Luwen Zhang, Zhiyong Wu, Yanyao Bian, Dan Su, Helen Meng"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2022-10054"
          ],
          [
            "Date",
            "2022-09-18 2022-9-18"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "2573-2577"
          ],
          [
            "Proceedings Title",
            "Interspeech 2022"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Title",
            "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2022/zhou22d_interspeech.html"
          ]
        ],
        "resource": "storage/i2572.pdf",
        "selectable": false
      },
      {
        "text": "Conventional and contemporary approaches used in text to speech synthesis",
        "item-id": "i2370",
        "nodes": [
          {
            "text": "Kaur_Singh_2022_Conventional and contemporary approaches used in text to speech synthesis.pdf",
            "item-id": "i2404",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kaur_Singh_2022_Conventional and contemporary approaches used in text to speech synthesis.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_77R3DZNU/1\">Abstract</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/2\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/2\">1.1 Motivation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/3\">2 Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/3\">2.1 Search strategy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/3\">2.2 Selection strategy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/3\">3 Classification of\u00a0speech synthesis techniques</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/3\">3.1 Articulatory synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/6\">3.2 Formant synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/6\">3.3 Concatenative speech synthesis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/8\">3.4 Statistical parametric speech synthesis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/8\">3.4.1 Hidden Markov model</a><ul style=\"list-style-type: none; padding-left:36px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/9\">3.4.1.1 Decision trees and\u00a0H(S)MMs </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/9\">3.4.1.2 Synthesis process </a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/10\">3.4.2 Speech synthesis based on\u00a0deep learning methods</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/11\">4 Elementary models used in\u00a0deep learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/11\">4.1 Restricted Boltzmann machines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/12\">4.2 Deep belief networks</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/14\">4.3 Deep neural network</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/14\">4.3.1 Basic principle of\u00a0working of\u00a0DNN based speech synthesis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/16\">4.4 Recurrent neural networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/17\">4.5 Deep Gaussian process</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/19\">4.6 Comparison between\u00a0various methods of\u00a0speech synthesis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/22\">4.7 Metrics used to\u00a0evaluate performance of\u00a0TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/22\">4.7.1 Objective test metrics</a><ul style=\"list-style-type: none; padding-left:36px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/22\">4.7.1.1 Itakura-Saito measure </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/22\">4.7.1.2 Root mean square (RMSE) of\u00a0log f0 </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/22\">4.7.1.3 Gross pith error (GPE) </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/23\">4.7.1.4 Voicedunvoiced (VUV) error rate </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/23\">4.7.1.5 Correlation coefficient </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/23\">4.7.1.6 Mel cepstral distortion (MCD) </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/23\">4.7.1.7 Band aperiodicity distortion (BAPD) </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/23\">4.7.1.8 Global distortion </a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/24\">4.7.2 Subjective testing metrics</a><ul style=\"list-style-type: none; padding-left:36px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/24\">4.7.2.1 Mean opinion score (MOS) </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/24\">4.7.2.2 Preference test </a></li></ul></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/24\">5 More challenging TTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/24\">5.1 Expressive TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/28\">5.2 Multi-speaker, multi-lingual TTS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/29\">6 State of\u00a0arts TTS employing deep learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/30\">6.1 TTS based on\u00a0autoregressive models</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/30\">6.1.1 WaveNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/31\">6.1.2 Tacotron</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/31\">6.1.3 Deep-voice</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_77R3DZNU/33\">6.2 TTS based on\u00a0non-autoregressive model</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/33\">6.2.1 ParaNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/33\">6.2.2 Parallel Tacotron</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/34\">6.2.3 FastSpeech</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/34\">6.2.4 Parallel WaveNet</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/37\">7 Speech corpus used for\u00a0training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/37\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/37\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_77R3DZNU/38\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kaur_Singh_2022_Conventional and contemporary approaches used in text to speech synthesis.pdf"
              ]
            ],
            "resource": "storage/i2404.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Conventional and contemporary approaches used in text to speech synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Nowadays speech synthesis or text to speech (TTS), an ability of system to produce human like natural sounding voice from the written text, is gaining popularity in the field of speech processing. For any TTS, intelligibility and naturalness are the two important measures for defining the quality of a synthesized sound which is highly dependent on the prosody modeling using acoustic model of synthesizer. The purpose of this review survey is firstly to study and analyze the various approaches used traditionally (articulatory synthesis, formant synthesis, concatenative speech synthesis and statistical parametric techniques based on hidden Markov model) and recently (statistical parametric based on deep learning approaches) for acoustic modeling with their pros and cons. The approaches based on deep learning to build the acoustic model has significantly contributed to the advancement of TTS as models based on deep learning are capable of modelling the complex context dependencies in the input data. Apart from these, this article also reviews the TTS approaches for generating speech with different voices and emotions to makes the TTS more realistic to use. It also addresses the subjective and objective metrics used to measure the quality of the synthesized voice. Various well known speech synthesis systems based on autoregressive and non-autoregressive models such as Tacotron, Deep Voice, WaveNet, Parallel WaveNet, Parallel Tacotron, FastSpeech by global tech-giant Google, Facebook, Microsoft employed the architecture of deep learning for end-to-end speech waveform generation and attained a remarkable mean opinion score (MOS)."
          ],
          [
            "Access Date",
            "2023-05-25 17:54:57"
          ],
          [
            "Creators",
            "Navdeep Kaur, Parminder Singh"
          ],
          [
            "DOI",
            "10.1007/s10462-022-10315-0"
          ],
          [
            "Date",
            "2022-11-13 2022-11-13"
          ],
          [
            "ISSN",
            "1573-7462"
          ],
          [
            "Journal Abbreviation",
            "Artif Intell Rev"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Publication Title",
            "Artificial Intelligence Review"
          ],
          [
            "Short Title",
            "Conventional and contemporary approaches used in text to speech synthesis"
          ],
          [
            "Title",
            "Conventional and contemporary approaches used in text to speech synthesis: a review"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s10462-022-10315-0"
          ]
        ],
        "resource": "storage/i2404.pdf",
        "selectable": false
      },
      {
        "text": "Expressive Neural Voice Cloning",
        "item-id": "i1433",
        "nodes": [
          {
            "text": "Neekhara et al_2021_Expressive Neural Voice Cloning.pdf",
            "item-id": "i1484",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Neekhara et al_2021_Expressive Neural Voice Cloning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_HKB5NM7P/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/3\">Background and Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/4\">Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/4\">Speaker Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/5\">Mel-Spectrogram Synthesizer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/7\">Cloning Techniques: Zero-Shot and Model Adaptation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/7\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/7\">Datasets and Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/8\">Model architecture and hyper-parameter details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/9\">Cloning Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/10\">Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/12\">Broader Impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HKB5NM7P/13\">Conclusion and Future Work</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Neekhara et al_2021_Expressive Neural Voice Cloning.pdf"
              ]
            ],
            "resource": "storage/i1484.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Expressive Neural Voice Cloning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Voice cloning is the task of learning to synthesize the voice of an unseen speaker from a few samples. While current voice cloning methods achieve promising results in Text-to-Speech (TTS) synthesis for a new voice, these approaches lack the ability to control the expressiveness of synthesized audio. In this work, we propose a controllable voice cloning method that allows fine-grained control over various style aspects of the synthesized speech for an unseen speaker. We achieve this by explicitly conditioning the speech synthesis model on a speaker encoding, pitch contour and latent style tokens during training. Through both quantitative and qualitative evaluations, we show that our framework can be used for various expressive voice cloning tasks using only a few transcribed or untranscribed speech samples for a new speaker. These cloning tasks include style transfer from a reference speech, synthesizing speech directly from text, and fine-grained style control by manipulating the style conditioning variables during inference."
          ],
          [
            "Access Date",
            "2022-03-29 13:00:09"
          ],
          [
            "Conference Name",
            "Asian Conference on Machine Learning"
          ],
          [
            "Creators",
            "Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley"
          ],
          [
            "Date",
            "2021-11-28 2021-11-28"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "252-267"
          ],
          [
            "Proceedings Title",
            "Proceedings of The 13th Asian Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Expressive Neural Voice Cloning"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v157/neekhara21a.html"
          ]
        ],
        "resource": "storage/i1484.pdf",
        "selectable": false
      },
      {
        "text": "FastSpeech",
        "item-id": "i1446",
        "nodes": [
          {
            "text": "Ren et al_2019_FastSpeech.pdf",
            "item-id": "i1505",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ren et al_2019_FastSpeech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ren et al_2019_FastSpeech.pdf"
              ]
            ],
            "resource": "storage/i1505.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FastSpeech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2022-03-29 09:36:38"
          ],
          [
            "Creators",
            "Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "FastSpeech"
          ],
          [
            "Title",
            "FastSpeech: Fast, Robust and Controllable Text to Speech"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i1505.pdf",
        "selectable": false
      },
      {
        "text": "FastSpeech 2",
        "item-id": "i2546",
        "nodes": [
          {
            "text": "Ren et al_2021_FastSpeech 2.pdf",
            "item-id": "i2608",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ren et al_2021_FastSpeech 2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_TQSQYQ8D/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/2\">2 FastSpeech 2 and 2s</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/3\">2.1 Motivation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/3\">2.2 Model Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/4\">2.3 Variance Adaptor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/5\">2.4 FastSpeech 2s</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/5\">2.5 Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/6\">3 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/6\">3.1 Experimental Setup</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/6\">3.2 Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/6\">3.2.1 Model Performance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/7\">3.2.2 Analyses on Variance Information</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/8\">3.2.3 Ablation Study</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/9\">4 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/13\">A Model Configuration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/13\">B Training and Inference</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/14\">C Modeling Pitch with Continuous Wavelet Transform</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/14\">C.1 Continuous Wavelet Transform</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/14\">C.2 Implementation Details</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/15\">D Case Study on Pitch Contour</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TQSQYQ8D/15\">E Variance Control</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ren et al_2021_FastSpeech 2.pdf"
              ]
            ],
            "resource": "storage/i2608.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FastSpeech 2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/."
          ],
          [
            "Access Date",
            "2023-06-07 15:46:51"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu"
          ],
          [
            "Date",
            "2021-01-12 2021/01/12"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "FastSpeech 2"
          ],
          [
            "Title",
            "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=piLPYqxtWuA"
          ]
        ],
        "resource": "storage/i2608.pdf",
        "selectable": false
      },
      {
        "text": "FoundationTTS",
        "item-id": "i2535",
        "nodes": [
          {
            "text": "Xue et al_2023_FoundationTTS.pdf",
            "item-id": "i2581",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xue et al_2023_FoundationTTS.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xue et al_2023_FoundationTTS.pdf"
              ]
            ],
            "resource": "storage/i2581.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FoundationTTS",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines."
          ],
          [
            "Access Date",
            "2023-06-07 17:11:49"
          ],
          [
            "Archiveid",
            "arXiv:2303.02939"
          ],
          [
            "Creators",
            "Ruiqing Xue, Yanqing Liu, Lei He, Xu Tan, Linquan Liu, Edward Lin, Sheng Zhao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.02939"
          ],
          [
            "Date",
            "2023-03-07 2023-03-07"
          ],
          [
            "Extra",
            "arXiv:2303.02939 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FoundationTTS"
          ],
          [
            "Title",
            "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.02939"
          ]
        ],
        "resource": "storage/i2581.pdf",
        "selectable": false
      },
      {
        "text": "Freevc",
        "item-id": "i3656",
        "nodes": [
          {
            "text": "IEEE Xplore Abstract Record",
            "item-id": "i3681",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "IEEE Xplore Abstract Record",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 15:49:40"
              ],
              [
                "Title",
                "IEEE Xplore Abstract Record"
              ],
              [
                "URL",
                "https://ieeexplore.ieee.org/abstract/document/10095191"
              ]
            ],
            "resource": "storage/i3681.html"
          },
          {
            "text": "Li et al_2023_Freevc.pdf",
            "item-id": "i3682",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2023_Freevc.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2023_Freevc.pdf"
              ]
            ],
            "resource": "storage/i3682.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Freevc",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Voice conversion (VC) can be achieved by first extracting source content information and target speaker information, and then reconstructing waveform with these information. However, current approaches normally either extract dirty content information with speaker information leaked in, or demand a large amount of annotated data for training. Besides, the quality of reconstructed waveform can be degraded by the mismatch between conversion model and vocoder. In this paper, we adopt the end-to-end framework of VITS for high-quality waveform reconstruction, and propose strategies for clean content information extraction without text annotation. We disentangle content information by imposing an information bottleneck to WavLM features, and propose the spectrogram-resize based data augmentation to improve the purity of extracted content information. Experimental results show that the proposed method outperforms the latest VC models trained with annotated data and has greater robustness."
          ],
          [
            "Access Date",
            "2024-01-14 15:49:27"
          ],
          [
            "Conference Name",
            "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Jingyi Li, Weiping Tu, Li Xiao"
          ],
          [
            "DOI",
            "10.1109/ICASSP49357.2023.10095191"
          ],
          [
            "Date",
            "2023-06-00 2023-06"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-5"
          ],
          [
            "Proceedings Title",
            "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Freevc"
          ],
          [
            "Title",
            "Freevc: Towards High-Quality Text-Free One-Shot Voice Conversion"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/10095191"
          ]
        ],
        "resource": "storage/i3682.pdf",
        "selectable": false
      },
      {
        "text": "Glow-TTS",
        "item-id": "i2539",
        "nodes": [
          {
            "text": "Kim et al_2020_Glow-TTS.pdf",
            "item-id": "i2606",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kim et al_2020_Glow-TTS.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kim et al_2020_Glow-TTS.pdf"
              ]
            ],
            "resource": "storage/i2606.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Glow-TTS",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting."
          ],
          [
            "Access Date",
            "2023-06-07 15:51:15"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Jaehyeon Kim, Sungwon Kim, Jungil Kong, Sungroh Yoon"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "8067\u20138077"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "Glow-TTS"
          ],
          [
            "Title",
            "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i2606.pdf",
        "selectable": false
      },
      {
        "text": "High quality, lightweight and adaptable TTS using LPCNet",
        "item-id": "i1436",
        "nodes": [
          {
            "text": "Comment: Accepted to Interspeech 2019",
            "item-id": "n1490",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted to Interspeech 2019",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted to Interspeech 2019</div>",
            "node_type": "note"
          },
          {
            "text": "Kons et al_2019_High quality, lightweight and adaptable TTS using LPCNet.pdf",
            "item-id": "i1489",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kons et al_2019_High quality, lightweight and adaptable TTS using LPCNet.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_D2QT4D7A/1\">High quality, lightweight and adaptable TTS using LPCNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/1\">1. Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/1\">2. System architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/2\">2.1. Prosody generator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/2\">2.2. Synthesizer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/3\">2.3. LPCNet decoder</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/3\">3. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/3\">3.1. High quality voices</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/3\">3.2. Voice adaptation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/4\">3.3. Performance</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/4\">4. Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2QT4D7A/4\">5. References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kons et al_2019_High quality, lightweight and adaptable TTS using LPCNet.pdf"
              ]
            ],
            "resource": "storage/i1489.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "High quality, lightweight and adaptable TTS using LPCNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a lightweight adaptable neural TTS system with high quality output. The system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and Linear Prediction Coding Net as a neural vocoder. This system can synthesize speech with close to natural quality while running 3 times faster than real-time on a standard CPU. The modular setup of the system allows for simple adaptation to new voices with a small amount of data. We first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9 % for female voices."
          ],
          [
            "Access Date",
            "2022-03-29 10:22:27"
          ],
          [
            "Creators",
            "Zvi Kons, Slava Shechtman, Alex Sorin, Carmel Rabinovitz, Ron Hoory"
          ],
          [
            "Date",
            "2019-06-26 2019-06-26"
          ],
          [
            "Extra",
            "arXiv: 1905.00590"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1905.00590 [cs, eess]"
          ],
          [
            "Title",
            "High quality, lightweight and adaptable TTS using LPCNet"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1905.00590"
          ]
        ],
        "resource": "storage/i1489.pdf",
        "selectable": false
      },
      {
        "text": "High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder",
        "item-id": "i2529",
        "nodes": [
          {
            "text": "Haque et al_2020_High-Fidelity Audio Generation and Representation Learning With Guided.pdf",
            "item-id": "i2738",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Haque et al_2020_High-Fidelity Audio Generation and Representation Learning With Guided.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_P782DFVB/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/2\">II Background and Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/2\">II-A Audio Representation Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">II-B Audio Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">II-C Closely Related Architectures</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III Proposed Research Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III-A Architecture of the GAAE</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III-A1 Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/3\">III-A2 Classifier</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/4\">III-A3 Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-A4 Discriminators</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-B Losses</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-B1 Encoder, Classifier and Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/5\">III-B2 Discriminators' loss</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV Data and Evaluation Metrics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV-A Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV-B Data Preprocessing</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/6\">IV-C Measurement Metrics</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">IV-C1 Inception Score (IS)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">IV-C2 Fr\u00e9chet Inception Distance (FID)</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V Experimental Setup, Results and Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V-A Impact of Labelled Data for Conditional Sample Generation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V-A1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/7\">V-A2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B Evaluation of Conditional Sample Generation based on Guidance</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B2 Result and Discussion: Manual test</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-B3 Results and Discussions: CNN based Classification accuracy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/8\">V-C Conditional Sample Generation using guidance from a different dataset</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/9\">V-C1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-C2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-D Guided representation Learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-D1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-D2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-E General Representation/Style Representation Learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/10\">V-E1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-E2 Results and Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-F Coherence of the General Representation/Latent Space</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-F1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/11\">V-F2 Results</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/12\">VI Hyperparameter Tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/12\">VII Classifier of the GAAE model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/12\">VIII Conclusion and Lesson Learnt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/1\">REFERENCES</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A ARCHITECTURAL DETAILS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A-A Supervised BigGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A-B Unsupervised BigGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/16\">A-C BiGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/17\">A-D GAAE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">A-E Simple Classifier</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">Kazi Nazmul Haque</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">Rajib Rana</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P782DFVB/18\">Bj\u00f6rn W. Schuller (M'05-SM'15-F'18)</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Haque et al_2020_High-Fidelity Audio Generation and Representation Learning With Guided.pdf"
              ]
            ],
            "resource": "storage/i2738.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating high-fidelity conditional audio samples and learning representation from unlabelled audio data are two challenging problems in machine learning research. Recent advances in the Generative Adversarial Neural Networks (GAN) architectures show great promise in addressing these challenges. To learn powerful representation using GAN architecture, it requires superior sample generation quality, which requires an enormous amount of labelled data. In this paper, we address this issue by proposing Guided Adversarial Autoencoder (GAAE), which can generate superior conditional audio samples from unlabelled audio data using a small percentage of labelled data as guidance. Representation learned from unlabelled data without any supervision does not guarantee its' usability for any downstream task. On the other hand, during the representation learning, if the model is highly biased towards the downstream task, it losses its generalisation capability. This makes the learned representation hardly useful for any other tasks that are not related to that downstream task. The proposed GAAE model also address these issues. Using this superior conditional generation, GAAE can learn representation specific to the downstream task. Furthermore, GAAE learns another type of representation capturing the general attributes of the data, which is independent of the downstream task at hand. Experimental results involving the S09 and the NSynth dataset attest the superior performance of GAAE compared to the state-of-the-art alternatives."
          ],
          [
            "Creators",
            "Kazi Nazmul Haque, Rajib Rana, Bj\u00f6rn W. Schuller"
          ],
          [
            "DOI",
            "10.1109/ACCESS.2020.3040797"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Access"
          ],
          [
            "ISSN",
            "2169-3536"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "223509-223528"
          ],
          [
            "Publication Title",
            "IEEE Access"
          ],
          [
            "Title",
            "High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i2738.pdf",
        "selectable": false
      },
      {
        "text": "Mega-TTS",
        "item-id": "i3209",
        "nodes": [
          {
            "text": "Jiang et al_2023_Mega-TTS.pdf",
            "item-id": "i3281",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2023_Mega-TTS.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_3PRRW9B3/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/3\">Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/3\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/4\">Disentangling speech into different components</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/5\">P-LLM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/6\">Speech prompting for inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/7\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/7\">Experimental setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/8\">Results of zero-shot synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/9\">Results of zero-shot speech editing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/9\">Results of zero-shot cross-lingual TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/9\">Results of robustness evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/10\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/15\">Detailed Experimental Settings</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/15\">Model Configurations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/15\">Details in Subjective Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/17\">Details of Speaker Diarization Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/17\">Details of Speaker Similarity Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/17\">Details of ASR Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/17\">Error Bars and Random Seeds</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/17\">Visualizations of Mel-Spectrograms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/17\">Visualization of Representations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/18\">Hyperparameter Selection for the Information Bottleneck</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/19\">Ablation Studies of Dataset Size and Model Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/20\">Limitations and Future Works</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3PRRW9B3/20\">Broader Impacts</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2023_Mega-TTS.pdf"
              ]
            ],
            "resource": "storage/i3281.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Mega-TTS",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Scaling text-to-speech to a large and wild dataset has been proven to be highly effective in achieving timbre and speech style generalization, particularly in zero-shot TTS. However, previous works usually encode speech into latent using audio codec and use autoregressive language models or diffusion models to generate it, which ignores the intrinsic nature of speech and may lead to inferior or uncontrollable results. We argue that speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases. From this perspective, we carefully design a novel and large zero-shot TTS system called Mega-TTS, which is trained with large-scale wild data and models different attributes in different ways: 1) Instead of using latent encoded by audio codec as the intermediate feature, we still choose spectrogram as it separates the phase and other attributes very well. Phase can be appropriately constructed by the GAN-based vocoder and does not need to be modeled by the language model. 2) We model the timbre using global vectors since timbre is a global attribute that changes slowly over time. 3) We further use a VQGAN-based acoustic model to generate the spectrogram and a latent code language model to fit the distribution of prosody, since prosody changes quickly over time in a sentence, and language models can capture both local and long-range dependencies. We scale Mega-TTS to multi-domain datasets with 20K hours of speech and evaluate its performance on unseen speakers. Experimental results demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, robustness, and speaker similarity due to the proper inductive bias of each module. Audio samples are available at https://mega-tts.github.io/demo-page."
          ],
          [
            "Access Date",
            "2023-11-14 04:51:38"
          ],
          [
            "Archiveid",
            "arXiv:2306.03509"
          ],
          [
            "Creators",
            "Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.03509"
          ],
          [
            "Date",
            "2023-06-06 2023-06-06"
          ],
          [
            "Extra",
            "arXiv:2306.03509 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Mega-TTS"
          ],
          [
            "Title",
            "Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.03509"
          ]
        ],
        "resource": "storage/i3281.pdf",
        "selectable": false
      },
      {
        "text": "Mega-TTS 2",
        "item-id": "i2998",
        "nodes": [
          {
            "text": "Jiang et al_2023_Mega-TTS 2.pdf",
            "item-id": "i3020",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2023_Mega-TTS 2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DFC63RGL/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/2\">Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFC63RGL/3\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/3\">Utilizing Prompts of Arbitrary Length</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/4\">Auto-Regressive Duration Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/4\">Prosody Interpolation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFC63RGL/5\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/5\">Experimental setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/6\">Results of Zero-Shot Speech Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/7\">Results of Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFC63RGL/7\">Conclusions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2023_Mega-TTS 2.pdf"
              ]
            ],
            "resource": "storage/i3020.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Mega-TTS 2",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Zero-shot text-to-speech aims at synthesizing voices with unseen speech prompts. Previous large-scale multispeaker TTS models have successfully achieved this goal with an enrolled recording within 10 seconds. However, most of them are designed to utilize only short speech prompts. The limited information in short speech prompts significantly hinders the performance of fine-grained identity imitation. In this paper, we introduce Mega-TTS 2, a generic zero-shot multispeaker TTS model that is capable of synthesizing speech for unseen speakers with arbitrary-length prompts. Specifically, we 1) design a multi-reference timbre encoder to extract timbre information from multiple reference speeches; 2) and train a prosody language model with arbitrary-length speech prompts; With these designs, our model is suitable for prompts of different lengths, which extends the upper bound of speech quality for zero-shot text-to-speech. Besides arbitrary-length prompts, we introduce arbitrary-source prompts, which leverages the probabilities derived from multiple P-LLM outputs to produce expressive and controlled prosody. Furthermore, we propose a phoneme-level auto-regressive duration model to introduce in-context learning capabilities to duration modeling. Experiments demonstrate that our method could not only synthesize identity-preserving speech with a short prompt of an unseen speaker but also achieve improved performance with longer speech prompts. Audio samples can be found in https://mega-tts.github.io/mega2_demo/."
          ],
          [
            "Access Date",
            "2023-08-09 13:07:00"
          ],
          [
            "Archiveid",
            "arXiv:2307.07218"
          ],
          [
            "Creators",
            "Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2307.07218"
          ],
          [
            "Date",
            "2023-07-14 2023-07-14"
          ],
          [
            "Extra",
            "arXiv:2307.07218 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Mega-TTS 2"
          ],
          [
            "Title",
            "Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2307.07218"
          ]
        ],
        "resource": "storage/i3020.pdf",
        "selectable": false
      },
      {
        "text": "Meta-StyleSpeech",
        "item-id": "i1435",
        "nodes": [
          {
            "text": "Min et al_2021_Meta-StyleSpeech.pdf",
            "item-id": "i1487",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Min et al_2021_Meta-StyleSpeech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Min et al_2021_Meta-StyleSpeech.pdf"
              ]
            ],
            "resource": "storage/i1487.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Meta-StyleSpeech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With rapid progress in neural text-to-speech (TTS) models, personalized speech generation is now in high demand for many applications. For practical applicability, a TTS model should generate high-quality speech with only a few audio samples from the given speaker, that are also short in length. However, existing methods either require to fine-tune the model or achieve low adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a new TTS model which not only synthesizes high-quality speech but also effectively adapts to new speakers. Specifically, we propose Style-Adaptive Layer Normalization (SALN) which aligns gain and bias of the text input according to the style extracted from a reference speech audio. With SALN, our model effectively synthesizes speech in the style of the target speaker even from a single speech audio. Furthermore, to enhance StyleSpeech\u2019s adaptation to speech from new speakers, we extend it to Meta-StyleSpeech by introducing two discriminators trained with style prototypes, and performing episodic training. The experimental results show that our models generate high-quality speech which accurately follows the speaker\u2019s voice with single short-duration (1-3 sec) speech audio, significantly outperforming baselines."
          ],
          [
            "Access Date",
            "2022-03-29 10:24:38"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Dongchan Min, Dong Bok Lee, Eunho Yang, Sung Ju Hwang"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "7748-7759"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 38th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Short Title",
            "Meta-StyleSpeech"
          ],
          [
            "Title",
            "Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v139/min21b.html"
          ]
        ],
        "resource": "storage/i1487.pdf",
        "selectable": false
      },
      {
        "text": "Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis",
        "item-id": "i1445",
        "nodes": [
          {
            "text": "Fan et al_2015_Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis.pdf",
            "item-id": "i1504",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Fan et al_2015_Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Fan et al_2015_Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis.pdf"
              ]
            ],
            "resource": "storage/i1504.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In DNN-based TTS synthesis, DNNs hidden layers can be viewed as deep transformation for linguistic features and the output layers as representation of acoustic space to regress the transformed linguistic features to acoustic parameters. The deep-layered architectures of DNN can not only represent highly-complex transformation compactly, but also take advantage of huge amount of training data. In this paper, we propose an approach to model multiple speakers TTS with a general DNN, where the same hidden layers are shared among different speakers while the output layers are composed of speaker-dependent nodes explaining the target of each speaker. The experimental results show that our approach can significantly improve the quality of synthesized speech objectively and subjectively, comparing with speech synthesized from the individual, speaker-dependent DNN-based TTS. We further transfer the hidden layers for a new speaker with limited training data and the resultant synthesized speech of the new speaker can also achieve a good quality in term of naturalness and speaker similarity."
          ],
          [
            "Conference Name",
            "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Yuchen Fan, Yao Qian, Frank K. Soong, Lei He"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2015.7178817"
          ],
          [
            "Date",
            "2015-04-00 2015-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4475-4479"
          ],
          [
            "Proceedings Title",
            "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis"
          ]
        ],
        "resource": "storage/i1504.pdf",
        "selectable": false
      },
      {
        "text": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
        "item-id": "i776",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n1040",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"4\"><p>Annotation</p>\n<p>Tacotron 2</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Shen et al_2018_Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions.pdf",
            "item-id": "i814",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shen et al_2018_Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shen et al_2018_Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions.pdf"
              ]
            ],
            "resource": "storage/i814.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, Rif A. Saurous, Yannis Agiomvrgiannakis, Yonghui Wu"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8461368"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4779-4783"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions"
          ]
        ],
        "resource": "storage/i814.pdf",
        "selectable": false
      },
      {
        "text": "NaturalSpeech",
        "item-id": "i2545",
        "nodes": [
          {
            "text": "Comment: 19 pages, 3 figures, 8 tables",
            "item-id": "n2604",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 19 pages, 3 figures, 8 tables",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 19 pages, 3 figures, 8 tables</div>",
            "node_type": "note"
          },
          {
            "text": "Tan et al_2022_NaturalSpeech.pdf",
            "item-id": "i2603",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tan et al_2022_NaturalSpeech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tan et al_2022_NaturalSpeech.pdf"
              ]
            ],
            "resource": "storage/i2603.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "NaturalSpeech",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text to speech (TTS) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a TTS system can achieve human-level quality, how to define/judge that quality and how to achieve it. In this paper, we answer these questions by first defining the human-level quality based on the statistical significance of subjective measure and introducing appropriate guidelines to judge it, and then developing a TTS system called NaturalSpeech that achieves human-level quality on a benchmark dataset. Specifically, we leverage a variational autoencoder (VAE) for end-to-end text to waveform generation, with several key modules to enhance the capacity of the prior from text and reduce the complexity of the posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and a memory mechanism in VAE. Experiment evaluations on popular LJSpeech dataset show that our proposed NaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human recordings at the sentence level, with Wilcoxon signed rank test at p-level p >> 0.05, which demonstrates no statistically significant difference from human recordings for the first time on this dataset."
          ],
          [
            "Access Date",
            "2023-06-07 15:55:43"
          ],
          [
            "Archiveid",
            "arXiv:2205.04421"
          ],
          [
            "Creators",
            "Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, Frank Soong, Tao Qin, Sheng Zhao, Tie-Yan Liu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2205.04421"
          ],
          [
            "Date",
            "2022-05-10 2022-05-10"
          ],
          [
            "Extra",
            "arXiv:2205.04421 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "NaturalSpeech"
          ],
          [
            "Title",
            "NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2205.04421"
          ]
        ],
        "resource": "storage/i2603.pdf",
        "selectable": false
      },
      {
        "text": "NaturalSpeech 2",
        "item-id": "i2550",
        "nodes": [
          {
            "text": "Comment: A large-scale text-to-speech and singing voice synthesis system with latent diffusion models. Update: NaturalSp",
            "item-id": "n2618",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: A large-scale text-to-speech and singing voice synthesis system with latent diffusion models. Update: NaturalSp",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: A large-scale text-to-speech and singing voice synthesis system with latent diffusion models. Update: NaturalSpeech 2 extension to voice conversion and speech enhancement</div>",
            "node_type": "note"
          },
          {
            "text": "Shen et al_2023_NaturalSpeech 2.pdf",
            "item-id": "i2617",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shen et al_2023_NaturalSpeech 2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shen et al_2023_NaturalSpeech 2.pdf"
              ]
            ],
            "resource": "storage/i2617.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "NaturalSpeech 2",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2."
          ],
          [
            "Access Date",
            "2023-06-07 15:20:29"
          ],
          [
            "Archiveid",
            "arXiv:2304.09116"
          ],
          [
            "Creators",
            "Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian"
          ],
          [
            "DOI",
            "10.48550/arXiv.2304.09116"
          ],
          [
            "Date",
            "2023-05-30 2023-05-30"
          ],
          [
            "Extra",
            "arXiv:2304.09116 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "NaturalSpeech 2"
          ],
          [
            "Title",
            "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2304.09116"
          ]
        ],
        "resource": "storage/i2617.pdf",
        "selectable": false
      },
      {
        "text": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
        "item-id": "i2180",
        "nodes": [
          {
            "text": "Comment: Working in progress",
            "item-id": "n2190",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Working in progress",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Working in progress</div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2023_Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers.pdf",
            "item-id": "i2189",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers.pdf"
              ]
            ],
            "resource": "storage/i2189.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work."
          ],
          [
            "Access Date",
            "2023-01-10 02:33:45"
          ],
          [
            "Archiveid",
            "arXiv:2301.02111"
          ],
          [
            "Creators",
            "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2301.02111"
          ],
          [
            "Date",
            "2023-01-05 2023-01-05"
          ],
          [
            "Extra",
            "arXiv:2301.02111 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2301.02111"
          ]
        ],
        "resource": "storage/i2189.pdf",
        "selectable": false
      },
      {
        "text": "Neural Speech Synthesis with Transformer Network",
        "item-id": "i2547",
        "nodes": [
          {
            "text": "Li et al_2019_Neural Speech Synthesis with Transformer Network.pdf",
            "item-id": "i2610",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2019_Neural Speech Synthesis with Transformer Network.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2019_Neural Speech Synthesis with Transformer Network.pdf"
              ]
            ],
            "resource": "storage/i2610.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Speech Synthesis with Transformer Network",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS)."
          ],
          [
            "Access Date",
            "2023-06-07 15:41:35"
          ],
          [
            "Creators",
            "Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu"
          ],
          [
            "DOI",
            "10.1609/aaai.v33i01.33016706"
          ],
          [
            "Date",
            "2019-07-17 2019-07-17"
          ],
          [
            "Extra",
            "Number: 01"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "01"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "6706-6713"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c) 2019 Association for the Advancement of Artificial Intelligence"
          ],
          [
            "Title",
            "Neural Speech Synthesis with Transformer Network"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/4642"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i2610.pdf",
        "selectable": false
      },
      {
        "text": "Non-Attentive Tacotron",
        "item-id": "i1419",
        "nodes": [
          {
            "text": "Shen et al_2021_Non-Attentive Tacotron.pdf",
            "item-id": "i1453",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shen et al_2021_Non-Attentive Tacotron.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shen et al_2021_Non-Attentive Tacotron.pdf"
              ]
            ],
            "resource": "storage/i1453.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Non-Attentive Tacotron",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
          ],
          [
            "Access Date",
            "2022-04-04 08:21:57"
          ],
          [
            "Creators",
            "Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, Yonghui Wu"
          ],
          [
            "Date",
            "2021-05-11 2021-05-11"
          ],
          [
            "Extra",
            "arXiv: 2010.04301"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2010.04301 [cs]"
          ],
          [
            "Short Title",
            "Non-Attentive Tacotron"
          ],
          [
            "Title",
            "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2010.04301"
          ]
        ],
        "resource": "storage/i1453.pdf",
        "selectable": false
      },
      {
        "text": "Parallel Tacotron 2",
        "item-id": "i1421",
        "nodes": [
          {
            "text": "Comment: Submitted to INTERSPEECH 2021",
            "item-id": "n1459",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Submitted to INTERSPEECH 2021",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Submitted to INTERSPEECH 2021</div>",
            "node_type": "note"
          },
          {
            "text": "Elias et al_2021_Parallel Tacotron 2.pdf",
            "item-id": "i1458",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Elias et al_2021_Parallel Tacotron 2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WCFWX8AS/1\">1  Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/1\">2  Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/2\">3  Duration Modeling in Parallel Tacotron</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/2\">4  Parallel Tacotron 2</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/2\">4.1  Differentiable Duration Modeling &amp;amp; Upsampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/3\">4.2  Reconstruction Loss using Soft-DTW</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/3\">4.3  Fine-grained Token Level VAE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/3\">4.4  Training Objective</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/3\">5  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/3\">5.1  Training Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/3\">5.2  Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/4\">5.3  Experimental Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/4\">5.4  Manual Control of Durations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/4\">6  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCFWX8AS/5\">7  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Elias et al_2021_Parallel Tacotron 2.pdf"
              ]
            ],
            "resource": "storage/i1458.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Parallel Tacotron 2",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces Parallel Tacotron 2, a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic Time Warping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations. Its duration control capability is also demonstrated."
          ],
          [
            "Access Date",
            "2022-04-04 08:20:19"
          ],
          [
            "Creators",
            "Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, R. J. Skerry-Ryan, Yonghui Wu"
          ],
          [
            "Date",
            "2021-08-29 2021-08-29"
          ],
          [
            "Extra",
            "arXiv: 2103.14574"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2103.14574 [cs, eess]"
          ],
          [
            "Short Title",
            "Parallel Tacotron 2"
          ],
          [
            "Title",
            "Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2103.14574"
          ]
        ],
        "resource": "storage/i1458.pdf",
        "selectable": false
      },
      {
        "text": "SC-GlowTTS",
        "item-id": "i2532",
        "nodes": [
          {
            "text": "Casanova et al. - 2021 - SC-GlowTTS An Efficient Zero-Shot Multi-Speaker T.pdf",
            "item-id": "i2599",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Casanova et al. - 2021 - SC-GlowTTS An Efficient Zero-Shot Multi-Speaker T.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-06-07 16:05:10"
              ],
              [
                "Title",
                "Casanova et al. - 2021 - SC-GlowTTS An Efficient Zero-Shot Multi-Speaker T.pdf"
              ],
              [
                "URL",
                "https://www.isca-speech.org/archive/pdfs/interspeech_2021/casanova21b_interspeech.pdf"
              ]
            ],
            "resource": "storage/i2599.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SC-GlowTTS",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose SC-GlowTTS: an ef\ufb01cient zeroshot multi-speaker text-to-speech model that improves similarity for speakers unseen during training. We propose a speaker-conditional architecture that explores a \ufb02ow-based decoder that works in a zero-shot scenario. As text encoders, we explore a dilated residual convolutional-based encoder, gated convolutional-based encoder, and transformer-based encoder. Additionally, we have shown that adjusting a GAN-based vocoder for the spectrograms predicted by the TTS model on the training dataset can signi\ufb01cantly improve the similarity and speech quality for new speakers. Our model converges using only 11 speakers, reaching state-of-the-art results for similarity with new speakers, as well as high speech quality."
          ],
          [
            "Access Date",
            "2023-06-07 16:05:14"
          ],
          [
            "Conference Name",
            "Interspeech 2021"
          ],
          [
            "Creators",
            "Edresson Casanova, Christopher Shulby, Eren G\u00f6lge, Nicolas Michael M\u00fcller, Frederico Santos De Oliveira, Arnaldo Candido Jr., Anderson Da Silva Soares, Sandra Maria Aluisio, Moacir Antonelli Ponti"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-1774"
          ],
          [
            "Date",
            "2021-08-30 2021-8-30"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "3645-3649"
          ],
          [
            "Proceedings Title",
            "Interspeech 2021"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "SC-GlowTTS"
          ],
          [
            "Title",
            "SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2021/casanova21b_interspeech.html"
          ]
        ],
        "resource": "storage/i2599.pdf",
        "selectable": false
      },
      {
        "text": "Sample Efficient Adaptive Text-to-Speech",
        "item-id": "i1438",
        "nodes": [
          {
            "text": "Chen et al_2019_Sample Efficient Adaptive Text-to-Speech.pdf",
            "item-id": "i1495",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2019_Sample Efficient Adaptive Text-to-Speech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2019_Sample Efficient Adaptive Text-to-Speech.pdf"
              ]
            ],
            "resource": "storage/i1495.pdf"
          },
          {
            "text": "Comment: Accepted by ICLR 2019",
            "item-id": "n1496",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted by ICLR 2019",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted by ICLR 2019</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Sample Efficient Adaptive Text-to-Speech",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, we learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. We introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker embedding with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers."
          ],
          [
            "Access Date",
            "2022-03-29 10:17:36"
          ],
          [
            "Creators",
            "Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan Wang, Luis C. Cobo, Andrew Trask, Ben Laurie, Caglar Gulcehre, A\u00e4ron van den Oord, Oriol Vinyals, Nando de Freitas"
          ],
          [
            "Date",
            "2019-01-16 2019-01-16"
          ],
          [
            "Extra",
            "arXiv: 1809.10460"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1809.10460 [cs, stat]"
          ],
          [
            "Title",
            "Sample Efficient Adaptive Text-to-Speech"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1809.10460"
          ]
        ],
        "resource": "storage/i1495.pdf",
        "selectable": false
      },
      {
        "text": "Speak, Read and Prompt",
        "item-id": "i2536",
        "nodes": [
          {
            "text": "Kharitonov et al_2023_Speak, Read and Prompt.pdf",
            "item-id": "i2584",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kharitonov et al_2023_Speak, Read and Prompt.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kharitonov et al_2023_Speak, Read and Prompt.pdf"
              ]
            ],
            "resource": "storage/i2584.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Speak, Read and Prompt",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests."
          ],
          [
            "Access Date",
            "2023-06-07 17:08:05"
          ],
          [
            "Archiveid",
            "arXiv:2302.03540"
          ],
          [
            "Creators",
            "Eugene Kharitonov, Damien Vincent, Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, Neil Zeghidour"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.03540"
          ],
          [
            "Date",
            "2023-02-07 2023-02-07"
          ],
          [
            "Extra",
            "arXiv:2302.03540 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Speak, Read and Prompt"
          ],
          [
            "Title",
            "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.03540"
          ]
        ],
        "resource": "storage/i2584.pdf",
        "selectable": false
      },
      {
        "text": "SpeechPainter",
        "item-id": "i2357",
        "nodes": [
          {
            "text": "Borsos et al_2022_SpeechPainter.pdf",
            "item-id": "i2457",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Borsos et al_2022_SpeechPainter.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LSJ9I93M/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/2\">2  Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/2\">2.1  Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/3\">2.2  Two-phase Training</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/3\">3  Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/5\">4  Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/5\">5  Broader Impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/5\">6  Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSJ9I93M/5\">7  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Borsos et al_2022_SpeechPainter.pdf"
              ]
            ],
            "resource": "storage/i2457.pdf"
          },
          {
            "text": "Comment: Submitted to Interspeech 2022",
            "item-id": "n2458",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Submitted to Interspeech 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Submitted to Interspeech 2022</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "SpeechPainter",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose SpeechPainter, a model for filling in gaps of up to one second in speech samples by leveraging an auxiliary textual input. We demonstrate that the model performs speech inpainting with the appropriate content, while maintaining speaker identity, prosody and recording environment conditions, and generalizing to unseen speakers. Our approach significantly outperforms baselines constructed using adaptive TTS, as judged by human raters in side-by-side preference and MOS tests."
          ],
          [
            "Access Date",
            "2023-05-11 09:17:27"
          ],
          [
            "Archiveid",
            "arXiv:2202.07273"
          ],
          [
            "Creators",
            "Zal\u00e1n Borsos, Matt Sharifi, Marco Tagliasacchi"
          ],
          [
            "DOI",
            "10.48550/arXiv.2202.07273"
          ],
          [
            "Date",
            "2022-03-30 2022-03-30"
          ],
          [
            "Extra",
            "arXiv:2202.07273 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "SpeechPainter"
          ],
          [
            "Title",
            "SpeechPainter: Text-conditioned Speech Inpainting"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2202.07273"
          ]
        ],
        "resource": "storage/i2457.pdf",
        "selectable": false
      },
      {
        "text": "Survey of Deep Learning Paradigms for Speech Processing",
        "item-id": "i2554",
        "nodes": [
          {
            "text": "Bhangale_Kothandaraman_2022_Survey of Deep Learning Paradigms for Speech Processing.pdf",
            "item-id": "i2625",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bhangale_Kothandaraman_2022_Survey of Deep Learning Paradigms for Speech Processing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bhangale_Kothandaraman_2022_Survey of Deep Learning Paradigms for Speech Processing.pdf"
              ]
            ],
            "resource": "storage/i2625.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Survey of Deep Learning Paradigms for Speech Processing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Over the past decades, a particular focus is given to research on machine learning techniques for speech processing applications. However, in the past few years, research has focused on using deep learning for speech processing applications. This new machine learning field has become a very attractive area of study and has remarkably better performance than the others in the various speech processing applications. This paper presents a brief survey of application deep learning for various speech processing applications such as speech separation, speech enhancement, speech recognition, speaker recognition, emotion recognition, language recognition, music recognition, speech data retrieval, etc. The survey goes on to cover the use of Auto-Encoder, Generative Adversarial Network, Restricted Boltzmann Machine, Deep Belief Network, Deep Neural Network, Convolutional Neural Network, Recurrent Neural Network and Deep Reinforcement Learning for speech processing. Additionally, it focuses on the various speech database and evaluation metrics used by deep learning algorithms for performance evaluation."
          ],
          [
            "Access Date",
            "2023-06-07 14:44:04"
          ],
          [
            "Creators",
            "Kishor Barasu Bhangale, Mohanaprasad Kothandaraman"
          ],
          [
            "DOI",
            "10.1007/s11277-022-09640-y"
          ],
          [
            "Date",
            "2022-07-01 2022-07-01"
          ],
          [
            "ISSN",
            "1572-834X"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Journal Abbreviation",
            "Wireless Pers Commun"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "1913-1949"
          ],
          [
            "Publication Title",
            "Wireless Personal Communications"
          ],
          [
            "Title",
            "Survey of Deep Learning Paradigms for Speech Processing"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11277-022-09640-y"
          ],
          [
            "Volume",
            "125"
          ]
        ],
        "resource": "storage/i2625.pdf",
        "selectable": false
      },
      {
        "text": "Tacotron",
        "item-id": "i2549",
        "nodes": [
          {
            "text": "Wang et al_2017_Tacotron.pdf",
            "item-id": "i2612",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2017_Tacotron.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_E586YTDL/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E586YTDL/3\">3 Model Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/3\">3.1 CBHG module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/3\">3.2 Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/4\">3.3 Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/5\">3.4 Post-processing net and waveform synthesis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/5\">4 Model Details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E586YTDL/5\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/6\">5.1 Ablation analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/7\">5.2 Mean opinion score tests</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E586YTDL/8\">6 Discussions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2017_Tacotron.pdf"
              ]
            ],
            "resource": "storage/i2612.pdf"
          },
          {
            "text": "[TLDR] Tacotron is presented, an end-to-end generative text- to-speech model that synthesizes speech directly from chara",
            "item-id": "n2614",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "[TLDR] Tacotron is presented, an end-to-end generative text- to-speech model that synthesizes speech directly from chara",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">[TLDR] Tacotron is presented, an end-to-end generative text- to-speech model that synthesizes speech directly from characters that achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Tacotron",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods."
          ],
          [
            "Access Date",
            "2023-06-07 15:37:19"
          ],
          [
            "Conference Name",
            "Interspeech 2017"
          ],
          [
            "Creators",
            "Yuxuan Wang, R.J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2017-1452"
          ],
          [
            "Date",
            "2017-08-20 2017-8-20"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Semantic Scholar"
          ],
          [
            "Pages",
            "4006-4010"
          ],
          [
            "Proceedings Title",
            "Interspeech 2017"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "Tacotron"
          ],
          [
            "Title",
            "Tacotron: Towards End-to-End Speech Synthesis"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2017/wang17n_interspeech.html"
          ]
        ],
        "resource": "storage/i2612.pdf",
        "selectable": false
      },
      {
        "text": "Text to Speech Synthesis",
        "item-id": "i2556",
        "nodes": [
          {
            "text": "ResearchGate Link",
            "item-id": "i2630",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "ResearchGate Link",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-06-07 14:41:10"
              ],
              [
                "Title",
                "ResearchGate Link"
              ],
              [
                "URL",
                "https://www.researchgate.net/publication/364280141_Text_to_Speech_Synthesis_A_Systematic_Review_Deep_Learning_Based_Architecture_and_Future_Research_Direction"
              ]
            ]
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Text to Speech Synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Creators",
            "Fahima Khanam, Farha Munmun, Nadia Ritu, Dr. Aloke Saha, M. Ph. D."
          ],
          [
            "DOI",
            "10.12720/jait.13.5.398-412"
          ],
          [
            "Date",
            "2022-10-00 2022-10"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Journal of Advances in Information Technology"
          ],
          [
            "Library Catalog",
            "ResearchGate"
          ],
          [
            "Pages",
            "398-412"
          ],
          [
            "Publication Title",
            "Journal of Advances in Information Technology"
          ],
          [
            "Short Title",
            "Text to Speech Synthesis"
          ],
          [
            "Title",
            "Text to Speech Synthesis: A Systematic Review, Deep Learning Based Architecture and Future Research Direction"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "selectable": false
      },
      {
        "text": "Text-to-Speech Synthesis",
        "item-id": "i2548",
        "icon": "glyphicon glyphicon-book",
        "item_title": "Text-to-Speech Synthesis",
        "item_type": "book",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-to-Speech Synthesis provides a complete, end-to-end account of the process of generating speech by computer. Giving an in-depth explanation of all aspects of current speech synthesis technology, it assumes no specialized prior knowledge. Introductory chapters on linguistics, phonetics, signal processing and speech signals lay the foundation, with subsequent material explaining how this knowledge is put to use in building practical systems that generate speech. Including coverage of the very latest techniques such as unit selection, hidden Markov model synthesis, and statistical text analysis, explanations of the more traditional techniques such as format synthesis and synthesis by rule are also provided. Weaving together the various strands of this multidisciplinary field, the book is designed for graduate students in electrical engineering, computer science, and linguistics. It is also an ideal reference for practitioners in the fields of human communication interaction and telephony."
          ],
          [
            "Creators",
            "Paul Taylor"
          ],
          [
            "Date",
            "2009-02-19 2009-02-19"
          ],
          [
            "ISBN",
            "978-0-521-89927-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Google Books"
          ],
          [
            "Num Pages",
            "626"
          ],
          [
            "Publisher",
            "Cambridge University Press"
          ],
          [
            "Title",
            "Text-to-Speech Synthesis"
          ]
        ]
      },
      {
        "text": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
        "item-id": "i775",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n1035",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"4\"><p>Annotation</p>\n<p>Voice cloning. SV2TTS</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Jia et al_2018_Transfer learning from speaker verification to multispeaker text-to-speech.pdf",
            "item-id": "i812",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jia et al_2018_Transfer learning from speaker verification to multispeaker text-to-speech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jia et al_2018_Transfer learning from speaker verification to multispeaker text-to-speech.pdf"
              ]
            ],
            "resource": "storage/i812.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation."
          ],
          [
            "Access Date",
            "2021-08-06"
          ],
          [
            "Conference Name",
            "Proceedings of the 32nd International Conference on Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu"
          ],
          [
            "Date",
            "2018-12-03 December 3, 2018"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "4485\u20134495"
          ],
          [
            "Place",
            "Red Hook, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 32nd International Conference on Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates Inc."
          ],
          [
            "Series",
            "NIPS'18"
          ],
          [
            "Title",
            "Transfer learning from speaker verification to multispeaker text-to-speech synthesis"
          ]
        ],
        "resource": "storage/i812.pdf",
        "selectable": false
      },
      {
        "text": "Transformers in Speech Processing",
        "item-id": "i2555",
        "nodes": [
          {
            "text": "Comment: under-review",
            "item-id": "n2629",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: under-review",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: under-review</div>",
            "node_type": "note"
          },
          {
            "text": "Latif et al_2023_Transformers in Speech Processing.pdf",
            "item-id": "i2628",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Latif et al_2023_Transformers in Speech Processing.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_IKFNYT3D/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-A Sequential Models for Speech Processing</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B Overview of Transformers</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B1 Self-Attention Layer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B2 Masked Self-Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B3 Multi-Head Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/3\">II-B4 Positional Encoding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/3\">II-C Popular Transformers for Speech</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/4\">II-C1 wav2vec</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C2 data2vec</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C3 Whisper</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C4 Tacotron</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C5 VALL-E</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C6 Conformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/6\">II-C7 UniSpeech</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/6\">II-C8 Speechformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/7\">II-C9 WavLM</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/7\">III Literature Review</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/7\">III-A Automatic Speech Recognition (ASR)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/8\">III-B Neural Speech Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/9\">III-C Speech Translation (ST)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/11\">III-D Speech Paralinguistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/11\">III-E Speech Enhancement and Separation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/13\">III-F Spoken Dialogue Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/15\">III-G Multi-Modal Applications</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/17\">IV Challenges and Future Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/17\">IV-A Training Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/17\">IV-B Computational Cost and Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/18\">IV-C Large Data Requirements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/18\">IV-D Generalization and Transferability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/18\">IV-E Multimodal Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/19\">IV-F Robustness</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/19\">V Summary and Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/20\">VI Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/20\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Latif et al_2023_Transformers in Speech Processing.pdf"
              ]
            ],
            "resource": "storage/i2628.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Transformers in Speech Processing",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues."
          ],
          [
            "Access Date",
            "2023-06-07 14:42:33"
          ],
          [
            "Archiveid",
            "arXiv:2303.11607"
          ],
          [
            "Creators",
            "Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, Junaid Qadir"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.11607"
          ],
          [
            "Date",
            "2023-03-21 2023-03-21"
          ],
          [
            "Extra",
            "arXiv:2303.11607 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Transformers in Speech Processing"
          ],
          [
            "Title",
            "Transformers in Speech Processing: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.11607"
          ]
        ],
        "resource": "storage/i2628.pdf",
        "selectable": false
      },
      {
        "text": "YourTTS",
        "item-id": "i2540",
        "nodes": [
          {
            "text": "Casanova et al_2022_YourTTS.pdf",
            "item-id": "i2594",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Casanova et al_2022_YourTTS.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Casanova et al_2022_YourTTS.pdf"
              ]
            ],
            "resource": "storage/i2594.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "YourTTS",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training."
          ],
          [
            "Access Date",
            "2023-06-07 16:19:47"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Edresson Casanova, Julian Weber, Christopher D. Shulby, Arnaldo Candido Junior, Eren G\u00f6lge, Moacir A. Ponti"
          ],
          [
            "Date",
            "2022-06-28 2022-06-28"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "2709-2720"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 39th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Short Title",
            "YourTTS"
          ],
          [
            "Title",
            "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v162/casanova22a.html"
          ]
        ],
        "resource": "storage/i2594.pdf",
        "selectable": false
      },
      {
        "text": "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings",
        "item-id": "i2542",
        "nodes": [
          {
            "text": "Cooper et al_2020_Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker.pdf",
            "item-id": "i2598",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cooper et al_2020_Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cooper et al_2020_Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker.pdf"
              ]
            ],
            "resource": "storage/i2598.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While speaker adaptation for end-to-end speech synthesis using speaker embeddings can produce good speaker similarity for speakers seen during training, there remains a gap for zero-shot adaptation to unseen speakers. We investigate multi-speaker modeling for end-to-end text-to-speech synthesis and study the effects of different types of state-of-the-art neural speaker embeddings on speaker similarity for unseen speakers. Learnable dictionary encoding-based speaker embeddings with angular softmax loss can improve equal error rates over x-vectors in a speaker verification task; these embeddings also improve speaker similarity and naturalness for unseen speakers when used for zero-shot adaptation to new speakers in end-to-end speech synthesis."
          ],
          [
            "Conference Name",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen, Junichi Yamagishi"
          ],
          [
            "DOI",
            "10.1109/ICASSP40776.2020.9054535"
          ],
          [
            "Date",
            "2020-05-00 2020-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6184-6188"
          ],
          [
            "Proceedings Title",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings"
          ]
        ],
        "resource": "storage/i2598.pdf",
        "selectable": false
      },
      {
        "text": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration",
        "item-id": "i1017",
        "nodes": [
          {
            "text": "Comment: Published in Interspeech'21",
            "item-id": "n1023",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Published in Interspeech'21",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Published in Interspeech'21</div>",
            "node_type": "note"
          },
          {
            "text": "Tang et al_2021_Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration.pdf",
            "item-id": "i1022",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tang et al_2021_Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KEDEF29M/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KEDEF29M/2\">2  Proposed Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/2\">2.1  Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/2\">2.2  Multi-modal alignment phase</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">2.3  Decoding phase</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3.1  Experiment setup</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3.1.1  Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3.1.2  Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3.1.3  Evaluation methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3.2  Speech naturalness</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/3\">3.2.1  Speech naturalness identification test</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/4\">3.2.2  Speech naturalness MOS test</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/4\">3.2.3  Duration prediction</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/4\">3.3  Speaker similarity</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/4\">4  Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KEDEF29M/5\">5  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tang et al_2021_Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration.pdf"
              ]
            ],
            "resource": "storage/i1022.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Given a piece of speech and its transcript text, text-based speech editing aims to generate speech that can be seamlessly inserted into the given speech by editing the transcript. Existing methods adopt a two-stage approach: synthesize the input text using a generic text-to-speech (TTS) engine and then transform the voice to the desired voice using voice conversion (VC). A major problem of this framework is that VC is a challenging problem which usually needs a moderate amount of parallel training data to work satisfactorily. In this paper, we propose a one-stage context-aware framework to generate natural and coherent target speech without any training data of the target speaker. In particular, we manage to perform accurate zero-shot duration prediction for the inserted text. The predicted duration is used to regulate both text embedding and speech embedding. Then, based on the aligned cross-modality input, we directly generate the mel-spectrogram of the edited speech with a transformer-based decoder. Subjective listening tests show that despite the lack of training data for the speaker, our method has achieved satisfactory results. It outperforms a recent zero-shot TTS engine by a large margin."
          ],
          [
            "Access Date",
            "2021-10-12 05:28:40"
          ],
          [
            "Creators",
            "Chuanxin Tang, Chong Luo, Zhiyuan Zhao, Dacheng Yin, Yucheng Zhao, Wenjun Zeng"
          ],
          [
            "Date",
            "2021-09-12 2021-09-12"
          ],
          [
            "Extra",
            "arXiv: 2109.05426"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2109.05426 [cs, eess]"
          ],
          [
            "Title",
            "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2109.05426"
          ]
        ],
        "resource": "storage/i1022.pdf",
        "selectable": false
      },
      {
        "text": "nnSpeech",
        "item-id": "i1789",
        "nodes": [
          {
            "text": "Zhao et al_2022_nnSpeech.pdf",
            "item-id": "i1801",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao et al_2022_nnSpeech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao et al_2022_nnSpeech.pdf"
              ]
            ],
            "resource": "storage/i1801.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "nnSpeech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multi-speaker text-to-speech (TTS) using a few adaption data is a challenge in practical applications. To address that, we propose a zero-shot multi-speaker TTS, named nnSpeech, that could synthesis a new speaker voice without fine-tuning and using only one adaption utterance. Compared with using a speaker representation module to extract the characteristics of new speakers, our method bases on a speaker-guided conditional variational autoencoder and can generate a variable Z, which contains both speaker characteristics and content information. The latent variable Z distribution is approximated by another variable conditioned on reference mel-spectrogram and phoneme. Experiments on the English corpus, Mandarin corpus, and cross-dataset proves that our model could generate natural and similar speech with only one adaption speech."
          ],
          [
            "Conference Name",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Botao Zhao, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao"
          ],
          [
            "DOI",
            "10.1109/ICASSP43922.2022.9746875"
          ],
          [
            "Date",
            "2022-05-00 2022-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4293-4297"
          ],
          [
            "Proceedings Title",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "nnSpeech"
          ],
          [
            "Title",
            "nnSpeech: Speaker-Guided Conditional Variational Autoencoder for Zero-Shot Multi-speaker text-to-speech"
          ]
        ],
        "resource": "storage/i1801.pdf",
        "selectable": false
      },
      {
        "text": "wu22f_interspeech.pdf",
        "item-id": "i2544",
        "icon": "glyphicon glyphicon-paperclip",
        "item_title": "wu22f_interspeech.pdf",
        "item_type": "attachment",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-06-07 16:02:15"
          ],
          [
            "Title",
            "wu22f_interspeech.pdf"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/pdfs/interspeech_2022/wu22f_interspeech.pdf"
          ]
        ],
        "resource": "storage/i2544.pdf"
      }
    ],
    "item_title": "Audio Generation",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Auto Encoder",
    "item-id": "c11,i2561",
    "nodes": [
      {
        "text": "Adversarial Autoencoders",
        "item-id": "i2525",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n2732",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotation</p>\n<p>Adversarial Autoencoder (AAE)</p>\n<p>The MCMC methods computing the gradient becomes more imprecise in training progress.</p>\n<p>Some generative models can avoid the difficulties of the training by being trained via direct back-propagation.</p>\n<p>The AAE can convert an autoencoder as a generative model. Both the reconstruction loss and adversarial loss are used in training, fitting the distribution of latent representation to any prior distribution (for example, normal distribution). The authors proposed several structures for unsupervised, semi-supervised and supervised lerning. In other word, the AAE has a discriminator classifing the latent representation if it is from encoder output or prior distribution. Therefore, with the discriminator loss, the encoder should confuse the discriminator to think all latent representations are from predefined distribution, which means these two distribution are identical. So the distribution of latent representation can be controlled.</p>\n<p>MNIST, Toronto face dataset (TFD) and SVHN.</p>\n<p>Use Log-likelihood of test data to compare with other generative networks. The results are the best. Use classification error for comparing the semi-supervised AAE structure with other generative networks. The results are the best compared to the others. Use clustering error rate to compare the clustering performance.</p>\n<p>+ve: Can use the prior distribution by its samples rather than the explicit functional form. Can be used in different situations (unsupervised, semi-unsupervised, clustering, ...).</p>\n<p>-ve:</p>\n<p>AAE is a great example for the improvement of AE, as it combined the concept of AE and GAN to regularize the distribution of latent representations.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Makhzani et al_2016_Adversarial Autoencoders.pdf",
            "item-id": "i2734",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Makhzani et al_2016_Adversarial Autoencoders.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_TL4LF463/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/2\">1.1 Generative Adversarial Networks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TL4LF463/2\">2 Adversarial Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/3\">2.1 Relationship to Variational Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/5\">2.2 Relationship to GANs and GMMNs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/5\">2.3 Incorporating Label Information in the Adversarial Regularization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/6\">3 Likelihood Analysis of Adversarial Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/8\">4 Supervised Adversarial Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/9\">5 Semi-Supervised Adversarial Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/10\">6 Unsupervised Clustering with Adversarial Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/11\">7 Dimensionality Reduction with Adversarial Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/13\">8 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TL4LF463/15\">Appendix A Experiment Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/15\">A.1 Likelihood Experiments</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TL4LF463/15\">A.2 Semi-Supervised Experiments</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/15\">A.2.1 MNIST</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/15\">A.2.2 SVHN</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TL4LF463/16\">A.3 Unsupervised Clustering Experiments</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Makhzani et al_2016_Adversarial Autoencoders.pdf"
              ]
            ],
            "resource": "storage/i2734.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Adversarial Autoencoders",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks."
          ],
          [
            "Access Date",
            "2023-06-07 18:22:22"
          ],
          [
            "Archiveid",
            "arXiv:1511.05644"
          ],
          [
            "Creators",
            "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey"
          ],
          [
            "DOI",
            "10.48550/arXiv.1511.05644"
          ],
          [
            "Date",
            "2016-05-24 2016-05-24"
          ],
          [
            "Extra",
            "arXiv:1511.05644 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Adversarial Autoencoders"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1511.05644"
          ]
        ],
        "resource": "storage/i2734.pdf",
        "selectable": false
      },
      {
        "text": "Adversarial Latent Autoencoders",
        "item-id": "i74",
        "nodes": [
          {
            "text": "Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf",
            "item-id": "i299",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf"
              ]
            ],
            "resource": "storage/i299.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Adversarial Latent Autoencoders",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture."
          ],
          [
            "Access Date",
            "2021-03-16 14:44:03"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Stanislav Pidhorskyi, Donald A. Adjeroh, Gianfranco Doretto"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14104-14113"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Adversarial Latent Autoencoders"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i299.pdf",
        "selectable": false
      },
      {
        "text": "Generating Diverse High-Fidelity Images with VQ-VAE-2",
        "item-id": "i1558",
        "nodes": [
          {
            "text": "Razavi et al_2019_Generating Diverse High-Fidelity Images with VQ-VAE-2.pdf",
            "item-id": "i1591",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Razavi et al_2019_Generating Diverse High-Fidelity Images with VQ-VAE-2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Razavi et al_2019_Generating Diverse High-Fidelity Images with VQ-VAE-2.pdf"
              ]
            ],
            "resource": "storage/i1591.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generating Diverse High-Fidelity Images with VQ-VAE-2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE  requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of  VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity."
          ],
          [
            "Access Date",
            "2022-05-21 05:34:56"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ali Razavi, Aaron van den Oord, Oriol Vinyals"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Generating Diverse High-Fidelity Images with VQ-VAE-2"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i1591.pdf",
        "selectable": false
      },
      {
        "text": "Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation",
        "item-id": "i2561",
        "nodes": [
          {
            "text": "Adiban et al_2022_Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder.pdf",
            "item-id": "i2638",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Adiban et al_2022_Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Adiban et al_2022_Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder.pdf"
              ]
            ],
            "resource": "storage/i2638.pdf"
          },
          {
            "text": "Other",
            "item-id": "n2640",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Other",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><h2>Other</h2>\n12 pages plus supplementary material. Submitted to BMVC 2022</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a multi-layer variational autoencoder method, we call HR-VQVAE, that learns hierarchical discrete representations of the data. By utilizing a novel objective function, each layer in HR-VQVAE learns a discrete representation of the residual from previous layers through a vector quantized encoder. Furthermore, the representations at each layer are hierarchically linked to those at previous layers. We evaluate our method on the tasks of image reconstruction and generation. Experimental results demonstrate that the discrete representations learned by HR-VQVAE enable the decoder to reconstruct high-quality images with less distortion than the baseline methods, namely VQVAE and VQVAE-2. HR-VQVAE can also generate high-quality and diverse images that outperform state-of-the-art generative models, providing further verification of the efficiency of the learned representations. The hierarchical nature of HR-VQVAE i) reduces the decoding search time, making the method particularly suitable for high-load tasks and ii) allows to increase the codebook size without incurring the codebook collapse problem."
          ],
          [
            "Access Date",
            "2023-06-02 07:17:42"
          ],
          [
            "Conference Name",
            "British Machine Vision Conference"
          ],
          [
            "Creators",
            "Mohammad Adiban, Kalin Stefanov, Sabato Marco Siniscalchi, Giampiero Salvi"
          ],
          [
            "DOI",
            "10.48550/ARXIV.2208.04554"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Version Number: 1"
          ],
          [
            "Library Catalog",
            "Semantic Scholar"
          ],
          [
            "Place",
            "London, UK"
          ],
          [
            "Proceedings Title",
            "British Machine Vision Conference"
          ],
          [
            "Publisher",
            "BMVA Press"
          ],
          [
            "Rights",
            "Creative Commons Attribution Non Commercial No Derivatives 4.0 International"
          ],
          [
            "Title",
            "Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation"
          ],
          [
            "URL",
            "https://bmvc2022.mpi-inf.mpg.de/0636.pdf"
          ]
        ],
        "resource": "storage/i2638.pdf",
        "selectable": false
      },
      {
        "text": "Neural Discrete Representation Learning",
        "item-id": "i1527",
        "nodes": [
          {
            "text": "van den Oord et al_2017_Neural Discrete Representation Learning.pdf",
            "item-id": "i1544",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "van den Oord et al_2017_Neural Discrete Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "van den Oord et al_2017_Neural Discrete Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i1544.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Discrete Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of posterior collapse'' -\u2014 where the latents are ignored when they are paired with a powerful autoregressive decoder -\u2014 typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations."
          ],
          [
            "Access Date",
            "2022-05-12 15:41:24"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Aaron van den Oord, Oriol Vinyals, koray kavukcuoglu"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Neural Discrete Representation Learning"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i1544.pdf",
        "selectable": false
      },
      {
        "text": "Nonlinear principal component analysis using autoassociative neural networks",
        "item-id": "i5",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n312",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Auto Encoder (Nonlinear Principal Component Analysis NLPCA)</p>\n<p>Engineers are often confronted with the problem of extracting information about poorly-known process of data</p>\n<p>The dimensionality reduction is closely related to feature extraction and it can capture the information contained in the original data.</p>\n<p>The proposed method is the using multi-layer neural networks with same input and output and a bottleneck. The training target is to minimize the reconstruction error. Compared to PCA, the nonlinearity improve the performance of feature extraction.</p>\n<p>Datasets are generated by some simple functions.</p>\n<p>The results of experiments shows the reconstruction error is less than PCA and ANN without mapping layer.</p>\n<p>+ve: Can find and eliminates nonlinear correlations in the data. Can remove redundant information. More effective than PCA.</p>\n<p>-ve: Limited by the practicalities of computing functional approximations from limited data.</p>\n<p>The origin of auto encoder. A great milestone.</p>\n<p></p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf",
            "item-id": "i357",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf"
              ]
            ],
            "resource": "storage/i357.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Nonlinear principal component analysis using autoassociative neural networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal \u201cbottleneck\u201d layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters."
          ],
          [
            "Access Date",
            "2021-03-16 13:41:25"
          ],
          [
            "Creators",
            "Mark A. Kramer"
          ],
          [
            "DOI",
            "https://doi.org/10.1002/aic.690370209"
          ],
          [
            "Date",
            "1991-00-00 1991"
          ],
          [
            "Extra",
            "_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690370209"
          ],
          [
            "ISSN",
            "1547-5905"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Wiley Online Library"
          ],
          [
            "Pages",
            "233-243"
          ],
          [
            "Publication Title",
            "AIChE Journal"
          ],
          [
            "Rights",
            "Copyright \u00a9 1991 American Institute of Chemical Engineers"
          ],
          [
            "Title",
            "Nonlinear principal component analysis using autoassociative neural networks"
          ],
          [
            "URL",
            "https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690370209"
          ],
          [
            "Volume",
            "37"
          ]
        ],
        "resource": "storage/i357.pdf",
        "selectable": false
      },
      {
        "text": "Reducing the Dimensionality of Data with Neural Networks",
        "item-id": "i4",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n310",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Using RBM to pretrain the Auto encoder for dimensionality reduction.</p>\n<p>Multi-layer auto encoder is hard to train.</p>\n<p>Use a very different type of algorithm to pretrain the model and prove it can be generalized to other datasets.</p>\n<p>The method introduced in this paper uses RBM as the algorithm to pretrain the weights of an autoencoder. For each layer of the autoencoder, the weights will be pretrained as RBM, and pretrain the next layer. With the RBM pretrained weights, the autoencoder has better performance for reconstruction.</p>\n<p>MNIST hand-written digits and Olivetti face dataset</p>\n<p>No quantitative comparison.</p>\n<p>+ve: Better reconstruction results than PCA. This pretraining can also be used for classification and regression. Pretraining helps generalization. The autoencoders can map in bi-direction between data and code spaces. Can apply to very large datasets.</p>\n<p>-ve:</p>\n<p>Provide a new pretrain method for auto encoder to improve the performance. Combine the RBM and the neural networks.</p>\n<p></p>\n<p></p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf",
            "item-id": "i349",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf"
              ]
            ],
            "resource": "storage/i349.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Reducing the Dimensionality of Data with Neural Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks."
          ],
          [
            "Access Date",
            "2021-03-16 14:00:23"
          ],
          [
            "Creators",
            "G. E. Hinton, R. R. Salakhutdinov"
          ],
          [
            "DOI",
            "10.1126/science.1127647"
          ],
          [
            "Date",
            "2006-07-28 2006/07/28"
          ],
          [
            "Extra",
            "Publisher: American Association for the Advancement of Science\nSection: Report\nPMID: 16873662"
          ],
          [
            "ISSN",
            "0036-8075, 1095-9203"
          ],
          [
            "Issue",
            "5786"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "science.sciencemag.org"
          ],
          [
            "Pages",
            "504-507"
          ],
          [
            "Publication Title",
            "Science"
          ],
          [
            "Rights",
            "American Association for the Advancement of Science"
          ],
          [
            "Title",
            "Reducing the Dimensionality of Data with Neural Networks"
          ],
          [
            "URL",
            "https://science.sciencemag.org/content/313/5786/504"
          ],
          [
            "Volume",
            "313"
          ]
        ],
        "resource": "storage/i349.pdf",
        "selectable": false
      },
      {
        "text": "Stacked Denoising Autoencoders",
        "item-id": "i3",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n313",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Denoising autoencoder (DAE)</p>\n<p>To overcome the gap between the stacking RBMs and stacking autoencoders. And find out what can shape a good, useful representation.</p>\n<p>The authors were looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns. And expected the latent representation is robust and stable, also can perform denoising task.</p>\n<p>The method proposed is the DAE which adds the noise to input data, and train the autoencoder to reconstruct the original data without noise. The noise added can be Gaussian noise or masking noise. With the noise inside, DAE can learn the features more stable and robust. So, an autoencoder with noisy data input can encode as higher level representations which have better performance than without noise added. The stacked DAE is the pretrained method to improve the performance of autoencoder.</p>\n<p>The dataset used is the 12 x 12 patches from Olshausen for DAE. The MNIST, and tzanetakis audio genre classification data set for stacked DAE.</p>\n<p>They compared the test error rate for classification task with other methods, and the SDAE (stacked DAE) performs the best.</p>\n<p>+ve: Better performance than DBNs. Establish the value of using the denoising criterion as an unsupervised objective for useful higher level presentations.</p>\n<p>-ve: The DAEs used in the paper are shallow.</p>\n<p>This paper introduced a simple way (add noise to input data) to improve the performance of encoder, which is easy to implement and prove the value for denoising task in the AE training. </p>\n<p></p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Vincent et al_2010_Stacked Denoising Autoencoders.pdf",
            "item-id": "i348",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Vincent et al_2010_Stacked Denoising Autoencoders.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Vincent et al_2010_Stacked Denoising Autoencoders.pdf"
              ]
            ],
            "resource": "storage/i348.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Stacked Denoising Autoencoders",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
          ],
          [
            "Creators",
            "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol"
          ],
          [
            "Date",
            "2010-12-01 December 1, 2010"
          ],
          [
            "ISSN",
            "1532-4435"
          ],
          [
            "Journal Abbreviation",
            "J. Mach. Learn. Res."
          ],
          [
            "Library Catalog",
            "3/1/2010"
          ],
          [
            "Pages",
            "3371\u20133408"
          ],
          [
            "Publication Title",
            "The Journal of Machine Learning Research"
          ],
          [
            "Short Title",
            "Stacked Denoising Autoencoders"
          ],
          [
            "Title",
            "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
          ],
          [
            "Volume",
            "11"
          ]
        ],
        "resource": "storage/i348.pdf",
        "selectable": false
      },
      {
        "text": "Tutorial on Variational Autoencoders",
        "item-id": "i68",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n346",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>This paper introduces a variant of autoencoder, which is the variational autoencoder. For the ordinary autoencoder, the code is a n-dimensional vector. As for VAE, the code trained is a Gaussian distribution. In the network structure, the mean and the variance of the distribution will be trained. And then, the generator resample the code by the Gaussian distribution encoded. Compared with the vanilla autoencoder, VAE has better performance.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Doersch_2021_Tutorial on Variational Autoencoders.pdf",
            "item-id": "i203",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Doersch_2021_Tutorial on Variational Autoencoders.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_QNPQUFA6/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/3\">1.1 Preliminaries: Latent Variable Models</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/4\">2 Variational Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/7\">2.1 Setting up the objective</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/9\">2.2 Optimizing the objective</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/11\">2.3 Testing the learned model</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/12\">2.4 Interpreting the objective</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/12\">2.4.1 The error from D[Q(z|X)P(z|X)]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/13\">2.4.2 The information-theoretic interpretation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/13\">2.4.3 VAEs and the regularization parameter</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/14\">3 Conditional Variational Autoencoders</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/16\">4 Examples</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/16\">4.1 MNIST variational autoencoder</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/19\">5 MNIST conditional variational autoencoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QNPQUFA6/19\">A Proof in 1D that VAEs have zero approximation error given arbitrarily powerful learners.</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Doersch_2021_Tutorial on Variational Autoencoders.pdf"
              ]
            ],
            "resource": "storage/i203.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Tutorial on Variational Autoencoders",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed."
          ],
          [
            "Access Date",
            "2021-03-16 14:05:22"
          ],
          [
            "Creators",
            "Carl Doersch"
          ],
          [
            "Date",
            "2021-01-03 2021-01-03"
          ],
          [
            "Extra",
            "arXiv: 1606.05908"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1606.05908 [cs, stat]"
          ],
          [
            "Title",
            "Tutorial on Variational Autoencoders"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1606.05908"
          ]
        ],
        "resource": "storage/i203.pdf",
        "selectable": false
      },
      {
        "text": "VideoMAE",
        "item-id": "i2397",
        "nodes": [
          {
            "text": "Tong et al_2022_VideoMAE.pdf",
            "item-id": "i2449",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tong et al_2022_VideoMAE.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tong et al_2022_VideoMAE.pdf"
              ]
            ],
            "resource": "storage/i2449.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VideoMAE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging and meaningful self-supervision task, thus encouraging extracting more effective video representations during the pre-training process. We obtain three important findings with VideoMAE: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance for VideoMAE. The temporally redundant video content enables higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. This is partially ascribed to the challenging task of video reconstruction to enforce high-level structure learning. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important factor. Notably, our VideoMAE with the vanilla ViT backbone can achieve 87.4% on Kinects-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE."
          ],
          [
            "Access Date",
            "2023-05-22 04:32:05"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Zhan Tong, Yibing Song, Jue Wang, Limin Wang"
          ],
          [
            "Date",
            "2022-10-31 2022/10/31"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Short Title",
            "VideoMAE"
          ],
          [
            "Title",
            "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=AhccnBXSne"
          ]
        ],
        "resource": "storage/i2449.pdf",
        "selectable": false
      }
    ],
    "item_title": "Auto Encoder",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Convolutional Neural Networks",
    "item-id": "c13,i2823",
    "nodes": [
      {
        "text": "Batch Normalization",
        "item-id": "i20",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n211",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Batch normalization and InceptionV2</p>\n<p>The hidden layers in neural networks need to continuously adapt to the new distribution or will cause the covariate shift (Shimodaira, 2000).</p>\n<p>The optimizer will train fast when the distribution of input remains more stable. </p>\n<p>They proposed a method called Batch Normalization. The BN transform applied to the mini-batch of the input. The BN layer will calculate the mini-batch mean and variance to normalize the input batch to similar to N(0, 1). And also will learn a scale and shift function to map the output better.</p>\n<p>MNIST</p>\n<p>The metric is classification error. The training is faster and the result is more accurate than the model without BN.</p>\n<p>+ve: The training speed is faster. The performance of model is better.</p>\n<p>-ve: Not explored the full range of other possibilities of BN.</p>\n<p>The BN contributes a lot for modern deep models to improve the speed in training and the performance. In the many models today, the BN layers will apply after each convolutional layers to improve the performance.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Ioffe_Szegedy_2015_Batch Normalization.pdf",
            "item-id": "i288",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ioffe_Szegedy_2015_Batch Normalization.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ioffe_Szegedy_2015_Batch Normalization.pdf"
              ]
            ],
            "resource": "storage/i288.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Batch Normalization",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the t..."
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Sergey Ioffe, Christian Szegedy"
          ],
          [
            "Date",
            "2015-06-01 2015/06/01"
          ],
          [
            "Extra",
            "ISSN: 1938-7228"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "448-456"
          ],
          [
            "Proceedings Title",
            "International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Short Title",
            "Batch Normalization"
          ],
          [
            "Title",
            "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
          ],
          [
            "URL",
            "http://proceedings.mlr.press/v37/ioffe15.html"
          ]
        ],
        "resource": "storage/i288.pdf",
        "selectable": false
      },
      {
        "text": "CBAM",
        "item-id": "i86",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n270",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>With Channel Attention Module and Spatial Attention Module, the CBAM can be inserted into conv blocks to improve the CNN performance.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Woo et al_2018_CBAM.pdf",
            "item-id": "i271",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Woo et al_2018_CBAM.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Woo et al_2018_CBAM.pdf"
              ]
            ],
            "resource": "storage/i271.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CBAM",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-18 03:50:54"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3-19"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "CBAM"
          ],
          [
            "Title",
            "CBAM: Convolutional Block Attention Module"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i271.pdf",
        "selectable": false
      },
      {
        "text": "Conditional Image Generation with PixelCNN Decoders",
        "item-id": "i1560",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n1589",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotation</p>\n<p>PixelCNN</p>\n<p></p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "van den Oord et al_2016_Conditional Image Generation with PixelCNN Decoders.pdf",
            "item-id": "i1593",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "van den Oord et al_2016_Conditional Image Generation with PixelCNN Decoders.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "van den Oord et al_2016_Conditional Image Generation with PixelCNN Decoders.pdf"
              ]
            ],
            "resource": "storage/i1593.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Conditional Image Generation with PixelCNN Decoders",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost."
          ],
          [
            "Access Date",
            "2022-05-21 04:11:11"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, Alex Graves"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Conditional Image Generation with PixelCNN Decoders"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html"
          ],
          [
            "Volume",
            "29"
          ]
        ],
        "resource": "storage/i1593.pdf",
        "selectable": false
      },
      {
        "text": "Deep Residual Learning for Image Recognition",
        "item-id": "i22",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n208",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>ResNet and Residual block</p>\n<p>Will the model be better with more layers? How to solve the problem of gradients vanishment and explosion.</p>\n<p>The hypothesis is the model constructed with some identity mappings can produce a better training error than shallower models.</p>\n<p>In the residual block, there is an identity mapping to the output of the block and sum them. In the residual networks (ResNet), &nbsp;they are formed by multiple residual blocks with different number of layers. </p>\n<p>ImageNet 2012 classification dataset. CIFAR-10. PASCAL VOC 2007 and 2012 and COCO.</p>\n<p>The comparison between the models of 18 layers and 34 layers indicates the ResNet can produce better performance when the number of layers is deeper. Then the ResNets are compared with other networks, leading to a better result. The compared applied to CIFAR-10 between different layer versions of ResNet shows extremely deep network does not produce a better result (ResNet-1202 vs ResNet-110). Also, the experiments for object detection on PASCAL VOC 2007, 2012 and COCO shows the ResNet has good generalization performance.</p>\n<p>+ve: Produce better performance with deeper ResNet. Has good generalization performance.</p>\n<p>-ve:</p>\n<p>A great method that release the limitation of the depth of model design to let the researchers available for any depth they want, which powered the community has better flexbility of networks architecture.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "He et al_2016_Deep Residual Learning for Image Recognition.pdf",
            "item-id": "i327",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2016_Deep Residual Learning for Image Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2016_Deep Residual Learning for Image Recognition.pdf"
              ]
            ],
            "resource": "storage/i327.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Residual Learning for Image Recognition",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "770-778"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Deep Residual Learning for Image Recognition"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i327.pdf",
        "selectable": false
      },
      {
        "text": "EfficientNet",
        "item-id": "i1633",
        "nodes": [
          {
            "text": "Tan_Le_2019_EfficientNet.pdf",
            "item-id": "i1638",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tan_Le_2019_EfficientNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tan_Le_2019_EfficientNet.pdf"
              ]
            ],
            "resource": "storage/i1638.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "EfficientNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters."
          ],
          [
            "Access Date",
            "2022-06-07 18:09:05"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Mingxing Tan, Quoc Le"
          ],
          [
            "Date",
            "2019-05-24 2019-05-24"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "6105-6114"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 36th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Short Title",
            "EfficientNet"
          ],
          [
            "Title",
            "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v97/tan19a.html"
          ]
        ],
        "resource": "storage/i1638.pdf",
        "selectable": false
      },
      {
        "text": "Going Deeper With Convolutions",
        "item-id": "i61",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n171",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Inception Module and GoogLeNet (InceptionV1)</p>\n<p>The mobile and embedded computting has limited power for models. A high-efficiency method is required.</p>\n<p>Decide to develop a high-efficiency method keeping a computational budget of 1.5 billion operations.</p>\n<p>The Inception module has multiple routes for input features. Each route has different kernel size of convolutions and max-pooling. And then, concatenate the outputs as the result of this module. Otherwise, the 1x1 convolutions can be used to reduce the number of channels. GoogLeNet is the model formed by Inception modules.</p>\n<p>ILSVRC 2014</p>\n<p>The metric is Top-5 error in ILSVRC 2014. The GoogLeNet ranked 1st in this competition.</p>\n<p>+ve: A significant quality improvement with slightly increase of computational requirements. Also available for object detection.</p>\n<p>-ve:</p>\n<p>The Inception Module is the new direction that the CNN can be not sequence. And, the high-efficiency also is an important metrics to judge the usability of a neural network structure.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Szegedy et al_2015_Going Deeper With Convolutions.pdf",
            "item-id": "i322",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Szegedy et al_2015_Going Deeper With Convolutions.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Szegedy et al_2015_Going Deeper With Convolutions.pdf"
              ]
            ],
            "resource": "storage/i322.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Going Deeper With Convolutions",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification."
          ],
          [
            "Access Date",
            "2021-03-16 14:27:59"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Library Catalog",
            "www.cv-foundation.org"
          ],
          [
            "Pages",
            "1-9"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Going Deeper With Convolutions"
          ],
          [
            "URL",
            "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html"
          ]
        ],
        "resource": "storage/i322.pdf",
        "selectable": false
      },
      {
        "text": "Gradient-based learning applied to document recognition",
        "item-id": "i104",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n315",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Convolutional Neural Networks (CNN) and LeNet</p>\n<p>To find a better pattern recognition systems can be built by relying more on automatic learning and less on hand-designed heuristics.</p>\n<p>The new method is made possible by recent progress in machine learning and computer technology. The hand-crafted feature extraction can be advantageously replaced by carefully designed learning machines that operate directly on pixel images.</p>\n<p>The architecture of CNN is based on convolutional layers, pooling layers, and some fully-connected layers. Convolutional layers and pooling layers are effective for extracting features from 2D images, and they can also share the parameters with nearby numbers which can dramatically reduce the parameters, and it is beneficial for model performance.</p>\n<p>The dataset is the called MNIST, with 60000 training images and 10000 test images, combined by SD-1 and SD-2 datasets.</p>\n<p>The classification error of LeNet is lower compared to other traditional methods and fully-connected NN.</p>\n<p>+ve: Better classification performance than traditional methods and fully-connected NN. Suitable for hardware implementations with low memory requirements. More robust for shape variance and noise.</p>\n<p>-ve: Longer training time.</p>\n<p>The origin of CNN, huge contribution to computer vision area.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Lecun et al_1998_Gradient-based learning applied to document recognition.pdf",
            "item-id": "i355",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lecun et al_1998_Gradient-based learning applied to document recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lecun et al_1998_Gradient-based learning applied to document recognition.pdf"
              ]
            ],
            "resource": "storage/i355.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Gradient-based learning applied to document recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
          ],
          [
            "Creators",
            "Y. Lecun, L. Bottou, Y. Bengio, P. Haffner"
          ],
          [
            "DOI",
            "10.1109/5.726791"
          ],
          [
            "Date",
            "1998-11-00 November 1998"
          ],
          [
            "Extra",
            "Conference Name: Proceedings of the IEEE"
          ],
          [
            "ISSN",
            "1558-2256"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2278-2324"
          ],
          [
            "Publication Title",
            "Proceedings of the IEEE"
          ],
          [
            "Title",
            "Gradient-based learning applied to document recognition"
          ],
          [
            "Volume",
            "86"
          ]
        ],
        "resource": "storage/i355.pdf",
        "selectable": false
      },
      {
        "text": "Identity Mappings in Deep Residual Networks",
        "item-id": "i1700",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n1745",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotation</p>\n<p>ResNetV2</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "He et al_2016_Identity Mappings in Deep Residual Networks.pdf",
            "item-id": "i1746",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2016_Identity Mappings in Deep Residual Networks.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_H6EI4J5V/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/2\">2 Analysis of Deep Residual Networks</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/4\">3 On the Importance of Identity Skip Connections</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/4\">3.1 Experiments on Skip Connections</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/8\">3.2 Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/8\">4 On the Usage of Activation Functions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/9\">4.1 Experiments on Activation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/11\">4.2 Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/12\">5 Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/14\">6 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H6EI4J5V/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2016_Identity Mappings in Deep Residual Networks.pdf"
              ]
            ],
            "resource": "storage/i1746.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Identity Mappings in Deep Residual Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\u00a0% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46493-0_38"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46493-0"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "630-645"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Identity Mappings in Deep Residual Networks"
          ]
        ],
        "resource": "storage/i1746.pdf",
        "selectable": false
      },
      {
        "text": "ImageNet Classification with Deep Convolutional Neural Networks",
        "item-id": "i95",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n319",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>AlexNet / ReLU</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf",
            "item-id": "i320",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf"
              ]
            ],
            "resource": "storage/i320.pdf"
          },
          {
            "text": "ResearchGate Link",
            "item-id": "i321",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "ResearchGate Link",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2021-03-16 14:39:50"
              ],
              [
                "Title",
                "ResearchGate Link"
              ],
              [
                "URL",
                "https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks"
              ]
            ]
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ImageNet Classification with Deep Convolutional Neural Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
          ],
          [
            "Creators",
            "Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton"
          ],
          [
            "DOI",
            "10.1145/3065386"
          ],
          [
            "Date",
            "2012-01-01 January 1, 2012"
          ],
          [
            "Journal Abbreviation",
            "Neural Information Processing Systems"
          ],
          [
            "Library Catalog",
            "ResearchGate"
          ],
          [
            "Pages",
            "1097\u20131105"
          ],
          [
            "Publication Title",
            "Advances in neural information processing systems"
          ],
          [
            "Title",
            "ImageNet Classification with Deep Convolutional Neural Networks"
          ],
          [
            "Volume",
            "25"
          ]
        ],
        "resource": "storage/i320.pdf",
        "selectable": false
      },
      {
        "text": "Improving neural networks by preventing co-adaptation of feature detectors",
        "item-id": "i67",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n309",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Dropout</p>\n<p>The overfitting, do worse on the test data than on the training data, happens.</p>\n<p>Finding a similar method with \"mean network\", which is formed by many separate networks, to reduce the test set error.</p>\n<p>The dropout is used as disable some proportion of hidden units in training process to prevent the hidden units replies to others. Normally, the drop probability is set to 0.5.</p>\n<p>Datasets are MNIST, TIMIT benchmark, CIFAR-10, ImageNet, and Reuters.</p>\n<p>The main metrics for comparison is the classification error for test set. The performance of dropout network is similar to \"mean network\", and better than the network without dropout.</p>\n<p>+ve: Almost all dropout probabilities can improve the generalization performance of the models. Dropout is simpler to implement.</p>\n<p>-ve: Some extreme probabilities will cause the performance worse.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf",
            "item-id": "i354",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_6HQXUSE5/8\">A Experiments on MNIST</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/8\">A.1 Details for dropout training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/9\">A.2 Details for dropout finetuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/9\">A.3 Effect on features</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/9\">B Experiments on TIMIT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/10\">B.1 Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/12\">B.2 Dropout Finetuning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/12\">C Experiments on Reuters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/13\">D Tiny Images and CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/14\">E ImageNet</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/14\">F Convolutional Neural Networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/15\">F.1 Pooling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/15\">F.2 Local response normalization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/16\">F.3 Neuron nonlinearities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/16\">F.4 Objective function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/16\">F.5 Weight initialization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/17\">F.6 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/17\">F.7 Learning rates</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/17\">G Models for CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6HQXUSE5/18\">H Models for ImageNet</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf"
              ]
            ],
            "resource": "storage/i354.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
          ],
          [
            "Access Date",
            "2021-03-16 13:52:06"
          ],
          [
            "Creators",
            "Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov"
          ],
          [
            "Date",
            "2012-07-03 2012-07-03"
          ],
          [
            "Extra",
            "arXiv: 1207.0580"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1207.0580 [cs]"
          ],
          [
            "Title",
            "Improving neural networks by preventing co-adaptation of feature detectors"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1207.0580"
          ]
        ],
        "resource": "storage/i354.pdf",
        "selectable": false
      },
      {
        "text": "Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks",
        "item-id": "i1877",
        "nodes": [
          {
            "text": "Qiu et al_2017_Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks.pdf",
            "item-id": "i1977",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Qiu et al_2017_Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Qiu et al_2017_Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks.pdf"
              ]
            ],
            "resource": "storage/i1977.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating 3*3*3 convolutions with 1*3*3 convolutional filters on spatial domain (equivalent to 2D CNN) plus 3*1*1 convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%, respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques."
          ],
          [
            "Access Date",
            "2022-10-31 02:30:00"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Zhaofan Qiu, Ting Yao, Tao Mei"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5533-5541"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Qiu_Learning_Spatio-Temporal_Representation_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i1977.pdf",
        "selectable": false
      },
      {
        "text": "MobileNetV2",
        "item-id": "i1788",
        "nodes": [
          {
            "text": "Sandler et al_2018_MobileNetV2.pdf",
            "item-id": "i1800",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sandler et al_2018_MobileNetV2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sandler et al_2018_MobileNetV2.pdf"
              ]
            ],
            "resource": "storage/i1800.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MobileNetV2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper we describe a new mobile architecture, mbox{MobileNetV2}, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call mbox{SSDLite}. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of mbox{DeepLabv3} which we call Mobile mbox{DeepLabv3}. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on mbox{ImageNet}~cite{Russakovsky:2015:ILS:2846547.2846559} classification, COCO object detection cite{COCO}, VOC image segmentation cite{PASCAL}. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters."
          ],
          [
            "Access Date",
            "2022-10-04 07:41:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4510-4520"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MobileNetV2"
          ],
          [
            "Title",
            "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i1800.pdf",
        "selectable": false
      },
      {
        "text": "MobileNets",
        "item-id": "i1702",
        "nodes": [
          {
            "text": "Howard et al_2017_MobileNets.pdf",
            "item-id": "i1749",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Howard et al_2017_MobileNets.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Howard et al_2017_MobileNets.pdf"
              ]
            ],
            "resource": "storage/i1749.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MobileNets",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
          ],
          [
            "Access Date",
            "2022-07-20 19:43:30"
          ],
          [
            "Archiveid",
            "arXiv:1704.04861"
          ],
          [
            "Creators",
            "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam"
          ],
          [
            "DOI",
            "10.48550/arXiv.1704.04861"
          ],
          [
            "Date",
            "2017-04-16 2017-04-16"
          ],
          [
            "Extra",
            "arXiv:1704.04861 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MobileNets"
          ],
          [
            "Title",
            "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1704.04861"
          ]
        ],
        "resource": "storage/i1749.pdf",
        "selectable": false
      },
      {
        "text": "Network In Network",
        "item-id": "i63",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n303",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>NIN</p>\n<p>The traditional convolutional layer as generalized linear model (GLM) has low level of abstraction.</p>\n<p>By replacing the GLM in convolutional layer with a non-linear function approximator can improve the &nbsp;abstraction ability of local model.</p>\n<p>The authors proposed two concepts, using MLP to replace the GLM in convolutional layer, and the usage of global average pooling layer. They use convolution layers with 1x1 kernel to simulate the MLP inside. And the global average pooling layer is used to replace the fully-connected layers in traditional CNN.</p>\n<p>CIFAR-10, CIFAR-100, SVHN, MNIST.</p>\n<p>The metric is test set classification error. In these 4 datasets, the NIN has an outstanding performance compared to other structures. Also the global average pooling as a regularizer is also better than fully connected + dropout set.</p>\n<p>+ve: The Mlpconv allows more complex and learnable interaction of cross channel information. Global average pooling is more meaningful and interpretable. Global average pooling is more robust to spatial translations of the input.</p>\n<p>-ve:</p>\n<p>The ideas in this paper is amazing, and many famous structures such as ResNet and GoogLeNet use these ideas.</p>\n<p></p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Comment: 10 pages, 4 figures, for iclr2014",
            "item-id": "n326",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 10 pages, 4 figures, for iclr2014",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 10 pages, 4 figures, for iclr2014</div>",
            "node_type": "note"
          },
          {
            "text": "Lin et al_2014_Network In Network.pdf",
            "item-id": "i325",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lin et al_2014_Network In Network.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_H3QKEDSD/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/2\">2 Convolutional Neural Networks</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/3\">3 Network In Network</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/3\">3.1 MLP Convolution Layers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/4\">3.2 Global Average Pooling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/4\">3.3 Network In Network Structure</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/5\">4.1 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/5\">4.2 CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/6\">4.3 CIFAR-100</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/6\">4.4 Street View House Numbers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/7\">4.5 MNIST</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/7\">4.6 Global Average Pooling as a Regularizer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/8\">4.7 Visualization of NIN</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H3QKEDSD/8\">5 Conclusions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lin et al_2014_Network In Network.pdf"
              ]
            ],
            "resource": "storage/i325.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Network In Network",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
          ],
          [
            "Access Date",
            "2021-03-16 14:27:20"
          ],
          [
            "Creators",
            "Min Lin, Qiang Chen, Shuicheng Yan"
          ],
          [
            "Date",
            "2014-03-04 2014-03-04"
          ],
          [
            "Extra",
            "arXiv: 1312.4400"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1312.4400 [cs]"
          ],
          [
            "Title",
            "Network In Network"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1312.4400"
          ]
        ],
        "resource": "storage/i325.pdf",
        "selectable": false
      },
      {
        "text": "Quo Vadis, Action Recognition?",
        "item-id": "i950",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2016",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>I3D</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf",
            "item-id": "i979",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf"
              ]
            ],
            "resource": "storage/i979.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Quo Vadis, Action Recognition?",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-09-23 04:12:37"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Joao Carreira, Andrew Zisserman"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "6299-6308"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Quo Vadis, Action Recognition?"
          ],
          [
            "Title",
            "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i979.pdf",
        "selectable": false
      },
      {
        "text": "Rethinking Spatiotemporal Feature Learning",
        "item-id": "i1865",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1966",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>S3D</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Xie et al_2018_Rethinking Spatiotemporal Feature Learning.pdf",
            "item-id": "i1967",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xie et al_2018_Rethinking Spatiotemporal Feature Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xie et al_2018_Rethinking Spatiotemporal Feature Learning.pdf"
              ]
            ],
            "resource": "storage/i1967.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Rethinking Spatiotemporal Feature Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level ``semantic'' features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24)."
          ],
          [
            "Access Date",
            "2022-10-31 04:39:35"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "305-321"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "Rethinking Spatiotemporal Feature Learning"
          ],
          [
            "Title",
            "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Saining_Xie_Rethinking_Spatiotemporal_Feature_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i1967.pdf",
        "selectable": false
      },
      {
        "text": "ShuffleNet",
        "item-id": "i102",
        "nodes": [
          {
            "text": "Zhang et al_2017_ShuffleNet.pdf",
            "item-id": "i232",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2017_ShuffleNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2017_ShuffleNet.pdf"
              ]
            ],
            "resource": "storage/i232.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ShuffleNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy."
          ],
          [
            "Access Date",
            "2021-03-29 04:17:08"
          ],
          [
            "Creators",
            "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun"
          ],
          [
            "Date",
            "2017-12-07 2017-12-07"
          ],
          [
            "Extra",
            "arXiv: 1707.01083"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1707.01083 [cs]"
          ],
          [
            "Short Title",
            "ShuffleNet"
          ],
          [
            "Title",
            "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1707.01083"
          ]
        ],
        "resource": "storage/i232.pdf",
        "selectable": false
      },
      {
        "text": "U-Net",
        "item-id": "i2184",
        "nodes": [
          {
            "text": "Ronneberger et al_2015_U-Net.pdf",
            "item-id": "i2194",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ronneberger et al_2015_U-Net.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_4AN4CFCZ/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4AN4CFCZ/4\">2 Network Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4AN4CFCZ/4\">3 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4AN4CFCZ/6\">4 Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4AN4CFCZ/7\">5 Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ronneberger et al_2015_U-Net.pdf"
              ]
            ],
            "resource": "storage/i2194.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "U-Net",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net."
          ],
          [
            "Conference Name",
            "Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015"
          ],
          [
            "Creators",
            "Olaf Ronneberger, Philipp Fischer, Thomas Brox, Nassir Navab, Joachim Hornegger, William M. Wells, Alejandro F. Frangi"
          ],
          [
            "DOI",
            "10.1007/978-3-319-24574-4_28"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "ISBN",
            "978-3-319-24574-4"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "234-241"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "U-Net"
          ],
          [
            "Title",
            "U-Net: Convolutional Networks for Biomedical Image Segmentation"
          ]
        ],
        "resource": "storage/i2194.pdf",
        "selectable": false
      },
      {
        "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "item-id": "i2823",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n304",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>VGG</p>\n<p>Improve the previous CNN structure for better accuracy for ILSVRC.</p>\n<p>The depth of the structure of CNN can be discussed for better performance of the model.</p>\n<p>They proposed two best VGG-16 and VGG-19 structures. Instead of using big kernel size, the VGG only use 3x3 kernel sizes and 16 layers or 19 layers.</p>\n<p>ILSVRC-2014</p>\n<p>The metric is test set classification error. Their score is better than the best of ILSVRC-2013.</p>\n<p>+ve: More depth is beneficial for the performance. VGG structures has widely usage for other tasks.</p>\n<p>-ve:</p>\n<p>Provide a famous VGG structure for CNN area, which indicates the depth contributes more than width of CNN.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf",
            "item-id": "i329",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_I72M66HF/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I72M66HF/2\">2 ConvNet Configurations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/2\">2.1 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/2\">2.2 Configurations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/2\">2.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I72M66HF/4\">3 Classification Framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/4\">3.1 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/5\">3.2 Testing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/5\">3.3 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I72M66HF/5\">4 Classification Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/6\">4.1 Single Scale Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/6\">4.2 Multi-Scale Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/7\">4.3 Multi-crop evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/7\">4.4 ConvNet Fusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/7\">4.5 Comparison with the State of the Art</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/8\">5 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I72M66HF/10\">A Localisation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/10\">A.1 Localisation ConvNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/11\">A.2 Localisation Experiments</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/11\">B Generalisation of Very Deep Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I72M66HF/14\">C Paper Revisions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf"
              ]
            ],
            "resource": "storage/i329.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
          ],
          [
            "Access Date",
            "2023-07-12 14:59:38"
          ],
          [
            "Conference Name",
            "3rd International Conference on Learning Representations, ICLR 2015"
          ],
          [
            "Creators",
            "Karen Simonyan, Andrew Zisserman, Yoshua Bengio, Yann LeCun"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Library Catalog",
            "DBLP Computer Science Bibliography"
          ],
          [
            "Place",
            "San Diego, CA, USA"
          ],
          [
            "Proceedings Title",
            "3rd International Conference on Learning Representations, ICLR 2015"
          ],
          [
            "Title",
            "Very Deep Convolutional Networks for Large-Scale Image Recognition"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1409.1556"
          ]
        ],
        "resource": "storage/i329.pdf",
        "selectable": false
      },
      {
        "text": "Xception",
        "item-id": "i1878",
        "nodes": [
          {
            "text": "Chollet_2017_Xception.pdf",
            "item-id": "i1979",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chollet_2017_Xception.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chollet_2017_Xception.pdf"
              ]
            ],
            "resource": "storage/i1979.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Xception",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
          ],
          [
            "Access Date",
            "2022-10-31 02:25:21"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Francois Chollet"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1251-1258"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Xception"
          ],
          [
            "Title",
            "Xception: Deep Learning With Depthwise Separable Convolutions"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i1979.pdf",
        "selectable": false
      }
    ],
    "item_title": "Convolutional Neural Networks",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Dataset",
    "item-id": "c31,i3660",
    "nodes": [
      {
        "text": "ADD 2022",
        "item-id": "i3145",
        "nodes": [
          {
            "text": "Comment: Accepted by ICASSP 2022",
            "item-id": "n3149",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted by ICASSP 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted by ICASSP 2022</div>",
            "node_type": "note"
          },
          {
            "text": "Yi et al_2022_ADD 2022.pdf",
            "item-id": "i3148",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yi et al_2022_ADD 2022.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yi et al_2022_ADD 2022.pdf"
              ]
            ],
            "resource": "storage/i3148.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ADD 2022",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio deepfake detection is an emerging topic, which was included in the ASVspoof 2021. However, the recent shared tasks have not covered many real-life and challenging scenarios. The first Audio Deep synthesis Detection challenge (ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks: low-quality fake audio detection (LF), partially fake audio detection (PF) and audio fake game (FG). The LF track focuses on dealing with bona fide and fully fake utterances with various real-world noises etc. The PF track aims to distinguish the partially fake audio from the real. The FG track is a rivalry game, which includes two tasks: an audio generation task and an audio fake detection task. In this paper, we describe the datasets, evaluation metrics, and protocols. We also report major findings that reflect the recent advances in audio deepfake detection tasks."
          ],
          [
            "Access Date",
            "2023-10-20 13:17:45"
          ],
          [
            "Archiveid",
            "arXiv:2202.08433"
          ],
          [
            "Creators",
            "Jiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, Haoxin Ma, Chenglong Wang, Tao Wang, Zhengkun Tian, Ye Bai, Cunhang Fan, Shan Liang, Shiming Wang, Shuai Zhang, Xinrui Yan, Le Xu, Zhengqi Wen, Haizhou Li, Zheng Lian, Bin Liu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2202.08433"
          ],
          [
            "Date",
            "2022-02-26 2022-02-26"
          ],
          [
            "Extra",
            "arXiv:2202.08433 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ADD 2022"
          ],
          [
            "Title",
            "ADD 2022: the First Audio Deep Synthesis Detection Challenge"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2202.08433"
          ]
        ],
        "resource": "storage/i3148.pdf",
        "selectable": false
      },
      {
        "text": "ASVspoof 2021",
        "item-id": "i3040",
        "nodes": [
          {
            "text": "Liu et al_2023_ASVspoof 2021.pdf",
            "item-id": "i3115",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2023_ASVspoof 2021.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2023_ASVspoof 2021.pdf"
              ]
            ],
            "resource": "storage/i3115.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ASVspoof 2021",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Benchmarking initiatives support the meaningful comparison of competing solutions to prominent problems in speech and language processing. Successive benchmarking evaluations typically reflect a progressive evolution from ideal lab conditions towards to those encountered in the wild. ASVspoof, the spoofing and deepfake detection initiative and challenge series, has followed the same trend. This article provides a summary of the ASVspoof 2021 challenge and the results of 54 participating teams that submitted to the evaluation phase. For the logical access (LA) task, results indicate that countermeasures are robust to newly introduced encoding and transmission effects. Results for the physical access (PA) task indicate the potential to detect replay attacks in real, as opposed to simulated physical spaces, but a lack of robustness to variations between simulated and real acoustic environments. The Deepfake (DF) task, new to the 2021 edition, targets solutions to the detection of manipulated, compressed speech data posted online. While detection solutions offer some resilience to compression effects, they lack generalization across different source datasets. In addition to a summary of the top-performing systems for each task, new analyses of influential data factors and results for hidden data subsets, the article includes a review of post-challenge results, an outline of the principal challenge limitations and a road-map for the future of ASVspoof."
          ],
          [
            "Creators",
            "Xuechen Liu, Xin Wang, Md Sahidullah, Jose Patino, H\u00e9ctor Delgado, Tomi Kinnunen, Massimiliano Todisco, Junichi Yamagishi, Nicholas Evans, Andreas Nautsch, Kong Aik Lee"
          ],
          [
            "DOI",
            "10.1109/TASLP.2023.3285283"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Extra",
            "Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "ISSN",
            "2329-9304"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2507-2522"
          ],
          [
            "Publication Title",
            "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "Short Title",
            "ASVspoof 2021"
          ],
          [
            "Title",
            "ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i3115.pdf",
        "selectable": false
      },
      {
        "text": "AV-Deepfake1M",
        "item-id": "i3243",
        "nodes": [
          {
            "text": "Cai et al_2023_AV-Deepfake1M.pdf",
            "item-id": "i3271",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2023_AV-Deepfake1M.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XTN5YQ85/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/3\">. AV-Deepfake1M Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/4\">. Data Generation Pipeline</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/4\">Transcript Manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/4\">Audio Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/5\">Video Generation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/5\">. Dataset Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Human Quality Assessment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Computational Cost</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Benchmarks and Metrics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Data Partitioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/7\">. Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/7\">. Results and Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/7\">. Audio-Visual Temporal Deepfake Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/8\">. Audio-Visual Deepfake Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/8\">. Unimodal Deepfake Detection and Localization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/8\">. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Transcript Manipulation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Human Quality Assessment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/14\">. Evaluation and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/14\">. Audio and Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/15\">. Label Access For Training</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2023_AV-Deepfake1M.pdf"
              ]
            ],
            "resource": "storage/i3271.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "AV-Deepfake1M",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The detection and localization of highly realistic deepfake audio-visual content are challenging even for the most advanced state-of-the-art methods. While most of the research efforts in this domain are focused on detecting high-quality deepfake images and videos, only a few works address the problem of the localization of small segments of audio-visual manipulations embedded in real videos. In this research, we emulate the process of such content generation and propose the AV-Deepfake1M dataset. The dataset contains content-driven (i) video manipulations, (ii) audio manipulations, and (iii) audio-visual manipulations for more than 2K subjects resulting in a total of more than 1M videos. The paper provides a thorough description of the proposed data generation pipeline accompanied by a rigorous analysis of the quality of the generated data. The comprehensive benchmark of the proposed dataset utilizing state-of-the-art deepfake detection and localization methods indicates a significant drop in performance compared to previous datasets. The proposed dataset will play a vital role in building the next-generation deepfake localization methods. The dataset and associated code are available at https://github.com/ControlNet/AV-Deepfake1M ."
          ],
          [
            "Access Date",
            "2023-11-28 06:09:08"
          ],
          [
            "Archiveid",
            "arXiv:2311.15308"
          ],
          [
            "Creators",
            "Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall, Kalin Stefanov"
          ],
          [
            "DOI",
            "10.48550/arXiv.2311.15308"
          ],
          [
            "Date",
            "2023-11-26 2023-11-26"
          ],
          [
            "Extra",
            "arXiv:2311.15308 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Rights",
            "All rights reserved"
          ],
          [
            "Short Title",
            "AV-Deepfake1M"
          ],
          [
            "Title",
            "AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2311.15308"
          ]
        ],
        "resource": "storage/i3271.pdf",
        "selectable": false
      },
      {
        "text": "ActivityNet",
        "item-id": "i1161",
        "nodes": [
          {
            "text": "Caba Heilbron et al_2015_ActivityNet.pdf",
            "item-id": "i1180",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Caba Heilbron et al_2015_ActivityNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Caba Heilbron et al_2015_ActivityNet.pdf"
              ]
            ],
            "resource": "storage/i1180.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ActivityNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper, we introduce ActivityNet: a new large-scale video benchmark for human activity understanding. Our new benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity categories with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 hours of video. We illustrate three scenarios in which ActivityNet can be used to benchmark and compare algorithms for human activity understanding: untrimmed video classification, trimmed activity classification and activity detection."
          ],
          [
            "Access Date",
            "2021-11-01 19:43:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, Juan Carlos Niebles"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "961-970"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "ActivityNet"
          ],
          [
            "Title",
            "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2015/html/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html"
          ]
        ],
        "resource": "storage/i1180.pdf",
        "selectable": false
      },
      {
        "text": "Audio Set",
        "item-id": "i2522",
        "nodes": [
          {
            "text": "Gemmeke et al_2017_Audio Set.pdf",
            "item-id": "i2726",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gemmeke et al_2017_Audio Set.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gemmeke et al_2017_Audio Set.pdf"
              ]
            ],
            "resource": "storage/i2726.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio Set",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers."
          ],
          [
            "Conference Name",
            "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2017.7952261"
          ],
          [
            "Date",
            "2017-03-00 2017-03"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "776-780"
          ],
          [
            "Proceedings Title",
            "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Audio Set"
          ],
          [
            "Title",
            "Audio Set: An ontology and human-labeled dataset for audio events"
          ]
        ],
        "resource": "storage/i2726.pdf",
        "selectable": false
      },
      {
        "text": "CATER",
        "item-id": "i3462",
        "nodes": [
          {
            "text": "Girdhar_Ramanan_2019_CATER.pdf",
            "item-id": "i3568",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Girdhar_Ramanan_2019_CATER.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Girdhar_Ramanan_2019_CATER.pdf"
              ]
            ],
            "resource": "storage/i3568.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CATER",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controllable. Using CATER, we provide insights into some of the most recent state of the art deep video architectures."
          ],
          [
            "Access Date",
            "2024-01-08 02:12:47"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Rohit Girdhar, Deva Ramanan"
          ],
          [
            "Date",
            "2019-09-23 2019/09/23"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "CATER"
          ],
          [
            "Title",
            "CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=HJgzt2VKPB"
          ]
        ],
        "resource": "storage/i3568.pdf",
        "selectable": false
      },
      {
        "text": "CLEVRER",
        "item-id": "i3466",
        "nodes": [
          {
            "text": "Yi et al_2019_CLEVRER.pdf",
            "item-id": "i3551",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yi et al_2019_CLEVRER.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DRVMZGMK/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/3\">The CLEVRER Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/3\">Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/4\">Questions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/5\">Baseline Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/5\">Model Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/6\">Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/7\">Neuro-Symbolic Dynamic Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/9\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/14\">Descriptive Question Sub-types and Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/14\">NS-DR Model Details and Training Paradigm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/15\">Extra Examples from CLEVRER</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yi et al_2019_CLEVRER.pdf"
              ]
            ],
            "resource": "storage/i3551.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CLEVRER",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations."
          ],
          [
            "Access Date",
            "2024-01-08 03:17:28"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum"
          ],
          [
            "Date",
            "2019-09-25 2019/09/25"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "CLEVRER"
          ],
          [
            "Title",
            "CLEVRER: Collision Events for Video Representation and Reasoning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=HkxYzANYDB"
          ]
        ],
        "resource": "storage/i3551.pdf",
        "selectable": false
      },
      {
        "text": "CSTR VCTK Corpus",
        "item-id": "i3659",
        "icon": "glyphicon glyphicon-file",
        "item_title": "CSTR VCTK Corpus",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. \n \nThe newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage.  \n \nThe details of the text selection algorithms are described in the following paper:  \nC. Veaux, J. Yamagishi and S. King,  \n\"The voice bank corpus: Design, collection and data analysis of  \na large regional accent speech database,\"  \nhttps://doi.org/10.1109/ICSDA.2013.6709856 \n \nThe rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at \nhttp://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf \n \nAll speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. \n \nThis corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. \n \nThe dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error."
          ],
          [
            "Access Date",
            "2024-01-14 12:06:53"
          ],
          [
            "Creators",
            "Junichi Yamagishi, Christophe Veaux, Kirsten MacDonald"
          ],
          [
            "DOI",
            "10.7488/ds/2645"
          ],
          [
            "Date",
            "2019-11-13 2019-11-13T17:09:33Z"
          ],
          [
            "Extra",
            "Accepted: 2019-11-13T17:09:33Z\nPublisher: University of Edinburgh. The Centre for Speech Technology Research (CSTR)"
          ],
          [
            "Language",
            "eng"
          ],
          [
            "Library Catalog",
            "datashare.ed.ac.uk"
          ],
          [
            "Publication Title",
            "The Rainbow Passage which the speakers read out can be found in the International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm)."
          ],
          [
            "Rights",
            "ODC-BY 1.0"
          ],
          [
            "Short Title",
            "CSTR VCTK Corpus"
          ],
          [
            "Title",
            "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)"
          ],
          [
            "URL",
            "https://datashare.ed.ac.uk/handle/10283/3443"
          ]
        ]
      },
      {
        "text": "Celeb-DF",
        "item-id": "i957",
        "nodes": [
          {
            "text": "Li et al_2020_Celeb-DF.pdf",
            "item-id": "i966",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2020_Celeb-DF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2020_Celeb-DF.pdf"
              ]
            ],
            "resource": "storage/i966.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Celeb-DF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for datasets of DeepFake videos. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF."
          ],
          [
            "Access Date",
            "2021-10-05 12:25:39"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3207-3216"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Celeb-DF"
          ],
          [
            "Title",
            "Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Celeb-DF_A_Large-Scale_Challenging_Dataset_for_DeepFake_Forensics_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i966.pdf",
        "selectable": false
      },
      {
        "text": "CelebV-HQ",
        "item-id": "i2501",
        "nodes": [
          {
            "text": "Zhu et al_2022_CelebV-HQ.pdf",
            "item-id": "i2695",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2022_CelebV-HQ.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_83PTNWB6/1\">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2022_CelebV-HQ.pdf"
              ]
            ],
            "resource": "storage/i2695.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CelebV-HQ",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35,\u00a0666 video clips with the resolution of $$512\\times 512$$512\u00d7512at least, involving 15,\u00a0653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. We finally envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available (Project page: https://celebv-hq.github.io/Code and models: https://github.com/CelebV-HQ/CelebV-HQ)."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, Chen Change Loy, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-20071-7_38"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-20071-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "650-667"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "CelebV-HQ"
          ],
          [
            "Title",
            "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset"
          ]
        ],
        "resource": "storage/i2695.pdf",
        "selectable": false
      },
      {
        "text": "Contributing Data to Deepfake Detection Research",
        "item-id": "i990",
        "icon": "glyphicon glyphicon-pencil",
        "item_title": "Contributing Data to Deepfake Detection Research",
        "item_type": "blogPost",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Posted by Nick Dufour, Google Research and Andrew Gully, Jigsaw   Deep learning has given rise to technologies that would have been thought ..."
          ],
          [
            "Access Date",
            "2021-10-10 07:46:15"
          ],
          [
            "Blog Title",
            "Google AI Blog"
          ],
          [
            "Creators",
            "Dufou Nick, Jigsaw Andrew"
          ],
          [
            "Date",
            "2019-09-24 2019-09-24"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Title",
            "Contributing Data to Deepfake Detection Research"
          ],
          [
            "URL",
            "http://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html"
          ]
        ]
      },
      {
        "text": "DF-Platter",
        "item-id": "i2775",
        "nodes": [
          {
            "text": "Narayan et al_2023_DF-Platter.pdf",
            "item-id": "i2910",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Narayan et al_2023_DF-Platter.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Narayan et al_2023_DF-Platter.pdf"
              ]
            ],
            "resource": "storage/i2910.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DF-Platter",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake detection is gaining significant importance in the research community. While most of the research efforts are focused around high-quality images and videos, deepfake generation algorithms today have the capability to generate low-resolution videos, occluded deepfakes, and multiple-subject deepfakes. In this research, we emulate the real-world scenario of deepfake generation and spreading, and propose the DF-Platter dataset, which contains (i) both low-resolution and high-resolution deepfakes generated using multiple generation techniques and (ii) single-subject and multiple-subject deepfakes, with face images of Indian ethnicity. Faces in the dataset are annotated for various attributes such as gender, age, skin tone, and occlusion. The database is prepared in 116 days with continuous usage of 32 GPUs accounting to 1,800 GB cumulative memory. With over 500 GBs in size, the dataset contains a total of 133,260 videos encompassing three sets. To the best of our knowledge, this is one of the largest datasets containing vast variability and multiple challenges. We also provide benchmark results under multiple evaluation settings using popular and state-of-the-art deepfake detection models. Further, benchmark results under c23 and c40 compression are provided. The results demonstrate a significant performance reduction in the deepfake detection task on low-resolution deepfakes and show that the existing techniques fail drastically on multiple-subject deepfakes. It is our assertion that this database will improve the state-of-the-art by extending the capabilities of deepfake detection algorithms to real-world scenarios. The database is available at: http://iab-rubric.org/df-platter-database."
          ],
          [
            "Access Date",
            "2023-07-01 07:21:42"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Kartik Narayan, Harsh Agarwal, Kartik Thakral, Surbhi Mittal, Mayank Vatsa, Richa Singh"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9739-9748"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DF-Platter"
          ],
          [
            "Title",
            "DF-Platter: Multi-Face Heterogeneous Deepfake Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2910.pdf",
        "selectable": false
      },
      {
        "text": "Dear Sir or Madam, May I introduce the GYAFC Dataset",
        "item-id": "i1432",
        "nodes": [
          {
            "text": "Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human ",
            "item-id": "n1482",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human ",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf",
            "item-id": "i1481",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf"
              ]
            ],
            "resource": "storage/i1481.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Dear Sir or Madam, May I introduce the GYAFC Dataset",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics."
          ],
          [
            "Access Date",
            "2022-03-29 13:28:17"
          ],
          [
            "Creators",
            "Sudha Rao, Joel Tetreault"
          ],
          [
            "Date",
            "2018-04-16 2018-04-16"
          ],
          [
            "Extra",
            "arXiv: 1803.06535"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1803.06535 [cs]"
          ],
          [
            "Short Title",
            "Dear Sir or Madam, May I introduce the GYAFC Dataset"
          ],
          [
            "Title",
            "Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1803.06535"
          ]
        ],
        "resource": "storage/i1481.pdf",
        "selectable": false
      },
      {
        "text": "DeepFakes",
        "item-id": "i993",
        "nodes": [
          {
            "text": "Comment: http://publications.idiap.ch/index.php/publications/show/3988",
            "item-id": "n1011",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: http://publications.idiap.ch/index.php/publications/show/3988",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: http://publications.idiap.ch/index.php/publications/show/3988</div>",
            "node_type": "note"
          },
          {
            "text": "Korshunov_Marcel_2018_DeepFakes.pdf",
            "item-id": "i1010",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Korshunov_Marcel_2018_DeepFakes.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PLYTFY24/1\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/2\">II Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PLYTFY24/2\">III Deepfake database</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">III-A Evaluation protocol</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">IV Analysis of deepfake videos</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">IV-A Vulnerability of face recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">IV-B Detection of Deepfake videos</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/4\">V Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/5\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Korshunov_Marcel_2018_DeepFakes.pdf"
              ]
            ],
            "resource": "storage/i1010.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "DeepFakes",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97% equal error rate on high quality Deepfakes. Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so."
          ],
          [
            "Access Date",
            "2021-10-10 07:13:43"
          ],
          [
            "Archiveid",
            "arXiv:1812.08685"
          ],
          [
            "Creators",
            "Pavel Korshunov, Sebastien Marcel"
          ],
          [
            "Date",
            "2018-12-20 2018-12-20"
          ],
          [
            "Extra",
            "arXiv:1812.08685 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "DeepFakes"
          ],
          [
            "Title",
            "DeepFakes: a New Threat to Face Recognition? Assessment and Detection"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1812.08685"
          ]
        ],
        "resource": "storage/i1010.pdf",
        "selectable": false
      },
      {
        "text": "DeeperForensics-1.0",
        "item-id": "i956",
        "nodes": [
          {
            "text": "Jiang et al_2020_DeeperForensics-1.pdf",
            "item-id": "i968",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2020_DeeperForensics-1.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2020_DeeperForensics-1.pdf"
              ]
            ],
            "resource": "storage/i968.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DeeperForensics-1.0",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present our on-going effort of constructing a large- scale benchmark for face forgery detection. The first version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far, with 60, 000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. We further contribute a comprehensive study that evaluates five representative detection baselines and make a thorough analysis of different settings."
          ],
          [
            "Access Date",
            "2021-10-05 12:23:53"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2889-2898"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DeeperForensics-1.0"
          ],
          [
            "Title",
            "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Jiang_DeeperForensics-1.0_A_Large-Scale_Dataset_for_Real-World_Face_Forgery_Detection_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i968.pdf",
        "selectable": false
      },
      {
        "text": "Do You Really Mean That?",
        "item-id": "i2289",
        "nodes": [
          {
            "text": "Cai et al_2022_Do You Really Mean That.pdf",
            "item-id": "i2291",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2022_Do You Really Mean That.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2022_Do You Really Mean That.pdf"
              ]
            ],
            "resource": "storage/i2291.pdf"
          },
          {
            "text": "lavdf_supp.pdf",
            "item-id": "i3356",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "lavdf_supp.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Proposed Method Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Proposed Dataset Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Additional Qualitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/3\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "lavdf_supp.pdf"
              ]
            ],
            "resource": "storage/i3356.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Do You Really Mean That?",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attributes, and adversarial perturbation-based spatio-temporal modifications at the whole video or random locations while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from a sentiment perspective. We introduce a content-driven audio-visual deepfake dataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of learning temporal forgery localization. Specifically, the content-driven audio-visual manipulations are performed strategically to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is guided via contrastive, boundary matching, and frame classification loss functions. Our extensive quantitative and qualitative analysis demonstrates the proposed method's strong performance for temporal forgery localization and deepfake detection tasks."
          ],
          [
            "Conference Name",
            "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
          ],
          [
            "Creators",
            "Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1109/DICTA56598.2022.10034605"
          ],
          [
            "Date",
            "2022-11-00 2022-11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-10"
          ],
          [
            "Place",
            "Sydney, Australia"
          ],
          [
            "Proceedings Title",
            "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
          ],
          [
            "Rights",
            "All rights reserved"
          ],
          [
            "Short Title",
            "Do You Really Mean That?"
          ],
          [
            "Title",
            "Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization"
          ]
        ],
        "selectable": false
      },
      {
        "text": "EgoSchema",
        "item-id": "i3457",
        "nodes": [
          {
            "text": "Comment: https://egoschema.github.io/",
            "item-id": "n3540",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: https://egoschema.github.io/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: https://egoschema.github.io/</div>",
            "node_type": "note"
          },
          {
            "text": "Mangalam et al_2023_EgoSchema.pdf",
            "item-id": "i3539",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mangalam et al_2023_EgoSchema.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_7Z3G4UR9/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/4\">Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">Collecting 0.96plus0.96minus0.96100.96 EgoSchema</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">0.96plus0.96minus0.96100.96 EgoSchema Pipeline</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">Stage I: Raw Data Filtering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">Stage II: Question Answer Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/6\">Stage III: Generated Question Answer Filtering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Stage IV: Manual QAW Curation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Temporal Certificates</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Benchmarking0.96plus0.96minus0.96100.96 EgoSchema</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Evaluating Certificate Lengths</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/8\">Evaluating Multiple-choice Question Answering on0.96plus0.96minus0.96100.96 EgoSchema</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/9\">Conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/21\">Set A</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/21\">Question prompt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/22\">Answer prompt</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/24\">Set B</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/24\">Question and answer prompt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/26\">Wrong answer prompt</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/29\">Our clip length and narration density choice</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/29\">Human curation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/29\">Curation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/30\">Benchmarking details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/30\">Violet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/31\">mPLUG-Owl</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/31\">InternVideo</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/31\">Human</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mangalam et al_2023_EgoSchema.pdf"
              ]
            ],
            "resource": "storage/i3539.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "EgoSchema",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over 5.7x longer than the second closest dataset and 10x to 100x longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that \\name{}{}, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code are open-sourced for both public and commercial use under the Ego4D license at http://egoschema.github.io"
          ],
          [
            "Access Date",
            "2024-01-08 03:38:58"
          ],
          [
            "Archiveid",
            "arXiv:2308.09126"
          ],
          [
            "Creators",
            "Karttikeya Mangalam, Raiymbek Akshulakov, Jitendra Malik"
          ],
          [
            "Date",
            "2023-08-17 2023-08-17"
          ],
          [
            "Extra",
            "arXiv:2308.09126 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "EgoSchema"
          ],
          [
            "Title",
            "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2308.09126"
          ]
        ],
        "resource": "storage/i3539.pdf",
        "selectable": false
      },
      {
        "text": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors",
        "item-id": "i1117",
        "nodes": [
          {
            "text": "Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfak",
            "item-id": "n1141",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfak",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection (ADGD '21) at ACM MM 2021</div>",
            "node_type": "note"
          },
          {
            "text": "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf",
            "item-id": "i1140",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U37X3UA3/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/2\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/2\">2.1 Fake Media Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/3\">2.2 Fake Media Detection Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3 Multimodal Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3.2 Evaluation Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4.1 Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4.2 Experiment Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/6\">4.3 Unimodal Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">4.4 Ensemble Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">4.5 Multimodal Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf"
              ]
            ],
            "resource": "storage/i1140.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Significant advancements made in the generation of deepfakes have caused security and privacy issues. Attackers can easily impersonate a person's identity in an image by replacing his face with the target person's face. Moreover, a new domain of cloning human voices using deep-learning technologies is also emerging. Now, an attacker can generate realistic cloned voices of humans using only a few seconds of audio of the target person. With the emerging threat of potential harm deepfakes can cause, researchers have proposed deepfake detection methods. However, they only focus on detecting a single modality, i.e., either video or audio. On the other hand, to develop a good deepfake detector that can cope with the recent advancements in deepfake generation, we need to have a detector that can detect deepfakes of multiple modalities, i.e., videos and audios. To build such a detector, we need a dataset that contains video and respective audio deepfakes. We were able to find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized fake audios as well. We used this multimodal deepfake dataset and performed detailed baseline experiments using state-of-the-art unimodal, ensemble-based, and multimodal detection methods to evaluate it. We conclude through detailed experimentation that unimodals, addressing only a single modality, video or audio, do not perform well compared to ensemble-based methods. Whereas purely multimodal-based baselines provide the worst performance."
          ],
          [
            "Access Date",
            "2021-10-24 02:58:21"
          ],
          [
            "Creators",
            "Hasam Khalid, Minha Kim, Shahroz Tariq, Simon S. Woo"
          ],
          [
            "DOI",
            "10.1145/3476099.3484315"
          ],
          [
            "Date",
            "2021-10-24 2021-10-24"
          ],
          [
            "Extra",
            "arXiv: 2109.02993"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "7-15"
          ],
          [
            "Publication Title",
            "Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection"
          ],
          [
            "Title",
            "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2109.02993"
          ]
        ],
        "resource": "storage/i1140.pdf",
        "selectable": false
      },
      {
        "text": "Explore Multi-Step Reasoning in Video Question Answering",
        "item-id": "i3473",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n3563",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>SVQA</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Song et al_2018_Explore Multi-Step Reasoning in Video Question Answering.pdf",
            "item-id": "i3564",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2018_Explore Multi-Step Reasoning in Video Question Answering.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YR62CMTZ/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/3\">3 The Proposed Benchmark SVQA</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/3\">3.1 Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">3.2 QA Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">3.3 Quality Control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">3.4 Balance Answer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">4 The Proposed Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/5\">4.1 Question Embedding Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/5\">4.2 Video Embedding Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/5\">4.3 Attention Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/6\">4.4 Answer Decoder Module</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/6\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/6\">5.1 Dataset Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/7\">5.2 Performance Comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/7\">5.3 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/7\">5.4 Generalization Ability</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2018_Explore Multi-Step Reasoning in Video Question Answering.pdf"
              ]
            ],
            "resource": "storage/i3564.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Explore Multi-Step Reasoning in Video Question Answering",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Video question answering (VideoQA) always involves visual reasoning. When answering questions composing of multiple logic correlations, models need to perform multi-step reasoning. In this paper, we formulate multi-step reasoning in VideoQA as a new task to answer compositional and logical structured questions based on video content. Existing VideoQA datasets are inadequate as benchmarks for the multi-step reasoning due to limitations such as lacking logical structure and having language biases. Thus we design a system to automatically generate a large-scale dataset, namely SVQA (Synthetic Video Question Answering). Compared with other VideoQA datasets, SVQA contains exclusively long and structured questions with various spatial and temporal relations between objects. More importantly, questions in SVQA can be decomposed into human readable logical tree or chain layouts, each node of which represents a sub-task requiring a reasoning operation such as comparison or arithmetic. Towards automatic question answering in SVQA, we develop a new VideoQA model. Particularly, we construct a new attention module, which contains spatial attention mechanism to address crucial and multiple logical sub-tasks embedded in questions, as well as a refined GRU called ta-GRU (temporal-attention GRU) to capture the long-term temporal dependency and gather complete visual cues. Experimental results show the capability of multi-step reasoning of SVQA and the effectiveness of our model when compared with other existing models."
          ],
          [
            "Access Date",
            "2024-01-07"
          ],
          [
            "Creators",
            "Xiaomeng Song, Yucheng Shi, Xin Chen, Yahong Han"
          ],
          [
            "DOI",
            "10.1145/3240508.3240563"
          ],
          [
            "Date",
            "2018-00-15 \u5341\u6708 15, 2018"
          ],
          [
            "ISBN",
            "978-1-4503-5665-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "239\u2013247"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 26th ACM international conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '18"
          ],
          [
            "Title",
            "Explore Multi-Step Reasoning in Video Question Answering"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3240508.3240563"
          ]
        ],
        "resource": "storage/i3564.pdf",
        "selectable": false
      },
      {
        "text": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
        "item-id": "i1211",
        "nodes": [
          {
            "text": "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf",
            "item-id": "i1210",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf"
              ]
            ],
            "resource": "storage/i1210.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as DeepFake videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classi\ufb01er, our method does not need DeepFake generated images as negative training examples since we target the artifacts in af\ufb01ne face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice."
          ],
          [
            "Conference Name",
            "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
          ],
          [
            "Creators",
            "Yuezun Li, Siwei Lyu"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Zotero"
          ],
          [
            "Pages",
            "7"
          ],
          [
            "Proceedings Title",
            "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
          ],
          [
            "Title",
            "Exposing DeepFake Videos By Detecting Face Warping Artifacts"
          ]
        ],
        "resource": "storage/i1210.pdf",
        "selectable": false
      },
      {
        "text": "Face Forensics in the Wild",
        "item-id": "i2778",
        "nodes": [
          {
            "text": "Zhou et al_2021_Face Forensics in the Wild.pdf",
            "item-id": "i2920",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhou et al_2021_Face Forensics in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhou et al_2021_Face Forensics in the Wild.pdf"
              ]
            ],
            "resource": "storage/i2920.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Face Forensics in the Wild",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "On existing public benchmarks, face forgery detection techniques have achieved great success. However, when used in multi-person videos, which often contain many people active in the scene with only a small subset having been manipulated, their performance remains far from being satisfactory. To take face forgery detection to a new level, we construct a novel large-scale dataset, called FFIW-10K, which comprises 10,000 high-quality forgery videos, with an average of three human faces in each frame. The manipulation procedure is fully automatic, controlled by a domain-adversarial quality assessment network, making our dataset highly scalable with low human cost. In addition, we propose a novel algorithm to tackle the task of multi-person face forgery detection. Supervised by only video-level label, the algorithm explores multiple instance learning and learns to automatically attend to tampered faces. Our algorithm outperforms representative approaches for both forgery classification and localization on FFIW-10K, and also shows high generalization ability on existing benchmarks. We hope that our dataset and study will help the community to explore this new field in more depth."
          ],
          [
            "Access Date",
            "2023-07-01 00:02:44"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, Jianbing Shen"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5778-5788"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Face Forensics in the Wild"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Face_Forensics_in_the_Wild_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i2920.pdf",
        "selectable": false
      },
      {
        "text": "FaceForensics++",
        "item-id": "i955",
        "nodes": [
          {
            "text": "Rossler et al_2019_FaceForensics++.pdf",
            "item-id": "i963",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rossler et al_2019_FaceForensics++.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rossler et al_2019_FaceForensics++.pdf"
              ]
            ],
            "resource": "storage/i963.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FaceForensics++",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers."
          ],
          [
            "Access Date",
            "2021-10-05 12:34:36"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Niessner"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1-11"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "FaceForensics++"
          ],
          [
            "Title",
            "FaceForensics++: Learning to Detect Manipulated Facial Images"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i963.pdf",
        "selectable": false
      },
      {
        "text": "FairFace",
        "item-id": "i1861",
        "nodes": [
          {
            "text": "K\u00e4rkk\u00e4inen_Joo_2019_FairFace.pdf",
            "item-id": "i1959",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "K\u00e4rkk\u00e4inen_Joo_2019_FairFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "K\u00e4rkk\u00e4inen_Joo_2019_FairFace.pdf"
              ]
            ],
            "resource": "storage/i1959.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FairFace",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups."
          ],
          [
            "Access Date",
            "2022-11-01 15:56:33"
          ],
          [
            "Archiveid",
            "arXiv:1908.04913"
          ],
          [
            "Creators",
            "Kimmo K\u00e4rkk\u00e4inen, Jungseock Joo"
          ],
          [
            "DOI",
            "10.48550/arXiv.1908.04913"
          ],
          [
            "Date",
            "2019-08-13 2019-08-13"
          ],
          [
            "Extra",
            "arXiv:1908.04913 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FairFace"
          ],
          [
            "Title",
            "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1908.04913"
          ]
        ],
        "resource": "storage/i1959.pdf",
        "selectable": false
      },
      {
        "text": "FakeAVCeleb",
        "item-id": "i918",
        "nodes": [
          {
            "text": "Khalid et al_2021_FakeAVCeleb.pdf",
            "item-id": "i933",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khalid et al_2021_FakeAVCeleb.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_9H4BYB9V/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/2\">2 BACKGROUND AND MOTIVATION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/3\">3 Dataset collection and Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/3\">3.1 Dataset Collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/4\">3.2 Dataset Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/6\">3.3 Different Multimodal Dataset Generation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4.1 Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4.2 Deepfake detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4.3 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5 Discussion and Future Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.1 Data Quality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.2 Data Availability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.3 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.4 Future Directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/9\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khalid et al_2021_FakeAVCeleb.pdf"
              ]
            ],
            "resource": "storage/i933.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FakeAVCeleb",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the significant advancements made in generation of forged video and audio, commonly known as deepfakes, using deep learning technologies, the problem of its misuse is a well-known issue now. Recently, a new problem of generating cloned or synthesized human voice of a person is emerging. AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake videos and audios, new deepfake detectors are need that focuses on both, video and audio. Detecting deepfakes is a challenging task and researchers have made numerous attempts and proposed several deepfake detection methods. To develop a good deepfake detector, a handsome amount of good quality dataset is needed that captures the real world scenarios. Many researchers have contributed in this cause and provided several deepfake dataset, self generated and in-the-wild. However, almost all of these datasets either contains deepfake videos or audio. Moreover, the recent deepfake datasets proposed by researchers have racial bias issues. Hence, there is a crucial need of a good deepfake video and audio deepfake dataset. To fill this gap, we propose a novel Audio-Video Deepfake dataset (FakeAVCeleb) that not only contains deepfake videos but respective synthesized cloned audios as well. We generated our dataset using recent most popular deepfake generation methods and the videos and audios are perfectly lip-synced with each other. To generate a more realistic dataset, we selected real YouTube videos of celebrities having four racial backgrounds (Caucasian, Black, East Asian and South Asian) to counter the racial bias issue. Lastly, we propose a novel multimodal detection method that detects deepfake videos and audios based on our multimodal Audio-Video deepfake dataset."
          ],
          [
            "Access Date",
            "2021-08-17 05:47:40"
          ],
          [
            "Archiveid",
            "arXiv:2108.05080"
          ],
          [
            "Creators",
            "Hasam Khalid, Shahroz Tariq, Simon S. Woo"
          ],
          [
            "Date",
            "2021-08-11 2021-08-11"
          ],
          [
            "Extra",
            "arXiv: 2108.05080 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FakeAVCeleb"
          ],
          [
            "Title",
            "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2108.05080"
          ]
        ],
        "resource": "storage/i933.pdf",
        "selectable": false
      },
      {
        "text": "FineAction",
        "item-id": "i2779",
        "nodes": [
          {
            "text": "Liu et al_2022_FineAction.pdf",
            "item-id": "i2900",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2022_FineAction.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BQ3CPN52/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/2\">II Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/2\">II-A Action Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/2\">II-B Temporal Action Localization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/3\">III FineAction Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/3\">III-A Dataset Preparation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/4\">III-B Dataset Annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/5\">III-C Dataset Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/5\">III-D Dataset Properties</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/6\">IV Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/6\">IV-A Dataset Split</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/6\">IV-B Untrimmed Video Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/7\">IV-C Temporal Action Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/8\">IV-D Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/10\">V Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/10\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2022_FineAction.pdf"
              ]
            ],
            "resource": "storage/i2900.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FineAction",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond. The dataset is available at https://deeperaction.github.io/datasets/fineaction."
          ],
          [
            "Creators",
            "Yi Liu, Limin Wang, Yali Wang, Xiao Ma, Yu Qiao"
          ],
          [
            "DOI",
            "10.1109/TIP.2022.3217368"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6937-6950"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Short Title",
            "FineAction"
          ],
          [
            "Title",
            "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i2900.pdf",
        "selectable": false
      },
      {
        "text": "ForgeryNet",
        "item-id": "i1199",
        "nodes": [
          {
            "text": "He et al_2021_ForgeryNet.pdf",
            "item-id": "i3601",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2021_ForgeryNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2021_ForgeryNet.pdf"
              ]
            ],
            "resource": "storage/i3601.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ForgeryNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapid progress of photorealistic synthesis techniques has reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis. To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations.We hope that the scale, quality, and variety of ForgeryNet dataset will foster further research and innovation in the area of face forgery classification, spatial and temporal forgery localization etc."
          ],
          [
            "Access Date",
            "2021-11-16 05:05:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, Ziwei Liu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4360-4369"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "ForgeryNet"
          ],
          [
            "Title",
            "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i3601.pdf",
        "selectable": false
      },
      {
        "text": "Glitch in the matrix",
        "item-id": "i2991",
        "nodes": [
          {
            "text": "Cai et al_2023_Glitch in the matrix.pdf",
            "item-id": "i3000",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2023_Glitch in the matrix.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2023_Glitch in the matrix.pdf"
              ]
            ],
            "resource": "storage/i3000.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Glitch in the matrix",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most deepfake detection methods focus on detecting spatial and/or spatio-temporal changes in facial attributes and are centered around the binary classification task of detecting whether a video is real or fake. This is because available benchmark datasets contain mostly visual-only modifications present in the entirety of the video. However, a sophisticated deepfake may include small segments of audio or audio\u2013visual manipulations that can completely change the meaning of the video content. To addresses this gap, we propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual and audio\u2013visual manipulations. The proposed baseline method, Boundary Aware Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture which effectively captures multimodal manipulations. We further improve (i.e. BA-TFD+) the baseline method by replacing the backbone with a Multiscale Vision Transformer and guide the training process with contrastive, frame classification, boundary matching and multimodal boundary matching loss functions. The quantitative analysis demonstrates the superiority of BA-TFD+ on temporal forgery localization and deepfake detection tasks using several benchmark datasets including our newly proposed dataset. The dataset, models and code are available at https://github.com/ControlNet/LAV-DF."
          ],
          [
            "Access Date",
            "2023-09-04 20:24:08"
          ],
          [
            "Creators",
            "Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1016/j.cviu.2023.103818"
          ],
          [
            "Date",
            "2023-11-01 2023-11-01"
          ],
          [
            "ISSN",
            "1077-3142"
          ],
          [
            "Journal Abbreviation",
            "Computer Vision and Image Understanding"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "103818"
          ],
          [
            "Publication Title",
            "Computer Vision and Image Understanding"
          ],
          [
            "Rights",
            "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)"
          ],
          [
            "Short Title",
            "Glitch in the matrix"
          ],
          [
            "Title",
            "Glitch in the matrix: A large scale benchmark for content driven audio\u2013visual forgery detection and localization"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1077314223001984"
          ],
          [
            "Volume",
            "236"
          ]
        ],
        "resource": "storage/i3000.pdf",
        "selectable": false
      },
      {
        "text": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
        "item-id": "i3444",
        "nodes": [
          {
            "text": "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf",
            "item-id": "i3549",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_H2CF698R/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2CF698R/3\">Dynamic Concept Learner</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/4\">Model Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/5\">Training and inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Comparisons on Temporal and Causal Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/7\">Evaluation of Object and Event Concept Grounding in Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/7\">Generalization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/9\">Extension to real videos and the new concept</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/9\">Discussion and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Dynamic Predictor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/14\">Program Parser</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/14\">CLEVRER Operations and Program Execution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/15\">Trajectory Performance Evaluation.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/16\">Statistics for CLEVRER-Grounding and CLEVRER-Retrieval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/16\">Training Objectives</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf"
              ]
            ],
            "resource": "storage/i3549.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from video and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity."
          ],
          [
            "Access Date",
            "2024-01-08 03:21:42"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, Chuang Gan"
          ],
          [
            "Date",
            "2020-10-02 2020/10/02"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=bhCDO_cEGCz"
          ]
        ],
        "resource": "storage/i3549.pdf",
        "selectable": false
      },
      {
        "text": "HACS",
        "item-id": "i2774",
        "nodes": [
          {
            "text": "Zhao et al_2019_HACS.pdf",
            "item-id": "i2906",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao et al_2019_HACS.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao et al_2019_HACS.pdf"
              ]
            ],
            "resource": "storage/i2906.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HACS",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large-scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transfer learning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1M as a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations."
          ],
          [
            "Access Date",
            "2023-07-01 08:00:28"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8668-8678"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "HACS"
          ],
          [
            "Title",
            "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_HACS_Human_Action_Clips_and_Segments_Dataset_for_Recognition_and_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i2906.pdf",
        "selectable": false
      },
      {
        "text": "KnowIT VQA",
        "item-id": "i3471",
        "nodes": [
          {
            "text": "Garcia et al_2020_KnowIT VQA.pdf",
            "item-id": "i3558",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Garcia et al_2020_KnowIT VQA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Garcia et al_2020_KnowIT VQA.pdf"
              ]
            ],
            "resource": "storage/i3558.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "KnowIT VQA",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations."
          ],
          [
            "Access Date",
            "2024-01-08 03:09:31"
          ],
          [
            "Creators",
            "Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima"
          ],
          [
            "DOI",
            "10.1609/aaai.v34i07.6713"
          ],
          [
            "Date",
            "2020-04-03 2020-04-03"
          ],
          [
            "Extra",
            "Number: 07"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "07"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "10826-10834"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c) 2020 Association for the Advancement of Artificial Intelligence"
          ],
          [
            "Short Title",
            "KnowIT VQA"
          ],
          [
            "Title",
            "KnowIT VQA: Answering Knowledge-Based Questions about Videos"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/6713"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i3558.pdf",
        "selectable": false
      },
      {
        "text": "KoDF",
        "item-id": "i1099",
        "nodes": [
          {
            "text": "Kwon et al_2021_KoDF.pdf",
            "item-id": "i1101",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kwon et al_2021_KoDF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kwon et al_2021_KoDF.pdf"
              ]
            ],
            "resource": "storage/i1101.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "KoDF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratizing the face synthesis technology to a great extent. Videos generated as such have come to be called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we provide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detection datasets, and underline the importance of using multiple datasets for real-world generalization. KoDF is publicly available at https://moneybrain-research.github.io/kodf in its entirety (i.e. real clips, synthesized clips, clips with adversarial attack, and metadata)."
          ],
          [
            "Access Date",
            "2021-10-20 05:29:23"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10744-10753"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "KoDF"
          ],
          [
            "Title",
            "KoDF: A Large-Scale Korean DeepFake Detection Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Kwon_KoDF_A_Large-Scale_Korean_DeepFake_Detection_Dataset_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1101.pdf",
        "selectable": false
      },
      {
        "text": "Labeled Faces in the Wild",
        "item-id": "i1844",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2008",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>LFW dataset</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Huang et al_2008_Labeled Faces in the Wild.pdf",
            "item-id": "i2010",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2008_Labeled Faces in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2008_Labeled Faces in the Wild.pdf"
              ]
            ],
            "resource": "storage/i2010.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Labeled Faces in the Wild",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version."
          ],
          [
            "Access Date",
            "2022-10-30 16:04:27"
          ],
          [
            "Conference Name",
            "Workshop on Faces in 'Real-Life' Images: Detection, Alignment, and Recognition"
          ],
          [
            "Creators",
            "Gary B. Huang, Marwan Mattar, Tamara Berg, Eric Learned-Miller"
          ],
          [
            "Date",
            "2008-10-00 2008/10"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "hal.inria.fr"
          ],
          [
            "Proceedings Title",
            "Workshop on Faces in 'Real-Life' Images: Detection, Alignment, and Recognition"
          ],
          [
            "Short Title",
            "Labeled Faces in the Wild"
          ],
          [
            "Title",
            "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"
          ],
          [
            "URL",
            "https://hal.inria.fr/inria-00321923"
          ]
        ],
        "resource": "storage/i2010.pdf",
        "selectable": false
      },
      {
        "text": "Labeled faces in the wild: Updates and new reporting procedures",
        "item-id": "i2803",
        "nodes": [
          {
            "text": "Learned-Miller_2014_Labeled faces in the wild.pdf",
            "item-id": "i2857",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Learned-Miller_2014_Labeled faces in the wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Learned-Miller_2014_Labeled faces in the wild.pdf"
              ]
            ],
            "resource": "storage/i2857.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-font",
        "item_title": "Labeled faces in the wild: Updates and new reporting procedures",
        "item_type": "report",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Creators",
            "Gary B. Huang Erik Learned-Miller"
          ],
          [
            "Date",
            "2014-05-00 2014-05"
          ],
          [
            "Extra",
            "Issue: UM-CS-2014-003"
          ],
          [
            "Institution",
            "University of Massachusetts, Amherst"
          ],
          [
            "Title",
            "Labeled faces in the wild: Updates and new reporting procedures"
          ]
        ],
        "resource": "storage/i2857.pdf",
        "selectable": false
      },
      {
        "text": "Libri-Light",
        "item-id": "i3660",
        "nodes": [
          {
            "text": "IEEE Xplore Abstract Record",
            "item-id": "i3687",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "IEEE Xplore Abstract Record",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 12:03:31"
              ],
              [
                "Title",
                "IEEE Xplore Abstract Record"
              ],
              [
                "URL",
                "https://ieeexplore.ieee.org/abstract/document/9052942"
              ]
            ],
            "resource": "storage/i3687.html"
          },
          {
            "text": "Kahn et al_2020_Libri-Light.pdf",
            "item-id": "i3688",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kahn et al_2020_Libri-Light.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kahn et al_2020_Libri-Light.pdf"
              ]
            ],
            "resource": "storage/i3688.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Libri-Light",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio, which is, to our knowledge, the largest freely-available corpus of speech. The audio has been segmented using voice activity detection and is tagged with SNR, speaker ID and genre descriptions. Additionally, we provide baseline systems and evaluation metrics working under three settings: (1) the zero resource/unsupervised setting (ABX), (2) the semi- supervised setting (PER, CER) and (3) the distant supervision setting (WER). Settings (2) and (3) use limited textual resources (10 minutes to 10 hours) aligned with the speech. Setting (3) uses large amounts of unaligned text. They are evaluated on the standard LibriSpeech dev and test sets for comparison with the supervised state-of-the-art."
          ],
          [
            "Access Date",
            "2024-01-14 12:03:19"
          ],
          [
            "Conference Name",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "J. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazar\u00e9, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, E. Dupoux"
          ],
          [
            "DOI",
            "10.1109/ICASSP40776.2020.9052942"
          ],
          [
            "Date",
            "2020-05-00 2020-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "7669-7673"
          ],
          [
            "Proceedings Title",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Libri-Light"
          ],
          [
            "Title",
            "Libri-Light: A Benchmark for ASR with Limited or No Supervision"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/9052942"
          ]
        ],
        "resource": "storage/i3688.pdf",
        "selectable": false
      },
      {
        "text": "Librispeech",
        "item-id": "i3653",
        "nodes": [
          {
            "text": "IEEE Xplore Abstract Record",
            "item-id": "i3689",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "IEEE Xplore Abstract Record",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 12:00:17"
              ],
              [
                "Title",
                "IEEE Xplore Abstract Record"
              ],
              [
                "URL",
                "https://ieeexplore.ieee.org/abstract/document/7178964"
              ]
            ],
            "resource": "storage/i3689.html"
          },
          {
            "text": "Panayotov et al_2015_Librispeech.pdf",
            "item-id": "i3690",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Panayotov et al_2015_Librispeech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Panayotov et al_2015_Librispeech.pdf"
              ]
            ],
            "resource": "storage/i3690.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Librispeech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems."
          ],
          [
            "Access Date",
            "2024-01-14 12:00:11"
          ],
          [
            "Conference Name",
            "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2015.7178964"
          ],
          [
            "Date",
            "2015-04-00 2015-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "5206-5210"
          ],
          [
            "Proceedings Title",
            "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Librispeech"
          ],
          [
            "Title",
            "Librispeech: An ASR corpus based on public domain audio books"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/7178964"
          ]
        ],
        "resource": "storage/i3690.pdf",
        "selectable": false
      },
      {
        "text": "MS-Celeb-1M",
        "item-id": "i1892",
        "nodes": [
          {
            "text": "Guo et al_2016_MS-Celeb-1M.pdf",
            "item-id": "i2011",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Guo et al_2016_MS-Celeb-1M.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_622H4S7N/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/4\">2 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_622H4S7N/6\">3 Benchmark Construction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/6\">3.1 One Million Celebrity List</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/7\">3.2 Celebrity Selection for Measurement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/8\">3.3 Labeling for Measurement</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_622H4S7N/10\">4 Celebrity Recognition</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/10\">4.1 Evaluation Protocol</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/11\">4.2 Training Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/13\">4.3 Baseline</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/14\">5 Discussion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Guo et al_2016_MS-Celeb-1M.pdf"
              ]
            ],
            "resource": "storage/i2011.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MS-Celeb-1M",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46487-9_6"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46487-9"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "87-102"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "MS-Celeb-1M"
          ],
          [
            "Title",
            "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition"
          ]
        ],
        "resource": "storage/i2011.pdf",
        "selectable": false
      },
      {
        "text": "MVBench",
        "item-id": "i3449",
        "nodes": [
          {
            "text": "Comment: 18 pages, 7 figures, 19 tables",
            "item-id": "n3541",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 18 pages, 7 figures, 19 tables",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 18 pages, 7 figures, 19 tables</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2023_MVBench.pdf",
            "item-id": "i3538",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2023_MVBench.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_I3MT7G7N/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/2\">. Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/3\">. MVBench</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/3\">. Temporal Task Definition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/4\">. Automatic QA Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/5\">. Prompt Design for Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/5\">. VideoChat2</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/5\">. Instruction-Tuning Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/6\">. Progressive Multi-Modal Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/6\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/7\">. Results on MVBench</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/7\">. More Comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/8\">. Ablations of VideoChat2</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/8\">. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/9\">. Training Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/9\">. More Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/10\">. Details of QA Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/10\">. Results on Challenging Video QA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/11\">. Comparisons with GPT-4V</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/11\">. Leaderboards and Analyses</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/13\">. Qualitative Results</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2023_MVBench.pdf"
              ]
            ],
            "resource": "storage/i3538.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MVBench",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiple-choice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https://github.com/OpenGVLab/Ask-Anything."
          ],
          [
            "Access Date",
            "2024-01-08 03:38:59"
          ],
          [
            "Archiveid",
            "arXiv:2311.17005"
          ],
          [
            "Creators",
            "Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao"
          ],
          [
            "Date",
            "2023-12-03 2023-12-03"
          ],
          [
            "Extra",
            "arXiv:2311.17005 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MVBench"
          ],
          [
            "Title",
            "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2311.17005"
          ]
        ],
        "resource": "storage/i3538.pdf",
        "selectable": false
      },
      {
        "text": "MoVQA",
        "item-id": "i3439",
        "nodes": [
          {
            "text": "Zhang et al_2023_MoVQA.pdf",
            "item-id": "i3534",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_MoVQA.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BXZY4RUU/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/3\">. MoVQA Benchmark</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/3\">. Question-Answering Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/4\">. Distractor Options Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/5\">. Data Statistics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/5\">. Proposed Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Keyframe Branch</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Context Branch</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. VideoQA Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/7\">. Multimodal LLMs Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/7\">. Qualitative Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/8\">. Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/12\">. More data statistics of our MoVQA.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/12\">. More examples</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/12\">. The 100 movies used in our MoVQA.</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_MoVQA.pdf"
              ]
            ],
            "resource": "storage/i3534.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MoVQA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While several long-form VideoQA datasets have been introduced, the length of both videos used to curate questions and sub-clips of clues leveraged to answer those questions have not yet reached the criteria for genuine long-form video understanding. Moreover, their QAs are unduly narrow and modality-biased, lacking a wider view of understanding long-term video content with rich dynamics and complex narratives. To remedy this, we introduce MoVQA, a long-form movie question-answering dataset, and benchmark to assess the diverse cognitive capabilities of multimodal systems rely on multi-level temporal lengths, with considering both video length and clue length. Additionally, to take a step towards human-level understanding in long-form video, versatile and multimodal question-answering is designed from the moviegoer-perspective to assess the model capabilities on various perceptual and cognitive axes.Through analysis involving various baselines reveals a consistent trend: the performance of all methods significantly deteriorate with increasing video and clue length. Meanwhile, our established baseline method has shown some improvements, but there is still ample scope for enhancement on our challenging MoVQA dataset. We expect our MoVQA to provide a new perspective and encourage inspiring works on long-form video understanding research."
          ],
          [
            "Access Date",
            "2024-01-08 03:38:58"
          ],
          [
            "Archiveid",
            "arXiv:2312.04817"
          ],
          [
            "Creators",
            "Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, Yu Qiao"
          ],
          [
            "Date",
            "2023-12-07 2023-12-07"
          ],
          [
            "Extra",
            "arXiv:2312.04817 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MoVQA"
          ],
          [
            "Title",
            "MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2312.04817"
          ]
        ],
        "resource": "storage/i3534.pdf",
        "selectable": false
      },
      {
        "text": "MovieQA",
        "item-id": "i3458",
        "nodes": [
          {
            "text": "Tapaswi et al_2016_MovieQA.pdf",
            "item-id": "i3545",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tapaswi et al_2016_MovieQA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tapaswi et al_2016_MovieQA.pdf"
              ]
            ],
            "resource": "storage/i3545.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MovieQA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2024-01-08 03:28:01"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4631-4640"
          ],
          [
            "Short Title",
            "MovieQA"
          ],
          [
            "Title",
            "MovieQA: Understanding Stories in Movies Through Question-Answering"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i3545.pdf",
        "selectable": false
      },
      {
        "text": "Multimodal Language Analysis in the Wild",
        "item-id": "i1889",
        "nodes": [
          {
            "text": "Bagher Zadeh et al_2018_Multimodal Language Analysis in the Wild.pdf",
            "item-id": "i2005",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bagher Zadeh et al_2018_Multimodal Language Analysis in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bagher Zadeh et al_2018_Multimodal Language Analysis in the Wild.pdf"
              ]
            ],
            "resource": "storage/i2005.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multimodal Language Analysis in the Wild",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art."
          ],
          [
            "Access Date",
            "2022-10-30 16:11:59"
          ],
          [
            "Conference Name",
            "ACL 2018"
          ],
          [
            "Creators",
            "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency"
          ],
          [
            "DOI",
            "10.18653/v1/P18-1208"
          ],
          [
            "Date",
            "2018-07-00 2018-07"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "2236\u20132246"
          ],
          [
            "Place",
            "Melbourne, Australia"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "Multimodal Language Analysis in the Wild"
          ],
          [
            "Title",
            "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph"
          ],
          [
            "URL",
            "https://aclanthology.org/P18-1208"
          ]
        ],
        "resource": "storage/i2005.pdf",
        "selectable": false
      },
      {
        "text": "NExT-QA",
        "item-id": "i3455",
        "nodes": [
          {
            "text": "Xiao et al_2021_NExT-QA.pdf",
            "item-id": "i3532",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xiao et al_2021_NExT-QA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xiao et al_2021_NExT-QA.pdf"
              ]
            ],
            "resource": "storage/i3532.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "NExT-QA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting at causal action reasoning, temporal action reasoning and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial description towards a deeper understanding of videos."
          ],
          [
            "Access Date",
            "2024-01-08 06:24:52"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9777-9786"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "NExT-QA"
          ],
          [
            "Title",
            "NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Xiao_NExT-QA_Next_Phase_of_Question-Answering_to_Explaining_Temporal_Actions_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i3532.pdf",
        "selectable": false
      },
      {
        "text": "One Millisecond Face Alignment with an Ensemble of Regression Trees",
        "item-id": "i2221",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2263",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>FFHQ</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Kazemi_Sullivan_2014_One Millisecond Face Alignment with an Ensemble of Regression Trees.pdf",
            "item-id": "i2264",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kazemi_Sullivan_2014_One Millisecond Face Alignment with an Ensemble of Regression Trees.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kazemi_Sullivan_2014_One Millisecond Face Alignment with an Ensemble of Regression Trees.pdf"
              ]
            ],
            "resource": "storage/i2264.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "One Millisecond Face Alignment with an Ensemble of Regression Trees",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data."
          ],
          [
            "Access Date",
            "2023-02-10 00:02:34"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Vahid Kazemi, Josephine Sullivan"
          ],
          [
            "Date",
            "2014-00-00 2014"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1867-1874"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "One Millisecond Face Alignment with an Ensemble of Regression Trees"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2014/html/Kazemi_One_Millisecond_Face_2014_CVPR_paper.html"
          ]
        ],
        "resource": "storage/i2264.pdf",
        "selectable": false
      },
      {
        "text": "OpenForensics",
        "item-id": "i1104",
        "nodes": [
          {
            "text": "Le et al_2021_OpenForensics.pdf",
            "item-id": "i1105",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Le et al_2021_OpenForensics.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Le et al_2021_OpenForensics.pdf"
              ]
            ],
            "resource": "storage/i1105.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "OpenForensics",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely OpenForensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios."
          ],
          [
            "Access Date",
            "2021-10-20 05:58:09"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10117-10127"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "OpenForensics"
          ],
          [
            "Title",
            "OpenForensics: Large-Scale Challenging Dataset for Multi-Face Forgery Detection and Segmentation In-the-Wild"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Le_OpenForensics_Large-Scale_Challenging_Dataset_for_Multi-Face_Forgery_Detection_and_Segmentation_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1105.pdf",
        "selectable": false
      },
      {
        "text": "Pre-training Strategies and\u00a0Datasets for\u00a0Facial Representation Learning",
        "item-id": "i2479",
        "nodes": [
          {
            "text": "Bulat et al_2022_Pre-training Strategies and Datasets for Facial Representation Learning.pdf",
            "item-id": "i2480",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bulat et al_2022_Pre-training Strategies and Datasets for Facial Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bulat et al_2022_Pre-training Strategies and Datasets for Facial Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i2480.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pre-training Strategies and\u00a0Datasets for\u00a0Facial Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "What is the best way to learn a universal face representation? Recent work on Deep Learning in the area of face analysis has focused on supervised learning for specific tasks of interest (e.g. face recognition, facial landmark localization etc.) but has overlooked the overarching question of how to find a facial representation that can be readily adapted to several facial analysis tasks and datasets. To this end, we make the following 4 contributions: (a) we introduce, for the first time, a comprehensive evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks. (b) We systematically investigate two ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of few-shot facial learning. (c) We investigate important properties of the training datasets including their size and quality (labelled, unlabelled or even uncurated). (d) To draw our conclusions, we conducted a very large number of experiments. Our main two findings are: (1) Unsupervised pre-training on completely in-the-wild, uncurated data provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (2) Many existing facial video datasets seem to have a large amount of redundancy. We will release code, and pre-trained models to facilitate future research."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Adrian Bulat, Shiyang Cheng, Jing Yang, Andrew Garbett, Enrique Sanchez, Georgios Tzimiropoulos, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19778-9_7"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19778-9"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "107-125"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Pre-training Strategies and\u00a0Datasets for\u00a0Facial Representation Learning"
          ]
        ],
        "resource": "storage/i2480.pdf",
        "selectable": false
      },
      {
        "text": "Quo Vadis, Action Recognition?",
        "item-id": "i950",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2016",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>I3D</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf",
            "item-id": "i979",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf"
              ]
            ],
            "resource": "storage/i979.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Quo Vadis, Action Recognition?",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-09-23 04:12:37"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Joao Carreira, Andrew Zisserman"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "6299-6308"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Quo Vadis, Action Recognition?"
          ],
          [
            "Title",
            "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i979.pdf",
        "selectable": false
      },
      {
        "text": "Rescaling Egocentric Vision",
        "item-id": "i2782",
        "nodes": [
          {
            "text": "Damen et al_2022_Rescaling Egocentric Vision.pdf",
            "item-id": "i2904",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Damen et al_2022_Rescaling Egocentric Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Damen et al_2022_Rescaling Egocentric Vision.pdf"
              ]
            ],
            "resource": "storage/i2904.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Rescaling Egocentric Vision",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces the pipeline to extend the largest dataset in egocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100\u00a0hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (Damen in Scaling egocentric vision: ECCV, 2018), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection enables new challenges such as action detection and evaluating the \u201ctest of time\u201d\u2014i.e. whether models trained on data collected in 2018 can generalise to new footage collected two years later. The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval\u00a0(from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics."
          ],
          [
            "Access Date",
            "2023-07-01 08:04:01"
          ],
          [
            "Creators",
            "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
          ],
          [
            "DOI",
            "10.1007/s11263-021-01531-2"
          ],
          [
            "Date",
            "2022-01-01 2022-01-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "33-55"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Short Title",
            "Rescaling Egocentric Vision"
          ],
          [
            "Title",
            "Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-021-01531-2"
          ],
          [
            "Volume",
            "130"
          ]
        ],
        "resource": "storage/i2904.pdf",
        "selectable": false
      },
      {
        "text": "SEWA DB",
        "item-id": "i1417",
        "nodes": [
          {
            "text": "Kossaifi et al_2021_SEWA DB.pdf",
            "item-id": "i1450",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kossaifi et al_2021_SEWA DB.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YS322WTS/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2 State-of-the-art in audio-visual emotion databases</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2.1 Elicitation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2.1.1 Posed behaviour</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2.1.2 Induced behaviour</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.1.3 Spontaneous behaviour</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.2 Representation of emotion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.3 Data Annotation and generation of the Gold Standard</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.4 The Existing Corpora</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.4.1 Elicitation using Conversational Context</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">2.4.2 Elicitation using Human-Machine Interfaces</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">2.4.3 Elicitation through Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">2.4.4 Corpus collected by segmenting existing recordings</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">3 SEWA database</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">3.1 Data collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/6\">3.2 The Data Statistics and subject demographics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/6\">3.3 Data annotation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/6\">3.3.1 Facial landmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/7\">3.3.2 Hand Gesture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/7\">3.3.3 Head Gesture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/7\">3.3.4 Transcript</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/8\">3.3.5 Facial Action Units annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/8\">3.3.6 Valence, Arousal, and Liking/Disliking annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/9\">3.3.7 Behaviour Templates</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/9\">3.3.8 Mimicry Episodes</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/9\">3.4 Database availability</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4 Baseline experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4.0.1 Methods</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4.1 Feature extraction</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4.1.1 Appearance features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/11\">4.1.2 Geometric features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/11\">4.1.3 Audio features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/11\">4.1.4 Feature fusion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.2 Experimental setting</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.2.1 Performance measure</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.3 Experimental results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.3.1 Action unit detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/13\">4.3.2 Estimation of valence, arousal and liking/disliking</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/13\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/16\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Jean Kossaifi</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Robert Walecki</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Yannis Panagakis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Jie Shen</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Maximilian Schmitt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Fabien Ringeval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Jing Han</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Vedhas Pandit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Antoine Toisoul</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Bj\u00f6rn Schuller</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Kam Star</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/20\">Elnar Hajiyev</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/20\">Maja Pantic</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kossaifi et al_2021_SEWA DB.pdf"
              ]
            ],
            "resource": "storage/i1450.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SEWA DB",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal and (dis)liking intensity estimation."
          ],
          [
            "Access Date",
            "2022-04-06 08:28:51"
          ],
          [
            "Creators",
            "Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn Schuller, Kam Star, Elnar Hajiyev, Maja Pantic"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2019.2944808"
          ],
          [
            "Date",
            "2021-03-01 2021-3-1"
          ],
          [
            "Extra",
            "arXiv: 1901.02839"
          ],
          [
            "ISSN",
            "0162-8828, 2160-9292, 1939-3539"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Journal Abbreviation",
            "IEEE Trans. Pattern Anal. Mach. Intell."
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "1022-1040"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "SEWA DB"
          ],
          [
            "Title",
            "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1901.02839"
          ],
          [
            "Volume",
            "43"
          ]
        ],
        "resource": "storage/i1450.pdf",
        "selectable": false
      },
      {
        "text": "STAR",
        "item-id": "i3454",
        "nodes": [
          {
            "text": "Wu et al_2021_STAR.pdf",
            "item-id": "i3546",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2021_STAR.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2021_STAR.pdf"
              ]
            ],
            "resource": "storage/i3546.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "STAR",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Reasoning in the real world is not divorced from situations. How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence. This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR). This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical. The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships). Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning. Questions and answers are procedurally generated. The answering logic of each question is represented by a functional program based on a situation hyper-graph. We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task. We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark."
          ],
          [
            "Access Date",
            "2024-01-08 03:25:45"
          ],
          [
            "Conference Name",
            "Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)"
          ],
          [
            "Creators",
            "Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, Chuang Gan"
          ],
          [
            "Date",
            "2021-08-29 2021/08/29"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)"
          ],
          [
            "Short Title",
            "STAR"
          ],
          [
            "Title",
            "STAR: A Benchmark for Situated Reasoning in Real-World Videos"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=EfgNF5-ZAjM"
          ]
        ],
        "resource": "storage/i3546.pdf",
        "selectable": false
      },
      {
        "text": "Scaling Egocentric Vision",
        "item-id": "i2780",
        "nodes": [
          {
            "text": "Damen et al_2018_Scaling Egocentric Vision.pdf",
            "item-id": "i2905",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Damen et al_2018_Scaling Egocentric Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Damen et al_2018_Scaling Egocentric Vision.pdf"
              ]
            ],
            "resource": "storage/i2905.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Scaling Egocentric Vision",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "First-person vision is gaining interest as it offers a unique viewpoint on people\u2019s interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict non-scripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens."
          ],
          [
            "Access Date",
            "2023-07-01 08:03:51"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "720-736"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "Scaling Egocentric Vision"
          ],
          [
            "Title",
            "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i2905.pdf",
        "selectable": false
      },
      {
        "text": "TGIF-QA",
        "item-id": "i2795",
        "nodes": [
          {
            "text": "Jang et al_2017_TGIF-QA.pdf",
            "item-id": "i2834",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jang et al_2017_TGIF-QA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jang et al_2017_TGIF-QA.pdf"
              ]
            ],
            "resource": "storage/i2834.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TGIF-QA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations."
          ],
          [
            "Access Date",
            "2023-07-18 06:59:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2758-2766"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "TGIF-QA"
          ],
          [
            "Title",
            "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i2834.pdf",
        "selectable": false
      },
      {
        "text": "TVQA",
        "item-id": "i3472",
        "nodes": [
          {
            "text": "Lei et al_2018_TVQA.pdf",
            "item-id": "i3562",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lei et al_2018_TVQA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lei et al_2018_TVQA.pdf"
              ]
            ],
            "resource": "storage/i3562.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TVQA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http://tvqa.cs.unc.edu."
          ],
          [
            "Access Date",
            "2024-01-08 03:02:07"
          ],
          [
            "Conference Name",
            "EMNLP 2018"
          ],
          [
            "Creators",
            "Jie Lei, Licheng Yu, Mohit Bansal, Tamara Berg, Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii"
          ],
          [
            "DOI",
            "10.18653/v1/D18-1167"
          ],
          [
            "Date",
            "2018-10-00 2018-10"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "1369\u20131379"
          ],
          [
            "Place",
            "Brussels, Belgium"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "TVQA"
          ],
          [
            "Title",
            "TVQA: Localized, Compositional Video Question Answering"
          ],
          [
            "URL",
            "https://aclanthology.org/D18-1167"
          ]
        ],
        "resource": "storage/i3562.pdf",
        "selectable": false
      },
      {
        "text": "TVQA+",
        "item-id": "i3460",
        "nodes": [
          {
            "text": "Lei et al_2020_TVQA+.pdf",
            "item-id": "i3561",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lei et al_2020_TVQA+.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lei et al_2020_TVQA+.pdf"
              ]
            ],
            "resource": "storage/i3561.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TVQA+",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations."
          ],
          [
            "Access Date",
            "2024-01-08 03:02:27"
          ],
          [
            "Conference Name",
            "ACL 2020"
          ],
          [
            "Creators",
            "Jie Lei, Licheng Yu, Tamara Berg, Mohit Bansal, Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault"
          ],
          [
            "DOI",
            "10.18653/v1/2020.acl-main.730"
          ],
          [
            "Date",
            "2020-07-00 2020-07"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "8211\u20138225"
          ],
          [
            "Place",
            "Online"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "TVQA+"
          ],
          [
            "Title",
            "TVQA+: Spatio-Temporal Grounding for Video Question Answering"
          ],
          [
            "URL",
            "https://aclanthology.org/2020.acl-main.730"
          ]
        ],
        "resource": "storage/i3561.pdf",
        "selectable": false
      },
      {
        "text": "The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense",
        "item-id": "i2504",
        "nodes": [
          {
            "text": "Goyal et al_2017_The Something Something Video Database for Learning and Evaluating Visual.pdf",
            "item-id": "i2700",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Goyal et al_2017_The Something Something Video Database for Learning and Evaluating Visual.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Goyal et al_2017_The Something Something Video Database for Learning and Evaluating Visual.pdf"
              ]
            ],
            "resource": "storage/i2700.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the \"something-something\" database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale."
          ],
          [
            "Access Date",
            "2023-06-08 12:05:19"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5842-5850"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Goyal_The_Something_Something_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i2700.pdf",
        "selectable": false
      },
      {
        "text": "The DeepFake Detection Challenge (DFDC) Dataset",
        "item-id": "i730",
        "nodes": [
          {
            "text": "Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf",
            "item-id": "i1153",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf"
              ]
            ],
            "resource": "storage/i1153.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "The DeepFake Detection Challenge (DFDC) Dataset",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real \"in-the-wild\" Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https://ai.facebook.com/datasets/dfdc."
          ],
          [
            "Access Date",
            "2021-07-19 03:52:43"
          ],
          [
            "Archiveid",
            "arXiv:2006.07397"
          ],
          [
            "Creators",
            "Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, Cristian Canton Ferrer"
          ],
          [
            "Date",
            "2020-10-27 2020-10-27"
          ],
          [
            "Extra",
            "arXiv: 2006.07397 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "The DeepFake Detection Challenge (DFDC) Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2006.07397"
          ]
        ],
        "resource": "storage/i1153.pdf",
        "selectable": false
      },
      {
        "text": "The EPIC-KITCHENS Dataset",
        "item-id": "i2781",
        "nodes": [
          {
            "text": "Damen et al_2021_The EPIC-KITCHENS Dataset.pdf",
            "item-id": "i2902",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Damen et al_2021_The EPIC-KITCHENS Dataset.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8VY4TJB5/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/2\">2 Related Datasets</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/3\">3 The EPIC-KITCHENS Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/3\">3.1 Data Collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/4\">3.2 Action Segment Annotations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/6\">3.3 Active Object Bounding Box Annotations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/6\">3.4 Verb and Noun Classes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/7\">3.5 Annotation Visualisations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/8\">3.6 Annotation Quality Assurance</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/8\">4 Benchmarks and Baseline Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/9\">4.1 Object Detection Benchmark</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/10\">4.2 Action Recognition Benchmark</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/13\">4.3 Action Anticipation Benchmark</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/14\">5 Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/15\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Dima Damen</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Hazel Doughty</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Giovanni Maria Farinella</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Sanja Fidler</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Antonino Furnari</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Evangelos Kazakos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Davide Moltisanti</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Jonathan Munro</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Toby Perrett</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Will Price</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Michael Wray</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/18\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Damen et al_2021_The EPIC-KITCHENS Dataset.pdf"
              ]
            ],
            "resource": "storage/i2902.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The EPIC-KITCHENS Dataset",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the largest egocentric video benchmark, offering a unique viewpoint on people\u2019s interaction with objects, their attention, and even intention. In this paper, we detail how this large-scale dataset was captured by 32 participants in their native kitchen environments, and densely annotated with actions and object interactions. Our videos depict nonscripted daily activities, as recording is started every time a participant entered their kitchen. Recording took place in four countries by participants belonging to ten different nationalities, resulting in highly diverse kitchen habits and cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.2K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. We introduce new baselines that highlight the multimodal nature of the dataset and the importance of explicit temporal modelling to discriminate fine-grained actions (e.g., \u2018closing a tap\u2019 from \u2018opening\u2019 it up)."
          ],
          [
            "Creators",
            "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2020.2991965"
          ],
          [
            "Date",
            "2021-11-00 2021-11"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4125-4141"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "The EPIC-KITCHENS Dataset"
          ],
          [
            "Title",
            "The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines"
          ],
          [
            "Volume",
            "43"
          ]
        ],
        "resource": "storage/i2902.pdf",
        "selectable": false
      },
      {
        "text": "The Kinetics Human Action Video Dataset",
        "item-id": "i2297",
        "nodes": [
          {
            "text": "Kay et al_2017_The Kinetics Human Action Video Dataset.pdf",
            "item-id": "i2299",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kay et al_2017_The Kinetics Human Action Video Dataset.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kay et al_2017_The Kinetics Human Action Video Dataset.pdf"
              ]
            ],
            "resource": "storage/i2299.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "The Kinetics Human Action Video Dataset",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers."
          ],
          [
            "Access Date",
            "2023-04-14 10:13:17"
          ],
          [
            "Archiveid",
            "arXiv:1705.06950"
          ],
          [
            "Creators",
            "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman"
          ],
          [
            "DOI",
            "10.48550/arXiv.1705.06950"
          ],
          [
            "Date",
            "2017-05-19 2017-05-19"
          ],
          [
            "Extra",
            "arXiv:1705.06950 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "The Kinetics Human Action Video Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1705.06950"
          ]
        ],
        "resource": "storage/i2299.pdf",
        "selectable": false
      },
      {
        "text": "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\"",
        "item-id": "i1162",
        "nodes": [
          {
            "text": "Comment: Preprint submitted to Computer Vision and Image Understanding",
            "item-id": "n1181",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Preprint submitted to Computer Vision and Image Understanding",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Preprint submitted to Computer Vision and Image Understanding</div>",
            "node_type": "note"
          },
          {
            "text": "Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf",
            "item-id": "i1178",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf"
              ]
            ],
            "resource": "storage/i1178.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\"",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Automatically recognizing and localizing wide ranges of human actions has crucial importance for video understanding. Towards this goal, the THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including THUMOS challenge, had focused primarily on the classification of pre-segmented (i.e., trimmed) videos, which is an artificial task. In THUMOS 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include `background videos' which share similar scenes and backgrounds as action videos, but are devoid of the specific actions. The three editions of the challenge organized in 2013--2015 have made THUMOS a common benchmark for action classification and detection and the annual challenge is widely attended by teams from around the world. In this paper we describe the THUMOS benchmark in detail and give an overview of data collection and annotation procedures. We present the evaluation protocols used to quantify results in the two THUMOS tasks of action classification and temporal detection. We also present results of submissions to the THUMOS 2015 challenge and review the participating approaches. Additionally, we include a comprehensive empirical study evaluating the differences in action recognition between trimmed and untrimmed videos, and how well methods trained on trimmed videos generalize to untrimmed videos. We conclude by proposing several directions and improvements for future THUMOS challenges."
          ],
          [
            "Access Date",
            "2021-11-01 19:43:05"
          ],
          [
            "Creators",
            "Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, Mubarak Shah"
          ],
          [
            "DOI",
            "10.1016/j.cviu.2016.10.018"
          ],
          [
            "Date",
            "2017-02-00 02/2017"
          ],
          [
            "Extra",
            "arXiv: 1604.06182"
          ],
          [
            "ISSN",
            "10773142"
          ],
          [
            "Journal Abbreviation",
            "Computer Vision and Image Understanding"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "1-23"
          ],
          [
            "Publication Title",
            "Computer Vision and Image Understanding"
          ],
          [
            "Title",
            "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\""
          ],
          [
            "URL",
            "http://arxiv.org/abs/1604.06182"
          ],
          [
            "Volume",
            "155"
          ]
        ],
        "resource": "storage/i1178.pdf",
        "selectable": false
      },
      {
        "text": "The VidTIMIT Database",
        "item-id": "i2203",
        "icon": "glyphicon glyphicon-book",
        "item_title": "The VidTIMIT Database",
        "item_type": "book",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This communication describes the multi-modal VidTIMIT database, which can be useful for research involving mono- or multi-modal speech recognition or person authentication. It is comprised of video and corresponding audio recordings of 43 volunteers, reciting short sentences selected from the NTIMIT corpus"
          ],
          [
            "Creators",
            "Conrad Sanderson"
          ],
          [
            "Date",
            "2002-00-00 2002"
          ],
          [
            "Library Catalog",
            "Infoscience"
          ],
          [
            "Publisher",
            "IDIAP"
          ],
          [
            "Title",
            "The VidTIMIT Database"
          ]
        ]
      },
      {
        "text": "Transferring Domain-Agnostic Knowledge in Video Question Answering",
        "item-id": "i3469",
        "nodes": [
          {
            "text": "Wu et al_2021_Transferring Domain-Agnostic Knowledge in Video Question Answering.pdf",
            "item-id": "i3556",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2021_Transferring Domain-Agnostic Knowledge in Video Question Answering.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2021_Transferring Domain-Agnostic Knowledge in Video Question Answering.pdf"
              ]
            ],
            "resource": "storage/i3556.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Transferring Domain-Agnostic Knowledge in Video Question Answering",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2024-01-08 03:12:44"
          ],
          [
            "Conference Name",
            "Proceedings of the British Machine Vision Conference (BMVC)"
          ],
          [
            "Creators",
            "Tianran Wu, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima, Haruo Takemura"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Google Scholar"
          ],
          [
            "Proceedings Title",
            "Proceedings of the British Machine Vision Conference (BMVC)"
          ],
          [
            "Title",
            "Transferring Domain-Agnostic Knowledge in Video Question Answering"
          ],
          [
            "URL",
            "https://www.bmvc2021-virtualconference.com/assets/papers/1187.pdf"
          ]
        ],
        "resource": "storage/i3556.pdf",
        "selectable": false
      },
      {
        "text": "VGGFace2",
        "item-id": "i1893",
        "nodes": [
          {
            "text": "Cao et al_2018_VGGFace2.pdf",
            "item-id": "i2013",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cao et al_2018_VGGFace2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cao et al_2018_VGGFace2.pdf"
              ]
            ],
            "resource": "storage/i2013.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VGGFace2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimise the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS-Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on the IJB-A and IJB-B face recognition benchmarks, exceeding the previous state-of-the-art by a large margin. The dataset and models are publicly available."
          ],
          [
            "Conference Name",
            "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
          ],
          [
            "Creators",
            "Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, Andrew Zisserman"
          ],
          [
            "DOI",
            "10.1109/FG.2018.00020"
          ],
          [
            "Date",
            "2018-05-00 2018-05"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "67-74"
          ],
          [
            "Proceedings Title",
            "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
          ],
          [
            "Short Title",
            "VGGFace2"
          ],
          [
            "Title",
            "VGGFace2: A Dataset for Recognising Faces across Pose and Age"
          ]
        ],
        "resource": "storage/i2013.pdf",
        "selectable": false
      },
      {
        "text": "Video Question Answering via Gradually Refined Attention over Appearance and Motion",
        "item-id": "i2794",
        "nodes": [
          {
            "text": "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf",
            "item-id": "i2833",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2QNGQXBI/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2.1 Video Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2.2 Image Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/3\">2.3 Video Question Answering</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/3\">3 Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/4\">3.1 Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/4\">3.2 Attention Memory Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/5\">3.3 Answer Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.1 Data Preparation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.2 Model details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.3 Baseline methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/7\">4.4 Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/7\">4.5 Results and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/8\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf"
              ]
            ],
            "resource": "storage/i2833.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently image question answering (ImageQA) has gained lots of attention in the research community. However, as its natural extension, video question answering (VideoQA) is less explored. Although both tasks look similar, VideoQA is more challenging mainly because of the complexity and diversity of videos. As such, simply extending the ImageQA methods to videos is insufficient and suboptimal. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this paper, we consider exploiting the appearance and motion information resided in the video with a novel attention mechanism. More specifically, we propose an end-to-end model which gradually refines its attention over the appearance and motion features of the video using the question as guidance. The question is processed word by word until the model generates the final optimized attention. The weighted representation of the video, as well as other contextual information, are used to generate the answer. Extensive experiments show the advantages of our model compared to other baseline models. We also demonstrate the effectiveness of our model by analyzing the refined attention weights during the question answering procedure."
          ],
          [
            "Access Date",
            "2023-07-17"
          ],
          [
            "Creators",
            "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, Yueting Zhuang"
          ],
          [
            "DOI",
            "10.1145/3123266.3123427"
          ],
          [
            "Date",
            "2017-00-23 \u5341\u6708 23, 2017"
          ],
          [
            "ISBN",
            "978-1-4503-4906-2"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1645\u20131653"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 25th ACM international conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '17"
          ],
          [
            "Title",
            "Video Question Answering via Gradually Refined Attention over Appearance and Motion"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3123266.3123427"
          ]
        ],
        "resource": "storage/i2833.pdf",
        "selectable": false
      },
      {
        "text": "VoxCeleb",
        "item-id": "i1016",
        "nodes": [
          {
            "text": "Comment: The dataset can be downloaded from http://www.robots.ox.ac.uk/~vgg/data/voxceleb . 1706.08612v2: minor fixes; 6",
            "item-id": "n1020",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: The dataset can be downloaded from http://www.robots.ox.ac.uk/~vgg/data/voxceleb . 1706.08612v2: minor fixes; 6",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: The dataset can be downloaded from http://www.robots.ox.ac.uk/~vgg/data/voxceleb . 1706.08612v2: minor fixes; 6 pages</div>",
            "node_type": "note"
          },
          {
            "text": "Nagrani et al_2017_VoxCeleb.pdf",
            "item-id": "i1019",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Nagrani et al_2017_VoxCeleb.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XJYZD72B/1\">1  Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/1\">2  Related Works</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/2\">3  Dataset Description</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/2\">4  Dataset Collection Pipeline</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/3\">5  CNN Design and Architecture</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XJYZD72B/4\">6  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/4\">6.1  Experimental setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/4\">6.2  Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/4\">6.3  Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/4\">7  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XJYZD72B/5\">8  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Nagrani et al_2017_VoxCeleb.pdf"
              ]
            ],
            "resource": "storage/i1019.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VoxCeleb",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification."
          ],
          [
            "Access Date",
            "2021-10-12 12:27:36"
          ],
          [
            "Creators",
            "Arsha Nagrani, Joon Son Chung, Andrew Zisserman"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2017-950"
          ],
          [
            "Date",
            "2017-08-20 2017-8-20"
          ],
          [
            "Extra",
            "arXiv: 1706.08612"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "2616-2620"
          ],
          [
            "Publication Title",
            "Interspeech 2017"
          ],
          [
            "Short Title",
            "VoxCeleb"
          ],
          [
            "Title",
            "VoxCeleb: a large-scale speaker identification dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1706.08612"
          ]
        ],
        "resource": "storage/i1019.pdf",
        "selectable": false
      },
      {
        "text": "VoxCeleb2",
        "item-id": "i2826",
        "nodes": [
          {
            "text": "Chung et al_2018_VoxCeleb2.pdf",
            "item-id": "i2889",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chung et al_2018_VoxCeleb2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chung et al_2018_VoxCeleb2.pdf"
              ]
            ],
            "resource": "storage/i2889.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VoxCeleb2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The objective of this paper is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset."
          ],
          [
            "Access Date",
            "2023-07-10 08:30:15"
          ],
          [
            "Conference Name",
            "Interspeech 2018"
          ],
          [
            "Creators",
            "Joon Son Chung, Arsha Nagrani, Andrew Zisserman"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2018-1929"
          ],
          [
            "Date",
            "2018-09-02 2018-9-2"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "1086-1090"
          ],
          [
            "Proceedings Title",
            "Interspeech 2018"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "VoxCeleb2"
          ],
          [
            "Title",
            "VoxCeleb2: Deep Speaker Recognition"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2018/chung18b_interspeech.html"
          ]
        ],
        "resource": "storage/i2889.pdf",
        "selectable": false
      },
      {
        "text": "WildDeepfake",
        "item-id": "i1123",
        "nodes": [
          {
            "text": "Zi et al_2020_WildDeepfake.pdf",
            "item-id": "i1152",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zi et al_2020_WildDeepfake.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8HCGFPL6/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/2\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/2\">2.1 Deepfake Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/3\">2.2 Deepfake Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/3\">3 Datasets for Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/3\">3.1 Existing Deepfake Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/4\">3.2 WildDeepfake Dataset</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/5\">4 Proposed ADDNets for Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/5\">4.1 Problem Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/5\">4.2 Proposed Detection Networks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/7\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/7\">5.1 Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/7\">5.2 Results and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zi et al_2020_WildDeepfake.pdf"
              ]
            ],
            "resource": "storage/i1152.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "WildDeepfake",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, the abuse of a face swap technique called deepfake has raised enormous public concerns. So far, a large number of deepfake videos (known as \"deepfakes\") have been crafted and uploaded to the internet, calling for effective countermeasures. One promising countermeasure against deepfakes is deepfake detection. Several deepfake datasets have been released to support the training and testing of deepfake detectors, such as DeepfakeDetection [1] and FaceForensics++ [23]. While this has greatly advanced deepfake detection, most of the real videos in these datasets are filmed with a few volunteer actors in limited scenes, and the fake videos are crafted by researchers using a few popular deepfake softwares. Detectors developed on these datasets may become less effective against real-world deepfakes on the internet. To better support detection against real-world deepfakes, in this paper, we introduce a new dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707 deepfake videos collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop and test the effectiveness of deepfake detectors against real-world deepfakes. We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a more challenging dataset, where the detection performance can decrease drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention masks on real/fake faces for improved detection. We empirically verify the effectiveness of ADDNets on both existing datasets and WildDeepfake. The dataset is available at: https://github.com/deepfakeinthewild/deepfake-in-the-wild."
          ],
          [
            "Access Date",
            "2021-10-20"
          ],
          [
            "Creators",
            "Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, Yu-Gang Jiang"
          ],
          [
            "DOI",
            "10.1145/3394171.3413769"
          ],
          [
            "Date",
            "2020-10-12 October 12, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7988-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2382\u20132390"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '20"
          ],
          [
            "Short Title",
            "WildDeepfake"
          ],
          [
            "Title",
            "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3394171.3413769"
          ]
        ],
        "resource": "storage/i1152.pdf",
        "selectable": false
      },
      {
        "text": "Yelp Dataset Challenge",
        "item-id": "i1429",
        "nodes": [
          {
            "text": "Asghar_2016_Yelp Dataset Challenge.pdf",
            "item-id": "i1473",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Asghar_2016_Yelp Dataset Challenge.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Asghar_2016_Yelp Dataset Challenge.pdf"
              ]
            ],
            "resource": "storage/i1473.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Yelp Dataset Challenge",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models."
          ],
          [
            "Access Date",
            "2022-03-29 13:31:08"
          ],
          [
            "Creators",
            "Nabiha Asghar"
          ],
          [
            "Date",
            "2016-05-17 2016-05-17"
          ],
          [
            "Extra",
            "arXiv: 1605.05362"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1605.05362 [cs]"
          ],
          [
            "Short Title",
            "Yelp Dataset Challenge"
          ],
          [
            "Title",
            "Yelp Dataset Challenge: Review Rating Prediction"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1605.05362"
          ]
        ],
        "resource": "storage/i1473.pdf",
        "selectable": false
      }
    ],
    "item_title": "Dataset",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Deepfake Dataset",
    "item-id": "c20,i3426",
    "nodes": [
      {
        "text": "ADD 2022",
        "item-id": "i3145",
        "nodes": [
          {
            "text": "Comment: Accepted by ICASSP 2022",
            "item-id": "n3149",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted by ICASSP 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted by ICASSP 2022</div>",
            "node_type": "note"
          },
          {
            "text": "Yi et al_2022_ADD 2022.pdf",
            "item-id": "i3148",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yi et al_2022_ADD 2022.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yi et al_2022_ADD 2022.pdf"
              ]
            ],
            "resource": "storage/i3148.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ADD 2022",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio deepfake detection is an emerging topic, which was included in the ASVspoof 2021. However, the recent shared tasks have not covered many real-life and challenging scenarios. The first Audio Deep synthesis Detection challenge (ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks: low-quality fake audio detection (LF), partially fake audio detection (PF) and audio fake game (FG). The LF track focuses on dealing with bona fide and fully fake utterances with various real-world noises etc. The PF track aims to distinguish the partially fake audio from the real. The FG track is a rivalry game, which includes two tasks: an audio generation task and an audio fake detection task. In this paper, we describe the datasets, evaluation metrics, and protocols. We also report major findings that reflect the recent advances in audio deepfake detection tasks."
          ],
          [
            "Access Date",
            "2023-10-20 13:17:45"
          ],
          [
            "Archiveid",
            "arXiv:2202.08433"
          ],
          [
            "Creators",
            "Jiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, Haoxin Ma, Chenglong Wang, Tao Wang, Zhengkun Tian, Ye Bai, Cunhang Fan, Shan Liang, Shiming Wang, Shuai Zhang, Xinrui Yan, Le Xu, Zhengqi Wen, Haizhou Li, Zheng Lian, Bin Liu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2202.08433"
          ],
          [
            "Date",
            "2022-02-26 2022-02-26"
          ],
          [
            "Extra",
            "arXiv:2202.08433 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ADD 2022"
          ],
          [
            "Title",
            "ADD 2022: the First Audio Deep Synthesis Detection Challenge"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2202.08433"
          ]
        ],
        "resource": "storage/i3148.pdf",
        "selectable": false
      },
      {
        "text": "ASVspoof 2021",
        "item-id": "i3040",
        "nodes": [
          {
            "text": "Liu et al_2023_ASVspoof 2021.pdf",
            "item-id": "i3115",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2023_ASVspoof 2021.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2023_ASVspoof 2021.pdf"
              ]
            ],
            "resource": "storage/i3115.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ASVspoof 2021",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Benchmarking initiatives support the meaningful comparison of competing solutions to prominent problems in speech and language processing. Successive benchmarking evaluations typically reflect a progressive evolution from ideal lab conditions towards to those encountered in the wild. ASVspoof, the spoofing and deepfake detection initiative and challenge series, has followed the same trend. This article provides a summary of the ASVspoof 2021 challenge and the results of 54 participating teams that submitted to the evaluation phase. For the logical access (LA) task, results indicate that countermeasures are robust to newly introduced encoding and transmission effects. Results for the physical access (PA) task indicate the potential to detect replay attacks in real, as opposed to simulated physical spaces, but a lack of robustness to variations between simulated and real acoustic environments. The Deepfake (DF) task, new to the 2021 edition, targets solutions to the detection of manipulated, compressed speech data posted online. While detection solutions offer some resilience to compression effects, they lack generalization across different source datasets. In addition to a summary of the top-performing systems for each task, new analyses of influential data factors and results for hidden data subsets, the article includes a review of post-challenge results, an outline of the principal challenge limitations and a road-map for the future of ASVspoof."
          ],
          [
            "Creators",
            "Xuechen Liu, Xin Wang, Md Sahidullah, Jose Patino, H\u00e9ctor Delgado, Tomi Kinnunen, Massimiliano Todisco, Junichi Yamagishi, Nicholas Evans, Andreas Nautsch, Kong Aik Lee"
          ],
          [
            "DOI",
            "10.1109/TASLP.2023.3285283"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Extra",
            "Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "ISSN",
            "2329-9304"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2507-2522"
          ],
          [
            "Publication Title",
            "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "Short Title",
            "ASVspoof 2021"
          ],
          [
            "Title",
            "ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i3115.pdf",
        "selectable": false
      },
      {
        "text": "AV-Deepfake1M",
        "item-id": "i3243",
        "nodes": [
          {
            "text": "Cai et al_2023_AV-Deepfake1M.pdf",
            "item-id": "i3271",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2023_AV-Deepfake1M.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XTN5YQ85/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/3\">. AV-Deepfake1M Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/4\">. Data Generation Pipeline</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/4\">Transcript Manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/4\">Audio Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/5\">Video Generation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/5\">. Dataset Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Human Quality Assessment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Computational Cost</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Benchmarks and Metrics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Data Partitioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/6\">. Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/7\">. Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/7\">. Results and Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/7\">. Audio-Visual Temporal Deepfake Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/8\">. Audio-Visual Deepfake Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/8\">. Unimodal Deepfake Detection and Localization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/8\">. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Transcript Manipulation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Human Quality Assessment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/13\">. Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/14\">. Evaluation and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/14\">. Audio and Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XTN5YQ85/15\">. Label Access For Training</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2023_AV-Deepfake1M.pdf"
              ]
            ],
            "resource": "storage/i3271.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "AV-Deepfake1M",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The detection and localization of highly realistic deepfake audio-visual content are challenging even for the most advanced state-of-the-art methods. While most of the research efforts in this domain are focused on detecting high-quality deepfake images and videos, only a few works address the problem of the localization of small segments of audio-visual manipulations embedded in real videos. In this research, we emulate the process of such content generation and propose the AV-Deepfake1M dataset. The dataset contains content-driven (i) video manipulations, (ii) audio manipulations, and (iii) audio-visual manipulations for more than 2K subjects resulting in a total of more than 1M videos. The paper provides a thorough description of the proposed data generation pipeline accompanied by a rigorous analysis of the quality of the generated data. The comprehensive benchmark of the proposed dataset utilizing state-of-the-art deepfake detection and localization methods indicates a significant drop in performance compared to previous datasets. The proposed dataset will play a vital role in building the next-generation deepfake localization methods. The dataset and associated code are available at https://github.com/ControlNet/AV-Deepfake1M ."
          ],
          [
            "Access Date",
            "2023-11-28 06:09:08"
          ],
          [
            "Archiveid",
            "arXiv:2311.15308"
          ],
          [
            "Creators",
            "Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall, Kalin Stefanov"
          ],
          [
            "DOI",
            "10.48550/arXiv.2311.15308"
          ],
          [
            "Date",
            "2023-11-26 2023-11-26"
          ],
          [
            "Extra",
            "arXiv:2311.15308 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Rights",
            "All rights reserved"
          ],
          [
            "Short Title",
            "AV-Deepfake1M"
          ],
          [
            "Title",
            "AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2311.15308"
          ]
        ],
        "resource": "storage/i3271.pdf",
        "selectable": false
      },
      {
        "text": "An Initial Investigation for Detecting Partially Spoofed Audio",
        "item-id": "i3258",
        "nodes": [
          {
            "text": "Comment: INTERSPEECH 2021",
            "item-id": "n3380",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: INTERSPEECH 2021",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: INTERSPEECH 2021</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang et al_2021_An Initial Investigation for Detecting Partially Spoofed Audio.pdf",
            "item-id": "i3379",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2021_An Initial Investigation for Detecting Partially Spoofed Audio.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_AUB9LVSV/1\">1  Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/1\">2  PartialSpoof Database</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/2\">3  Spoofing Countermeasures</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/2\">3.1  CM training for utterance-level detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/2\">3.2  Deriving segmental scores from an utterance-level score</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/3\">3.3  CM training for segmental-level detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/3\">4  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/3\">4.1  Experimental configurations</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/3\">4.2  Experiments for utterance-level detection</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/3\">4.2.1  Ablation study of LCNN CMs against PartialSpoof</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/3\">4.2.2  Cross-database investigation for training data mismatch</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/4\">4.2.3  Analysis based on spoof segment ratios</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/4\">4.3  Experiments for segmental-level detection</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/4\">5  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/5\">6  References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/6\">A  Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/6\">A.1  Details of PartialSpoof DatabaseSamples can be found at https://nii-yamagishilab.github.io/zlin-demo/IS2021/index.html</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/6\">A.1.1  Database collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/6\">A.1.2  Statistics of spoof trails in PartialSpoof dataset</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/6\">A.2  Multiple random initialization for Section 4</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/6\">A.2.1  Ablation study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/7\">A.2.2  Cross-database study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AUB9LVSV/7\">A.2.3  Segmental-level detection</a></li></ul></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2021_An Initial Investigation for Detecting Partially Spoofed Audio.pdf"
              ]
            ],
            "resource": "storage/i3379.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "An Initial Investigation for Detecting Partially Spoofed Audio",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "All existing databases of spoofed speech contain attack data that is spoofed in its entirety. In practice, it is entirely plausible that successful attacks can be mounted with utterances that are only partially spoofed. By definition, partially-spoofed utterances contain a mix of both spoofed and bona fide segments, which will likely degrade the performance of countermeasures trained with entirely spoofed utterances. This hypothesis raises the obvious question: 'Can we detect partially-spoofed audio?' This paper introduces a new database of partially-spoofed data, named PartialSpoof, to help address this question. This new database enables us to investigate and compare the performance of countermeasures on both utterance- and segmental- level labels. Experimental results using the utterance-level labels reveal that the reliability of countermeasures trained to detect fully-spoofed data is found to degrade substantially when tested with partially-spoofed data, whereas training on partially-spoofed data performs reliably in the case of both fully- and partially-spoofed utterances. Additional experiments using segmental-level labels show that spotting injected spoofed segments included in an utterance is a much more challenging task even if the latest countermeasure models are used."
          ],
          [
            "Access Date",
            "2023-12-09 08:22:06"
          ],
          [
            "Archiveid",
            "arXiv:2104.02518"
          ],
          [
            "Creators",
            "Lin Zhang, Xin Wang, Erica Cooper, Junichi Yamagishi, Jose Patino, Nicholas Evans"
          ],
          [
            "DOI",
            "10.48550/arXiv.2104.02518"
          ],
          [
            "Date",
            "2021-06-15 2021-06-15"
          ],
          [
            "Extra",
            "arXiv:2104.02518 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "An Initial Investigation for Detecting Partially Spoofed Audio"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2104.02518"
          ]
        ],
        "resource": "storage/i3379.pdf",
        "selectable": false
      },
      {
        "text": "Celeb-DF",
        "item-id": "i957",
        "nodes": [
          {
            "text": "Li et al_2020_Celeb-DF.pdf",
            "item-id": "i966",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2020_Celeb-DF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2020_Celeb-DF.pdf"
              ]
            ],
            "resource": "storage/i966.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Celeb-DF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for datasets of DeepFake videos. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF."
          ],
          [
            "Access Date",
            "2021-10-05 12:25:39"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3207-3216"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Celeb-DF"
          ],
          [
            "Title",
            "Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Celeb-DF_A_Large-Scale_Challenging_Dataset_for_DeepFake_Forensics_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i966.pdf",
        "selectable": false
      },
      {
        "text": "Contributing Data to Deepfake Detection Research",
        "item-id": "i990",
        "icon": "glyphicon glyphicon-pencil",
        "item_title": "Contributing Data to Deepfake Detection Research",
        "item_type": "blogPost",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Posted by Nick Dufour, Google Research and Andrew Gully, Jigsaw   Deep learning has given rise to technologies that would have been thought ..."
          ],
          [
            "Access Date",
            "2021-10-10 07:46:15"
          ],
          [
            "Blog Title",
            "Google AI Blog"
          ],
          [
            "Creators",
            "Dufou Nick, Jigsaw Andrew"
          ],
          [
            "Date",
            "2019-09-24 2019-09-24"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Title",
            "Contributing Data to Deepfake Detection Research"
          ],
          [
            "URL",
            "http://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html"
          ]
        ]
      },
      {
        "text": "DF-Platter",
        "item-id": "i2775",
        "nodes": [
          {
            "text": "Narayan et al_2023_DF-Platter.pdf",
            "item-id": "i2910",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Narayan et al_2023_DF-Platter.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Narayan et al_2023_DF-Platter.pdf"
              ]
            ],
            "resource": "storage/i2910.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DF-Platter",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake detection is gaining significant importance in the research community. While most of the research efforts are focused around high-quality images and videos, deepfake generation algorithms today have the capability to generate low-resolution videos, occluded deepfakes, and multiple-subject deepfakes. In this research, we emulate the real-world scenario of deepfake generation and spreading, and propose the DF-Platter dataset, which contains (i) both low-resolution and high-resolution deepfakes generated using multiple generation techniques and (ii) single-subject and multiple-subject deepfakes, with face images of Indian ethnicity. Faces in the dataset are annotated for various attributes such as gender, age, skin tone, and occlusion. The database is prepared in 116 days with continuous usage of 32 GPUs accounting to 1,800 GB cumulative memory. With over 500 GBs in size, the dataset contains a total of 133,260 videos encompassing three sets. To the best of our knowledge, this is one of the largest datasets containing vast variability and multiple challenges. We also provide benchmark results under multiple evaluation settings using popular and state-of-the-art deepfake detection models. Further, benchmark results under c23 and c40 compression are provided. The results demonstrate a significant performance reduction in the deepfake detection task on low-resolution deepfakes and show that the existing techniques fail drastically on multiple-subject deepfakes. It is our assertion that this database will improve the state-of-the-art by extending the capabilities of deepfake detection algorithms to real-world scenarios. The database is available at: http://iab-rubric.org/df-platter-database."
          ],
          [
            "Access Date",
            "2023-07-01 07:21:42"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Kartik Narayan, Harsh Agarwal, Kartik Thakral, Surbhi Mittal, Mayank Vatsa, Richa Singh"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9739-9748"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DF-Platter"
          ],
          [
            "Title",
            "DF-Platter: Multi-Face Heterogeneous Deepfake Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2910.pdf",
        "selectable": false
      },
      {
        "text": "DeepFakes",
        "item-id": "i993",
        "nodes": [
          {
            "text": "Comment: http://publications.idiap.ch/index.php/publications/show/3988",
            "item-id": "n1011",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: http://publications.idiap.ch/index.php/publications/show/3988",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: http://publications.idiap.ch/index.php/publications/show/3988</div>",
            "node_type": "note"
          },
          {
            "text": "Korshunov_Marcel_2018_DeepFakes.pdf",
            "item-id": "i1010",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Korshunov_Marcel_2018_DeepFakes.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PLYTFY24/1\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/2\">II Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PLYTFY24/2\">III Deepfake database</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">III-A Evaluation protocol</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">IV Analysis of deepfake videos</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">IV-A Vulnerability of face recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/3\">IV-B Detection of Deepfake videos</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/4\">V Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PLYTFY24/5\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Korshunov_Marcel_2018_DeepFakes.pdf"
              ]
            ],
            "resource": "storage/i1010.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "DeepFakes",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97% equal error rate on high quality Deepfakes. Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so."
          ],
          [
            "Access Date",
            "2021-10-10 07:13:43"
          ],
          [
            "Archiveid",
            "arXiv:1812.08685"
          ],
          [
            "Creators",
            "Pavel Korshunov, Sebastien Marcel"
          ],
          [
            "Date",
            "2018-12-20 2018-12-20"
          ],
          [
            "Extra",
            "arXiv:1812.08685 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "DeepFakes"
          ],
          [
            "Title",
            "DeepFakes: a New Threat to Face Recognition? Assessment and Detection"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1812.08685"
          ]
        ],
        "resource": "storage/i1010.pdf",
        "selectable": false
      },
      {
        "text": "DeeperForensics-1.0",
        "item-id": "i956",
        "nodes": [
          {
            "text": "Jiang et al_2020_DeeperForensics-1.pdf",
            "item-id": "i968",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2020_DeeperForensics-1.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2020_DeeperForensics-1.pdf"
              ]
            ],
            "resource": "storage/i968.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DeeperForensics-1.0",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present our on-going effort of constructing a large- scale benchmark for face forgery detection. The first version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far, with 60, 000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. We further contribute a comprehensive study that evaluates five representative detection baselines and make a thorough analysis of different settings."
          ],
          [
            "Access Date",
            "2021-10-05 12:23:53"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2889-2898"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DeeperForensics-1.0"
          ],
          [
            "Title",
            "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Jiang_DeeperForensics-1.0_A_Large-Scale_Dataset_for_Real-World_Face_Forgery_Detection_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i968.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Video Detection Using Recurrent Neural Networks",
        "item-id": "i3223",
        "nodes": [
          {
            "text": "G\u00fcera_Delp_2018_Deepfake Video Detection Using Recurrent Neural Networks.pdf",
            "item-id": "i3312",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "G\u00fcera_Delp_2018_Deepfake Video Detection Using Recurrent Neural Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "G\u00fcera_Delp_2018_Deepfake Video Detection Using Recurrent Neural Networks.pdf"
              ]
            ],
            "resource": "storage/i3312.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake Video Detection Using Recurrent Neural Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent months a machine learning based free software tool has made it easy to create believable face swaps in videos that leaves few traces of manipulation, in what are known as \"deepfake\" videos. Scenarios where these realistic fake videos are used to create political distress, blackmail someone or fake terrorism events are easily envisioned. This paper proposes a temporal-aware pipeline to automatically detect deepfake videos. Our system uses a convolutional neural network (CNN) to extract frame-level features. These features are then used to train a recurrent neural network (RNN) that learns to classify if a video has been subject to manipulation or not. We evaluate our method against a large set of deepfake videos collected from multiple video websites. We show how our system can achieve competitive results in this task while using a simple architecture."
          ],
          [
            "Access Date",
            "2024-01-03 09:23:34"
          ],
          [
            "Conference Name",
            "2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)"
          ],
          [
            "Creators",
            "David G\u00fcera, Edward J. Delp"
          ],
          [
            "DOI",
            "10.1109/AVSS.2018.8639163"
          ],
          [
            "Date",
            "2018-11-00 2018-11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-6"
          ],
          [
            "Proceedings Title",
            "2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)"
          ],
          [
            "Title",
            "Deepfake Video Detection Using Recurrent Neural Networks"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/document/8639163"
          ]
        ],
        "resource": "storage/i3312.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake generation and detection, a survey",
        "item-id": "i2777",
        "nodes": [
          {
            "text": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item-id": "i2921",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/2\">2 Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3 Deepfake generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.1 Types of\u00a0Deepfake</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.2 Face-based generation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.1 Face swapping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.2 Facial reenactment</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4 Deepfake detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1 Detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1.1 Features based detection methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/10\">4.1.2 Machine learning-based detection methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/11\">4.2 Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5 Discussions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5.1 Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">5.2 Future directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang_2022_Deepfake generation and detection, a survey.pdf"
              ]
            ],
            "resource": "storage/i2921.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake generation and detection, a survey",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake\u00a0refers to realistic, but\u00a0fake images, sounds, and videos generated by articial intelligence methods. Recent advances in deepfake generation make\u00a0deepfake more realistic and easier to make. Deepfake has been a signicant threat to national security, democracy, society, and\u00a0our privacy, which calls for deepfake detection methods to combat potential threats. In the paper, we make a survey on state-ofthe-art deepfake generation methods, detection methods, and existing datasets. Current deepfake generation methods can be\u00a0classified into face swapping and facial reenactment. Deepfake detection methods are mainly based features and machine\u00a0learning methods. There are still some challenges for deepfake detection, such as progress on deepfake generation, lack of high\u00a0quality datasets and benchmark. Future trends on deepfake detection can be efficient, robust and systematical detection methods\u00a0and high quality datasets."
          ],
          [
            "Access Date",
            "2023-06-30 23:52:04"
          ],
          [
            "Creators",
            "Tao Zhang"
          ],
          [
            "DOI",
            "10.1007/s11042-021-11733-y"
          ],
          [
            "Date",
            "2022-02-01 2022-02-01"
          ],
          [
            "ISSN",
            "1573-7721"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Multimed Tools Appl"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "6259-6276"
          ],
          [
            "Publication Title",
            "Multimedia Tools and Applications"
          ],
          [
            "Title",
            "Deepfake generation and detection, a survey"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11042-021-11733-y"
          ],
          [
            "Volume",
            "81"
          ]
        ],
        "resource": "storage/i2921.pdf",
        "selectable": false
      },
      {
        "text": "Do You Really Mean That?",
        "item-id": "i2289",
        "nodes": [
          {
            "text": "Cai et al_2022_Do You Really Mean That.pdf",
            "item-id": "i2291",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2022_Do You Really Mean That.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2022_Do You Really Mean That.pdf"
              ]
            ],
            "resource": "storage/i2291.pdf"
          },
          {
            "text": "lavdf_supp.pdf",
            "item-id": "i3356",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "lavdf_supp.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Proposed Method Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Proposed Dataset Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Additional Qualitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/3\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "lavdf_supp.pdf"
              ]
            ],
            "resource": "storage/i3356.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Do You Really Mean That?",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attributes, and adversarial perturbation-based spatio-temporal modifications at the whole video or random locations while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from a sentiment perspective. We introduce a content-driven audio-visual deepfake dataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of learning temporal forgery localization. Specifically, the content-driven audio-visual manipulations are performed strategically to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is guided via contrastive, boundary matching, and frame classification loss functions. Our extensive quantitative and qualitative analysis demonstrates the proposed method's strong performance for temporal forgery localization and deepfake detection tasks."
          ],
          [
            "Conference Name",
            "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
          ],
          [
            "Creators",
            "Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1109/DICTA56598.2022.10034605"
          ],
          [
            "Date",
            "2022-11-00 2022-11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-10"
          ],
          [
            "Place",
            "Sydney, Australia"
          ],
          [
            "Proceedings Title",
            "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
          ],
          [
            "Rights",
            "All rights reserved"
          ],
          [
            "Short Title",
            "Do You Really Mean That?"
          ],
          [
            "Title",
            "Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization"
          ]
        ],
        "selectable": false
      },
      {
        "text": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors",
        "item-id": "i1117",
        "nodes": [
          {
            "text": "Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfak",
            "item-id": "n1141",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfak",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection (ADGD '21) at ACM MM 2021</div>",
            "node_type": "note"
          },
          {
            "text": "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf",
            "item-id": "i1140",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U37X3UA3/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/2\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/2\">2.1 Fake Media Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/3\">2.2 Fake Media Detection Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3 Multimodal Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3.2 Evaluation Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4.1 Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4.2 Experiment Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/6\">4.3 Unimodal Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">4.4 Ensemble Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">4.5 Multimodal Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf"
              ]
            ],
            "resource": "storage/i1140.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Significant advancements made in the generation of deepfakes have caused security and privacy issues. Attackers can easily impersonate a person's identity in an image by replacing his face with the target person's face. Moreover, a new domain of cloning human voices using deep-learning technologies is also emerging. Now, an attacker can generate realistic cloned voices of humans using only a few seconds of audio of the target person. With the emerging threat of potential harm deepfakes can cause, researchers have proposed deepfake detection methods. However, they only focus on detecting a single modality, i.e., either video or audio. On the other hand, to develop a good deepfake detector that can cope with the recent advancements in deepfake generation, we need to have a detector that can detect deepfakes of multiple modalities, i.e., videos and audios. To build such a detector, we need a dataset that contains video and respective audio deepfakes. We were able to find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized fake audios as well. We used this multimodal deepfake dataset and performed detailed baseline experiments using state-of-the-art unimodal, ensemble-based, and multimodal detection methods to evaluate it. We conclude through detailed experimentation that unimodals, addressing only a single modality, video or audio, do not perform well compared to ensemble-based methods. Whereas purely multimodal-based baselines provide the worst performance."
          ],
          [
            "Access Date",
            "2021-10-24 02:58:21"
          ],
          [
            "Creators",
            "Hasam Khalid, Minha Kim, Shahroz Tariq, Simon S. Woo"
          ],
          [
            "DOI",
            "10.1145/3476099.3484315"
          ],
          [
            "Date",
            "2021-10-24 2021-10-24"
          ],
          [
            "Extra",
            "arXiv: 2109.02993"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "7-15"
          ],
          [
            "Publication Title",
            "Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection"
          ],
          [
            "Title",
            "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2109.02993"
          ]
        ],
        "resource": "storage/i1140.pdf",
        "selectable": false
      },
      {
        "text": "Exposing Deep Fakes Using Inconsistent Head Poses",
        "item-id": "i992",
        "nodes": [
          {
            "text": "Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf",
            "item-id": "i1007",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf"
              ]
            ],
            "resource": "storage/i1007.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exposing Deep Fakes Using Inconsistent Head Poses",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes). Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue. Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes."
          ],
          [
            "Conference Name",
            "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Xin Yang, Yuezun Li, Siwei Lyu"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2019.8683164"
          ],
          [
            "Date",
            "2019-05-00 2019-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "8261-8265"
          ],
          [
            "Proceedings Title",
            "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Exposing Deep Fakes Using Inconsistent Head Poses"
          ]
        ],
        "resource": "storage/i1007.pdf",
        "selectable": false
      },
      {
        "text": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
        "item-id": "i1211",
        "nodes": [
          {
            "text": "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf",
            "item-id": "i1210",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf"
              ]
            ],
            "resource": "storage/i1210.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as DeepFake videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classi\ufb01er, our method does not need DeepFake generated images as negative training examples since we target the artifacts in af\ufb01ne face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice."
          ],
          [
            "Conference Name",
            "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
          ],
          [
            "Creators",
            "Yuezun Li, Siwei Lyu"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Zotero"
          ],
          [
            "Pages",
            "7"
          ],
          [
            "Proceedings Title",
            "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
          ],
          [
            "Title",
            "Exposing DeepFake Videos By Detecting Face Warping Artifacts"
          ]
        ],
        "resource": "storage/i1210.pdf",
        "selectable": false
      },
      {
        "text": "Face Forensics in the Wild",
        "item-id": "i2778",
        "nodes": [
          {
            "text": "Zhou et al_2021_Face Forensics in the Wild.pdf",
            "item-id": "i2920",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhou et al_2021_Face Forensics in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhou et al_2021_Face Forensics in the Wild.pdf"
              ]
            ],
            "resource": "storage/i2920.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Face Forensics in the Wild",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "On existing public benchmarks, face forgery detection techniques have achieved great success. However, when used in multi-person videos, which often contain many people active in the scene with only a small subset having been manipulated, their performance remains far from being satisfactory. To take face forgery detection to a new level, we construct a novel large-scale dataset, called FFIW-10K, which comprises 10,000 high-quality forgery videos, with an average of three human faces in each frame. The manipulation procedure is fully automatic, controlled by a domain-adversarial quality assessment network, making our dataset highly scalable with low human cost. In addition, we propose a novel algorithm to tackle the task of multi-person face forgery detection. Supervised by only video-level label, the algorithm explores multiple instance learning and learns to automatically attend to tampered faces. Our algorithm outperforms representative approaches for both forgery classification and localization on FFIW-10K, and also shows high generalization ability on existing benchmarks. We hope that our dataset and study will help the community to explore this new field in more depth."
          ],
          [
            "Access Date",
            "2023-07-01 00:02:44"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, Jianbing Shen"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5778-5788"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Face Forensics in the Wild"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Face_Forensics_in_the_Wild_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i2920.pdf",
        "selectable": false
      },
      {
        "text": "FaceForensics",
        "item-id": "i3411",
        "nodes": [
          {
            "text": "Comment: Video: https://youtu.be/Tle7YaPkO_k",
            "item-id": "n3412",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Video: https://youtu.be/Tle7YaPkO_k",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Video: https://youtu.be/Tle7YaPkO_k</div>",
            "node_type": "note"
          },
          {
            "text": "R\u00f6ssler et al_2018_FaceForensics.pdf",
            "item-id": "i3414",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "R\u00f6ssler et al_2018_FaceForensics.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SQLV76MT/1\">FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "R\u00f6ssler et al_2018_FaceForensics.pdf"
              ]
            ],
            "resource": "storage/i3414.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FaceForensics",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With recent advances in computer vision and graphics, it is now possible to generate videos with extremely realistic synthetic faces, even in real time. Countless applications are possible, some of which raise a legitimate alarm, calling for reliable detectors of fake videos. In fact, distinguishing between original and manipulated video can be a challenge for humans and computers alike, especially when the videos are compressed or have low resolution, as it often happens on social networks. Research on the detection of face manipulations has been seriously hampered by the lack of adequate datasets. To this end, we introduce a novel face manipulation dataset of about half a million edited images (from over 1000 videos). The manipulations have been generated with a state-of-the-art face editing approach. It exceeds all existing video manipulation datasets by at least an order of magnitude. Using our new dataset, we introduce benchmarks for classical image forensic tasks, including classification and segmentation, considering videos compressed at various quality levels. In addition, we introduce a benchmark evaluation for creating indistinguishable forgeries with known ground truth; for instance with generative refinement models."
          ],
          [
            "Access Date",
            "2024-01-04 03:45:26"
          ],
          [
            "Archiveid",
            "arXiv:1803.09179"
          ],
          [
            "Creators",
            "Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Nie\u00dfner"
          ],
          [
            "DOI",
            "10.48550/arXiv.1803.09179"
          ],
          [
            "Date",
            "2018-03-24 2018-03-24"
          ],
          [
            "Extra",
            "arXiv:1803.09179 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FaceForensics"
          ],
          [
            "Title",
            "FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1803.09179"
          ]
        ],
        "resource": "storage/i3414.pdf",
        "selectable": false
      },
      {
        "text": "FaceForensics++",
        "item-id": "i955",
        "nodes": [
          {
            "text": "Rossler et al_2019_FaceForensics++.pdf",
            "item-id": "i963",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rossler et al_2019_FaceForensics++.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rossler et al_2019_FaceForensics++.pdf"
              ]
            ],
            "resource": "storage/i963.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FaceForensics++",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers."
          ],
          [
            "Access Date",
            "2021-10-05 12:34:36"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Niessner"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1-11"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "FaceForensics++"
          ],
          [
            "Title",
            "FaceForensics++: Learning to Detect Manipulated Facial Images"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i963.pdf",
        "selectable": false
      },
      {
        "text": "FakeAVCeleb",
        "item-id": "i918",
        "nodes": [
          {
            "text": "Khalid et al_2021_FakeAVCeleb.pdf",
            "item-id": "i933",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khalid et al_2021_FakeAVCeleb.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_9H4BYB9V/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/2\">2 BACKGROUND AND MOTIVATION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/3\">3 Dataset collection and Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/3\">3.1 Dataset Collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/4\">3.2 Dataset Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/6\">3.3 Different Multimodal Dataset Generation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4.1 Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4.2 Deepfake detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/7\">4.3 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5 Discussion and Future Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.1 Data Quality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.2 Data Availability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.3 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/8\">5.4 Future Directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/9\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9H4BYB9V/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khalid et al_2021_FakeAVCeleb.pdf"
              ]
            ],
            "resource": "storage/i933.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FakeAVCeleb",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the significant advancements made in generation of forged video and audio, commonly known as deepfakes, using deep learning technologies, the problem of its misuse is a well-known issue now. Recently, a new problem of generating cloned or synthesized human voice of a person is emerging. AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake videos and audios, new deepfake detectors are need that focuses on both, video and audio. Detecting deepfakes is a challenging task and researchers have made numerous attempts and proposed several deepfake detection methods. To develop a good deepfake detector, a handsome amount of good quality dataset is needed that captures the real world scenarios. Many researchers have contributed in this cause and provided several deepfake dataset, self generated and in-the-wild. However, almost all of these datasets either contains deepfake videos or audio. Moreover, the recent deepfake datasets proposed by researchers have racial bias issues. Hence, there is a crucial need of a good deepfake video and audio deepfake dataset. To fill this gap, we propose a novel Audio-Video Deepfake dataset (FakeAVCeleb) that not only contains deepfake videos but respective synthesized cloned audios as well. We generated our dataset using recent most popular deepfake generation methods and the videos and audios are perfectly lip-synced with each other. To generate a more realistic dataset, we selected real YouTube videos of celebrities having four racial backgrounds (Caucasian, Black, East Asian and South Asian) to counter the racial bias issue. Lastly, we propose a novel multimodal detection method that detects deepfake videos and audios based on our multimodal Audio-Video deepfake dataset."
          ],
          [
            "Access Date",
            "2021-08-17 05:47:40"
          ],
          [
            "Archiveid",
            "arXiv:2108.05080"
          ],
          [
            "Creators",
            "Hasam Khalid, Shahroz Tariq, Simon S. Woo"
          ],
          [
            "Date",
            "2021-08-11 2021-08-11"
          ],
          [
            "Extra",
            "arXiv: 2108.05080 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FakeAVCeleb"
          ],
          [
            "Title",
            "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2108.05080"
          ]
        ],
        "resource": "storage/i933.pdf",
        "selectable": false
      },
      {
        "text": "ForgeryNet",
        "item-id": "i1199",
        "nodes": [
          {
            "text": "He et al_2021_ForgeryNet.pdf",
            "item-id": "i3601",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2021_ForgeryNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2021_ForgeryNet.pdf"
              ]
            ],
            "resource": "storage/i3601.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ForgeryNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapid progress of photorealistic synthesis techniques has reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis. To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations.We hope that the scale, quality, and variety of ForgeryNet dataset will foster further research and innovation in the area of face forgery classification, spatial and temporal forgery localization etc."
          ],
          [
            "Access Date",
            "2021-11-16 05:05:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, Ziwei Liu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4360-4369"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "ForgeryNet"
          ],
          [
            "Title",
            "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i3601.pdf",
        "selectable": false
      },
      {
        "text": "Generation and detection of manipulated multimodal audiovisual content",
        "item-id": "i3222",
        "nodes": [
          {
            "text": "Liz-L\u00f3pez et al_2024_Generation and detection of manipulated multimodal audiovisual content.pdf",
            "item-id": "i3304",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liz-L\u00f3pez et al_2024_Generation and detection of manipulated multimodal audiovisual content.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GZPZVRVS/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/3\">Methodology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Initial Analysis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Manipulation techniques</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Video manipulation techniques</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Face morphing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/5\">Entire face synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/6\">Face Swap</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/7\">Facial Reenactment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/7\">Lip sync</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/7\">Facial attribute manipulation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/8\">Audio manipulation techniques</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/8\">Voice conversion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/8\">Text-to-speech synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/9\">Voice cloning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/9\">Voice morphing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/9\">Replay Attacks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/10\">Multimodal manipulation techniques</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/10\">Dataset of manipulated multimedia content</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/11\">Video</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/11\">Audio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Multimodal video and audio</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Multimedia data forensics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Techniques for video forensics</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Techniques for detecting manipulated metadata</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/14\">Techniques for detecting manipulated video</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/15\">Techniques for detecting manipulated video-temporal continuity features</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/18\">Techniques for detecting manipulated audio</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/18\">Audio forensics based on feature selection and extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/19\">Audio forensics based on CNN architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/22\">Audio forensics based on attention layers</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/24\">Techniques for detecting manipulated multimodal content</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/24\">Inconsistencies between modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/26\">Emotional inconsistencies</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/27\">Available tools for non-technical end-users</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/28\">Discussion and conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/28\">Answer to research questions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/29\">Future trends and challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/30\">Conclusion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Declaration of competing interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Data availability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Appendix A. Datasets download links</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Appendix B. Codes availables from forensics systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/32\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liz-L\u00f3pez et al_2024_Generation and detection of manipulated multimodal audiovisual content.pdf"
              ]
            ],
            "resource": "storage/i3304.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generation and detection of manipulated multimodal audiovisual content",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative deep learning techniques have invaded the public discourse recently. Despite the advantages, the applications to disinformation are concerning as the counter-measures advance slowly. As the manipulation of multimedia content becomes easier, faster, and more credible, developing effective forensics becomes invaluable. Other works have identified this need but neglect that disinformation is inherently multimodal. Overall in this survey, we exhaustively describe modern manipulation and forensic techniques from the lens of video, audio and their multimodal fusion. For manipulation techniques, we give a classification of the most commonly applied manipulations. Generative techniques can be exploited to generate datasets; we provide a list of current datasets useful for forensics. We have reviewed forensic techniques from 2018 to 2023, examined the usage of datasets, and given a comparative analysis of each modality. Finally, we give another comparison of end-to-end forensics tools for end-users. From our analysis clear trends are found with diffusion models, dataset granularity, explainability techniques, synchronisation improvements, and learning task diversity. We find a roadmap of deep challenges ahead, including multilinguality, multimodality, improving data quality (and variety), all in an adversarial ever-changing environment."
          ],
          [
            "Access Date",
            "2023-11-07 11:27:46"
          ],
          [
            "Creators",
            "Helena Liz-L\u00f3pez, Mamadou Keita, Abdelmalik Taleb-Ahmed, Abdenour Hadid, Javier Huertas-Tato, David Camacho"
          ],
          [
            "DOI",
            "10.1016/j.inffus.2023.102103"
          ],
          [
            "Date",
            "2024-03-01 2024-03-01"
          ],
          [
            "ISSN",
            "1566-2535"
          ],
          [
            "Journal Abbreviation",
            "Information Fusion"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "102103"
          ],
          [
            "Publication Title",
            "Information Fusion"
          ],
          [
            "Short Title",
            "Generation and detection of manipulated multimodal audiovisual content"
          ],
          [
            "Title",
            "Generation and detection of manipulated multimodal audiovisual content: Advances, trends and open challenges"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1566253523004190"
          ],
          [
            "Volume",
            "103"
          ]
        ],
        "resource": "storage/i3304.pdf",
        "selectable": false
      },
      {
        "text": "Glitch in the matrix",
        "item-id": "i2991",
        "nodes": [
          {
            "text": "Cai et al_2023_Glitch in the matrix.pdf",
            "item-id": "i3000",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2023_Glitch in the matrix.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2023_Glitch in the matrix.pdf"
              ]
            ],
            "resource": "storage/i3000.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Glitch in the matrix",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most deepfake detection methods focus on detecting spatial and/or spatio-temporal changes in facial attributes and are centered around the binary classification task of detecting whether a video is real or fake. This is because available benchmark datasets contain mostly visual-only modifications present in the entirety of the video. However, a sophisticated deepfake may include small segments of audio or audio\u2013visual manipulations that can completely change the meaning of the video content. To addresses this gap, we propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual and audio\u2013visual manipulations. The proposed baseline method, Boundary Aware Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture which effectively captures multimodal manipulations. We further improve (i.e. BA-TFD+) the baseline method by replacing the backbone with a Multiscale Vision Transformer and guide the training process with contrastive, frame classification, boundary matching and multimodal boundary matching loss functions. The quantitative analysis demonstrates the superiority of BA-TFD+ on temporal forgery localization and deepfake detection tasks using several benchmark datasets including our newly proposed dataset. The dataset, models and code are available at https://github.com/ControlNet/LAV-DF."
          ],
          [
            "Access Date",
            "2023-09-04 20:24:08"
          ],
          [
            "Creators",
            "Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1016/j.cviu.2023.103818"
          ],
          [
            "Date",
            "2023-11-01 2023-11-01"
          ],
          [
            "ISSN",
            "1077-3142"
          ],
          [
            "Journal Abbreviation",
            "Computer Vision and Image Understanding"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "103818"
          ],
          [
            "Publication Title",
            "Computer Vision and Image Understanding"
          ],
          [
            "Rights",
            "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)"
          ],
          [
            "Short Title",
            "Glitch in the matrix"
          ],
          [
            "Title",
            "Glitch in the matrix: A large scale benchmark for content driven audio\u2013visual forgery detection and localization"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1077314223001984"
          ],
          [
            "Volume",
            "236"
          ]
        ],
        "resource": "storage/i3000.pdf",
        "selectable": false
      },
      {
        "text": "KoDF",
        "item-id": "i1099",
        "nodes": [
          {
            "text": "Kwon et al_2021_KoDF.pdf",
            "item-id": "i1101",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kwon et al_2021_KoDF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kwon et al_2021_KoDF.pdf"
              ]
            ],
            "resource": "storage/i1101.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "KoDF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratizing the face synthesis technology to a great extent. Videos generated as such have come to be called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we provide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detection datasets, and underline the importance of using multiple datasets for real-world generalization. KoDF is publicly available at https://moneybrain-research.github.io/kodf in its entirety (i.e. real clips, synthesized clips, clips with adversarial attack, and metadata)."
          ],
          [
            "Access Date",
            "2021-10-20 05:29:23"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10744-10753"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "KoDF"
          ],
          [
            "Title",
            "KoDF: A Large-Scale Korean DeepFake Detection Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Kwon_KoDF_A_Large-Scale_Korean_DeepFake_Detection_Dataset_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1101.pdf",
        "selectable": false
      },
      {
        "text": "Learning realistic human actions from movies",
        "item-id": "i3426",
        "nodes": [
          {
            "text": "Laptev et al_2008_Learning realistic human actions from movies.pdf",
            "item-id": "i3428",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Laptev et al_2008_Learning realistic human actions from movies.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Laptev et al_2008_Learning realistic human actions from movies.pdf"
              ]
            ],
            "resource": "storage/i3428.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning realistic human actions from movies",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results."
          ],
          [
            "Access Date",
            "2024-01-04 04:07:37"
          ],
          [
            "Conference Name",
            "2008 IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ivan Laptev, Marcin Marszalek, Cordelia Schmid, Benjamin Rozenfeld"
          ],
          [
            "DOI",
            "10.1109/CVPR.2008.4587756"
          ],
          [
            "Date",
            "2008-06-00 2008-06"
          ],
          [
            "Extra",
            "ISSN: 1063-6919"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-8"
          ],
          [
            "Proceedings Title",
            "2008 IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Learning realistic human actions from movies"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/4587756"
          ]
        ],
        "resource": "storage/i3428.pdf",
        "selectable": false
      },
      {
        "text": "OpenForensics",
        "item-id": "i1104",
        "nodes": [
          {
            "text": "Le et al_2021_OpenForensics.pdf",
            "item-id": "i1105",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Le et al_2021_OpenForensics.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Le et al_2021_OpenForensics.pdf"
              ]
            ],
            "resource": "storage/i1105.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "OpenForensics",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely OpenForensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios."
          ],
          [
            "Access Date",
            "2021-10-20 05:58:09"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10117-10127"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "OpenForensics"
          ],
          [
            "Title",
            "OpenForensics: Large-Scale Challenging Dataset for Multi-Face Forgery Detection and Segmentation In-the-Wild"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Le_OpenForensics_Large-Scale_Challenging_Dataset_for_Multi-Face_Forgery_Detection_and_Segmentation_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1105.pdf",
        "selectable": false
      },
      {
        "text": "The Creation and Detection of Deepfakes",
        "item-id": "i958",
        "nodes": [
          {
            "text": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item-id": "i969",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf"
              ]
            ],
            "resource": "storage/i969.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The Creation and Detection of Deepfakes",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these \u201cdeepfakes\u201d have advanced significantly. In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention."
          ],
          [
            "Access Date",
            "2021-10-05 09:05:27"
          ],
          [
            "Creators",
            "Yisroel Mirsky, Wenke Lee"
          ],
          [
            "DOI",
            "10.1145/3425780"
          ],
          [
            "Date",
            "2021-01-02 2021-01-02"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "April 2021"
          ],
          [
            "Pages",
            "7:1\u20137:41"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "The Creation and Detection of Deepfakes"
          ],
          [
            "Title",
            "The Creation and Detection of Deepfakes: A Survey"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3425780"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i969.pdf",
        "selectable": false
      },
      {
        "text": "The DeepFake Detection Challenge (DFDC) Dataset",
        "item-id": "i730",
        "nodes": [
          {
            "text": "Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf",
            "item-id": "i1153",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf"
              ]
            ],
            "resource": "storage/i1153.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "The DeepFake Detection Challenge (DFDC) Dataset",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real \"in-the-wild\" Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https://ai.facebook.com/datasets/dfdc."
          ],
          [
            "Access Date",
            "2021-07-19 03:52:43"
          ],
          [
            "Archiveid",
            "arXiv:2006.07397"
          ],
          [
            "Creators",
            "Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, Cristian Canton Ferrer"
          ],
          [
            "Date",
            "2020-10-27 2020-10-27"
          ],
          [
            "Extra",
            "arXiv: 2006.07397 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "The DeepFake Detection Challenge (DFDC) Dataset"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2006.07397"
          ]
        ],
        "resource": "storage/i1153.pdf",
        "selectable": false
      },
      {
        "text": "What's Left?",
        "item-id": "i3245",
        "nodes": [
          {
            "text": "Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/lef",
            "item-id": "n3368",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/lef",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/left_neurips_2023</div>",
            "node_type": "note"
          },
          {
            "text": "Hsu et al_2023_What's Left.pdf",
            "item-id": "i3367",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hsu et al_2023_What's Left.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_J5VBVVPX/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/3\">Logic-Enhanced Foundation Model (LEFT)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/3\">Domain-independent LLM interpreter</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/5\">Domain-independent first-order logic executor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/6\">Domain-specific grounding modules</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/7\">Concept learning across domains</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/7\">Accuracy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/8\">Data efficiency</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/9\">Reasoning generalization across tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/9\">Accuracy</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/10\">Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/14\">LEFT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/14\">Function definitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/15\">Broader impact</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Error bars</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Transfer task construction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Compute</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Code</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hsu et al_2023_What's Left.pdf"
              ]
            ],
            "resource": "storage/i3367.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "What's Left?",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like \"left\" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains."
          ],
          [
            "Access Date",
            "2023-12-20 06:23:46"
          ],
          [
            "Archiveid",
            "arXiv:2310.16035"
          ],
          [
            "Creators",
            "Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2310.16035"
          ],
          [
            "Date",
            "2023-10-24 2023-10-24"
          ],
          [
            "Extra",
            "arXiv:2310.16035 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "What's Left?"
          ],
          [
            "Title",
            "What's Left? Concept Grounding with Logic-Enhanced Foundation Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2310.16035"
          ]
        ],
        "resource": "storage/i3367.pdf",
        "selectable": false
      },
      {
        "text": "WildDeepfake",
        "item-id": "i1123",
        "nodes": [
          {
            "text": "Zi et al_2020_WildDeepfake.pdf",
            "item-id": "i1152",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zi et al_2020_WildDeepfake.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8HCGFPL6/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/2\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/2\">2.1 Deepfake Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/3\">2.2 Deepfake Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/3\">3 Datasets for Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/3\">3.1 Existing Deepfake Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/4\">3.2 WildDeepfake Dataset</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/5\">4 Proposed ADDNets for Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/5\">4.1 Problem Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/5\">4.2 Proposed Detection Networks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/7\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/7\">5.1 Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/7\">5.2 Results and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8HCGFPL6/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zi et al_2020_WildDeepfake.pdf"
              ]
            ],
            "resource": "storage/i1152.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "WildDeepfake",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, the abuse of a face swap technique called deepfake has raised enormous public concerns. So far, a large number of deepfake videos (known as \"deepfakes\") have been crafted and uploaded to the internet, calling for effective countermeasures. One promising countermeasure against deepfakes is deepfake detection. Several deepfake datasets have been released to support the training and testing of deepfake detectors, such as DeepfakeDetection [1] and FaceForensics++ [23]. While this has greatly advanced deepfake detection, most of the real videos in these datasets are filmed with a few volunteer actors in limited scenes, and the fake videos are crafted by researchers using a few popular deepfake softwares. Detectors developed on these datasets may become less effective against real-world deepfakes on the internet. To better support detection against real-world deepfakes, in this paper, we introduce a new dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707 deepfake videos collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop and test the effectiveness of deepfake detectors against real-world deepfakes. We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a more challenging dataset, where the detection performance can decrease drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention masks on real/fake faces for improved detection. We empirically verify the effectiveness of ADDNets on both existing datasets and WildDeepfake. The dataset is available at: https://github.com/deepfakeinthewild/deepfake-in-the-wild."
          ],
          [
            "Access Date",
            "2021-10-20"
          ],
          [
            "Creators",
            "Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, Yu-Gang Jiang"
          ],
          [
            "DOI",
            "10.1145/3394171.3413769"
          ],
          [
            "Date",
            "2020-10-12 October 12, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7988-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2382\u20132390"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '20"
          ],
          [
            "Short Title",
            "WildDeepfake"
          ],
          [
            "Title",
            "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3394171.3413769"
          ]
        ],
        "resource": "storage/i1152.pdf",
        "selectable": false
      }
    ],
    "item_title": "Deepfake Dataset",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Deepfake Detection",
    "item-id": "c22,i3753",
    "nodes": [
      {
        "text": "A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer",
        "item-id": "i1875",
        "nodes": [
          {
            "text": "Bayar_Stamm_2016_A Deep Learning Approach to Universal Image Manipulation Detection Using a New.pdf",
            "item-id": "i1984",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bayar_Stamm_2016_A Deep Learning Approach to Universal Image Manipulation Detection Using a New.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bayar_Stamm_2016_A Deep Learning Approach to Universal Image Manipulation Detection Using a New.pdf"
              ]
            ],
            "resource": "storage/i1984.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "When creating a forgery, a forger can modify an image using many different image editing operations. Since a forensic examiner must test for each of these, significant interest has arisen in the development of universal forensic algorithms capable of detecting many different image editing operations and manipulations. In this paper, we propose a universal forensic approach to performing manipulation detection using deep learning. Specifically, we propose a new convolutional network architecture capable of automatically learning manipulation detection features directly from training data. In their current form, convolutional neural networks will learn features that capture an image's content as opposed to manipulation detection features. To overcome this issue, we develop a new form of convolutional layer that is specifically designed to suppress an image's content and adaptively learn manipulation detection features. Through a series of experiments, we demonstrate that our proposed approach can automatically learn how to detect multiple image manipulations without relying on pre-selected features or any preprocessing. The results of these experiments show that our proposed approach can automatically detect several different manipulations with an average accuracy of 99.10%."
          ],
          [
            "Access Date",
            "2022-10-30"
          ],
          [
            "Creators",
            "Belhassen Bayar, Matthew C. Stamm"
          ],
          [
            "DOI",
            "10.1145/2909827.2930786"
          ],
          [
            "Date",
            "2016-06-20 2016-06-20"
          ],
          [
            "ISBN",
            "978-1-4503-4290-2"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "5\u201310"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "IH&amp;MMSec '16"
          ],
          [
            "Title",
            "A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer"
          ],
          [
            "URL",
            "https://doi.org/10.1145/2909827.2930786"
          ]
        ],
        "resource": "storage/i1984.pdf",
        "selectable": false
      },
      {
        "text": "A detailed analysis of image and video forgery detection techniques",
        "item-id": "i2216",
        "nodes": [
          {
            "text": "Tyagi_Yadav_2022_A detailed analysis of image and video forgery detection techniques.pdf",
            "item-id": "i2255",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tyagi_Yadav_2022_A detailed analysis of image and video forgery detection techniques.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G58JDD4K/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/2\">2 Prerequisites for visual imagery manipulation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/3\">2.1 Image forgery (IF)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/3\">2.2 Image tampering (IT)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/4\">2.3 Image generation (IG)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/4\">2.4 Image warping and morphing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/4\">3 Image and video manipulation datasets</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1 Image tampering datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1.1 The Columbia gray dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1.2 The Columbia color dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1.3 The CASIA datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.4 The MICC datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.5 The DRESDEN image dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.6 The IMD dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.7 The CoFoMo dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/8\">3.1.8 The IEEE IFS-TC dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/8\">3.1.9 The wild web dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/8\">3.1.10 Retouching forgery datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.1.11 Other dataset resources</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.2 Video tampering datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.2.1 First-generation datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.2.2 Second-generation datasets</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/10\">4 Image manipulation and detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/10\">4.1 Image manipulation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/10\">4.1.1 Graphics-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/11\">4.1.2 Learning-based methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/11\">4.2 Image forgery detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/11\">4.2.1 Traditional methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">4.2.2 Deep learning-based approaches</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">5 Video manipulation and detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">5.1 Types of video manipulation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">5.1.1  Missing context</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.1.2 Deceptive editing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.1.3 Malicious transformation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.2 Video forgery detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.2.1 Identity swap</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.2.2 Attribute manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/17\">5.2.3 Expression swap</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/17\">6 Evaluation and findings</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/17\">6.1 Research challenges and future scopes</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/18\">7 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/18\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tyagi_Yadav_2022_A detailed analysis of image and video forgery detection techniques.pdf"
              ]
            ],
            "resource": "storage/i2255.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A detailed analysis of image and video forgery detection techniques",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site\u2019s or can be as malicious as to defame or hurt one\u2019s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one\u2019s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery."
          ],
          [
            "Access Date",
            "2023-02-16 23:08:34"
          ],
          [
            "Creators",
            "Shobhit Tyagi, Divakar Yadav"
          ],
          [
            "DOI",
            "10.1007/s00371-021-02347-4"
          ],
          [
            "Date",
            "2022-01-13 2022-01-13"
          ],
          [
            "ISSN",
            "1432-2315"
          ],
          [
            "Journal Abbreviation",
            "Vis Comput"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Publication Title",
            "The Visual Computer"
          ],
          [
            "Title",
            "A detailed analysis of image and video forgery detection techniques"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s00371-021-02347-4"
          ]
        ],
        "resource": "storage/i2255.pdf",
        "selectable": false
      },
      {
        "text": "AVFakeNet",
        "item-id": "i2306",
        "nodes": [
          {
            "text": "Ilyas et al_2023_AVFakeNet.pdf",
            "item-id": "i2328",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ilyas et al_2023_AVFakeNet.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GSRYV8I9/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/3\">Literature Review</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/3\">Video deepfakes detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/3\">Audio deepfakes detection</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/4\">Analysis of the real and fake videos of FakeAVCeleb dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/4\">Proposed methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/5\">Workflow of proposed unified framework</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/5\">Dense Swin Transformer network</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/6\">Input block</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/6\">Feature extraction block</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/7\">Output block</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/7\">Experiments and results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/7\">Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/8\">Experimental setup and training parameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/9\">Detection performance on different spectrograms</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/9\">Performance evaluation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/9\">Performance evaluation on FakeAVCeleb dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/12\">Performance evaluation on Celeb-DF dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/13\">Performance evaluation on ASVSpoof-2019 LA dataset</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/13\">Comparison with state-of-the-art methods on FakeAVCeleb dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/13\">Comparison with existing methods on ASVSpoof-2019 LA dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/13\">Cross-corpora evaluation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/14\">Cross-set evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/14\">Cross-dataset evaluation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/14\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/15\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/15\">Declaration of Competing Interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/15\">Data availability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/15\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GSRYV8I9/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ilyas et al_2023_AVFakeNet.pdf"
              ]
            ],
            "resource": "storage/i2328.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AVFakeNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent advances in the field of machine learning and social media platforms facilitate the creation and rapid dissemination of realistic fake content (i.e., images, videos, audios). Initially, the fake content generation involved the manipulation of either audio or video streams but currently, more realistic deepfakes content is being produced via modifying both audio\u2013visual streams. Researchers in the field of deepfakes detection mostly focus on identifying fake videos exploiting solely visual or audio modality. However, there exist a few approaches for audio\u2013visual deepfakes detection but mostly are not evaluated on a multimodal dataset with deepfakes videos having the manipulations in both streams. The unified approaches evaluated on the audio\u2013visual deepfakes dataset have reported low detection accuracies and failed when the faces are side-posed. Therefore, in this paper, we introduced a novel AVFakeNet framework that focuses on both the audio and visual modalities of a video for deepfakes detection. More specifically, our unified AVFakeNet model is a novel Dense Swin Transformer Net (DST-Net) which consists of an input block, feature extraction block, and output block. The input and output block comprises dense layers while the feature extraction block employs a customized swin transformer module. We have performed extensive experimentation on five different datasets (FakeAVCeleb, Celeb-DF, ASVSpoof-2019 LA, World Leaders dataset, Presidential Deepfakes dataset) comprising audio, visual, and audio\u2013visual deepfakes along with a cross-corpora evaluation to signify the effectiveness and generalizability of our unified framework. Experimental results highlight the effectiveness of the proposed framework in terms of accurately detecting deepfakes videos via scrutinizing both the audio and visual streams."
          ],
          [
            "Access Date",
            "2023-04-21 13:23:13"
          ],
          [
            "Creators",
            "Hafsa Ilyas, Ali Javed, Khalid Mahmood Malik"
          ],
          [
            "DOI",
            "10.1016/j.asoc.2023.110124"
          ],
          [
            "Date",
            "2023-03-01 2023-03-01"
          ],
          [
            "ISSN",
            "1568-4946"
          ],
          [
            "Journal Abbreviation",
            "Applied Soft Computing"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "110124"
          ],
          [
            "Publication Title",
            "Applied Soft Computing"
          ],
          [
            "Short Title",
            "AVFakeNet"
          ],
          [
            "Title",
            "AVFakeNet: A unified end-to-end Dense Swin Transformer deep learning model for audio\u2013visual deepfakes detection"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1568494623001424"
          ],
          [
            "Volume",
            "136"
          ]
        ],
        "resource": "storage/i2328.pdf",
        "selectable": false
      },
      {
        "text": "AVForensics",
        "item-id": "i2565",
        "nodes": [
          {
            "text": "Zhu et al_2023_AVForensics.pdf",
            "item-id": "i2644",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2023_AVForensics.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2023_AVForensics.pdf"
              ]
            ],
            "resource": "storage/i2644.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AVForensics",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Existing cross-dataset deepfake detection approaches exploit mouth-related mismatches between the auditory and visual modalities in fake videos to enhance generalisation to unseen forgeries. However, such methods inevitably suffer performance degradation with limited or unaltered mouth motions, we argue that face forgery detection consistently benefits from using high-level cues across the whole face region. In this paper, we propose a two-phase audio-driven multi-modal transformer-based framework, termed AVForensics, to perform deepfake video content detection from an audio-visual matching view related to full face. In the first pre-training phase, we apply the novel uniform masking strategy to model global facial features and learn temporally dense video representations in a self-supervised cross-modal manner, by capturing the natural correspondence between the visual and auditory modalities regardless of large-scaled labelled data and heavy memory usage. Then we use these learned representations to fine-tune for the down-stream deepfake detection task in the second phase, which encourages the model to offer accurate predictions based on captured global facial movement features. Extensive experiments and visualizations on various public datasets demonstrate the superiority of our self-supervised pre-trained method for achieving generalisable and robust deepfake video detection."
          ],
          [
            "Access Date",
            "2023-06-13"
          ],
          [
            "Creators",
            "Yizhe Zhu, Jialin Gao, Xi Zhou"
          ],
          [
            "DOI",
            "10.1145/3591106.3592218"
          ],
          [
            "Date",
            "2023-00-12 \u516d\u6708 12, 2023"
          ],
          [
            "ISBN",
            "9798400701788"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "162\u2013171"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2023 ACM International Conference on Multimedia Retrieval"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "ICMR '23"
          ],
          [
            "Short Title",
            "AVForensics"
          ],
          [
            "Title",
            "AVForensics: Audio-driven Deepfake Video Detection with Masking Strategy in Self-supervision"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3591106.3592218"
          ]
        ],
        "resource": "storage/i2644.pdf",
        "selectable": false
      },
      {
        "text": "AVoiD-DF",
        "item-id": "i2307",
        "nodes": [
          {
            "text": "Yang et al_2023_AVoiD-DF.pdf",
            "item-id": "i2314",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2023_AVoiD-DF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2023_AVoiD-DF.pdf"
              ]
            ],
            "resource": "storage/i2314.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AVoiD-DF",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, deepfakes have raised severe concerns about the authenticity of online media. Prior works for deepfake detection have made many efforts to capture the intra-modal artifacts. However, deepfake videos in real-world scenarios often consist of a combination of audio and visual. In this paper, we propose an Audio-Visual Joint Learning for Detecting Deepfake (AVoiD-DF), which exploits audio-visual inconsistency for multi-modal forgery detection. Specifically, AVoiD-DF begins by embedding temporal-spatial information in Temporal-Spatial Encoder. A Multi-Modal Joint-Decoder is then designed to fuse multi-modal features and jointly learn inherent relationships. Afterward, a Cross-Modal Classifier is devised to detect manipulation with inter-modal and intra-modal disharmony. Since existing datasets for deepfake detection mainly focus on one modality and only cover a few forgery methods, we build a novel benchmark DefakeAVMiT for multi-modal deepfake detection. DefakeAVMiT contains sufficient visuals with corresponding audios, where any one of the modalities may be maliciously modified by multiple deepfake methods. The experimental results on DefakeAVMiT, FakeAVCeleb, and DFDC demonstrate that the AVoiD-DF outperforms many state-of-the-arts in deepfake detection. Our proposed method also yields superior generalization on various forgery techniques."
          ],
          [
            "Creators",
            "Wenyuan Yang, Xiaoyu Zhou, Zhikai Chen, Bofei Guo, Zhongjie Ba, Zhihua Xia, Xiaochun Cao, Kui Ren"
          ],
          [
            "DOI",
            "10.1109/TIFS.2023.3262148"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Information Forensics and Security"
          ],
          [
            "ISSN",
            "1556-6021"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2015-2029"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Information Forensics and Security"
          ],
          [
            "Short Title",
            "AVoiD-DF"
          ],
          [
            "Title",
            "AVoiD-DF: Audio-Visual Joint Learning for Detecting Deepfake"
          ],
          [
            "Volume",
            "18"
          ]
        ],
        "resource": "storage/i2314.pdf",
        "selectable": false
      },
      {
        "text": "Audio-Visual Person-of-Interest DeepFake Detection",
        "item-id": "i2310",
        "nodes": [
          {
            "text": "Cozzolino et al_2022_Audio-Visual Person-of-Interest DeepFake Detection.pdf",
            "item-id": "i2321",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cozzolino et al_2022_Audio-Visual Person-of-Interest DeepFake Detection.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_7VKRM5CZ/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/2\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/4\">3 . Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/4\">3.1 . Contrastive Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/5\">3.2 . Testing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/6\">4 . Experimental Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/6\">4.1 . Datasets and Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/6\">4.2 . Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/7\">4.3 . Comparative Performance Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/8\">5 . Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/9\">A . Additional ablation studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/10\">B . Additional robustness analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/10\">C . State-of-the-art methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/11\">D . Deepfake video datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VKRM5CZ/11\">E . Limitations</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cozzolino et al_2022_Audio-Visual Person-of-Interest DeepFake Detection.pdf"
              ]
            ],
            "resource": "storage/i2321.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Audio-Visual Person-of-Interest DeepFake Detection",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face manipulation technology is advancing very rapidly, and new methods are being proposed day by day. The aim of this work is to propose a deepfake detector that can cope with the wide variety of manipulation methods and scenarios encountered in the real world. Our key insight is that each person has specific biometric characteristics that a synthetic generator cannot likely reproduce. Accordingly, we extract high-level audio-visual biometric features which characterize the identity of a person, and use them to create a person-of-interest (POI) deepfake detector. We leverage a contrastive learning paradigm to learn the moving-face and audio segment embeddings that are most discriminative for each identity. As a result, when the video and/or audio of a person is manipulated, its representation in the embedding space becomes inconsistent with the real identity, allowing reliable detection. Training is carried out exclusively on real talking-face videos, thus the detector does not depend on any specific manipulation method and yields the highest generalization ability. In addition, our method can detect both single-modality (audio-only, video-only) and multi-modality (audio-video) attacks, and is robust to low-quality or corrupted videos by building only on high-level semantic features. Experiments on a wide variety of datasets confirm that our method ensures a SOTA performance, with an average improvement in terms of AUC of around 3%, 10%, and 4% for high-quality, low quality, and attacked videos, respectively. https://github.com/grip-unina/poi-forensics"
          ],
          [
            "Access Date",
            "2023-04-21 13:37:49"
          ],
          [
            "Archiveid",
            "arXiv:2204.03083"
          ],
          [
            "Creators",
            "Davide Cozzolino, Matthias Nie\u00dfner, Luisa Verdoliva"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.03083"
          ],
          [
            "Date",
            "2022-12-21 2022-12-21"
          ],
          [
            "Extra",
            "arXiv:2204.03083 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Audio-Visual Person-of-Interest DeepFake Detection"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.03083"
          ]
        ],
        "resource": "storage/i2321.pdf",
        "selectable": false
      },
      {
        "text": "Combining EfficientNet and\u00a0Vision Transformers for\u00a0Video Deepfake Detection",
        "item-id": "i1641",
        "nodes": [
          {
            "text": "Coccomini et al_2022_Combining EfficientNet and Vision Transformers for Video Deepfake Detection.pdf",
            "item-id": "i1650",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Coccomini et al_2022_Combining EfficientNet and Vision Transformers for Video Deepfake Detection.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XSEMNE5H/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/2\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/2\">2.1 Deepfake Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/3\">2.2 Deepfake Detection</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/3\">3 Method</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/5\">4.1 Datasets and Face Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/6\">4.2 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/6\">4.3 Inference</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/7\">4.4 Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/9\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XSEMNE5H/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Coccomini et al_2022_Combining EfficientNet and Vision Transformers for Video Deepfake Detection.pdf"
              ]
            ],
            "resource": "storage/i1650.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Combining EfficientNet and\u00a0Vision Transformers for\u00a0Video Deepfake Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfakes are the result of digital manipulation to forge realistic yet fake imagery. With the astonishing advances in deep generative models, fake images or videos are nowadays obtained using variational autoencoders (VAEs) or Generative Adversarial Networks (GANs). These technologies are becoming more accessible and accurate, resulting in fake videos that are very difficult to be detected. Traditionally, Convolutional Neural Networks (CNNs) have been used to perform video deepfake detection, with the best results obtained using methods based on EfficientNet B7. In this study, we focus on video deep fake detection on faces, given that most methods are becoming extremely accurate in the generation of realistic human faces. Specifically, we combine various types of Vision Transformers with a convolutional EfficientNet B0 used as a feature extractor, obtaining comparable results with some very recent methods that use Vision Transformers. Differently from the state-of-the-art approaches, we use neither distillation nor ensemble methods. Furthermore, we present a straightforward inference procedure based on a simple voting scheme for handling multiple faces in the same video shot. The best model achieved an AUC of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the DeepFake Detection Challenge (DFDC). The code for reproducing our results is publicly available here: https://tinyurl.com/cnn-vit-dfd."
          ],
          [
            "Conference Name",
            "Image Analysis and Processing \u2013 ICIAP 2022"
          ],
          [
            "Creators",
            "Davide Alessandro Coccomini, Nicola Messina, Claudio Gennaro, Fabrizio Falchi, Stan Sclaroff, Cosimo Distante, Marco Leo, Giovanni M. Farinella, Federico Tombari"
          ],
          [
            "DOI",
            "10.1007/978-3-031-06433-3_19"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-06433-3"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "219-229"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Image Analysis and Processing \u2013 ICIAP 2022"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Combining EfficientNet and\u00a0Vision Transformers for\u00a0Video Deepfake Detection"
          ]
        ],
        "resource": "storage/i1650.pdf",
        "selectable": false
      },
      {
        "text": "Contrastive learning-based general Deepfake detection with multi-scale RGB frequency clues",
        "item-id": "i3594",
        "nodes": [
          {
            "text": "Dong et al_2023_Contrastive learning-based general Deepfake detection with multi-scale RGB.pdf",
            "item-id": "i3596",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dong et al_2023_Contrastive learning-based general Deepfake detection with multi-scale RGB.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dong et al_2023_Contrastive learning-based general Deepfake detection with multi-scale RGB.pdf"
              ]
            ],
            "resource": "storage/i3596.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Contrastive learning-based general Deepfake detection with multi-scale RGB frequency clues",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake is a type of image and video face manipulation methods which could cause security and society threats. Although some related databases and detection models have been proposed for detecting face forgery media, achieving a generalizable detector for both known and unknown manipulations remains challenging. In this study, a novel deepfake detection model with high generalizability is proposed to tackle this issue. We employ supervised contrastive learning to enhance the generalizability to unknown manipulations and datasets. In addition, we design a cross-modality data augmentation method by combining SRM and RGB features to extract detection clues comprehensively. Furthermore, we propose a multi-scale feature enhancement module to enhance textural and semantic information. Extensive experiments have demonstrated that our method improves model generalization in both intra- and cross- dataset scenarios."
          ],
          [
            "Access Date",
            "2024-01-09 07:48:01"
          ],
          [
            "Creators",
            "Fengkai Dong, Xiaoqiang Zou, Jiahui Wang, Xiyao Liu"
          ],
          [
            "DOI",
            "10.1016/j.jksuci.2023.03.005"
          ],
          [
            "Date",
            "2023-04-01 2023-04-01"
          ],
          [
            "ISSN",
            "1319-1578"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "Journal of King Saud University - Computer and Information Sciences"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "90-99"
          ],
          [
            "Publication Title",
            "Journal of King Saud University - Computer and Information Sciences"
          ],
          [
            "Title",
            "Contrastive learning-based general Deepfake detection with multi-scale RGB frequency clues"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1319157823000654"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i3596.pdf",
        "selectable": false
      },
      {
        "text": "DeepFake Detection by Analyzing Convolutional Traces",
        "item-id": "i1165",
        "nodes": [
          {
            "text": "Guarnera et al_2020_DeepFake Detection by Analyzing Convolutional Traces.pdf",
            "item-id": "i1187",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Guarnera et al_2020_DeepFake Detection by Analyzing Convolutional Traces.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Guarnera et al_2020_DeepFake Detection by Analyzing Convolutional Traces.pdf"
              ]
            ],
            "resource": "storage/i1187.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DeepFake Detection by Analyzing Convolutional Traces",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The Deepfake phenomenon has become very popular nowadays thanks to the possibility to create incredibly realistic images using deep learning tools, based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we focus on the analysis of Deepfakes of human faces with the objective of creating a new detection method able to detect a forensics trace hidden in images: a sort of fingerprint left in the image generation process. The proposed technique, by means of an Expectation Maximization (EM) algorithm, extracts a set of local features specifically addressed to model the underlying convolutional generative process. Ad-hoc validation has been employed through experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as ground-truth for non-fakes. Results demonstrated the effectiveness of the technique in distinguishing the different architectures and the corresponding generation process."
          ],
          [
            "Access Date",
            "2021-11-01 18:51:50"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Creators",
            "Luca Guarnera, Oliver Giudice, Sebastiano Battiato"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "666-667"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Title",
            "DeepFake Detection by Analyzing Convolutional Traces"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPRW_2020/html/w39/Guarnera_DeepFake_Detection_by_Analyzing_Convolutional_Traces_CVPRW_2020_paper.html"
          ]
        ],
        "resource": "storage/i1187.pdf",
        "selectable": false
      },
      {
        "text": "DeepFake detection algorithm based on improved vision transformer",
        "item-id": "i2336",
        "nodes": [
          {
            "text": "Heo et al_2023_DeepFake detection algorithm based on improved vision transformer.pdf",
            "item-id": "i2351",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Heo et al_2023_DeepFake detection algorithm based on improved vision transformer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PI4UQMFU/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/2\">Related works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/2\">DeepFake detection using spatial properties</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/3\">DeepFake detection using temporal properties</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/3\">Proposed DeepFake detection algorithm</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/3\">ViT for DeepFake detection</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/5\">The proposed method</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/5\">Data preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/5\">Basic network architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/6\">Combination of patch embedding and CNN features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/7\">Distillation method and teacher network</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/7\">Experimental results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/7\">Datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/7\">DFDC Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/8\">Celeb-DF (v2) datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/10\">Training and testing detail</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/10\">Training detail</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/10\">Testing detail</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/10\">Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/10\">Performance analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/14\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PI4UQMFU/14\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Heo et al_2023_DeepFake detection algorithm based on improved vision transformer.pdf"
              ]
            ],
            "resource": "storage/i2351.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DeepFake detection algorithm based on improved vision transformer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A DeepFake is a manipulated video made with generative deep learning technologies, such as generative adversarial networks or auto encoders that anyone can utilize. With the increase in DeepFakes, classifiers consisting of convolutional neural networks (CNN) that can distinguish them have been actively created. However, CNNs have a problem with overfitting and cannot consider the relation between local regions as global feature of image, resulting in misclassification. In this paper, we propose an efficient vision transformer model for DeepFake detection to extract both local and global features. We combine vector-concatenated CNN feature and patch-based positioning to interact with all positions to specify the artifact region. For the distillation token, the logit is trained using binary cross entropy through the sigmoid function. By adding this distillation, the proposed model is generalized to improve performance. From experiments, the proposed model outperforms the SOTA model by 0.006 AUC and 0.013 f1 score on the DFDC test dataset. For 2,500 fake videos, the proposed model correctly predicts 2,313 as fake, whereas the SOTA model predicts 2,276 in the best performance. With the ensemble method, the proposed model outperformed the SOTA model by 0.01 AUC. For Celeb-DF (v2) dataset, the proposed model achieves a high performance of 0.993 AUC and 0.978 f1 score, respectively."
          ],
          [
            "Access Date",
            "2023-04-25 14:20:48"
          ],
          [
            "Creators",
            "Young-Jin Heo, Woon-Ha Yeo, Byung-Gyu Kim"
          ],
          [
            "DOI",
            "10.1007/s10489-022-03867-9"
          ],
          [
            "Date",
            "2023-04-01 2023-04-01"
          ],
          [
            "ISSN",
            "1573-7497"
          ],
          [
            "Issue",
            "7"
          ],
          [
            "Journal Abbreviation",
            "Appl Intell"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "7512-7527"
          ],
          [
            "Publication Title",
            "Applied Intelligence"
          ],
          [
            "Title",
            "DeepFake detection algorithm based on improved vision transformer"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s10489-022-03867-9"
          ],
          [
            "Volume",
            "53"
          ]
        ],
        "resource": "storage/i2351.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Detection Scheme Based on Vision Transformer and Distillation",
        "item-id": "i936",
        "nodes": [
          {
            "text": "Comment: 7 pages, 5 figures",
            "item-id": "n942",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 7 pages, 5 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 7 pages, 5 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf",
            "item-id": "i941",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf"
              ]
            ],
            "resource": "storage/i941.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Deepfake Detection Scheme Based on Vision Transformer and Distillation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition."
          ],
          [
            "Access Date",
            "2021-09-15 22:04:42"
          ],
          [
            "Archiveid",
            "arXiv:2104.01353"
          ],
          [
            "Creators",
            "Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim"
          ],
          [
            "Date",
            "2021-04-03 2021-04-03"
          ],
          [
            "Extra",
            "arXiv:2104.01353 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Deepfake Detection Scheme Based on Vision Transformer and Distillation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2104.01353"
          ]
        ],
        "resource": "storage/i941.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Detection using Spatiotemporal Convolutional Networks",
        "item-id": "i1167",
        "nodes": [
          {
            "text": "de Lima et al_2020_Deepfake Detection using Spatiotemporal Convolutional Networks.pdf",
            "item-id": "i1191",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "de Lima et al_2020_Deepfake Detection using Spatiotemporal Convolutional Networks.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_4VU8GXXG/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/1\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/2\">3 . Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/2\">3.1 . Pre-Processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/2\">3.2 . DFT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/2\">3.3 . RCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/2\">3.4 . R3D</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/3\">3.5 . ResNet Mixed 3D-2D Convolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/3\">3.6 . ResNet (2+1)D</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/3\">3.7 . I3D</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/4\">4 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/4\">4.1 . Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/4\">4.2 . Result and analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4VU8GXXG/4\">5 . Conclusions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "de Lima et al_2020_Deepfake Detection using Spatiotemporal Convolutional Networks.pdf"
              ]
            ],
            "resource": "storage/i1191.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Deepfake Detection using Spatiotemporal Convolutional Networks",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Better generative models and larger datasets have led to more realistic fake videos that can fool the human eye but produce temporal and spatial artifacts that deep learning approaches can detect. Most current Deepfake detection methods only use individual video frames and therefore fail to learn from temporal information. We created a benchmark of the performance of spatiotemporal convolutional methods using the Celeb-DF dataset. Our methods outperformed state-of-the-art frame-based detection methods. Code for our paper is publicly available at https://github.com/oidelima/Deepfake-Detection."
          ],
          [
            "Access Date",
            "2021-11-01 18:49:03"
          ],
          [
            "Archiveid",
            "arXiv:2006.14749"
          ],
          [
            "Creators",
            "Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake Karwoski, Annet George"
          ],
          [
            "Date",
            "2020-06-25 2020-06-25"
          ],
          [
            "Extra",
            "arXiv:2006.14749 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Deepfake Detection using Spatiotemporal Convolutional Networks"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2006.14749"
          ]
        ],
        "resource": "storage/i1191.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Video Detection Based on Spatial, Spectral, and Temporal Inconsistencies Using Multimodal Deep Learning",
        "item-id": "i1447",
        "nodes": [
          {
            "text": "Lewis et al_2020_Deepfake Video Detection Based on Spatial, Spectral, and Temporal.pdf",
            "item-id": "i1506",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lewis et al_2020_Deepfake Video Detection Based on Spatial, Spectral, and Temporal.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lewis et al_2020_Deepfake Video Detection Based on Spatial, Spectral, and Temporal.pdf"
              ]
            ],
            "resource": "storage/i1506.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake Video Detection Based on Spatial, Spectral, and Temporal Inconsistencies Using Multimodal Deep Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Authentication of digital media has become an ever-pressing necessity for modern society. Since the introduction of Generative Adversarial Networks (GANs), synthetic media has become increasingly difficult to identify. Synthetic videos that contain altered faces and/or voices of a person are known as deepfakes and threaten trust and privacy in digital media. Deep-fakes can be weaponized for political advantage, slander, and to undermine the reputation of public figures. Despite imperfections of deepfakes, people struggle to distinguish between authentic and manipulated images and videos. Consequently, it is important to have automated systems that accurately and efficiently classify the validity of digital content. Many recent deepfake detection methods use single frames of video and focus on the spatial information in the image to infer the authenticity of the video. Some promising approaches exploit the temporal inconsistencies of manipulated videos; however, research primarily focuses on spatial features. We propose a hybrid deep learning approach that uses spatial, spectral, and temporal content that is coupled in a consistent way to differentiate real and fake videos. We show that the Discrete Cosine transform can improve deepfake detection by capturing spectral features of individual frames. In this work, we build a multimodal network that explores new features to detect deepfake videos, achieving 61.95% accuracy on the Facebook Deepfake Detection Challenge (DFDC) dataset."
          ],
          [
            "Conference Name",
            "2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)"
          ],
          [
            "Creators",
            "John K. Lewis, Imad Eddine Toubal, Helen Chen, Vishal Sandesera, Michael Lomnitz, Zigfried Hampel-Arias, Calyam Prasad, Kannappan Palaniappan"
          ],
          [
            "DOI",
            "10.1109/AIPR50011.2020.9425167"
          ],
          [
            "Date",
            "2020-03-00 2020-03"
          ],
          [
            "Extra",
            "ISSN: 2332-5615"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-9"
          ],
          [
            "Proceedings Title",
            "2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)"
          ],
          [
            "Title",
            "Deepfake Video Detection Based on Spatial, Spectral, and Temporal Inconsistencies Using Multimodal Deep Learning"
          ]
        ],
        "resource": "storage/i1506.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Video Detection Using Convolutional Vision Transformer",
        "item-id": "i1051",
        "nodes": [
          {
            "text": "Comment: 9 pages, 6 figures",
            "item-id": "n1078",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 9 pages, 6 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 9 pages, 6 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Wodajo_Atnafu_2021_Deepfake Video Detection Using Convolutional Vision Transformer.pdf",
            "item-id": "i1077",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wodajo_Atnafu_2021_Deepfake Video Detection Using Convolutional Vision Transformer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wodajo_Atnafu_2021_Deepfake Video Detection Using Convolutional Vision Transformer.pdf"
              ]
            ],
            "resource": "storage/i1077.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Deepfake Video Detection Using Convolutional Vision Transformer",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapid advancement of deep learning models that can generate and synthesis hyper-realistic videos known as Deepfakes and their ease of access to the general public have raised concern from all concerned bodies to their possible malicious intent use. Deep learning techniques can now generate faces, swap faces between two subjects in a video, alter facial expressions, change gender, and alter facial features, to list a few. These powerful video manipulation methods have potential use in many fields. However, they also pose a looming threat to everyone if used for harmful purposes such as identity theft, phishing, and scam. In this work, we propose a Convolutional Vision Transformer for the detection of Deepfakes. The Convolutional Vision Transformer has two components: Convolutional Neural Network (CNN) and Vision Transformer (ViT). The CNN extracts learnable features while the ViT takes in the learned features as input and categorizes them using an attention mechanism. We trained our model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5 percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our contribution is that we have added a CNN module to the ViT architecture and have achieved a competitive result on the DFDC dataset."
          ],
          [
            "Access Date",
            "2021-10-18 15:44:55"
          ],
          [
            "Archiveid",
            "arXiv:2102.11126"
          ],
          [
            "Creators",
            "Deressa Wodajo, Solomon Atnafu"
          ],
          [
            "Date",
            "2021-03-11 2021-03-11"
          ],
          [
            "Extra",
            "arXiv:2102.11126 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Deepfake Video Detection Using Convolutional Vision Transformer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2102.11126"
          ]
        ],
        "resource": "storage/i1077.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Video Detection through Optical Flow Based CNN",
        "item-id": "i1166",
        "nodes": [
          {
            "text": "Amerini et al_2019_Deepfake Video Detection through Optical Flow Based CNN.pdf",
            "item-id": "i1189",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Amerini et al_2019_Deepfake Video Detection through Optical Flow Based CNN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Amerini et al_2019_Deepfake Video Detection through Optical Flow Based CNN.pdf"
              ]
            ],
            "resource": "storage/i1189.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake Video Detection through Optical Flow Based CNN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent advances in visual media technology have led to new tools for processing and, above all, generating multimedia contents. In particular, modern AI-based technologies have provided easy-to-use tools to create extremely realistic manipulated videos. Such synthetic videos, named Deep Fakes, may constitute a serious threat to attack the reputation of public subjects or to address the general opinion on a certain event. According to this, being able to individuate this kind of fake information becomes fundamental. In this work, a new forensic technique able to discern between fake and original video sequences is given; unlike other state-of-the-art methods which resorts at single video frames, we propose the adoption of optical flow fields to exploit possible inter-frame dissimilarities. Such a clue is then used as feature to be learned by CNN classifiers. Preliminary results obtained on FaceForensics++ dataset highlight very promising performances."
          ],
          [
            "Access Date",
            "2021-11-01 18:49:24"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops"
          ],
          [
            "Creators",
            "Irene Amerini, Leonardo Galteri, Roberto Caldelli, Alberto Del Bimbo"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "0-0"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops"
          ],
          [
            "Title",
            "Deepfake Video Detection through Optical Flow Based CNN"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCVW_2019/html/HBU/Amerini_Deepfake_Video_Detection_through_Optical_Flow_Based_CNN_ICCVW_2019_paper.html"
          ]
        ],
        "resource": "storage/i1189.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake generation and detection, a survey",
        "item-id": "i2777",
        "nodes": [
          {
            "text": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item-id": "i2921",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/2\">2 Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3 Deepfake generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.1 Types of\u00a0Deepfake</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.2 Face-based generation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.1 Face swapping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.2 Facial reenactment</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4 Deepfake detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1 Detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1.1 Features based detection methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/10\">4.1.2 Machine learning-based detection methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/11\">4.2 Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5 Discussions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5.1 Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">5.2 Future directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang_2022_Deepfake generation and detection, a survey.pdf"
              ]
            ],
            "resource": "storage/i2921.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake generation and detection, a survey",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake\u00a0refers to realistic, but\u00a0fake images, sounds, and videos generated by articial intelligence methods. Recent advances in deepfake generation make\u00a0deepfake more realistic and easier to make. Deepfake has been a signicant threat to national security, democracy, society, and\u00a0our privacy, which calls for deepfake detection methods to combat potential threats. In the paper, we make a survey on state-ofthe-art deepfake generation methods, detection methods, and existing datasets. Current deepfake generation methods can be\u00a0classified into face swapping and facial reenactment. Deepfake detection methods are mainly based features and machine\u00a0learning methods. There are still some challenges for deepfake detection, such as progress on deepfake generation, lack of high\u00a0quality datasets and benchmark. Future trends on deepfake detection can be efficient, robust and systematical detection methods\u00a0and high quality datasets."
          ],
          [
            "Access Date",
            "2023-06-30 23:52:04"
          ],
          [
            "Creators",
            "Tao Zhang"
          ],
          [
            "DOI",
            "10.1007/s11042-021-11733-y"
          ],
          [
            "Date",
            "2022-02-01 2022-02-01"
          ],
          [
            "ISSN",
            "1573-7721"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Multimed Tools Appl"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "6259-6276"
          ],
          [
            "Publication Title",
            "Multimedia Tools and Applications"
          ],
          [
            "Title",
            "Deepfake generation and detection, a survey"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11042-021-11733-y"
          ],
          [
            "Volume",
            "81"
          ]
        ],
        "resource": "storage/i2921.pdf",
        "selectable": false
      },
      {
        "text": "Deepfakes Detection With Automatic Face Weighting",
        "item-id": "i1168",
        "nodes": [
          {
            "text": "Montserrat et al_2020_Deepfakes Detection With Automatic Face Weighting.pdf",
            "item-id": "i1193",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Montserrat et al_2020_Deepfakes Detection With Automatic Face Weighting.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Montserrat et al_2020_Deepfakes Detection With Automatic Face Weighting.pdf"
              ]
            ],
            "resource": "storage/i1193.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfakes Detection With Automatic Face Weighting",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Altered and manipulated multimedia is increasingly present and widely distributed via social media platforms. Advanced video manipulation tools enable the generation of highly realistic-looking altered multimedia. While many methods have been presented to detect manipulations, most of them fail when evaluated with data outside of the datasets used in research environments. In order to address this problem, the Deepfake Detection Challenge (DFDC) provides a large dataset of videos containing realistic manipulations and an evaluation system that ensures that methods work quickly and accurately, even when faced with challenging data. In this paper, we introduce a method based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that extracts visual and temporal features from faces present in videos to accurately detect manipulations. The method is evaluated with the DFDC dataset, providing competitive results compared to other techniques."
          ],
          [
            "Access Date",
            "2021-11-01 18:46:00"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Creators",
            "Daniel Mas Montserrat, Hanxiang Hao, Sri K. Yarlagadda, Sriram Baireddy, Ruiting Shao, Janos Horvath, Emily Bartusiak, Justin Yang, David Guera, Fengqing Zhu, Edward J. Delp"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "668-669"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Title",
            "Deepfakes Detection With Automatic Face Weighting"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPRW_2020/html/w39/Montserrat_Deepfakes_Detection_With_Automatic_Face_Weighting_CVPRW_2020_paper.html"
          ]
        ],
        "resource": "storage/i1193.pdf",
        "selectable": false
      },
      {
        "text": "Detecting deepfake videos based on spatiotemporal attention and convolutional LSTM",
        "item-id": "i2224",
        "nodes": [
          {
            "text": "Chen et al_2022_Detecting deepfake videos based on spatiotemporal attention and convolutional.pdf",
            "item-id": "i2268",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2022_Detecting deepfake videos based on spatiotemporal attention and convolutional.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_7VV9QPQZ/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/2\">2 Preliminaries</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/2\">2.1 Xception</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/2\">2.2 ConvLSTM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/3\">2.3 Attention mechanism</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/3\">3 Our algorithm</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/3\">3.1 Spatiotemporal attention mechanism</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/6\">3.2 Overall architecture</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/7\">4 Experimental results and analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/7\">4.1 Experimental setups</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/8\">4.2 Evaluation metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/9\">4.3 Effect of the number of frames per video sampled</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/9\">4.4 Ablation experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/10\">4.5 Detection accuracy comparison with the state-of-the-art algorithms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/11\">4.6 Computational complexity comparison with the state-of-the-art algorithms</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/11\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/12\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/12\">Declaration of Competing Interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/12\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7VV9QPQZ/12\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2022_Detecting deepfake videos based on spatiotemporal attention and convolutional.pdf"
              ]
            ],
            "resource": "storage/i2268.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Detecting deepfake videos based on spatiotemporal attention and convolutional LSTM",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Fake face detection is in dilemma with the rapid development of face manipulation technology. One way to improve the effectiveness of detector is to make full use of intra and inter frame information. In this paper, a novel Xception-LSTM algorithm is proposed by using our new spatiotemporal attention mechanism and convolutional long short-term memory (ConvLSTM). In the algorithm, the spatiotemporal attention mechanism, including spatial and temporal attention mechanism, is proposed to capture and enhance spatiotemporal correlations before dimension reduction of Xception. Thereafter, the ConvLSTM is introduced to consider frame structure information while modeling temporal information. The experimental results on three widely used datasets demonstrate that the proposed algorithms perform better than the state-of-the-art algorithms. In addition, the effectiveness of the spatiotemporal attention mechanism and ConvLSTM are illustrated in ablation experiments."
          ],
          [
            "Access Date",
            "2023-01-30 20:36:51"
          ],
          [
            "Creators",
            "Beijing Chen, Tianmu Li, Weiping Ding"
          ],
          [
            "DOI",
            "10.1016/j.ins.2022.04.014"
          ],
          [
            "Date",
            "2022-07-01 2022-07-01"
          ],
          [
            "ISSN",
            "0020-0255"
          ],
          [
            "Journal Abbreviation",
            "Information Sciences"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "58-70"
          ],
          [
            "Publication Title",
            "Information Sciences"
          ],
          [
            "Title",
            "Detecting deepfake videos based on spatiotemporal attention and convolutional LSTM"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0020025522003504"
          ],
          [
            "Volume",
            "601"
          ]
        ],
        "resource": "storage/i2268.pdf",
        "selectable": false
      },
      {
        "text": "Distinguishing computer graphics from natural images using convolution neural networks",
        "item-id": "i1880",
        "nodes": [
          {
            "text": "Rahmouni et al_2017_Distinguishing computer graphics from natural images using convolution neural.pdf",
            "item-id": "i1983",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rahmouni et al_2017_Distinguishing computer graphics from natural images using convolution neural.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rahmouni et al_2017_Distinguishing computer graphics from natural images using convolution neural.pdf"
              ]
            ],
            "resource": "storage/i1983.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Distinguishing computer graphics from natural images using convolution neural networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a deep-learning method for distinguishing computer generated graphics from real photographic images. The proposed method uses a Convolutional Neural Network (CNN) with a custom pooling layer to optimize current best-performing algorithms feature extraction scheme. Local estimates of class probabilities are computed and aggregated to predict the label of the whole picture. We evaluate our work on recent photo-realistic computer graphics and show that it outperforms state of the art methods for both local and full image classification."
          ],
          [
            "Conference Name",
            "2017 IEEE Workshop on Information Forensics and Security (WIFS)"
          ],
          [
            "Creators",
            "Nicolas Rahmouni, Vincent Nozick, Junichi Yamagishi, Isao Echizen"
          ],
          [
            "DOI",
            "10.1109/WIFS.2017.8267647"
          ],
          [
            "Date",
            "2017-12-00 2017-12"
          ],
          [
            "Extra",
            "ISSN: 2157-4774"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-6"
          ],
          [
            "Proceedings Title",
            "2017 IEEE Workshop on Information Forensics and Security (WIFS)"
          ],
          [
            "Title",
            "Distinguishing computer graphics from natural images using convolution neural networks"
          ]
        ],
        "resource": "storage/i1983.pdf",
        "selectable": false
      },
      {
        "text": "Do You Really Mean That?",
        "item-id": "i2289",
        "nodes": [
          {
            "text": "Cai et al_2022_Do You Really Mean That.pdf",
            "item-id": "i2291",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2022_Do You Really Mean That.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2022_Do You Really Mean That.pdf"
              ]
            ],
            "resource": "storage/i2291.pdf"
          },
          {
            "text": "lavdf_supp.pdf",
            "item-id": "i3356",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "lavdf_supp.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Proposed Method Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Proposed Dataset Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/1\">Additional Qualitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EJ8V5AGL/3\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "lavdf_supp.pdf"
              ]
            ],
            "resource": "storage/i3356.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Do You Really Mean That?",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attributes, and adversarial perturbation-based spatio-temporal modifications at the whole video or random locations while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from a sentiment perspective. We introduce a content-driven audio-visual deepfake dataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of learning temporal forgery localization. Specifically, the content-driven audio-visual manipulations are performed strategically to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is guided via contrastive, boundary matching, and frame classification loss functions. Our extensive quantitative and qualitative analysis demonstrates the proposed method's strong performance for temporal forgery localization and deepfake detection tasks."
          ],
          [
            "Conference Name",
            "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
          ],
          [
            "Creators",
            "Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1109/DICTA56598.2022.10034605"
          ],
          [
            "Date",
            "2022-11-00 2022-11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-10"
          ],
          [
            "Place",
            "Sydney, Australia"
          ],
          [
            "Proceedings Title",
            "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
          ],
          [
            "Rights",
            "All rights reserved"
          ],
          [
            "Short Title",
            "Do You Really Mean That?"
          ],
          [
            "Title",
            "Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization"
          ]
        ],
        "selectable": false
      },
      {
        "text": "Emotions Don't Lie",
        "item-id": "i1203",
        "nodes": [
          {
            "text": "Mittal et al_2020_Emotions Don't Lie.pdf",
            "item-id": "i1205",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mittal et al_2020_Emotions Don't Lie.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_4JXGDJ85/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/2\">2.1 Multimedia Forensics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/2\">2.2 Unimodal DeepFake Detection Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/2\">2.3 Multimodal DeepFake Detection Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/3\">2.4 DeepFake Video Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/3\">2.5 Affective Computing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/3\">3 Our Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/3\">3.1 Problem Statement and Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/3\">3.2 F1 and S1: Video/Audio Modality Embeddings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/4\">3.3 F2 and S2: Video/Audio Perceived Emotion Embedding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">3.4 Training Routine</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">3.5 Testing Routine</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">4 Implementation and Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">4.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">4.2 Training Parameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">4.3 Feature Extraction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">5 Results and Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/5\">5.1 Comparison with SOTA Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/6\">5.2 Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/6\">5.3 Interpreting the Correlations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/6\">5.4 Ablation Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/7\">5.5 Failure Cases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/7\">5.6 Results on Videos in the Wild</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/8\">6 Conclusion, Limitations and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/8\">7 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JXGDJ85/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mittal et al_2020_Emotions Don't Lie.pdf"
              ]
            ],
            "resource": "storage/i1205.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Emotions Don't Lie",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a learning-based method for detecting real and fake deepfake multimedia content. To maximize information for learning, we extract and analyze the similarity between the two audio and visual modalities from within the same video. Additionally, we extract and compare affective cues corresponding to perceived emotion from the two modalities within a video to infer whether the input video is \"real\" or \"fake\". We propose a deep learning network, inspired by the Siamese network architecture and the triplet loss. To validate our model, we report the AUC metric on two large-scale deepfake detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach with several SOTA deepfake detection methods and report per-video AUC of 84.4% on the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of our knowledge, ours is the first approach that simultaneously exploits audio and video modalities and also perceived emotions from the two modalities for deepfake detection."
          ],
          [
            "Access Date",
            "2021-11-16"
          ],
          [
            "Creators",
            "Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, Dinesh Manocha"
          ],
          [
            "DOI",
            "10.1145/3394171.3413570"
          ],
          [
            "Date",
            "2020-10-12 October 12, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7988-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2823\u20132832"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '20"
          ],
          [
            "Short Title",
            "Emotions Don't Lie"
          ],
          [
            "Title",
            "Emotions Don't Lie: An Audio-Visual Deepfake Detection Method using Affective Cues"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3394171.3413570"
          ]
        ],
        "resource": "storage/i1205.pdf",
        "selectable": false
      },
      {
        "text": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors",
        "item-id": "i1117",
        "nodes": [
          {
            "text": "Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfak",
            "item-id": "n1141",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfak",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection (ADGD '21) at ACM MM 2021</div>",
            "node_type": "note"
          },
          {
            "text": "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf",
            "item-id": "i1140",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U37X3UA3/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/2\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/2\">2.1 Fake Media Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/3\">2.2 Fake Media Detection Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3 Multimodal Deepfake Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/4\">3.2 Evaluation Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4.1 Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/5\">4.2 Experiment Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/6\">4.3 Unimodal Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">4.4 Ensemble Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">4.5 Multimodal Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U37X3UA3/7\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf"
              ]
            ],
            "resource": "storage/i1140.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Significant advancements made in the generation of deepfakes have caused security and privacy issues. Attackers can easily impersonate a person's identity in an image by replacing his face with the target person's face. Moreover, a new domain of cloning human voices using deep-learning technologies is also emerging. Now, an attacker can generate realistic cloned voices of humans using only a few seconds of audio of the target person. With the emerging threat of potential harm deepfakes can cause, researchers have proposed deepfake detection methods. However, they only focus on detecting a single modality, i.e., either video or audio. On the other hand, to develop a good deepfake detector that can cope with the recent advancements in deepfake generation, we need to have a detector that can detect deepfakes of multiple modalities, i.e., videos and audios. To build such a detector, we need a dataset that contains video and respective audio deepfakes. We were able to find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized fake audios as well. We used this multimodal deepfake dataset and performed detailed baseline experiments using state-of-the-art unimodal, ensemble-based, and multimodal detection methods to evaluate it. We conclude through detailed experimentation that unimodals, addressing only a single modality, video or audio, do not perform well compared to ensemble-based methods. Whereas purely multimodal-based baselines provide the worst performance."
          ],
          [
            "Access Date",
            "2021-10-24 02:58:21"
          ],
          [
            "Creators",
            "Hasam Khalid, Minha Kim, Shahroz Tariq, Simon S. Woo"
          ],
          [
            "DOI",
            "10.1145/3476099.3484315"
          ],
          [
            "Date",
            "2021-10-24 2021-10-24"
          ],
          [
            "Extra",
            "arXiv: 2109.02993"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "7-15"
          ],
          [
            "Publication Title",
            "Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection"
          ],
          [
            "Title",
            "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2109.02993"
          ]
        ],
        "resource": "storage/i1140.pdf",
        "selectable": false
      },
      {
        "text": "Exposing Deep Fakes Using Inconsistent Head Poses",
        "item-id": "i992",
        "nodes": [
          {
            "text": "Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf",
            "item-id": "i1007",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf"
              ]
            ],
            "resource": "storage/i1007.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exposing Deep Fakes Using Inconsistent Head Poses",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes). Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue. Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes."
          ],
          [
            "Conference Name",
            "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Xin Yang, Yuezun Li, Siwei Lyu"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2019.8683164"
          ],
          [
            "Date",
            "2019-05-00 2019-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "8261-8265"
          ],
          [
            "Proceedings Title",
            "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Exposing Deep Fakes Using Inconsistent Head Poses"
          ]
        ],
        "resource": "storage/i1007.pdf",
        "selectable": false
      },
      {
        "text": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
        "item-id": "i1211",
        "nodes": [
          {
            "text": "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf",
            "item-id": "i1210",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf"
              ]
            ],
            "resource": "storage/i1210.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as DeepFake videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classi\ufb01er, our method does not need DeepFake generated images as negative training examples since we target the artifacts in af\ufb01ne face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice."
          ],
          [
            "Conference Name",
            "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
          ],
          [
            "Creators",
            "Yuezun Li, Siwei Lyu"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Zotero"
          ],
          [
            "Pages",
            "7"
          ],
          [
            "Proceedings Title",
            "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
          ],
          [
            "Title",
            "Exposing DeepFake Videos By Detecting Face Warping Artifacts"
          ]
        ],
        "resource": "storage/i1210.pdf",
        "selectable": false
      },
      {
        "text": "Face X-Ray for More General Face Forgery Detection",
        "item-id": "i1879",
        "nodes": [
          {
            "text": "Li et al_2020_Face X-Ray for More General Face Forgery Detection.pdf",
            "item-id": "i1981",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2020_Face X-Ray for More General Face Forgery Detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2020_Face X-Ray for More General Face Forgery Detection.pdf"
              ]
            ],
            "resource": "storage/i1981.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Face X-Ray for More General Face Forgery Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper we propose a novel image representation called face X-ray for detecting forgery in face images. The face X-ray of an input face image is a greyscale image that reveals whether the input image can be decomposed into the blending of two images from different sources. It does so by showing the blending boundary for a forged image and the absence of blending for a real image. We observe that most existing face manipulation methods share a common step: blending the altered face into an existing background image. For this reason, face X-ray provides an effective way for detecting forgery generated by most existing face manipulation algorithms. Face X-ray is general in the sense that it only assumes the existence of a blending step and does not rely on any knowledge of the artifacts associated with a specific face manipulation technique. Indeed, the algorithm for computing face X-ray can be trained without fake images generated by any of the state-of-the-art face manipulation methods. Extensive experiments show that face X-ray remains effective when applied to forgery generated by unseen face manipulation techniques, while most existing face forgery detection or deepfake detection algorithms experience a significant performance drop."
          ],
          [
            "Access Date",
            "2022-10-31 02:24:28"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, Baining Guo"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5001-5010"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Face X-Ray for More General Face Forgery Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Face_X-Ray_for_More_General_Face_Forgery_Detection_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i1981.pdf",
        "selectable": false
      },
      {
        "text": "Generalized Zero and Few-Shot Transfer for Facial Forgery Detection",
        "item-id": "i1845",
        "nodes": [
          {
            "text": "Aneja_Nie\u00dfner_2020_Generalized Zero and Few-Shot Transfer for Facial Forgery Detection.pdf",
            "item-id": "i1932",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Aneja_Nie\u00dfner_2020_Generalized Zero and Few-Shot Transfer for Facial Forgery Detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Aneja_Nie\u00dfner_2020_Generalized Zero and Few-Shot Transfer for Facial Forgery Detection.pdf"
              ]
            ],
            "resource": "storage/i1932.pdf"
          },
          {
            "text": "Comment: Project page: https://shivangi-aneja.github.io/ddt/",
            "item-id": "n1933",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page: https://shivangi-aneja.github.io/ddt/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page: https://shivangi-aneja.github.io/ddt/</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Generalized Zero and Few-Shot Transfer for Facial Forgery Detection",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose Deep Distribution Transfer(DDT), a new transfer learning approach to address the problem of zero and few-shot transfer in the context of facial forgery detection. We examine how well a model (pre-)trained with one forgery creation method generalizes towards a previously unseen manipulation technique or different dataset. To facilitate this transfer, we introduce a new mixture model-based loss formulation that learns a multi-modal distribution, with modes corresponding to class categories of the underlying data of the source forgery method. Our core idea is to first pre-train an encoder neural network, which maps each mode of this distribution to the respective class labels, i.e., real or fake images in the source domain by minimizing wasserstein distance between them. In order to transfer this model to a new domain, we associate a few target samples with one of the previously trained modes. In addition, we propose a spatial mixup augmentation strategy that further helps generalization across domains. We find this learning strategy to be surprisingly effective at domain transfer compared to a traditional classification or even state-of-the-art domain adaptation/few-shot learning methods. For instance, compared to the best baseline, our method improves the classification accuracy by 4.88% for zero-shot and by 8.38% for the few-shot case transferred from the FaceForensics++ to Dessa dataset."
          ],
          [
            "Access Date",
            "2022-11-03 03:43:23"
          ],
          [
            "Archiveid",
            "arXiv:2006.11863"
          ],
          [
            "Creators",
            "Shivangi Aneja, Matthias Nie\u00dfner"
          ],
          [
            "DOI",
            "10.48550/arXiv.2006.11863"
          ],
          [
            "Date",
            "2020-06-21 2020-06-21"
          ],
          [
            "Extra",
            "arXiv:2006.11863 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Generalized Zero and Few-Shot Transfer for Facial Forgery Detection"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2006.11863"
          ]
        ],
        "resource": "storage/i1932.pdf",
        "selectable": false
      },
      {
        "text": "Generalizing Face Forgery Detection With High-Frequency Features",
        "item-id": "i3168",
        "nodes": [
          {
            "text": "Luo et al_2021_Generalizing Face Forgery Detection With High-Frequency Features.pdf",
            "item-id": "i3176",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Luo et al_2021_Generalizing Face Forgery Detection With High-Frequency Features.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Luo et al_2021_Generalizing Face Forgery Detection With High-Frequency Features.pdf"
              ]
            ],
            "resource": "storage/i3176.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generalizing Face Forgery Detection With High-Frequency Features",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Current face forgery detection methods achieve high accuracy under the within-database scenario where training and testing forgeries are synthesized by the same algorithm. However, few of them gain satisfying performance under the cross-database scenario where training and testing forgeries are synthesized by different algorithms. In this paper, we find that current CNN-based detectors tend to overfit to method-specific color textures and thus fail to generalize. Observing that image noises remove color textures and expose discrepancies between authentic and tampered regions, we propose to utilize the high-frequency noises for face forgery detection. We carefully devise three functional modules to take full advantage of the high-frequency features. The first is the multi-scale high-frequency feature extraction module that extracts high-frequency noises at multiple scales and composes a novel modality. The second is the residual-guided spatial attention module that guides the low-level RGB feature extractor to concentrate more on forgery traces from a new perspective. The last is the cross-modality attention module that leverages the correlation between the two complementary modalities to promote feature learning for each other. Comprehensive evaluations on several benchmark databases corroborate the superior generalization performance of our proposed method."
          ],
          [
            "Access Date",
            "2023-10-28 02:00:41"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yuchen Luo, Yong Zhang, Junchi Yan, Wei Liu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "16317-16326"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Generalizing Face Forgery Detection With High-Frequency Features"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Generalizing_Face_Forgery_Detection_With_High-Frequency_Features_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i3176.pdf",
        "selectable": false
      },
      {
        "text": "Glitch in the matrix",
        "item-id": "i2991",
        "nodes": [
          {
            "text": "Cai et al_2023_Glitch in the matrix.pdf",
            "item-id": "i3000",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2023_Glitch in the matrix.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2023_Glitch in the matrix.pdf"
              ]
            ],
            "resource": "storage/i3000.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Glitch in the matrix",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most deepfake detection methods focus on detecting spatial and/or spatio-temporal changes in facial attributes and are centered around the binary classification task of detecting whether a video is real or fake. This is because available benchmark datasets contain mostly visual-only modifications present in the entirety of the video. However, a sophisticated deepfake may include small segments of audio or audio\u2013visual manipulations that can completely change the meaning of the video content. To addresses this gap, we propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual and audio\u2013visual manipulations. The proposed baseline method, Boundary Aware Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture which effectively captures multimodal manipulations. We further improve (i.e. BA-TFD+) the baseline method by replacing the backbone with a Multiscale Vision Transformer and guide the training process with contrastive, frame classification, boundary matching and multimodal boundary matching loss functions. The quantitative analysis demonstrates the superiority of BA-TFD+ on temporal forgery localization and deepfake detection tasks using several benchmark datasets including our newly proposed dataset. The dataset, models and code are available at https://github.com/ControlNet/LAV-DF."
          ],
          [
            "Access Date",
            "2023-09-04 20:24:08"
          ],
          [
            "Creators",
            "Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1016/j.cviu.2023.103818"
          ],
          [
            "Date",
            "2023-11-01 2023-11-01"
          ],
          [
            "ISSN",
            "1077-3142"
          ],
          [
            "Journal Abbreviation",
            "Computer Vision and Image Understanding"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "103818"
          ],
          [
            "Publication Title",
            "Computer Vision and Image Understanding"
          ],
          [
            "Rights",
            "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)"
          ],
          [
            "Short Title",
            "Glitch in the matrix"
          ],
          [
            "Title",
            "Glitch in the matrix: A large scale benchmark for content driven audio\u2013visual forgery detection and localization"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1077314223001984"
          ],
          [
            "Volume",
            "236"
          ]
        ],
        "resource": "storage/i3000.pdf",
        "selectable": false
      },
      {
        "text": "Hierarchical supervisions with two-stream network for Deepfake detection",
        "item-id": "i3750",
        "nodes": [
          {
            "text": "Liang et al_2023_Hierarchical supervisions with two-stream network for Deepfake detection.pdf",
            "item-id": "i3758",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liang et al_2023_Hierarchical supervisions with two-stream network for Deepfake detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liang et al_2023_Hierarchical supervisions with two-stream network for Deepfake detection.pdf"
              ]
            ],
            "resource": "storage/i3758.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Hierarchical supervisions with two-stream network for Deepfake detection",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, the quality of face generation and manipulation has reached impressive levels, making it difficult even for humans to distinguish real and fake faces. At the same time, methods to distinguish fake faces from reals came out, such as Deepfake detection. However, the task of Deepfake detection remains challenging, especially the low-quality fake images circulating on the Internet and the diversity of face generation methods. In this work, we propose a new Deepfake detection network that could effectively distinguish both high-quality and low-quality faces generated by various generation methods. First, we design a two-stream framework that incorporates a regular spatial stream and a frequency stream to handle the low-quality problem since we find that the frequency domain artifacts of low-quality images will be preserved. Second, we introduce hierarchical supervisions in a coarse-to-fine manner, which consists of a coarse binary classification branch to classify reals and fakes and a five-category classification branch to classify reals and four different types of fakes. Extensive experiments have proved the effectiveness of our framework on several widely used datasets."
          ],
          [
            "Access Date",
            "2024-01-19 11:37:12"
          ],
          [
            "Creators",
            "Yufei Liang, Mengmeng Wang, Yining Jin, Shuwen Pan, Yong Liu"
          ],
          [
            "DOI",
            "10.1016/j.patrec.2023.05.029"
          ],
          [
            "Date",
            "2023-08-01 2023-08-01"
          ],
          [
            "ISSN",
            "0167-8655"
          ],
          [
            "Journal Abbreviation",
            "Pattern Recognition Letters"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "121-127"
          ],
          [
            "Publication Title",
            "Pattern Recognition Letters"
          ],
          [
            "Title",
            "Hierarchical supervisions with two-stream network for Deepfake detection"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0167865523001678"
          ],
          [
            "Volume",
            "172"
          ]
        ],
        "resource": "storage/i3758.pdf",
        "selectable": false
      },
      {
        "text": "Implicit Identity Leakage",
        "item-id": "i3163",
        "nodes": [
          {
            "text": "Dong et al_2023_Implicit Identity Leakage.pdf",
            "item-id": "i3182",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dong et al_2023_Implicit Identity Leakage.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dong et al_2023_Implicit Identity Leakage.pdf"
              ]
            ],
            "resource": "storage/i3182.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Implicit Identity Leakage",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we analyse the generalization ability of binary classifiers for the task of deepfake detection. We find that the stumbling block to their generalization is caused by the unexpected learned identity representation on images. Termed as the Implicit Identity Leakage, this phenomenon has been qualitatively and quantitatively verified among various DNNs. Furthermore, based on such understanding, we propose a simple yet effective method named the ID-unaware Deepfake Detection Model to reduce the influence of this phenomenon. Extensive experimental results demonstrate that our method outperforms the state-of-the-art in both in-dataset and cross-dataset evaluation. The code is available at https://github.com/megvii-research/CADDM."
          ],
          [
            "Access Date",
            "2023-10-28 01:48:04"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, Haoqiang Fan, Zheng Ge"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3994-4004"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Implicit Identity Leakage"
          ],
          [
            "Title",
            "Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Implicit_Identity_Leakage_The_Stumbling_Block_to_Improving_Deepfake_Detection_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i3182.pdf",
        "selectable": false
      },
      {
        "text": "Lip Sync Matters",
        "item-id": "i2309",
        "nodes": [
          {
            "text": "Shahzad et al_2022_Lip Sync Matters.pdf",
            "item-id": "i2319",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shahzad et al_2022_Lip Sync Matters.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shahzad et al_2022_Lip Sync Matters.pdf"
              ]
            ],
            "resource": "storage/i2319.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Lip Sync Matters",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake technology has advanced a lot, but it is a double-sided sword for the community. One can use it for beneficial purposes, such as restoring vintage content in old movies, or for nefarious purposes, such as creating fake footage to manipulate the public and distribute non-consensual pornography. A lot of work has been done to combat its improper use by detecting fake footage with good performance thanks to the availability of numerous public datasets and unimodal deep learning-based models. However, these methods are insufficient to detect multimodal manipulations, such as both visual and acoustic. This work proposes a novel lip-reading-based multi-modal Deepfake detection method called \u201cLip Sync Matters.\u201d It targets high-level semantic features to exploit the mismatch between the lip sequence extracted from the video and the synthetic lip sequence generated from the audio by the Wav2lip model to detect forged videos. Experimental results show that the proposed method outperforms several existing unimodal, ensemble, and multimodal methods on the publicly available multimodal FakeAVCeleb dataset."
          ],
          [
            "Conference Name",
            "2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
          ],
          [
            "Creators",
            "Sahibzada Adil Shahzad, Ammarah Hashmi, Sarwar Khan, Yan-Tsung Peng, Yu Tsao, Hsin-Min Wang"
          ],
          [
            "DOI",
            "10.23919/APSIPAASC55919.2022.9980296"
          ],
          [
            "Date",
            "2022-11-00 2022-11"
          ],
          [
            "Extra",
            "ISSN: 2640-0103"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1885-1892"
          ],
          [
            "Proceedings Title",
            "2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
          ],
          [
            "Short Title",
            "Lip Sync Matters"
          ],
          [
            "Title",
            "Lip Sync Matters: A Novel Multimodal Forgery Detector"
          ]
        ],
        "resource": "storage/i2319.pdf",
        "selectable": false
      },
      {
        "text": "M2TR",
        "item-id": "i1790",
        "nodes": [
          {
            "text": "Wang et al_2022_M2TR.pdf",
            "item-id": "i1803",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_M2TR.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_IEHYFW95/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IEHYFW95/3\">3 Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/3\">3.1 Multi-scale Transformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/3\">3.2 Frequency Filter</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/3\">3.3 Cross Modality Fusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/4\">3.4 Loss functions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IEHYFW95/4\">4 SR-DF DATASET</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/4\">4.1 Dataset Construction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/5\">4.2 Comparisons to current Deepfake Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IEHYFW95/5\">5 EXPERIMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/5\">5.1 Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/6\">5.2 Evaluation on FaceForensics++</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/6\">5.3 Evaluation on Celeb-DF and SR-DF</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/7\">5.4 Evaluation on ForgeryNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/7\">5.5 From Frames to Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/7\">5.6 Generalization Ability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/7\">5.7 Ablation Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IEHYFW95/8\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_M2TR.pdf"
              ]
            ],
            "resource": "storage/i1803.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "M2TR",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The widespread dissemination of Deepfakes demands effective approaches that can detect perceptually convincing forged images. In this paper, we aim to capture the subtle manipulation artifacts at different scales using transformer models. In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which operates on patches of different sizes to detect local inconsistencies in images at different spatial levels. M2TR further learns to detect forgery artifacts in the frequency domain to complement RGB information through a carefully designed cross modality fusion block. In addition, to stimulate Deepfake detection research, we introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. We conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods by clear margins."
          ],
          [
            "Access Date",
            "2022-09-29"
          ],
          [
            "Conference Name",
            "Proceedings of the 2022 International Conference on Multimedia Retrieval"
          ],
          [
            "Creators",
            "Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Yu-Gang Jiang, Ser-Nam Li"
          ],
          [
            "DOI",
            "10.1145/3512527.3531415"
          ],
          [
            "Date",
            "2022-06-27 2022-06-27"
          ],
          [
            "ISBN",
            "978-1-4503-9238-9"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "615\u2013623"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2022 International Conference on Multimedia Retrieval"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "ICMR '22"
          ],
          [
            "Short Title",
            "M2TR"
          ],
          [
            "Title",
            "M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3512527.3531415"
          ]
        ],
        "resource": "storage/i1803.pdf",
        "selectable": false
      },
      {
        "text": "MesoNet",
        "item-id": "i1212",
        "nodes": [
          {
            "text": "Afchar et al_2018_MesoNet.pdf",
            "item-id": "i1215",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Afchar et al_2018_MesoNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Afchar et al_2018_MesoNet.pdf"
              ]
            ],
            "resource": "storage/i1215.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MesoNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a method to automatically and efficiently detect face tampering in videos, and particularly focuses on two recent techniques used to generate hyper-realistic forged videos: Deepfake and Face2Face. Traditional image forensics techniques are usually not well suited to videos due to the compression that strongly degrades the data. Thus, this paper follows a deep learning approach and presents two networks, both with a low number of layers to focus on the mesoscopic properties of images. We evaluate those fast networks on both an existing dataset and a dataset we have constituted from online videos. The tests demonstrate a very successful detection rate with more than 98% for Deepfake and 95% for Face2Face."
          ],
          [
            "Conference Name",
            "2018 IEEE International Workshop on Information Forensics and Security (WIFS)"
          ],
          [
            "Creators",
            "Darius Afchar, Vincent Nozick, Junichi Yamagishi, Isao Echizen"
          ],
          [
            "DOI",
            "10.1109/WIFS.2018.8630761"
          ],
          [
            "Date",
            "2018-12-00 2018-12"
          ],
          [
            "Extra",
            "ISSN: 2157-4774"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-7"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Workshop on Information Forensics and Security (WIFS)"
          ],
          [
            "Short Title",
            "MesoNet"
          ],
          [
            "Title",
            "MesoNet: a Compact Facial Video Forgery Detection Network"
          ]
        ],
        "resource": "storage/i1215.pdf",
        "selectable": false
      },
      {
        "text": "Multi-Attentional Deepfake Detection",
        "item-id": "i3162",
        "nodes": [
          {
            "text": "Zhao et al_2021_Multi-Attentional Deepfake Detection.pdf",
            "item-id": "i3179",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao et al_2021_Multi-Attentional Deepfake Detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao et al_2021_Multi-Attentional Deepfake Detection.pdf"
              ]
            ],
            "resource": "storage/i3179.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multi-Attentional Deepfake Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face forgery by deepfake is widely spread over the internet and has raised severe societal concerns. Recently, how to detect such forgery contents has become a hot research topic and many deepfake detection methods have been proposed. Most of them model deepfake detection as a vanilla binary classification problem, i.e, first use a backbone network to extract a global feature and then feed it into a binary classifier (real/fake). But since the difference between the real and fake images in this task is often subtle and local, we argue this vanilla solution is not optimal. In this paper, we instead formulate deepfake detection as a fine-grained classification problem and propose a new multi-attentional deepfake detection network. Specifically, it consists of three key components: 1) multiple spatial attention heads to make the network attend to different local parts; 2) textural feature enhancement block to zoom in the subtle artifacts in shallow features; 3) aggregate the low-level textural feature and high-level semantic features guided by the attention maps. Moreover, to address the learning difficulty of this network, we further introduce a new regional independence loss and an attention guided data augmentation strategy. Through extensive experiments on different datasets, we demonstrate the superiority of our method over the vanilla binary classifier counterparts, and achieve state-of-the-art performance."
          ],
          [
            "Access Date",
            "2023-10-28 01:53:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang, Nenghai Yu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2185-2194"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Multi-Attentional Deepfake Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Multi-Attentional_Deepfake_Detection_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i3179.pdf",
        "selectable": false
      },
      {
        "text": "Multimodaltrace",
        "item-id": "i2564",
        "nodes": [
          {
            "text": "Raza_Malik_2023_Multimodaltrace.pdf",
            "item-id": "i2643",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Raza_Malik_2023_Multimodaltrace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Raza_Malik_2023_Multimodaltrace.pdf"
              ]
            ],
            "resource": "storage/i2643.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multimodaltrace",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "By employing generative deep learning techniques, Deepfakes are created with the intent to create mistrust in society, manipulate public opinion and political decisions, and for other malicious purposes such as blackmail, scamming, and even cyberstalking. As realistic deepfake may involve manipulation of either audio or video or both, thus it is important to explore the possibility of detecting deepfakes through the inadequacy of generative algorithms to synchronize audio and visual modalities. Prevailing performant methods, either detect audio or video cues for deepfakes detection while few ensemble the results after predictions on both modalities without inspecting relationship between audio and video cues. Deepfake detection using joint audiovisual representation learning is not explored much. Therefore, this paper proposes a unified multimodal framework, Multimodaltrace, which extracts learned channels from audio and visual modalities, mixes them independently in IntrAmodality Mixer Layer (IAML), processes them jointly in IntErModality Mixer Layers (IEML) from where it is fed to multilabel classification head. Empirical results show the effectiveness of the proposed framework giving state-of-the-art accuracy of 92.9% on the FakeAVCeleb dataset. The cross-dataset evaluation of the proposed framework on World Leaders and Presidential Deepfake Detection Datasets gives an accuracy of 83.61% and 70% respectively. The study also provides insights into how the model focuses on different parts of audio and visual features through integrated gradient analysis."
          ],
          [
            "Access Date",
            "2023-06-13 18:24:20"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Muhammad Anas Raza, Khalid Mahmood Malik"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "993-1000"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Multimodaltrace"
          ],
          [
            "Title",
            "Multimodaltrace: Deepfake Detection Using Audiovisual Representation Learning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Raza_Multimodaltrace_Deepfake_Detection_Using_Audiovisual_Representation_Learning_CVPRW_2023_paper.html"
          ]
        ],
        "resource": "storage/i2643.pdf",
        "selectable": false
      },
      {
        "text": "Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization",
        "item-id": "i1066",
        "nodes": [
          {
            "text": "Chugh et al_2020_Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and.pdf",
            "item-id": "i1069",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chugh et al_2020_Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_QET95886/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QET95886/2\">2 Literature Review</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/2\">2.1 Image-based</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/2\">2.2 Video-based</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/2\">2.3 Audio-visual features based</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/2\">2.4 Bimodal Approaches</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/3\">2.5 Analysis of Related Work</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QET95886/3\">3 MDS-based fake video detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/3\">3.1 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/3\">3.2 Visual Stream</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/4\">3.3 Audio Stream</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/4\">3.4 Loss functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/4\">3.5 Test Inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QET95886/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/5\">4.1 Dataset Description</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/6\">4.2 Training Parameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/6\">4.3 Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/7\">4.4 Evaluation on DFDC Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/7\">4.5 Evaluation on DFTIMIT Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/7\">4.6 Temporal Forgery Localization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/8\">5 Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/8\">6 Acknowledgement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QET95886/8\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chugh et al_2020_Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and.pdf"
              ]
            ],
            "resource": "storage/i1069.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose detection of deepfake videos based on the dissimilarity between the audio and visual modalities, termed as the Modality Dissonance Score (MDS). We hypothesize that manipulation of either modality will lead to dis-harmony between the two modalities, e.g., loss of lip-sync, unnatural facial and lip movements, etc. MDS is computed as the mean aggregate of dissimilarity scores between audio and visual segments in a video. Discriminative features are learnt for the audio and visual channels in a chunk-wise manner, employing the cross-entropy loss for individual modalities, and a contrastive loss that models inter-modality similarity. Extensive experiments on the DFDC and DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art by up to 7%. We also demonstrate temporal forgery localization, and show how our technique identifies the manipulated video segments."
          ],
          [
            "Access Date",
            "2021-10-18"
          ],
          [
            "Creators",
            "Komal Chugh, Parul Gupta, Abhinav Dhall, Ramanathan Subramanian"
          ],
          [
            "DOI",
            "10.1145/3394171.3413700"
          ],
          [
            "Date",
            "2020-10-12 October 12, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7988-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "439\u2013447"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '20"
          ],
          [
            "Title",
            "Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3394171.3413700"
          ]
        ],
        "resource": "storage/i1069.pdf",
        "selectable": false
      },
      {
        "text": "Protecting World Leaders Against Deep Fakes.",
        "item-id": "i1061",
        "nodes": [
          {
            "text": "Agarwal et al_2019_Protecting World Leaders Against Deep Fakes.pdf",
            "item-id": "i1090",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Agarwal et al_2019_Protecting World Leaders Against Deep Fakes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Agarwal et al_2019_Protecting World Leaders Against Deep Fakes.pdf"
              ]
            ],
            "resource": "storage/i1090.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Protecting World Leaders Against Deep Fakes.",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The creation of sophisticated fake videos has been largely relegated to Hollywood studios or state actors. Recent advances in deep learning, however, have made it significantly easier to create sophisticated and compelling fake videos. With relatively modest amounts of data and computing power, the average person can, for example, create a video of a world leader confessing to illegal activity leading to a constitutional crisis, a military leader saying something racially insensitive leading to civil unrest in an area of military activity, or a corporate titan claiming that their profits are weak leading to global stock manipulation. These so called deep fakes pose a significant threat to our democracy, national security, and society. To contend with this growing threat, we describe a forensic technique that models facial expressions and movements that typify an individual's speaking pattern. Although not visually apparent, these correlations are often violated by the nature of how deep-fake videos are created and can, therefore, be used for authentication."
          ],
          [
            "Access Date",
            "2021-10-18 14:31:30"
          ],
          [
            "Conference Name",
            "CVPR Workshops"
          ],
          [
            "Creators",
            "Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, Hao Li"
          ],
          [
            "Date",
            "2019-01-01 2019/01/01"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "CVPR Workshops"
          ],
          [
            "Title",
            "Protecting World Leaders Against Deep Fakes."
          ],
          [
            "URL",
            "https://openreview.net/forum?id=roZ-eTMgOpH"
          ]
        ],
        "resource": "storage/i1090.pdf",
        "selectable": false
      },
      {
        "text": "Recasting Residual-based Local Descriptors as Convolutional Neural Networks",
        "item-id": "i1874",
        "nodes": [
          {
            "text": "Cozzolino et al_2017_Recasting Residual-based Local Descriptors as Convolutional Neural Networks.pdf",
            "item-id": "i1985",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cozzolino et al_2017_Recasting Residual-based Local Descriptors as Convolutional Neural Networks.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_6ATPREYZ/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/2\">2 Residual-based local descriptors</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/2\">3 Recasting local features as CNN</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/2\">3.1 From local features to Bag-of-Words</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/3\">3.2 From Bag-of-Words to CNN</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/4\">4 Experimental analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/5\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/6\">6 Acknowledgement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6ATPREYZ/6\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cozzolino et al_2017_Recasting Residual-based Local Descriptors as Convolutional Neural Networks.pdf"
              ]
            ],
            "resource": "storage/i1985.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Recasting Residual-based Local Descriptors as Convolutional Neural Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Local descriptors based on the image noise residual have proven extremely effective for a number of forensic applications, like forgery detection and localization. Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shifting on deep learning. In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). Then, by relaxing the constraints, and fine-tuning the net on a relatively small training set, we obtain a significant performance improvement with respect to the conventional detector."
          ],
          [
            "Access Date",
            "2022-10-30"
          ],
          [
            "Conference Name",
            "Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security"
          ],
          [
            "Creators",
            "Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva"
          ],
          [
            "DOI",
            "10.1145/3082031.3083247"
          ],
          [
            "Date",
            "2017-06-20 2017-06-20"
          ],
          [
            "ISBN",
            "978-1-4503-5061-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "159\u2013164"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "IH&amp;MMSec '17"
          ],
          [
            "Short Title",
            "Recasting Residual-based Local Descriptors as Convolutional Neural Networks"
          ],
          [
            "Title",
            "Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3082031.3083247"
          ]
        ],
        "resource": "storage/i1985.pdf",
        "selectable": false
      },
      {
        "text": "Self-Supervised Learning of Adversarial Example",
        "item-id": "i3166",
        "nodes": [
          {
            "text": "Chen et al_2022_Self-Supervised Learning of Adversarial Example.pdf",
            "item-id": "i3181",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2022_Self-Supervised Learning of Adversarial Example.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2022_Self-Supervised Learning of Adversarial Example.pdf"
              ]
            ],
            "resource": "storage/i3181.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Self-Supervised Learning of Adversarial Example",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent studies in deepfake detection have yielded promising results when the training and testing face forgeries are from the same dataset. However, the problem remains challenging when one tries to generalize the detector to forgeries created by unseen methods in the training dataset. This work addresses the generalizable deepfake detection from a simple principle: a generalizable representation should be sensitive to diverse types of forgeries. Following this principle, we propose to enrich the \"diversity\" of forgeries by synthesizing augmented forgeries with a pool of forgery configurations and strengthen the \"sensitivity\" to the forgeries by enforcing the model to predict the forgery configurations. To effectively explore the large forgery augmentation space, we further propose to use the adversarial training strategy to dynamically synthesize the most challenging forgeries to the current model. Through extensive experiments, we show that the proposed strategies are surprisingly effective (see Figure 1), and they could achieve superior performance than the current state-of the-art methods. Code is available at https://github.com/liangchen527/SLADD."
          ],
          [
            "Access Date",
            "2023-10-28 01:49:32"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, Jue Wang"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "18710-18719"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Self-Supervised Learning of Adversarial Example"
          ],
          [
            "Title",
            "Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Self-Supervised_Learning_of_Adversarial_Example_Towards_Good_Generalizations_for_Deepfake_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i3181.pdf",
        "selectable": false
      },
      {
        "text": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection",
        "item-id": "i2784",
        "nodes": [
          {
            "text": "Feng et al_2023_Self-Supervised Video Forensics by Audio-Visual Anomaly Detection.pdf",
            "item-id": "i2909",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Feng et al_2023_Self-Supervised Video Forensics by Audio-Visual Anomaly Detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Feng et al_2023_Self-Supervised Video Forensics by Audio-Visual Anomaly Detection.pdf"
              ]
            ],
            "resource": "storage/i2909.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics."
          ],
          [
            "Access Date",
            "2023-07-01 07:30:05"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Chao Feng, Ziyang Chen, Andrew Owens"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10491-10503"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2909.pdf",
        "selectable": false
      },
      {
        "text": "Spatial-Phase Shallow Learning",
        "item-id": "i3161",
        "nodes": [
          {
            "text": "Liu et al_2021_Spatial-Phase Shallow Learning.pdf",
            "item-id": "i3178",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2021_Spatial-Phase Shallow Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2021_Spatial-Phase Shallow Learning.pdf"
              ]
            ],
            "resource": "storage/i3178.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Spatial-Phase Shallow Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The remarkable success in face forgery techniques has received considerable attention in computer vision due to security concerns. We observe that up-sampling is a necessary step of most face forgery techniques, and cumulative up-sampling will result in obvious changes in the frequency domain, especially in the phase spectrum. According to the property of natural images, the phase spectrum preserves abundant frequency components that provide extra information and complement the loss of the amplitude spectrum. To this end, we present a novel Spatial-Phase Shallow Learning (SPSL) method, which combines spatial image and phase spectrum to capture the up-sampling artifacts of face forgery to improve the transferability, for face forgery detection. And we also theoretically analyze the validity of utilizing the phase spectrum. Moreover, we notice that local texture information is more crucial than high-level semantic information for the face forgery detection task. So we reduce the receptive fields by shallowing the network to suppress high-level features and focus on the local region. Extensive experiments show that SPSL can achieve the state-of-the-art performance on cross-datasets evaluation as well as multi-class classification and obtain comparable results on single dataset evaluation."
          ],
          [
            "Access Date",
            "2023-10-28 01:58:14"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Honggu Liu, Xiaodan Li, Wenbo Zhou, Yuefeng Chen, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "772-781"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Spatial-Phase Shallow Learning"
          ],
          [
            "Title",
            "Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Spatial-Phase_Shallow_Learning_Rethinking_Face_Forgery_Detection_in_Frequency_Domain_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i3178.pdf",
        "selectable": false
      },
      {
        "text": "Spatiotemporal Inconsistency Learning for DeepFake Video Detection",
        "item-id": "i1424",
        "nodes": [
          {
            "text": "Gu et al_2021_Spatiotemporal Inconsistency Learning for DeepFake Video Detection.pdf",
            "item-id": "i1465",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gu et al_2021_Spatiotemporal Inconsistency Learning for DeepFake Video Detection.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_3EQHXPVN/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/2\">2 RELATED WORK</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/2\">2.1 DeepFake Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/2\">2.2 DeepFake Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/3\">2.3 Video Temporal Modeling</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/3\">3 Overview</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/4\">3.1 Spatial Inconsistency Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/4\">3.2 Temporal Inconsistency Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/5\">3.3 Information Supplement Module</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/5\">4 EXPERIMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/5\">4.1 Experimental Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/6\">4.2 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/6\">4.3 Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/6\">4.4 Intra Test Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/7\">4.5 Cross Test Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/7\">5 ABLATION STUDY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/8\">6 Visualization and Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/8\">7 CONCLUSION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EQHXPVN/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gu et al_2021_Spatiotemporal Inconsistency Learning for DeepFake Video Detection.pdf"
              ]
            ],
            "resource": "storage/i1465.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Spatiotemporal Inconsistency Learning for DeepFake Video Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapid development of facial manipulation techniques has aroused public concerns in recent years. Following the success of deep learning, existing methods always formulate DeepFake video detection as a binary classification problem and develop frame-based and video-based solutions. However, little attention has been paid to capturing the spatial-temporal inconsistency in forged videos. To address this issue, we term this task as a Spatial-Temporal Inconsistency Learning (STIL) process and instantiate it into a novel STIL block, which consists of a Spatial Inconsistency Module (SIM), a Temporal Inconsistency Module (TIM), and an Information Supplement Module (ISM). Specifically, we present a novel temporal modeling paradigm in TIM by exploiting the temporal difference over adjacent frames along with both horizontal and vertical directions. And the ISM simultaneously utilizes the spatial information from SIM and temporal information from TIM to establish a more comprehensive spatial-temporal representation. Moreover, our STIL block is flexible and could be plugged into existing 2D CNNs. Extensive experiments and visualizations are presented to demonstrate the effectiveness of our method against the state-of-the-art competitors."
          ],
          [
            "Access Date",
            "2022-04-04"
          ],
          [
            "Creators",
            "Zhihao Gu, Yang Chen, Taiping Yao, Shouhong Ding, Jilin Li, Feiyue Huang, Lizhuang Ma"
          ],
          [
            "Date",
            "2021-10-17 2021-10-17"
          ],
          [
            "ISBN",
            "978-1-4503-8651-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "3473\u20133481"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 29th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Title",
            "Spatiotemporal Inconsistency Learning for DeepFake Video Detection"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3474085.3475508"
          ]
        ],
        "resource": "storage/i1465.pdf",
        "selectable": false
      },
      {
        "text": "The Creation and Detection of Deepfakes",
        "item-id": "i958",
        "nodes": [
          {
            "text": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item-id": "i969",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf"
              ]
            ],
            "resource": "storage/i969.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The Creation and Detection of Deepfakes",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these \u201cdeepfakes\u201d have advanced significantly. In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention."
          ],
          [
            "Access Date",
            "2021-10-05 09:05:27"
          ],
          [
            "Creators",
            "Yisroel Mirsky, Wenke Lee"
          ],
          [
            "DOI",
            "10.1145/3425780"
          ],
          [
            "Date",
            "2021-01-02 2021-01-02"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "April 2021"
          ],
          [
            "Pages",
            "7:1\u20137:41"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "The Creation and Detection of Deepfakes"
          ],
          [
            "Title",
            "The Creation and Detection of Deepfakes: A Survey"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3425780"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i969.pdf",
        "selectable": false
      },
      {
        "text": "Thinking in Frequency",
        "item-id": "i1631",
        "nodes": [
          {
            "text": "Qian et al_2020_Thinking in Frequency.pdf",
            "item-id": "i1634",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Qian et al_2020_Thinking in Frequency.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Z34ZMQQ9/2\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/4\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/5\">3 Our Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/6\">3.1 FAD: Frequency-Aware Decomposition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/7\">3.2 LFS: Local Frequency Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/8\">3.3 Two-Stream Collaborative Learning Framework</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/9\">4 Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/9\">4.1 Setting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/10\">4.2 Comparing with Previous Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/12\">4.3 Ablation Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/14\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z34ZMQQ9/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Qian et al_2020_Thinking in Frequency.pdf"
              ]
            ],
            "resource": "storage/i1634.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Thinking in Frequency",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We find that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F$$^3$$3-Net), taking advantages of two different but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F$$^3$$3-Net significantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, Jing Shao, Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58610-2_6"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58610-2"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "86-103"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "Thinking in Frequency"
          ],
          [
            "Title",
            "Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues"
          ]
        ],
        "resource": "storage/i1634.pdf",
        "selectable": false
      },
      {
        "text": "Transferring Audio Deepfake Detection Capability across Languages",
        "item-id": "i3752",
        "nodes": [
          {
            "text": "Ba et al_2023_Transferring Audio Deepfake Detection Capability across Languages.pdf",
            "item-id": "i3756",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ba et al_2023_Transferring Audio Deepfake Detection Capability across Languages.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RRXX5J3V/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/2\">2 Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/2\">2.1 Audio Deepfake</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/2\">2.2 Audio Deepfake Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/3\">2.3 Domain Adaptation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/3\">3 Motivation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/3\">3.1 Motivation and Challenge for Non-English Deepfake Detection </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/3\">3.2 Utilizing the Key Observation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/3\">3.3 Giving Attackers the Upperhand</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/3\">4 DECRO Dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/4\">5 Cross-Lingual Performance Benchmarking</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/4\">5.1 Detection Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/4\">5.2 Evaluation Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/4\">5.3 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/4\">6 Cross-Lingual Domain Adaptation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/4\">6.1 Backbone Network for Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/5\">6.2 Cross-Lingual Domain Adaptation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/6\">7 Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/6\">7.1 Experiment Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/7\">7.2 Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/8\">7.3 Unsupervised Domain Adaptation.</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/8\">8 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/8\">9 Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/8\">10 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/9\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/9\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/11\">A Proof for hypothesis in Section 7.2.3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RRXX5J3V/11\">B Figures</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ba et al_2023_Transferring Audio Deepfake Detection Capability across Languages.pdf"
              ]
            ],
            "resource": "storage/i3756.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Transferring Audio Deepfake Detection Capability across Languages",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The proliferation of deepfake content has motivated a surge of detection studies. However, existing detection methods in the audio area exclusively work in English, and there is a lack of data resources in other languages. Cross-lingual deepfake detection, a critical but rarely explored area, urges more study. This paper conducts the first comprehensive study on the cross-lingual perspective of deepfake detection. We observe that English data enriched in deepfake algorithms can teach a detector the knowledge of various spoofing artifacts, contributing to performing detection across language domains. Based on the observation, we first construct a first-of-its-kind cross-lingual evaluation dataset including heterogeneous spoofed speech uttered in the two most widely spoken languages, then explored domain adaptation (DA) techniques to transfer the artifacts detection capability and propose effective and practical DA strategies fitting the cross-lingual scenario. Our adversarial-based DA paradigm teaches the model to learn real/fake knowledge while losing language dependency. Extensive experiments over 137-hour audio clips validate the adapted models can detect fake audio generated by unseen algorithms in the new domain."
          ],
          [
            "Access Date",
            "2024-01-19"
          ],
          [
            "Creators",
            "Zhongjie Ba, Qing Wen, Peng Cheng, Yuwei Wang, Feng Lin, Li Lu, Zhenguang Liu"
          ],
          [
            "DOI",
            "10.1145/3543507.3583222"
          ],
          [
            "Date",
            "2023-00-30 \u56db\u6708 30, 2023"
          ],
          [
            "ISBN",
            "978-1-4503-9416-1"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2033\u20132044"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the ACM Web Conference 2023"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "WWW '23"
          ],
          [
            "Title",
            "Transferring Audio Deepfake Detection Capability across Languages"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3543507.3583222"
          ]
        ],
        "resource": "storage/i3756.pdf",
        "selectable": false
      },
      {
        "text": "UMMAFormer",
        "item-id": "i3203",
        "nodes": [
          {
            "text": "Zhang et al_2023_UMMAFormer.pdf",
            "item-id": "i3307",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_UMMAFormer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_V7UCRAVT/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/3\">2 RELATED WORK</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/3\">2.1 Image-Level Forgery Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/3\">2.2 Temporal-Level Forgery Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/3\">2.3 Temporal Action Localization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/3\">3 METHODOLOGY</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/3\">3.1 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/4\">3.2 Temporal Feature Abnormal Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/5\">3.3 Parallel Cross-Attention Feature Pyramid Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/5\">3.4 Training and Inference</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/6\">4 Temporal Video Inpainting Localization</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/6\">5 EXPERIMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/6\">5.1 Experimental Setup.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/7\">5.2 Results for Temporal Face Forgery Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/7\">5.3 Results for Temporal Video Inpainting Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/7\">5.4 Results for Partial Synthetic Speech Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/7\">5.5 Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/8\">6 CONCLUSIONS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/9\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/11\">A Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/11\">A.1 Comparison between Existing Audio and Video Forensics Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/11\">A.2 More Experiments Results for LAV-DF Subset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7UCRAVT/11\">A.3 More Experiments Results for Video-level Face Forgery Classification</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_UMMAFormer.pdf"
              ]
            ],
            "resource": "storage/i3307.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "UMMAFormer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The emergence of artificial intelligence-generated content (AIGC) has raised concerns about the authenticity of multimedia content in various fields. However, existing research for forgery content detection has focused mainly on binary classification tasks of complete videos, which has limited applicability in industrial settings. To address this gap, we propose UMMAFormer, a novel universal transformer framework for temporal forgery localization (TFL) that predicts forgery segments with multimodal adaptation. Our approach introduces a Temporal Feature Abnormal Attention (TFAA) module based on temporal feature reconstruction to enhance the detection of temporal differences. We also design a Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) to optimize the Feature Pyramid Network (FPN) for subtle feature enhancement. To evaluate the proposed method, we contribute a novel Temporal Video Inpainting Localization (TVIL) dataset specifically tailored for video inpainting scenes. Our experiments show that our approach achieves state-of-the-art performance on benchmark datasets, including Lav-DF, TVIL, and Psynd, significantly outperforming previous methods. The code and data are available at https://github.com/ymhzyj/UMMAFormer/."
          ],
          [
            "Access Date",
            "2023-10-29"
          ],
          [
            "Creators",
            "Rui Zhang, Hongxia Wang, Mingshan Du, Hanqing Liu, Yang Zhou, Qiang Zeng"
          ],
          [
            "DOI",
            "10.1145/3581783.3613767"
          ],
          [
            "Date",
            "2023-00-27 \u5341\u6708 27, 2023"
          ],
          [
            "ISBN",
            "9798400701085"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "8749\u20138759"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 31st ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '23"
          ],
          [
            "Short Title",
            "UMMAFormer"
          ],
          [
            "Title",
            "UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3581783.3613767"
          ]
        ],
        "resource": "storage/i3307.pdf",
        "selectable": false
      },
      {
        "text": "Undercover Deepfakes",
        "item-id": "i3165",
        "nodes": [
          {
            "text": "Comment: ICCV 2023 Workshop and Challenge on DeepFake Analysis and Detection",
            "item-id": "n3185",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ICCV 2023 Workshop and Challenge on DeepFake Analysis and Detection",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ICCV 2023 Workshop and Challenge on DeepFake Analysis and Detection</div>",
            "node_type": "note"
          },
          {
            "text": "Saha et al_2023_Undercover Deepfakes.pdf",
            "item-id": "i3184",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Saha et al_2023_Undercover Deepfakes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Saha et al_2023_Undercover Deepfakes.pdf"
              ]
            ],
            "resource": "storage/i3184.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Undercover Deepfakes",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The recent renaissance in generative models, driven primarily by the advent of diffusion models and iterative improvement in GAN methods, has enabled many creative applications. However, each advancement is also accompanied by a rise in the potential for misuse. In the arena of the deepfake generation, this is a key societal issue. In particular, the ability to modify segments of videos using such generative techniques creates a new paradigm of deepfakes which are mostly real videos altered slightly to distort the truth. This paradigm has been under-explored by the current deepfake detection methods in the academic literature. In this paper, we present a deepfake detection method that can address this issue by performing deepfake prediction at the frame and video levels. To facilitate testing our method, we prepared a new benchmark dataset where videos have both real and fake frame sequences with very subtle transitions. We provide a benchmark on the proposed dataset with our detection method which utilizes the Vision Transformer based on Scaling and Shifting to learn spatial features, and a Timeseries Transformer to learn temporal features of the videos to help facilitate the interpretation of possible deepfakes. Extensive experiments on a variety of deepfake generation methods show excellent results by the proposed method on temporal segmentation and classical video-level predictions as well. In particular, the paradigm we address will form a powerful tool for the moderation of deepfakes, where human oversight can be better targeted to the parts of videos suspected of being deepfakes. All experiments can be reproduced at: github.com/rgb91/temporal-deepfake-segmentation."
          ],
          [
            "Access Date",
            "2023-10-28 01:39:08"
          ],
          [
            "Archiveid",
            "arXiv:2305.06564"
          ],
          [
            "Creators",
            "Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.06564"
          ],
          [
            "Date",
            "2023-08-24 2023-08-24"
          ],
          [
            "Extra",
            "arXiv:2305.06564 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Undercover Deepfakes"
          ],
          [
            "Title",
            "Undercover Deepfakes: Detecting Fake Segments in Videos"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.06564"
          ]
        ],
        "resource": "storage/i3184.pdf",
        "selectable": false
      },
      {
        "text": "Use of a Capsule Network to Detect Fake Images and Videos",
        "item-id": "i3167",
        "nodes": [
          {
            "text": "Comment: Fixing Table 2's scale",
            "item-id": "n3195",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Fixing Table 2's scale",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Fixing Table 2's scale</div>",
            "node_type": "note"
          },
          {
            "text": "Nguyen et al_2019_Use of a Capsule Network to Detect Fake Images and Videos.pdf",
            "item-id": "i3194",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Nguyen et al_2019_Use of a Capsule Network to Detect Fake Images and Videos.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Nguyen et al_2019_Use of a Capsule Network to Detect Fake Images and Videos.pdf"
              ]
            ],
            "resource": "storage/i3194.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Use of a Capsule Network to Detect Fake Images and Videos",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The revolution in computer hardware, especially in graphics processing units and tensor processing units, has enabled significant advances in computer graphics and artificial intelligence algorithms. In addition to their many beneficial applications in daily life and business, computer-generated/manipulated images and videos can be used for malicious purposes that violate security systems, privacy, and social trust. The deepfake phenomenon and its variations enable a normal user to use his or her personal computer to easily create fake videos of anybody from a short real online video. Several countermeasures have been introduced to deal with attacks using such videos. However, most of them are targeted at certain domains and are ineffective when applied to other domains or new attacks. In this paper, we introduce a capsule network that can detect various kinds of attacks, from presentation attacks using printed images and replayed videos to attacks using fake videos created using deep learning. It uses many fewer parameters than traditional convolutional neural networks with similar performance. Moreover, we explain, for the first time ever in the literature, the theory behind the application of capsule networks to the forensics problem through detailed analysis and visualization."
          ],
          [
            "Access Date",
            "2023-10-27 14:07:57"
          ],
          [
            "Archiveid",
            "arXiv:1910.12467"
          ],
          [
            "Creators",
            "Huy H. Nguyen, Junichi Yamagishi, Isao Echizen"
          ],
          [
            "DOI",
            "10.48550/arXiv.1910.12467"
          ],
          [
            "Date",
            "2019-10-29 2019-10-29"
          ],
          [
            "Extra",
            "arXiv:1910.12467 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Use of a Capsule Network to Detect Fake Images and Videos"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1910.12467"
          ]
        ],
        "resource": "storage/i3194.pdf",
        "selectable": false
      },
      {
        "text": "Voice-Face Homogeneity Tells Deepfake",
        "item-id": "i2311",
        "nodes": [
          {
            "text": "Cheng et al_2022_Voice-Face Homogeneity Tells Deepfake.pdf",
            "item-id": "i2323",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cheng et al_2022_Voice-Face Homogeneity Tells Deepfake.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_9IQPKDFI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/3\">2.1 Deepfake</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/3\">2.2 Deepfake Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/3\">2.3 Cross-modal Biometric Matching</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/3\">3 Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/3\">3.1 Method Intuition</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/4\">3.1.1 Voice-Face Mismatching in Deepfake Data (Q1)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/4\">3.1.2 Voice Discrimination over Identities (Q2)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/4\">3.2 VFD for Pre-training</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/5\">3.2.1 Data Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/5\">3.2.2 Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/6\">3.2.3 Pre-training Protocol</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/6\">3.3 VFD for Fine-tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/6\">3.4 VFD for Deepfake Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/6\">4 Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/6\">4.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/7\">4.2 Implement Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/7\">4.3 Compared Baselines and Evaluation Metrics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/7\">4.4 Performance Comparison</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/7\">4.4.1 Comparison on modality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/8\">4.4.2 Comparison on auxiliary data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/8\">4.4.3 Comparison on fine-tuning strategy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/9\">4.5 Qualitative Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/9\">4.5.1 Heatmap</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/9\">4.5.2 Similarity comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/9\">4.5.3 Detection failure cases</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/10\">4.6 Hyperparameter Study</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/10\">4.7 Ablation Study</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/10\">4.7.1 Module evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/10\">4.7.2 Strategy evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/11\">4.7.3 Feature extractor evaluation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/11\">5 Conclusion and Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9IQPKDFI/11\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cheng et al_2022_Voice-Face Homogeneity Tells Deepfake.pdf"
              ]
            ],
            "resource": "storage/i2323.pdf"
          },
          {
            "text": "Comment: 13 pages for peer review. Code will be released at https://github.com/xaCheng1996/VFD",
            "item-id": "n2324",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 13 pages for peer review. Code will be released at https://github.com/xaCheng1996/VFD",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 13 pages for peer review. Code will be released at https://github.com/xaCheng1996/VFD</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Voice-Face Homogeneity Tells Deepfake",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Detecting forgery videos is highly desirable due to the abuse of deepfake. Existing detection approaches contribute to exploring the specific artifacts in deepfake videos and fit well on certain data. However, the growing technique on these artifacts keeps challenging the robustness of traditional deepfake detectors. As a result, the development of generalizability of these approaches has reached a blockage. To address this issue, given the empirical results that the identities behind voices and faces are often mismatched in deepfake videos, and the voices and faces have homogeneity to some extent, in this paper, we propose to perform the deepfake detection from an unexplored voice-face matching view. To this end, a voice-face matching method is devised to measure the matching degree of these two. Nevertheless, training on specific deepfake datasets makes the model overfit certain traits of deepfake algorithms. We instead, advocate a method that quickly adapts to untapped forgery, with a pre-training then fine-tuning paradigm. Specifically, we first pre-train the model on a generic audio-visual dataset, followed by the fine-tuning on downstream deepfake data. We conduct extensive experiments over three widely exploited deepfake datasets - DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method obtains significant performance gains as compared to other state-of-the-art competitors. It is also worth noting that our method already achieves competitive results when fine-tuned on limited deepfake data."
          ],
          [
            "Access Date",
            "2023-04-21 13:37:23"
          ],
          [
            "Archiveid",
            "arXiv:2203.02195"
          ],
          [
            "Creators",
            "Harry Cheng, Yangyang Guo, Tianyi Wang, Qi Li, Xiaojun Chang, Liqiang Nie"
          ],
          [
            "DOI",
            "10.48550/arXiv.2203.02195"
          ],
          [
            "Date",
            "2022-06-13 2022-06-13"
          ],
          [
            "Extra",
            "arXiv:2203.02195 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Voice-Face Homogeneity Tells Deepfake"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2203.02195"
          ]
        ],
        "resource": "storage/i2323.pdf",
        "selectable": false
      },
      {
        "text": "Xception",
        "item-id": "i1878",
        "nodes": [
          {
            "text": "Chollet_2017_Xception.pdf",
            "item-id": "i1979",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chollet_2017_Xception.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chollet_2017_Xception.pdf"
              ]
            ],
            "resource": "storage/i1979.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Xception",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
          ],
          [
            "Access Date",
            "2022-10-31 02:25:21"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Francois Chollet"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1251-1258"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Xception"
          ],
          [
            "Title",
            "Xception: Deep Learning With Depthwise Separable Convolutions"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i1979.pdf",
        "selectable": false
      }
    ],
    "item_title": "Deepfake Detection",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Deepfake Generation",
    "item-id": "c21,i3417",
    "nodes": [
      {
        "text": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild",
        "item-id": "i778",
        "nodes": [
          {
            "text": "Prajwal et al_2020_A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild.pdf",
            "item-id": "i818",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Prajwal et al_2020_A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Prajwal et al_2020_A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild.pdf"
              ]
            ],
            "resource": "storage/i818.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model, and also publicly release the code, models, and evaluation benchmarks on our website."
          ],
          [
            "Access Date",
            "2021-08-06"
          ],
          [
            "Creators",
            "K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C.V. Jawahar"
          ],
          [
            "DOI",
            "10.1145/3394171.3413532"
          ],
          [
            "Date",
            "2020-10-12 October 12, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7988-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "484\u2013492"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '20"
          ],
          [
            "Title",
            "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3394171.3413532"
          ]
        ],
        "resource": "storage/i818.pdf",
        "selectable": false
      },
      {
        "text": "A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation",
        "item-id": "i2092",
        "nodes": [
          {
            "text": "Kadam et al_2021_A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation.pdf",
            "item-id": "i2121",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kadam et al_2021_A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kadam et al_2021_A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation.pdf"
              ]
            ],
            "resource": "storage/i2121.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The fields like Media, Education and Corporations etc have started focusing on content creation. This has led to the huge demand for synthetic media generation using less data. To synthesize a high-grade artificial video, the lip must be synchronized with the audio. Here we have compared the various methods for voice-cloning and lip synchronization. Voice cloning procedure include state of the art methods like wavenet and other text-to-speech approaches. Lip synchronization methods describe constrained and unconstrained methods. Various recent research like LipGan, Wav2Lip are discussed. The methods are compared and the best method is suggested. Apart from studying and comparing the various methods, their drawbacks, future scopes, and application are also there. Different social and ethical issues are also discussed."
          ],
          [
            "Access Date",
            "2022-11-11 06:26:50"
          ],
          [
            "Creators",
            "Anup Kadam, Sagar Rane, Arpit Kumar Mishra, Shailesh Kumar Sahu, Shubham Singh, Shivam Kumar Pathak"
          ],
          [
            "DOI",
            "10.4108/eai.14-4-2021.169187"
          ],
          [
            "Date",
            "2021-04-14 2021-04-14"
          ],
          [
            "ISSN",
            "2409-9708"
          ],
          [
            "Issue",
            "28"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "publications.eai.eu"
          ],
          [
            "Pages",
            "e2-e2"
          ],
          [
            "Publication Title",
            "EAI Endorsed Transactions on Creative Technologies"
          ],
          [
            "Title",
            "A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation"
          ],
          [
            "URL",
            "https://publications.eai.eu/index.php/ct/article/view/1417"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i2121.pdf",
        "selectable": false
      },
      {
        "text": "AD-NeRF",
        "item-id": "i1320",
        "nodes": [
          {
            "text": "Guo et al_2021_AD-NeRF.pdf",
            "item-id": "i1389",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Guo et al_2021_AD-NeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Guo et al_2021_AD-NeRF.pdf"
              ]
            ],
            "resource": "storage/i1389.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AD-NeRF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF."
          ],
          [
            "Access Date",
            "2022-02-14 06:56:13"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, Juyong Zhang"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5784-5794"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "AD-NeRF"
          ],
          [
            "Title",
            "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1389.pdf",
        "selectable": false
      },
      {
        "text": "APB2FaceV2",
        "item-id": "i1321",
        "nodes": [
          {
            "text": "Comment: ICASSP'21",
            "item-id": "n1392",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ICASSP'21",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ICASSP'21</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang et al_2020_APB2FaceV2.pdf",
            "item-id": "i1391",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2020_APB2FaceV2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MEMYG3QD/1\">1  Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MEMYG3QD/1\">2  Related Works</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MEMYG3QD/2\">3  Method</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MEMYG3QD/3\">4  Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MEMYG3QD/4\">5  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MEMYG3QD/5\">6  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2020_APB2FaceV2.pdf"
              ]
            ],
            "resource": "storage/i1391.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "APB2FaceV2",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio-guided face reenactment aims to generate a photorealistic face that has matched facial expression with the input audio. However, current methods can only reenact a special person once the model is trained or need extra operations such as 3D rendering and image post-fusion on the premise of generating vivid faces. To solve the above challenge, we propose a novel \\emph{R}eal-time \\emph{A}udio-guided \\emph{M}ulti-face reenactment approach named \\emph{APB2FaceV2}, which can reenact different target faces among multiple persons with corresponding reference face and drive audio signal as inputs. Enabling the model to be trained end-to-end and have a faster speed, we design a novel module named Adaptive Convolution (AdaConv) to infuse audio information into the network, as well as adopt a lightweight network as our backbone so that the network can run in real time on CPU and GPU. Comparison experiments prove the superiority of our approach than existing state-of-the-art methods, and further experiments demonstrate that our method is efficient and flexible for practical applications https://github.com/zhangzjn/APB2FaceV2"
          ],
          [
            "Access Date",
            "2022-02-13 11:01:38"
          ],
          [
            "Archiveid",
            "arXiv:2010.13017"
          ],
          [
            "Creators",
            "Jiangning Zhang, Xianfang Zeng, Chao Xu, Jun Chen, Yong Liu, Yunliang Jiang"
          ],
          [
            "Date",
            "2020-10-24 2020-10-24"
          ],
          [
            "Extra",
            "arXiv:2010.13017 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "APB2FaceV2"
          ],
          [
            "Title",
            "APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2010.13017"
          ]
        ],
        "resource": "storage/i1391.pdf",
        "selectable": false
      },
      {
        "text": "AnyoneNet",
        "item-id": "i2149",
        "nodes": [
          {
            "text": "Wang et al_2022_AnyoneNet.pdf",
            "item-id": "i2164",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_AnyoneNet.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8I25BFJF/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8I25BFJF/2\">Related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/2\">Text-to-speech synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/2\">Speech-driven talking head generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8I25BFJF/3\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/3\">Overall framework</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/4\">Face Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/4\">Face-conditioned multi-speaker TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/5\">Talking head generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/6\">Landmark to photo-realistic image</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8I25BFJF/6\">Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8I25BFJF/6\">Database</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/7\">Database for the TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/7\">Database for face encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/7\">Database for the talking head generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/7\">Data processing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8I25BFJF/7\">Evaluation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/7\">Face-conditioned multi-speaker TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/8\">Talking head generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8I25BFJF/8\">Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/9\">Face-conditioned multi-speaker TTS.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/9\">Talking head generation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/9\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/11\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8I25BFJF/11\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_AnyoneNet.pdf"
              ]
            ],
            "resource": "storage/i2164.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AnyoneNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Automatically generating videos in which synthesized speech is synchronized with lip movements in a talking head has great potential in many human-computer interaction scenarios. In this paper, we present an automatic method to generate synchronized speech and talking-head videos on the basis of text and a single face image of an arbitrary person as input. In contrast to previous text-driven talking head generation methods, which can only synthesize the voice of a specific person, the proposed method is capable of synthesizing speech for any person. Specifically, the proposed method decomposes the generation of synchronized speech and talking head videos into two stages, i.e., a text-to-speech (TTS) stage and a speech-driven talking head generation stage. The proposed TTS module is a face-conditioned multi-speaker TTS model that gets the speaker identity information from face images instead of speech, which allows us to synthesize a personalized voice on the basis of the input face image. To generate the talking head videos from the face images, a facial landmark-based method that can predict both lip movements and head rotations is proposed. Extensive experiments demonstrate that the proposed method is able to generate synchronized speech and talking head videos for arbitrary persons, in which the timbre of the synthesized voice is in harmony with the input face, and the proposed landmark-based talking head method outperforms the state-of-the-art landmark-based method on generating natural talking head videos."
          ],
          [
            "Creators",
            "Xinsheng Wang, Qicong Xie, Jihua Zhu, Lei Xie, Odette Scharenborg"
          ],
          [
            "DOI",
            "10.1109/TMM.2022.3214100"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Multimedia"
          ],
          [
            "ISSN",
            "1941-0077"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-12"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Multimedia"
          ],
          [
            "Short Title",
            "AnyoneNet"
          ],
          [
            "Title",
            "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Persons"
          ]
        ],
        "resource": "storage/i2164.pdf",
        "selectable": false
      },
      {
        "text": "Arbitrary talking face generation via attentional audio-visual coherence learning",
        "item-id": "i1298",
        "nodes": [
          {
            "text": "Zhu et al_2021_Arbitrary talking face generation via attentional audio-visual coherence.pdf",
            "item-id": "i1347",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2021_Arbitrary talking face generation via attentional audio-visual coherence.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Z4LQSNPE/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/2\">Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/2\">Talking Face Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/2\">Mutual Information Estimation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/3\">Proposed Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/3\">Overview</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/3\">Asymmetric Mutual Information Estimator</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/3\">Preliminary Theory of 'MINE'</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/4\">'AMIE' in Talking Face Generation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/4\">Dynamic Attention on Lip Area</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/5\">Training Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/5\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/5\">Dataset and Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/5\">Quantitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/5\">Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/6\">Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/6\">User Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z4LQSNPE/6\">Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2021_Arbitrary talking face generation via attentional audio-visual coherence.pdf"
              ]
            ],
            "resource": "storage/i1347.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Arbitrary talking face generation via attentional audio-visual coherence learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking face generation aims to synthesize a face video with precise lip synchronization as well as a smooth transition of facial motion over the entire video via the given speech clip and facial image. Most existing methods mainly focus on either disentangling the information in a single image or learning temporal information between frames. However, cross-modality coherence between audio and video information has not been well addressed during synthesis. In this paper, we propose a novel arbitrary talking face generation framework by discovering the audio-visual coherence via the proposed Asymmetric Mutual Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA) block by selectively focusing the lip area of the input image during the training stage, to further enhance lip synchronization. Experimental results on benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods on prevalent metrics with robust high-resolution synthesizing on gender and pose variations."
          ],
          [
            "Access Date",
            "2022-02-19"
          ],
          [
            "Conference Name",
            "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence"
          ],
          [
            "Creators",
            "Hao Zhu, Huaibo Huang, Yi Li, Aihua Zheng, Ran He"
          ],
          [
            "Date",
            "2021-01-07 2021-01-07"
          ],
          [
            "ISBN",
            "978-0-9992411-6-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2362\u20132368"
          ],
          [
            "Place",
            "Yokohama, Yokohama, Japan"
          ],
          [
            "Proceedings Title",
            "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence"
          ],
          [
            "Series",
            "IJCAI'20"
          ],
          [
            "Title",
            "Arbitrary talking face generation via attentional audio-visual coherence learning"
          ]
        ],
        "resource": "storage/i1347.pdf",
        "selectable": false
      },
      {
        "text": "Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild",
        "item-id": "i1827",
        "nodes": [
          {
            "text": "Wang et al_2022_Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the.pdf",
            "item-id": "i2071",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the.pdf"
              ]
            ],
            "resource": "storage/i2071.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking face generation with great practical significance has attracted more attention in recent audio-visual studies. How to achieve accurate lip synchronization is a long-standing challenge to be further investigated. Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by incorporating spatial attention module and channel attention module into lip-syncing strategy. Rather than focusing on the unimportant regions of the face image, the proposed AttnWav2Lip model is able to pay more attention on the lip region reconstruction. To our limited knowledge, this is the first attempt to introduce attention mechanism to the scheme of talking face generation. An extensive experiments have been conducted to evaluate the effectiveness of the proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a superior performance has been demonstrated on the benchmark lip synthesis datasets, including LRW, LRS2 and LRS3."
          ],
          [
            "Access Date",
            "2022-10-16 21:16:12"
          ],
          [
            "Archiveid",
            "arXiv:2203.03984"
          ],
          [
            "Creators",
            "Ganglai Wang, Peng Zhang, Lei Xie, Wei Huang, Yufei Zha"
          ],
          [
            "DOI",
            "10.48550/arXiv.2203.03984"
          ],
          [
            "Date",
            "2022-03-08 2022-03-08"
          ],
          [
            "Extra",
            "arXiv:2203.03984 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2203.03984"
          ]
        ],
        "resource": "storage/i2071.pdf",
        "selectable": false
      },
      {
        "text": "Audio-Driven Emotional Video Portraits",
        "item-id": "i1518",
        "nodes": [
          {
            "text": "Ji et al_2021_Audio-Driven Emotional Video Portraits.pdf",
            "item-id": "i1520",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ji et al_2021_Audio-Driven Emotional Video Portraits.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ji et al_2021_Audio-Driven Emotional Video Portraits.pdf"
              ]
            ],
            "resource": "storage/i1520.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio-Driven Emotional Video Portraits",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Despite previous success in generating audio-driven talking heads, most of the previous studies focus on the correlation between speech content and the mouth shape. Facial emotion, which is one of the most important features on natural human faces, is always neglected in their methods. In this work, we present Emotional Video Portraits (EVP), a system for synthesizing high-quality video portraits with vivid emotional dynamics driven by audios. Specifically, we propose the Cross-Reconstructed Emotion Disentanglement technique to decompose speech into two decoupled spaces, i.e., a duration-independent emotion space and a duration dependent content space. With the disentangled features, dynamic 2D emotional facial landmarks can be deduced. Then we propose the Target-Adaptive Face Synthesis technique to generate the final high-quality video portraits, by bridging the gap between the deduced landmarks and the natural head poses of target videos. Extensive experiments demonstrate the effectiveness of our method both qualitatively and quantitatively."
          ],
          [
            "Access Date",
            "2022-05-07 12:09:36"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14080-14089"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Audio-Driven Emotional Video Portraits"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Ji_Audio-Driven_Emotional_Video_Portraits_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1520.pdf",
        "selectable": false
      },
      {
        "text": "Audio-Visual Face Reenactment",
        "item-id": "i2207",
        "nodes": [
          {
            "text": "Agarwal et al_2023_Audio-Visual Face Reenactment.pdf",
            "item-id": "i2240",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Agarwal et al_2023_Audio-Visual Face Reenactment.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Agarwal et al_2023_Audio-Visual Face Reenactment.pdf"
              ]
            ],
            "resource": "storage/i2240.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio-Visual Face Reenactment",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This work proposes a novel method to generate realistic talking head videos using audio and visual streams. We animate a source image by transferring head motion from a driving video using a dense motion field generated using learnable keypoints. We improve the quality of lip sync using audio as an additional input, helping the network to attend to the mouth region. We use additional priors using face segmentation and face mesh to improve the structure of the reconstructed faces. Finally, we improve the visual quality of the generations by incorporating a carefully designed identity-aware generator module. The identity-aware generator takes the source image and the warped motion features as input to generate a high-quality output with fine-grained details. Our method produces state-of-the-art results and generalizes well to unseen faces, languages, and voices. We comprehensively evaluate our approach using multiple metrics and outperforming the current techniques both qualitative and quantitatively. Our work opens up several applications, including enabling low bandwidth video calls. We release a demo video and additional information at http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr"
          ],
          [
            "Access Date",
            "2023-02-25 06:43:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
          ],
          [
            "Creators",
            "Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C. V. Jawahar"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5178-5187"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
          ],
          [
            "Title",
            "Audio-Visual Face Reenactment"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Audio-Visual_Face_Reenactment_WACV_2023_paper.html"
          ]
        ],
        "resource": "storage/i2240.pdf",
        "selectable": false
      },
      {
        "text": "Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose",
        "item-id": "i1295",
        "nodes": [
          {
            "text": "Comment: 12 pages, 9 figures",
            "item-id": "n1342",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 12 pages, 9 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 12 pages, 9 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Yi et al_2020_Audio-driven Talking Face Video Generation with Learning-based Personalized.pdf",
            "item-id": "i1341",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yi et al_2020_Audio-driven Talking Face Video Generation with Learning-based Personalized.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_T37IJ3KC/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/2\">2.1 Talking face generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/2\">2.2 3D face reconstruction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/2\">2.3 GANs and memory networks.</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/3\">3 Our Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/3\">3.1 3D face reconstruction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/3\">3.2 Mapping from audio to expression and pose</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/4\">3.3 Rendering and background matching</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/4\">3.3.1 Rendering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/4\">3.3.2 Background matching</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/4\">3.4 Memory-augmented GAN for refining frames</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/6\">4.1 Experiment setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/7\">4.2 Ablation study</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/7\">4.3 Comparison with state of the arts</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/7\">4.3.1 Comparison with Ours-P</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/8\">4.3.2 Comparison with Ours-G</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/9\">4.4 Analysis of head pose behavior</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/9\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/10\">Appendix A: Correlation between Audio and Head Pose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/10\">Appendix B: User Study on the Length of Input Short Video</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T37IJ3KC/11\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yi et al_2020_Audio-driven Talking Face Video Generation with Learning-based Personalized.pdf"
              ]
            ],
            "resource": "storage/i1341.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Real-world talking faces often accompany with natural head movement. However, most existing talking face video generation methods only consider facial animation with fixed head pose. In this paper, we address this problem by proposing a deep neural network model that takes an audio signal A of a source person and a very short video V of a target person as input, and outputs a synthesized high-quality talking face video with personalized head pose (making use of the visual information in V), expression and lip synchronization (by considering both A and V). The most challenging issue in our work is that natural poses often cause in-plane and out-of-plane head rotations, which makes synthesized talking face video far from realistic. To address this challenge, we reconstruct 3D face animation and re-render it into synthesized frames. To fine tune these frames into realistic ones with smooth background transition, we propose a novel memory-augmented GAN module. By first training a general mapping based on a publicly available dataset and fine-tuning the mapping using the input short video of target person, we develop an effective strategy that only requires a small number of frames (about 300 frames) to learn personalized talking behavior including head pose. Extensive experiments and two user studies show that our method can generate high-quality (i.e., personalized head movements, expressions and good lip synchronization) talking face videos, which are naturally looking with more distinguishing head movement effects than the state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-02-21 02:30:11"
          ],
          [
            "Archiveid",
            "arXiv:2002.10137"
          ],
          [
            "Creators",
            "Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu"
          ],
          [
            "Date",
            "2020-03-05 2020-03-05"
          ],
          [
            "Extra",
            "arXiv:2002.10137 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2002.10137"
          ]
        ],
        "resource": "storage/i1341.pdf",
        "selectable": false
      },
      {
        "text": "Bringing portraits to life",
        "item-id": "i3234",
        "nodes": [
          {
            "text": "Averbuch-Elor et al_2017_Bringing portraits to life.pdf",
            "item-id": "i3332",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Averbuch-Elor et al_2017_Bringing portraits to life.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_L7EDK6T7/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/2\">2 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/3\">3 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/3\">4 Coarse target video synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/4\">5 Transferring Hidden Regions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/5\">6 Transferring Fine-Scale Details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/6\">7 Results and Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/6\">7.1 User study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/7\">7.2 Pipeline evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/7\">7.3 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/8\">7.4 Runtime</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/8\">8 Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/8\">8.1 Reactive profiles</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/9\">8.2 Non photo-realistic faces</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/10\">9 Conclusions and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_L7EDK6T7/10\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Averbuch-Elor et al_2017_Bringing portraits to life.pdf"
              ]
            ],
            "resource": "storage/i3332.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Bringing portraits to life",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a technique to automatically animate a still portrait, making it possible for the subject in the photo to come to life and express various emotions. We use a driving video (of a different subject) and develop means to transfer the expressiveness of the subject in the driving video to the target portrait. In contrast to previous work that requires an input video of the target face to reenact a facial performance, our technique uses only a single target image. We animate the target image through 2D warps that imitate the facial transformations in the driving video. As warps alone do not carry the full expressiveness of the face, we add fine-scale dynamic details which are commonly associated with facial expressions such as creases and wrinkles. Furthermore, we hallucinate regions that are hidden in the input target face, most notably in the inner mouth. Our technique gives rise to reactive profiles, where people in still images can automatically interact with their viewers. We demonstrate our technique operating on numerous still portraits from the internet."
          ],
          [
            "Access Date",
            "2023-12-27 13:27:39"
          ],
          [
            "Creators",
            "Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, Michael F. Cohen"
          ],
          [
            "DOI",
            "10.1145/3130800.3130818"
          ],
          [
            "Date",
            "2017-00-20 \u5341\u4e00\u6708 20, 2017"
          ],
          [
            "ISSN",
            "0730-0301"
          ],
          [
            "Issue",
            "6"
          ],
          [
            "Journal Abbreviation",
            "ACM Trans. Graph."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "196:1\u2013196:13"
          ],
          [
            "Publication Title",
            "ACM Transactions on Graphics"
          ],
          [
            "Title",
            "Bringing portraits to life"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3130800.3130818"
          ],
          [
            "Volume",
            "36"
          ]
        ],
        "resource": "storage/i3332.pdf",
        "selectable": false
      },
      {
        "text": "Capture, Learning, and Synthesis of 3D Speaking Styles",
        "item-id": "i1300",
        "nodes": [
          {
            "text": "Cudeiro et al_2019_Capture, Learning, and Synthesis of 3D Speaking Styles.pdf",
            "item-id": "i1351",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cudeiro et al_2019_Capture, Learning, and Synthesis of 3D Speaking Styles.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cudeiro et al_2019_Capture, Learning, and Synthesis of 3D Speaking Styles.pdf"
              ]
            ],
            "resource": "storage/i1351.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Capture, Learning, and Synthesis of 3D Speaking Styles",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input--even speech in languages other than English--and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de."
          ],
          [
            "Access Date",
            "2022-02-20 07:05:50"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael J. Black"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10101-10111"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Capture, Learning, and Synthesis of 3D Speaking Styles"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Cudeiro_Capture_Learning_and_Synthesis_of_3D_Speaking_Styles_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i1351.pdf",
        "selectable": false
      },
      {
        "text": "Cloth Interactive Transformer for Virtual Try-On",
        "item-id": "i1530",
        "nodes": [
          {
            "text": "Comment: 11 pages, 6 figures,",
            "item-id": "n1553",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 11 pages, 6 figures,",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 11 pages, 6 figures,</div>",
            "node_type": "note"
          },
          {
            "text": "Ren et al_2021_Cloth Interactive Transformer for Virtual Try-On.pdf",
            "item-id": "i1552",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ren et al_2021_Cloth Interactive Transformer for Virtual Try-On.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ren et al_2021_Cloth Interactive Transformer for Virtual Try-On.pdf"
              ]
            ],
            "resource": "storage/i1552.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Cloth Interactive Transformer for Virtual Try-On",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "2D image-based virtual try-on has attracted increased attention from the multimedia and computer vision communities. However, most of the existing image-based virtual try-on methods directly put both person and the in-shop clothing representations together, without considering the mutual correlation between them. What is more, the long-range information, which is crucial for generating globally consistent results, is also hard to be established via the regular convolution operation. To alleviate these two problems, in this paper we propose a novel two-stage Cloth Interactive Transformer (CIT) for virtual try-on. In the first stage, we design a CIT matching block, aiming to perform a learnable thin-plate spline transformation that can capture more reasonable long-range relation. As a result, the warped in-shop clothing looks more natural. In the second stage, we propose a novel CIT reasoning block for establishing the global mutual interactive dependence. Based on this mutual dependence, the significant region within the input data can be highlighted, and consequently, the try-on results can become more realistic. Extensive experiments on a public fashion dataset demonstrate that our CIT can achieve the new state-of-the-art virtual try-on performance both qualitatively and quantitatively. The source code and trained models are available at https://github.com/Amazingren/CIT."
          ],
          [
            "Access Date",
            "2022-05-12 15:30:19"
          ],
          [
            "Archiveid",
            "arXiv:2104.05519"
          ],
          [
            "Creators",
            "Bin Ren, Hao Tang, Fanyang Meng, Runwei Ding, Ling Shao, Philip H. S. Torr, Nicu Sebe"
          ],
          [
            "Date",
            "2021-04-12 2021-04-12"
          ],
          [
            "Extra",
            "arXiv:2104.05519 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Cloth Interactive Transformer for Virtual Try-On"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2104.05519"
          ]
        ],
        "resource": "storage/i1552.pdf",
        "selectable": false
      },
      {
        "text": "DFA-NeRF",
        "item-id": "i1319",
        "nodes": [
          {
            "text": "Yao et al_2022_DFA-NeRF.pdf",
            "item-id": "i1385",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yao et al_2022_DFA-NeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yao et al_2022_DFA-NeRF.pdf"
              ]
            ],
            "resource": "storage/i1385.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "DFA-NeRF",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While recent advances in deep neural networks have made it possible to render high-quality images, generating photo-realistic and personalized talking head remains challenging. With given audio, the key to tackling this task is synchronizing lip movement and simultaneously generating personalized attributes like head movement and eye blink. In this work, we observe that the input audio is highly correlated to lip motion while less correlated to other personalized attributes (e.g., head movements). Inspired by this, we propose a novel framework based on neural radiance field to pursue high-fidelity and personalized talking head generation. Specifically, neural radiance field takes lip movements features and personalized attributes as two disentangled conditions, where lip movements are directly predicted from the audio inputs to achieve lip-synchronized generation. In the meanwhile, personalized attributes are sampled from a probabilistic model, where we design a Transformer-based variational autoencoder sampled from Gaussian Process to learn plausible and natural-looking head pose and eye blink. Experiments on several benchmarks demonstrate that our method achieves significantly better results than state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-02-14 07:04:37"
          ],
          [
            "Archiveid",
            "arXiv:2201.00791"
          ],
          [
            "Creators",
            "Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai, Xiaokang Yang"
          ],
          [
            "Date",
            "2022-01-03 2022-01-03"
          ],
          [
            "Extra",
            "arXiv:2201.00791 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "DFA-NeRF"
          ],
          [
            "Title",
            "DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2201.00791"
          ]
        ],
        "resource": "storage/i1385.pdf",
        "selectable": false
      },
      {
        "text": "Deep Person Generation",
        "item-id": "i2147",
        "nodes": [
          {
            "text": "Sha et al_2022_Deep Person Generation.pdf",
            "item-id": "i2162",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sha et al_2022_Deep Person Generation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NYKSXY4R/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/4\">2 Talking-head Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/5\">2.1 Motion-driven Talking-Head Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/7\">2.2 Audio-driven Talking-Head Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/9\">2.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/10\">3 Pose-guided Person Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/10\">3.1 Pose-guided Person Image Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/14\">3.2 Pose-guided Person Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/14\">3.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/15\">4 Garment-Oriented Person Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/15\">4.1 Virtual try-on</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">4.2 Garment Manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">4.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">5 Benchmarks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">5.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/20\">5.2 Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/21\">5.3 Performance Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/22\">6 Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7 Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7.1 Generative Data Augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7.2 Virtual Fitting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/26\">7.3 Digital Human</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/27\">8 Future Directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/28\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/28\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sha et al_2022_Deep Person Generation.pdf"
              ]
            ],
            "resource": "storage/i2162.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Person Generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep person generation has attracted extensive research attention due to its wide applications in virtual agents, video conferencing, online shopping and art/movie production. With the advancement of deep learning, visual appearances (face, pose, cloth) of a person image can be easily generated on demand. In this survey, we first summarize the scope of person generation, and then systematically review recent progress and technical trends in identity-preserving deep person generation, covering three major tasks: talking-head generation (face), pose-guided person generation (pose) and garment-oriented person generation (cloth). More than two hundred papers are covered for a thorough overview, and the milestone works are highlighted to witness the major technical breakthrough. Based on these fundamental tasks, many applications are investigated, e.g., virtual fitting, digital human, generative data augmentation. We hope this survey could shed some light on the future prospects of identity-preserving deep person generation, and provide a helpful foundation for full applications towards the digital human."
          ],
          [
            "Access Date",
            "2022-12-12 11:08:18"
          ],
          [
            "Creators",
            "Tong Sha, Wei Zhang, Tong Shen, Zhoujun Li, Tao Mei"
          ],
          [
            "DOI",
            "10.1145/3575656"
          ],
          [
            "Date",
            "2022-00-07 \u5341\u4e8c\u6708 7, 2022"
          ],
          [
            "Extra",
            "Just Accepted"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Deep Person Generation"
          ],
          [
            "Title",
            "Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3575656"
          ]
        ],
        "resource": "storage/i2162.pdf",
        "selectable": false
      },
      {
        "text": "Deep video portraits",
        "item-id": "i1057",
        "nodes": [
          {
            "text": "Kim et al_2018_Deep video portraits.pdf",
            "item-id": "i1084",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kim et al_2018_Deep video portraits.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2WL4AWEM/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/2\">2 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/4\">3 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/4\">4 Monocular Face Reconstruction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/5\">5 Synthetic Conditioning Input</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/5\">6 Rendering-to-Video Translation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/6\">7 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/6\">7.1 Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/8\">7.2 Quantitative Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/10\">7.3 Comparisons to the State of the Art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/10\">7.4 User Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/12\">8 Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/12\">9 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/12\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/13\">A Appendix</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2WL4AWEM/13\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kim et al_2018_Deep video portraits.pdf"
              ]
            ],
            "resource": "storage/i1084.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep video portraits",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect."
          ],
          [
            "Access Date",
            "2021-10-18 14:51:50"
          ],
          [
            "Creators",
            "Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick P\u00e9rez, Christian Richardt, Michael Zollh\u00f6fer, Christian Theobalt"
          ],
          [
            "DOI",
            "10.1145/3197517.3201283"
          ],
          [
            "Date",
            "2018-07-30 July 30, 2018"
          ],
          [
            "ISSN",
            "0730-0301"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "ACM Trans. Graph."
          ],
          [
            "Library Catalog",
            "August 2018"
          ],
          [
            "Pages",
            "163:1\u2013163:14"
          ],
          [
            "Publication Title",
            "ACM Transactions on Graphics"
          ],
          [
            "Title",
            "Deep video portraits"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3197517.3201283"
          ],
          [
            "Volume",
            "37"
          ]
        ],
        "resource": "storage/i1084.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake generation and detection, a survey",
        "item-id": "i2777",
        "nodes": [
          {
            "text": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item-id": "i2921",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/2\">2 Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3 Deepfake generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.1 Types of\u00a0Deepfake</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.2 Face-based generation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.1 Face swapping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.2 Facial reenactment</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4 Deepfake detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1 Detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1.1 Features based detection methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/10\">4.1.2 Machine learning-based detection methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/11\">4.2 Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5 Discussions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5.1 Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">5.2 Future directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang_2022_Deepfake generation and detection, a survey.pdf"
              ]
            ],
            "resource": "storage/i2921.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake generation and detection, a survey",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake\u00a0refers to realistic, but\u00a0fake images, sounds, and videos generated by articial intelligence methods. Recent advances in deepfake generation make\u00a0deepfake more realistic and easier to make. Deepfake has been a signicant threat to national security, democracy, society, and\u00a0our privacy, which calls for deepfake detection methods to combat potential threats. In the paper, we make a survey on state-ofthe-art deepfake generation methods, detection methods, and existing datasets. Current deepfake generation methods can be\u00a0classified into face swapping and facial reenactment. Deepfake detection methods are mainly based features and machine\u00a0learning methods. There are still some challenges for deepfake detection, such as progress on deepfake generation, lack of high\u00a0quality datasets and benchmark. Future trends on deepfake detection can be efficient, robust and systematical detection methods\u00a0and high quality datasets."
          ],
          [
            "Access Date",
            "2023-06-30 23:52:04"
          ],
          [
            "Creators",
            "Tao Zhang"
          ],
          [
            "DOI",
            "10.1007/s11042-021-11733-y"
          ],
          [
            "Date",
            "2022-02-01 2022-02-01"
          ],
          [
            "ISSN",
            "1573-7721"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Multimed Tools Appl"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "6259-6276"
          ],
          [
            "Publication Title",
            "Multimedia Tools and Applications"
          ],
          [
            "Title",
            "Deepfake generation and detection, a survey"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11042-021-11733-y"
          ],
          [
            "Volume",
            "81"
          ]
        ],
        "resource": "storage/i2921.pdf",
        "selectable": false
      },
      {
        "text": "Deepfakes and beyond",
        "item-id": "i2093",
        "nodes": [
          {
            "text": "Tolosana et al_2020_Deepfakes and beyond.pdf",
            "item-id": "i2122",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tolosana et al_2020_Deepfakes and beyond.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KWB4V9ZH/1\">plantilla_actualizada_ps_ARTICULO (1)</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/2\">deepkakes and beyond aythami</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/2\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/4\">II Types of Facial Manipulations</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/4\">III Entire Face Synthesis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/4\">III-A Manipulation Techniques and Public Databases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/5\">III-B Manipulation Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/6\">IV Identity Swap</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/6\">IV-A Manipulation Techniques and Public Databases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/9\">IV-B Manipulation Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/12\">V Attribute Manipulation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/12\">V-A Manipulation Techniques and Public Databases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/13\">V-B Manipulation Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/14\">VI Expression Swap</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/14\">VI-A Manipulation Techniques and Public Databases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/15\">VI-B Manipulation Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/16\">VII Other Face Manipulation Directions</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/16\">VII-A Face Morphing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/17\">VII-B Face De-Identification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/17\">VII-C Audio-to-Video and Text-to-Video</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/18\">VIII Concluding Remarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/19\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/23\">Biographies</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/23\">Ruben Tolosana</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/23\">Ruben Vera-Rodriguez</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/23\">Julian Fierrez</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/24\">Aythami Morales</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KWB4V9ZH/24\">Javier Ortega-Garcia</a></li></ul></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tolosana et al_2020_Deepfakes and beyond.pdf"
              ]
            ],
            "resource": "storage/i2122.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfakes and beyond",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The free access to large-scale public databases, together with the fast progress of deep learning techniques, in particular Generative Adversarial Networks, have led to the generation of very realistic fake content with its corresponding implications towards society in this era of fake news. This survey provides a thorough review of techniques for manipulating face images including DeepFake methods, and methods to detect such manipulations. In particular, four types of facial manipulation are reviewed: i) entire face synthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv) expression swap. For each manipulation group, we provide details regarding manipulation techniques, existing public databases, and key benchmarks for technology evaluation of fake detection methods, including a summary of results from those evaluations. Among all the aspects discussed in the survey, we pay special attention to the latest generation of DeepFakes, highlighting its improvements and challenges for fake detection. In addition to the survey information, we also discuss open issues and future trends that should be considered to advance in the field."
          ],
          [
            "Access Date",
            "2022-11-11 06:24:23"
          ],
          [
            "Creators",
            "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, Javier Ortega-Garcia"
          ],
          [
            "DOI",
            "10.1016/j.inffus.2020.06.014"
          ],
          [
            "Date",
            "2020-12-01 2020-12-01"
          ],
          [
            "ISSN",
            "1566-2535"
          ],
          [
            "Journal Abbreviation",
            "Information Fusion"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "131-148"
          ],
          [
            "Publication Title",
            "Information Fusion"
          ],
          [
            "Short Title",
            "Deepfakes and beyond"
          ],
          [
            "Title",
            "Deepfakes and beyond: A Survey of face manipulation and fake detection"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1566253520303110"
          ],
          [
            "Volume",
            "64"
          ]
        ],
        "resource": "storage/i2122.pdf",
        "selectable": false
      },
      {
        "text": "Deferred neural rendering",
        "item-id": "i3417",
        "nodes": [
          {
            "text": "Thies et al_2019_Deferred neural rendering.pdf",
            "item-id": "i3419",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Thies et al_2019_Deferred neural rendering.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Thies et al_2019_Deferred neural rendering.pdf"
              ]
            ],
            "resource": "storage/i3419.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deferred neural rendering",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks."
          ],
          [
            "Access Date",
            "2024-01-04 03:56:13"
          ],
          [
            "Creators",
            "Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner"
          ],
          [
            "DOI",
            "10.1145/3306346.3323035"
          ],
          [
            "Date",
            "2019-00-12 \u4e03\u6708 12, 2019"
          ],
          [
            "ISSN",
            "0730-0301"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "ACM Trans. Graph."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "66:1\u201366:12"
          ],
          [
            "Publication Title",
            "ACM Transactions on Graphics"
          ],
          [
            "Short Title",
            "Deferred neural rendering"
          ],
          [
            "Title",
            "Deferred neural rendering: image synthesis using neural textures"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3306346.3323035"
          ],
          [
            "Volume",
            "38"
          ]
        ],
        "resource": "storage/i3419.pdf",
        "selectable": false
      },
      {
        "text": "Depth-Aware Generative Adversarial Network for Talking Head Video Generation",
        "item-id": "i2157",
        "nodes": [
          {
            "text": "Hong et al_2022_Depth-Aware Generative Adversarial Network for Talking Head Video Generation.pdf",
            "item-id": "i2174",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hong et al_2022_Depth-Aware Generative Adversarial Network for Talking Head Video Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hong et al_2022_Depth-Aware Generative Adversarial Network for Talking Head Video Generation.pdf"
              ]
            ],
            "resource": "storage/i2174.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Depth-Aware Generative Adversarial Network for Talking Head Video Generation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking head video generation aims to produce a synthetic human face video that contains the identity and pose information respectively from a given source image and a driving video. Existing works for this task heavily rely on 2D representations (e.g. appearance and motion) learned from the input images. However, dense 3D facial geometry (e.g. pixel-wise depth) is extremely important for this task as it is particularly beneficial for us to essentially generate accurate 3D face structures and distinguish noisy information from the possibly cluttered background. Nevertheless, dense 3D geometry annotations are prohibitively costly for videos and are typically not available for this video generation task. In this paper, we introduce a self-supervised face-depth learning method to automatically recover dense 3D facial geometry (i.e. depth) from the face videos without the requirement of any expensive 3D annotation data. Based on the learned dense depth maps, we further propose to leverage them to estimate sparse facial keypoints that capture the critical movement of the human head. In a more dense way, the depth is also utilized to learn 3D-aware cross-modal (i.e. appearance and depth) attention to guide the generation of motion fields for warping source image representations. All these contributions compose a novel depth-aware generative adversarial network (DaGAN) for talking head generation. Extensive experiments conducted demonstrate that our proposed method can generate highly realistic faces, and achieve significant results on the unseen human faces."
          ],
          [
            "Access Date",
            "2022-12-12 09:08:58"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Fa-Ting Hong, Longhao Zhang, Li Shen, Dan Xu"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3397-3406"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Depth-Aware Generative Adversarial Network for Talking Head Video Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Hong_Depth-Aware_Generative_Adversarial_Network_for_Talking_Head_Video_Generation_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2174.pdf",
        "selectable": false
      },
      {
        "text": "DiffTalk",
        "item-id": "i2743",
        "nodes": [
          {
            "text": "Shen et al_2023_DiffTalk.pdf",
            "item-id": "i2745",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shen et al_2023_DiffTalk.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SWXUWEGE/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/3\">. Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/3\">. Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/3\">. Conditional Diffusion Model for Talking Head</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/5\">. Progressive Inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/5\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/5\">. Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/6\">. Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/7\">. Method Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/8\">. Expand to Higher Resolution</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/8\">. Conclusion and Discussion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shen et al_2023_DiffTalk.pdf"
              ]
            ],
            "resource": "storage/i2745.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DiffTalk",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the generation quality or enhance the model generalization. However, there are few works able to address both issues simultaneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturally generalized across different identities without any further fine-tuning. Additionally, our DiffTalk can be gracefully tailored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identities. For more video results, please refer to https://sstzal.github.io/DiffTalk/."
          ],
          [
            "Access Date",
            "2023-06-22 01:58:40"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1982-1991"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DiffTalk"
          ],
          [
            "Title",
            "DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper"
          ]
        ],
        "resource": "storage/i2745.pdf",
        "selectable": false
      },
      {
        "text": "EAMM",
        "item-id": "i2155",
        "nodes": [
          {
            "text": "Ji et al_2022_EAMM.pdf",
            "item-id": "i2172",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ji et al_2022_EAMM.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CLH6H8R2/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/2\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/3\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/3\">3.1 Audio2Facial-Dynamics Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/5\">3.2 Implicit Emotion Displacement Learner</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/6\">3.3 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/6\">4 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/6\">4.1 Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/7\">4.2 User Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/7\">4.3 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/8\">4.4 Limitations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/8\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/8\">6 Ethical Considerations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CLH6H8R2/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ji et al_2022_EAMM.pdf"
              ]
            ],
            "resource": "storage/i2172.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "EAMM",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate one-shot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zero- and first-order key-points motion. Then through exploring the motion model\u2019s properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired motion representations. Comprehensive experiments demonstrate that by incorporating the results from both modules, our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns."
          ],
          [
            "Access Date",
            "2022-12-12"
          ],
          [
            "Creators",
            "Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, Xun Cao"
          ],
          [
            "DOI",
            "10.1145/3528233.3530745"
          ],
          [
            "Date",
            "2022-00-24 \u4e03\u6708 24, 2022"
          ],
          [
            "ISBN",
            "978-1-4503-9337-9"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1\u201310"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "ACM SIGGRAPH 2022 Conference Proceedings"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "SIGGRAPH '22"
          ],
          [
            "Short Title",
            "EAMM"
          ],
          [
            "Title",
            "EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3528233.3530745"
          ]
        ],
        "resource": "storage/i2172.pdf",
        "selectable": false
      },
      {
        "text": "End-To-End Generation of Talking Faces from Noisy Speech",
        "item-id": "i1297",
        "nodes": [
          {
            "text": "Eskimez et al_2020_End-To-End Generation of Talking Faces from Noisy Speech.pdf",
            "item-id": "i1345",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Eskimez et al_2020_End-To-End Generation of Talking Faces from Noisy Speech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Eskimez et al_2020_End-To-End Generation of Talking Faces from Noisy Speech.pdf"
              ]
            ],
            "resource": "storage/i1345.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "End-To-End Generation of Talking Faces from Noisy Speech",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Acoustic cues are not the only component in speech communication; if the visual counterpart is present, it is shown to benefit speech comprehension. In this work, we propose an end-to-end (no pre- or post-processing) system that can generate talking faces from arbitrarily long noisy speech. We propose a mouth region mask to encourage the network to focus on mouth movements rather than speech irrelevant movements. In addition, we use generative adversarial network (GAN) training to improve the image quality and mouth-speech synchronization. Furthermore, we employ noise-resilient training to make our network robust to unseen non-stationary noise. We evaluate our system with image quality and mouth shape (landmark) measures on noisy speech utterances with five types of unseen non-stationary noise between -10 dB and 30 dB signal-to-noise ratio (SNR) with increments of 1 dB SNR. Results show that our system outperforms a state-of-the-art baseline system significantly, and our noise-resilient training improves performance for noisy speech in a wide range of SNR."
          ],
          [
            "Conference Name",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Sefik Emre Eskimez, Ross K. Maddox, Chenliang Xu, Zhiyao Duan"
          ],
          [
            "DOI",
            "10.1109/ICASSP40776.2020.9054103"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1948-1952"
          ],
          [
            "Proceedings Title",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "End-To-End Generation of Talking Faces from Noisy Speech"
          ]
        ],
        "resource": "storage/i1345.pdf",
        "selectable": false
      },
      {
        "text": "Everybody\u2019s Talkin\u2019",
        "item-id": "i1306",
        "nodes": [
          {
            "text": "Song et al_2022_Everybody\u2019s Talkin\u2019.pdf",
            "item-id": "i1365",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2022_Everybody\u2019s Talkin\u2019.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2022_Everybody\u2019s Talkin\u2019.pdf"
              ]
            ],
            "resource": "storage/i1365.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Everybody\u2019s Talkin\u2019",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photo-realistic video. This method is unique because it is highly dynamic. It does not assume a person-specific rendering network yet capable of translating one source audio into one random chosen video output within a set of speech videos. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we first factorize each target video frame into orthogonal parameter spaces, i.e., expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely mapped to the source audio. The geometry and pose parameters of the target human portrait are retained, therefore preserving the context of the original video footage. Finally, we introduce a novel video rendering network and a dynamic programming method to construct a temporally coherent and photo-realistic video. Extensive experiments demonstrate the superiority of our method over existing approaches. Our method is end-to-end learnable and robust to voice variations in the source audio."
          ],
          [
            "Creators",
            "Linsen Song, Wayne Wu, Chen Qian, Ran He, Chen Change Loy"
          ],
          [
            "DOI",
            "10.1109/TIFS.2022.3146783"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Information Forensics and Security"
          ],
          [
            "ISSN",
            "1556-6021"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "585-598"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Information Forensics and Security"
          ],
          [
            "Short Title",
            "Everybody\u2019s Talkin\u2019"
          ],
          [
            "Title",
            "Everybody\u2019s Talkin\u2019: Let Me Talk as You Want"
          ],
          [
            "Volume",
            "17"
          ]
        ],
        "resource": "storage/i1365.pdf",
        "selectable": false
      },
      {
        "text": "Expressive Talking Head Generation With Granular Audio-Visual Control",
        "item-id": "i1645",
        "nodes": [
          {
            "text": "Liang et al_2022_Expressive Talking Head Generation With Granular Audio-Visual Control.pdf",
            "item-id": "i1661",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liang et al_2022_Expressive Talking Head Generation With Granular Audio-Visual Control.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liang et al_2022_Expressive Talking Head Generation With Granular Audio-Visual Control.pdf"
              ]
            ],
            "resource": "storage/i1661.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Expressive Talking Head Generation With Granular Audio-Visual Control",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating expressive talking heads is essential for creating virtual humans. However, existing one- or few-shot methods focus on lip-sync and head motion, ignoring the emotional expressions that make talking faces realistic. In this paper, we propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT), which controls lip movements, head poses, and facial expressions of a talking head in a granular manner. Our insight is to decouple the audio-visual driving sources through prior-based pre-processing designs. Detailedly, we disassemble the driving image into three complementary parts including: 1) a cropped mouth that facilitates lip-sync; 2) a masked head that implicitly learns pose; and 3) the upper face which works corporately and complementarily with a time-shifted mouth to contribute the expression. Interestingly, the encoded features from the three sources are integrally balanced through reconstruction training. Extensive experiments show that our method generates expressive faces with not only synced mouth shapes, controllable poses, but precisely animated emotional expressions as well."
          ],
          [
            "Access Date",
            "2022-06-27 09:15:04"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin Hong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3387-3396"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Expressive Talking Head Generation With Granular Audio-Visual Control"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1661.pdf",
        "selectable": false
      },
      {
        "text": "FACIAL",
        "item-id": "i2150",
        "nodes": [
          {
            "text": "Zhang et al_2021_FACIAL.pdf",
            "item-id": "i2166",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2021_FACIAL.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2021_FACIAL.pdf"
              ]
            ],
            "resource": "storage/i2166.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FACIAL",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the attention map of eye blinks as input to generate the photo-realistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-12-12 11:03:36"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng, Saifeng Ni, Madhukar Budagavi, Xiaohu Guo"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3867-3876"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "FACIAL"
          ],
          [
            "Title",
            "FACIAL: Synthesizing Dynamic Talking Face With Implicit Attribute Learning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_FACIAL_Synthesizing_Dynamic_Talking_Face_With_Implicit_Attribute_Learning_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i2166.pdf",
        "selectable": false
      },
      {
        "text": "FENeRF",
        "item-id": "i2151",
        "nodes": [
          {
            "text": "Sun et al_2022_FENeRF.pdf",
            "item-id": "i2167",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sun et al_2022_FENeRF.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Y2Z4RU2I/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/2\">. Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/3\">. Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/3\">. Locally Editable NeRF Generator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/4\">. Discriminators</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/4\">. Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/5\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/5\">. Comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/6\">. Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/8\">. Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Y2Z4RU2I/8\">. Discussions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sun et al_2022_FENeRF.pdf"
              ]
            ],
            "resource": "storage/i2167.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FENeRF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Previous portrait image generation methods roughly fall into two categories: 2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but with low view consistency. 3D-aware GAN methods can maintain view consistency but their generated images are not locally editable. To overcome these limitations, we propose FENeRF, a 3D-aware generator that can produce view-consistent and locally-editable portrait images. Our method uses two decoupled latent codes to generate corresponding facial semantics and texture in a spatial-aligned 3D volume with shared geometry. Benefiting from such underlying 3D representation, FENeRF can jointly render the boundary-aligned image and semantic mask and use the semantic mask to edit the 3D volume via GAN inversion. We further show such 3D representation can be learned from widely available monocular image and semantic mask pairs. Moreover, we reveal that joint learning semantics and texture helps to generate finer geometry. Our experiments demonstrate that FENeRF outperforms state-of-the-art methods in various face editing tasks."
          ],
          [
            "Access Date",
            "2022-12-12 11:02:02"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, Jue Wang"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7672-7682"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "FENeRF"
          ],
          [
            "Title",
            "FENeRF: Face Editing in Neural Radiance Fields"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2167.pdf",
        "selectable": false
      },
      {
        "text": "FLNet",
        "item-id": "i3233",
        "nodes": [
          {
            "text": "Gu et al_2020_FLNet.pdf",
            "item-id": "i3330",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gu et al_2020_FLNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gu et al_2020_FLNet.pdf"
              ]
            ],
            "resource": "storage/i3330.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FLNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking face synthesis has been widely studied in either appearance-based or warping-based methods. Previous works mostly utilize single face image as a source, and generate novel facial animations by merging other person's facial features. However, some facial regions like eyes or teeth, which may be hidden in the source image, can not be synthesized faithfully and stably. In this paper, We present a landmark driven two-stream network to generate faithful talking facial animation, in which more facial details are created, preserved and transferred from multiple source images instead of a single one. Specifically, we propose a network consisting of a learning and fetching stream. The fetching sub-net directly learns to attentively warp and merge facial regions from five source images of distinctive landmarks, while the learning pipeline renders facial organs from the training face space to compensate. Compared to baseline algorithms, extensive experiments demonstrate that the proposed method achieves a higher performance both quantitatively and qualitatively. Codes are at https://github.com/kgu3/FLNet_AAAI2020."
          ],
          [
            "Access Date",
            "2023-12-27 13:28:49"
          ],
          [
            "Creators",
            "Kuangxiao Gu, Yuqian Zhou, Thomas Huang"
          ],
          [
            "DOI",
            "10.1609/aaai.v34i07.6717"
          ],
          [
            "Date",
            "2020-04-03 2020-04-03"
          ],
          [
            "Extra",
            "Number: 07"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "07"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "10861-10868"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c) 2020 Association for the Advancement of Artificial Intelligence"
          ],
          [
            "Short Title",
            "FLNet"
          ],
          [
            "Title",
            "FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/6717"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i3330.pdf",
        "selectable": false
      },
      {
        "text": "FSGAN",
        "item-id": "i1063",
        "nodes": [
          {
            "text": "Nirkin et al_2019_FSGAN.pdf",
            "item-id": "i1094",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Nirkin et al_2019_FSGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Nirkin et al_2019_FSGAN.pdf"
              ]
            ],
            "resource": "storage/i1094.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FSGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior."
          ],
          [
            "Access Date",
            "2021-10-18 14:23:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Yuval Nirkin, Yosi Keller, Tal Hassner"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7184-7193"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "FSGAN"
          ],
          [
            "Title",
            "FSGAN: Subject Agnostic Face Swapping and Reenactment"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i1094.pdf",
        "selectable": false
      },
      {
        "text": "Face2Face",
        "item-id": "i1062",
        "nodes": [
          {
            "text": "Thies et al_2016_Face2Face.pdf",
            "item-id": "i1092",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Thies et al_2016_Face2Face.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Thies et al_2016_Face2Face.pdf"
              ]
            ],
            "resource": "storage/i1092.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Face2Face",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time."
          ],
          [
            "Access Date",
            "2021-10-18 14:25:42"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, Matthias Niessner"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2387-2395"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Face2Face"
          ],
          [
            "Title",
            "Face2Face: Real-Time Face Capture and Reenactment of RGB Videos"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i1092.pdf",
        "selectable": false
      },
      {
        "text": "FaceFormer",
        "item-id": "i2156",
        "nodes": [
          {
            "text": "Fan et al_2022_FaceFormer.pdf",
            "item-id": "i2173",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Fan et al_2022_FaceFormer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Fan et al_2022_FaceFormer.pdf"
              ]
            ],
            "resource": "storage/i2173.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FaceFormer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Speech-driven 3D facial animation is challenging due to the complex geometry of human faces and the limited availability of 3D audio-visual data. Prior works typically focus on learning phoneme-level features of short audio windows with limited context, occasionally resulting in inaccurate lip movements. To tackle this limitation, we propose a Transformer-based autoregressive model, FaceFormer, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes. To cope with the data scarcity issue, we integrate the self-supervised pre-trained speech representations. Also, we devise two biased attention mechanisms well suited to this specific task, including the biased cross-modal multi-head (MH) attention and the biased causal MH self-attention with a periodic positional encoding strategy. The former effectively aligns the audio-motion modalities, whereas the latter offers abilities to generalize to longer audio sequences. Extensive experiments and a perceptual user study show that our approach outperforms the existing state-of-the-arts. The code and the video are available at: https://evelynfan.github.io/audio2face/."
          ],
          [
            "Access Date",
            "2022-12-12 09:09:55"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, Taku Komura"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "18770-18780"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "FaceFormer"
          ],
          [
            "Title",
            "FaceFormer: Speech-Driven 3D Facial Animation With Transformers"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Fan_FaceFormer_Speech-Driven_3D_Facial_Animation_With_Transformers_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2173.pdf",
        "selectable": false
      },
      {
        "text": "FaceShifter",
        "item-id": "i1060",
        "nodes": [
          {
            "text": "Comment: Accepted to CVPR 2020 (Oral), generated dataset and project webpage: lingzhili.com/FaceShifterPage/",
            "item-id": "n1089",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted to CVPR 2020 (Oral), generated dataset and project webpage: lingzhili.com/FaceShifterPage/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted to CVPR 2020 (Oral), generated dataset and project webpage: lingzhili.com/FaceShifterPage/</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2020_FaceShifter.pdf",
            "item-id": "i1088",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2020_FaceShifter.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2020_FaceShifter.pdf"
              ]
            ],
            "resource": "storage/i1088.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "FaceShifter",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we propose a novel two-stage framework, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, our framework, in its first stage, generates the swapped face in high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. We propose a novel attributes encoder for extracting multi-level target face attributes, and a new generator with carefully designed Adaptive Attentional Denormalization (AAD) layers to adaptively integrate the identity and the attributes for face synthesis. To address the challenging facial occlusions, we append a second stage consisting of a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is trained to recover anomaly regions in a self-supervised way without any manual annotations. Extensive experiments on wild faces demonstrate that our face swapping results are not only considerably more perceptually appealing, but also better identity preserving in comparison to other state-of-the-art methods."
          ],
          [
            "Access Date",
            "2021-10-18 14:34:46"
          ],
          [
            "Archiveid",
            "arXiv:1912.13457"
          ],
          [
            "Creators",
            "Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen"
          ],
          [
            "Date",
            "2020-09-15 2020-09-15"
          ],
          [
            "Extra",
            "arXiv:1912.13457 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "FaceShifter"
          ],
          [
            "Title",
            "FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1912.13457"
          ]
        ],
        "resource": "storage/i1088.pdf",
        "selectable": false
      },
      {
        "text": "Fast Face-Swap Using Convolutional Neural Networks",
        "item-id": "i1064",
        "nodes": [
          {
            "text": "Korshunova et al_2017_Fast Face-Swap Using Convolutional Neural Networks.pdf",
            "item-id": "i1097",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Korshunova et al_2017_Fast Face-Swap Using Convolutional Neural Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Korshunova et al_2017_Fast Face-Swap Using Convolutional Neural Networks.pdf"
              ]
            ],
            "resource": "storage/i1097.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Fast Face-Swap Using Convolutional Neural Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We consider the problem of face swapping in images, where an input identity is transformed into a target identity while preserving pose, facial expression and lighting. To perform this mapping, we use convolutional neural networks trained to capture the appearance of the target identity from an unstructured collection of his/her photographs. This approach is enabled by framing the face swapping problem in terms of style transfer, where the goal is to render an image in the style of another one. Building on recent advances in this area, we devise a new loss function that enables the network to produce highly photorealistic results. By combining neural networks with simple pre- and post-processing steps, we aim at making face swap work in real-time with no input from the user."
          ],
          [
            "Access Date",
            "2021-10-18 14:23:13"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Iryna Korshunova, Wenzhe Shi, Joni Dambre, Lucas Theis"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3677-3685"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Fast Face-Swap Using Convolutional Neural Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Korshunova_Fast_Face-Swap_Using_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i1097.pdf",
        "selectable": false
      },
      {
        "text": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
        "item-id": "i1056",
        "nodes": [
          {
            "text": "Zakharov et al_2019_Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.pdf",
            "item-id": "i1083",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zakharov et al_2019_Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zakharov et al_2019_Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.pdf"
              ]
            ],
            "resource": "storage/i1083.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings."
          ],
          [
            "Access Date",
            "2021-10-18 14:54:03"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9459-9468"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i1083.pdf",
        "selectable": false
      },
      {
        "text": "Few-shot Adversarial Audio Driving Talking Face Generation",
        "item-id": "i1324",
        "nodes": [
          {
            "text": "Chen_Xiong_2021_Few-shot Adversarial Audio Driving Talking Face Generation.pdf",
            "item-id": "i1397",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen_Xiong_2021_Few-shot Adversarial Audio Driving Talking Face Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen_Xiong_2021_Few-shot Adversarial Audio Driving Talking Face Generation.pdf"
              ]
            ],
            "resource": "storage/i1397.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Few-shot Adversarial Audio Driving Talking Face Generation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking-face generation is an interesting and challenging problem in computer vision and has become a research focus. This project aims to generate real talking-face video sequences, especially lip synchronization and head motion. In order to create a personalized talking-face model, these works require training on large-scale audio-visual datasets. However, in many practical scenarios, the personalized appearance features, and audio-video synchronization relationships need to be learned from a few lip synchronization sequences. In this paper, we consider it as a few-shot image synchronization problem: synthesizing talking-face with audio if there are additionally a few lip-synchronized video sequences as the learning task? We apply the reptile methods to train the meta adversarial networks and this meta-model could be adapted on just a few references sequences and done quickly to learn the personalized references models. With meta-learning on the dataset, the model can learn the initialization parameters. And with few adapt steps on the reference sequences, the model can learn quickly and generate highly realistic images with more facial texture and lip-sync. Experiments on several datasets demonstrated significantly better results obtained by our methods than the state-of-the-art methods in both quantitative and quantitative comparisons."
          ],
          [
            "Access Date",
            "2022-02-12"
          ],
          [
            "Conference Name",
            "2021 3rd International Conference on Advanced Information Science and System (AISS 2021)"
          ],
          [
            "Creators",
            "Ruyi Chen, Shengwu Xiong"
          ],
          [
            "DOI",
            "10.1145/3503047.3503054"
          ],
          [
            "Date",
            "2021-11-26 2021-11-26"
          ],
          [
            "ISBN",
            "978-1-4503-8586-2"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1\u20136"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "2021 3rd International Conference on Advanced Information Science and System (AISS 2021)"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "AISS 2021"
          ],
          [
            "Title",
            "Few-shot Adversarial Audio Driving Talking Face Generation"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3503047.3503054"
          ]
        ],
        "resource": "storage/i1397.pdf",
        "selectable": false
      },
      {
        "text": "Generative Adversarial Networks for face generation",
        "item-id": "i1529",
        "nodes": [
          {
            "text": "Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf",
            "item-id": "i1550",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf"
              ]
            ],
            "resource": "storage/i1550.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative Adversarial Networks for face generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues."
          ],
          [
            "Access Date",
            "2022-05-12 15:31:57"
          ],
          [
            "Creators",
            "Amina Kammoun, Rim Slama, Hedi Tabia, Tarek Ouni, Mohmed Abid"
          ],
          [
            "DOI",
            "10.1145/1122445.1122456"
          ],
          [
            "Date",
            "2022-03-20 2022-03-20"
          ],
          [
            "Extra",
            "Just Accepted"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Generative Adversarial Networks for face generation"
          ],
          [
            "Title",
            "Generative Adversarial Networks for face generation: A survey"
          ],
          [
            "URL",
            "http://doi.org/10.1145/1122445.1122456"
          ]
        ],
        "resource": "storage/i1550.pdf",
        "selectable": false
      },
      {
        "text": "Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss",
        "item-id": "i1330",
        "nodes": [
          {
            "text": "Chen et al_2019_Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss.pdf",
            "item-id": "i1408",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2019_Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2019_Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss.pdf"
              ]
            ],
            "resource": "storage/i1408.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers sequence-level information along with frame-level information. Thoughtful experiments on several datasets and real-world samples demonstrate significantly better results obtained by our method than the state-of-the-art methods in both quantitative and qualitative comparisons."
          ],
          [
            "Access Date",
            "2022-02-02 06:51:05"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Lele Chen, Ross K. Maddox, Zhiyao Duan, Chenliang Xu"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7832-7841"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Hierarchical_Cross-Modal_Talking_Face_Generation_With_Dynamic_Pixel-Wise_Loss_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i1408.pdf",
        "selectable": false
      },
      {
        "text": "High-Speed and High-Quality Text-to-Lip Generation",
        "item-id": "i916",
        "nodes": [
          {
            "text": "Comment: Author draft",
            "item-id": "n929",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Author draft",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Author draft</div>",
            "node_type": "note"
          },
          {
            "text": "Liu et al_2021_High-Speed and High-Quality Text-to-Lip Generation.pdf",
            "item-id": "i928",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2021_High-Speed and High-Quality Text-to-Lip Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2021_High-Speed and High-Quality Text-to-Lip Generation.pdf"
              ]
            ],
            "resource": "storage/i928.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "High-Speed and High-Quality Text-to-Lip Generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video. Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner. However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation. This encourages the research of parallel T2L generation. In this work, we propose a novel parallel decoding model for high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner. Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem. Extensive experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L generates lip movements with competitive quality compared with the state-of-the-art AR T2L model DualLip and exceeds the baseline AR model TransformerT2L by a notable margin benefiting from the mitigation of the error propagation problem; and 2) exhibits distinct superiority in inference speed (an average speedup of 19$\\times$ than DualLip on TCD-TIMIT)."
          ],
          [
            "Access Date",
            "2021-08-24 04:32:43"
          ],
          [
            "Creators",
            "Jinglin Liu, Zhiying Zhu, Yi Ren, Zhou Zhao"
          ],
          [
            "Date",
            "2021-07-14 2021-07-14"
          ],
          [
            "Extra",
            "arXiv: 2107.06831"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2107.06831 [cs]"
          ],
          [
            "Title",
            "High-Speed and High-Quality Text-to-Lip Generation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2107.06831"
          ]
        ],
        "resource": "storage/i928.pdf",
        "selectable": false
      },
      {
        "text": "Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis",
        "item-id": "i1311",
        "nodes": [
          {
            "text": "Wu et al_2021_Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face.pdf",
            "item-id": "i1374",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2021_Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RHXYPBHG/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/3\">2 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/3\">3 Problem Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/3\">4 Talking Style Observation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/5\">5 Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/5\">5.1 Stylized 3D Talking Face Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/5\">5.2 Photorealistic Render</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/6\">6 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/6\">6.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/6\">6.2 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/6\">6.3 Comparison with VOCA on Style Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/7\">6.4 Study on the Style Space</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/7\">6.5 Comparison with One-Shot Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/8\">6.6 Effectiveness of Latent Style Fusion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/8\">7 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/8\">8 Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RHXYPBHG/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2021_Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face.pdf"
              ]
            ],
            "resource": "storage/i1374.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-book",
        "item_title": "Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis",
        "item_type": "bookSection",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "People talk with diversified styles. For one piece of speech, different talking styles exhibit significant differences in the facial and head pose movements. For example, the \"excited\" style usually talks with the mouth wide open, while the \"solemn\" style is more standardized and seldomly exhibits exaggerated motions. Due to such huge differences between different styles, it is necessary to incorporate the talking style into audio-driven talking face synthesis framework. In this paper, we propose to inject style into the talking face synthesis framework through imitating arbitrary talking style of the particular reference video. Specifically, we systematically investigate talking styles with our collected Ted-HD dataset and construct style codes as several statistics of 3D morphable model (3DMM) parameters. Afterwards, we devise a latent-style-fusion (LSF) model to synthesize stylized talking faces by imitating talking styles from the style codes. We emphasize the following novel characteristics of our framework: (1) It doesn't require any annotation of the style, the talking style is learned in an unsupervised manner from talking videos in the wild. (2) It can imitate arbitrary styles from arbitrary videos, and the style codes can also be interpolated to generate new styles. Extensive experiments demonstrate that the proposed framework has the ability to synthesize more natural and expressive talking styles compared with baseline methods."
          ],
          [
            "Access Date",
            "2022-02-14"
          ],
          [
            "Book Title",
            "Proceedings of the 29th ACM International Conference on Multimedia"
          ],
          [
            "Creators",
            "Haozhe Wu, Jia Jia, Haoyu Wang, Yishun Dou, Chao Duan, Qingshan Deng"
          ],
          [
            "Date",
            "2021-10-17 2021-10-17"
          ],
          [
            "ISBN",
            "978-1-4503-8651-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1478\u20131486"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Title",
            "Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3474085.3475280"
          ]
        ],
        "resource": "storage/i1374.pdf",
        "selectable": false
      },
      {
        "text": "Learning Dynamic Facial Radiance Fields for\u00a0Few-Shot Talking Head Synthesis",
        "item-id": "i2185",
        "nodes": [
          {
            "text": "Full Text PDF",
            "item-id": "i2195",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Full Text PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-12-23 14:30:43"
              ],
              [
                "Title",
                "Full Text PDF"
              ],
              [
                "URL",
                "https://link.springer.com/content/pdf/10.1007%2F978-3-031-19775-8_39.pdf"
              ]
            ],
            "resource": "storage/i2195.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Dynamic Facial Radiance Fields for\u00a0Few-Shot Talking Head Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking head synthesis is an emerging technology with wide applications in film dubbing, virtual avatars and online education. Recent NeRF-based methods generate more natural talking videos, as they better capture the 3D structural information of faces. However, a specific model needs to be trained for each identity with a large dataset. In this paper, we propose Dynamic Facial Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly generalize to an unseen identity with few training data. Different from the existing NeRF-based methods which directly encode the 3D geometry and appearance of a specific person into the network, our DFRF conditions face radiance field on 2D appearance images to learn the face prior. Thus the facial radiance field can be flexibly adjusted to the new identity with few reference images. Additionally, for better modeling of the facial deformations, we propose a differentiable face warping module conditioned on audio signals to deform all reference images to the query space. Extensive experiments show that with only tens of seconds of training clip available, our proposed DFRF can synthesize natural and high-quality audio-driven talking head videos for novel identities with only 40k iterations. We highly recommend readers view our supplementary video for intuitive comparisons. Code is available in https://sstzal.github.io/DFRF/."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou, Jiwen Lu, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19775-8_39"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19775-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "666-682"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Learning Dynamic Facial Radiance Fields for\u00a0Few-Shot Talking Head Synthesis"
          ]
        ],
        "resource": "storage/i2195.pdf",
        "selectable": false
      },
      {
        "text": "LipSync3D",
        "item-id": "i917",
        "nodes": [
          {
            "text": "Lahiri et al_2021_LipSync3D.pdf",
            "item-id": "i930",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lahiri et al_2021_LipSync3D.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lahiri et al_2021_LipSync3D.pdf"
              ]
            ],
            "resource": "storage/i930.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "LipSync3D",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we present a video-based learning framework for animating personalized 3D talking faces from audio. We introduce two training-time data normalizations that significantly improve data sample efficiency. First, we isolate and represent faces in a normalized space that decouples 3D geometry, head pose, and texture. This decomposes the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo constancy of skin to isolate and remove spatiotemporal lighting variations. Together, these normalizations allow simple networks to generate high fidelity lip-sync videos under novel ambient illumination while training with just a single video (of usually < 5 minutes). Further, to stabilize temporal dynamics, we introduce an auto-regressive approach that conditions the model on its previous visual state. Human ratings and objective metrics demonstrate that our method outperforms contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications enabled by our framework."
          ],
          [
            "Access Date",
            "2021-08-24 04:31:06"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis, Chris Bregler"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2755-2764"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "LipSync3D"
          ],
          [
            "Title",
            "LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces From Video Using Pose and Lighting Normalization"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i930.pdf",
        "selectable": false
      },
      {
        "text": "MakeltTalk",
        "item-id": "i1675",
        "nodes": [
          {
            "text": "Zhou et al_2020_MakeltTalk.pdf",
            "item-id": "i1677",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhou et al_2020_MakeltTalk.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_HS3CENX8/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HS3CENX8/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/4\">3.1 Speech Content Animation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/5\">3.2 Speaker-Aware Animation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/5\">3.3 Single-Image Animation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HS3CENX8/6\">4 Training</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/6\">4.1 Speech Content Animation Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/7\">4.2 Speaker-Aware Animation Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/7\">4.3 Image-to-Image Translation Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/7\">4.4 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HS3CENX8/8\">5 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/8\">5.1 Animating Non-Photorealistic Images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/8\">5.2 Animating Human Facial Images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/9\">5.3 Evaluation Protocol</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/9\">5.4 Content Animation Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/11\">5.5 Speaker-Aware Animation Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/11\">5.6 Ablation study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/12\">5.7 User Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/13\">5.8 Applications</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/13\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/14\">7 Ethical Considerations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/14\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/14\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/15\">A Speaker-Aware Animation network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HS3CENX8/15\">B Image-to-image translation network</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhou et al_2020_MakeltTalk.pdf"
              ]
            ],
            "resource": "storage/i1677.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MakeltTalk",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a method that generates expressive talking-head videos from a single facial image with audio as the only input. In contrast to previous attempts to learn direct mappings from audio to raw pixels for creating talking faces, our method first disentangles the content and speaker information in the input audio signal. The audio content robustly controls the motion of lips and nearby facial regions, while the speaker information determines the specifics of facial expressions and the rest of the talking-head dynamics. Another key component of our method is the prediction of facial landmarks reflecting the speaker-aware dynamics. Based on this intermediate representation, our method works with many portrait images in a single unified framework, including artistic paintings, sketches, 2D cartoon characters, Japanese mangas, and stylized caricatures. In addition, our method generalizes well for faces and characters that were not observed during training. We present extensive quantitative and qualitative evaluation of our method, in addition to user studies, demonstrating generated talking-heads of significantly higher quality compared to prior state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-07-06 01:48:52"
          ],
          [
            "Creators",
            "Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, Dingzeyu Li"
          ],
          [
            "DOI",
            "10.1145/3414685.3417774"
          ],
          [
            "Date",
            "2020-11-26 2020-11-26"
          ],
          [
            "ISSN",
            "0730-0301"
          ],
          [
            "Issue",
            "6"
          ],
          [
            "Journal Abbreviation",
            "ACM Trans. Graph."
          ],
          [
            "Library Catalog",
            "December 2020"
          ],
          [
            "Pages",
            "221:1\u2013221:15"
          ],
          [
            "Publication Title",
            "ACM Transactions on Graphics"
          ],
          [
            "Short Title",
            "MakeltTalk"
          ],
          [
            "Title",
            "MakeltTalk: speaker-aware talking-head animation"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3414685.3417774"
          ],
          [
            "Volume",
            "39"
          ]
        ],
        "resource": "storage/i1677.pdf",
        "selectable": false
      },
      {
        "text": "MeshTalk",
        "item-id": "i1314",
        "nodes": [
          {
            "text": "Richard et al_2021_MeshTalk.pdf",
            "item-id": "i1377",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Richard et al_2021_MeshTalk.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Richard et al_2021_MeshTalk.pdf"
              ]
            ],
            "resource": "storage/i1377.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MeshTalk",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a generic method for generating full facial 3D animation from speech. Existing approaches to audio-driven facial animation exhibit uncanny or static upper face animation, fail to produce accurate and plausible co-articulation or rely on person-specific models that limit their scalability. To improve upon existing models, we propose a generic audio-driven facial animation approach that achieves highly realistic motion synthesis results for the entire face. At the core of our approach is a categorical latent space for facial animation that disentangles audio-correlated and audio-uncorrelated information based on a novel cross-modality loss. Our approach ensures highly accurate lip motion, while also synthesizing plausible animation of the parts of the face that are uncorrelated to the audio signal, such as eye blinks and eye brow motion. We demonstrate that our approach outperforms several baselines and obtains state-of-the-art quality both qualitatively and quantitatively. A perceptual user study demonstrates that our approach is deemed more realistic than the current state-of-the-art in over 75% of cases. We recommend watching the supplemental video before reading the paper: https://github.com/facebookresearch/meshtalk"
          ],
          [
            "Access Date",
            "2022-02-14 07:36:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Alexander Richard, Michael Zollh\u00f6fer, Yandong Wen, Fernando de la Torre, Yaser Sheikh"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1173-1182"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "MeshTalk"
          ],
          [
            "Title",
            "MeshTalk: 3D Face Animation From Speech Using Cross-Modality Disentanglement"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1377.pdf",
        "selectable": false
      },
      {
        "text": "MoCoGAN",
        "item-id": "i1118",
        "nodes": [
          {
            "text": "Tulyakov et al_2018_MoCoGAN.pdf",
            "item-id": "i1142",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tulyakov et al_2018_MoCoGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tulyakov et al_2018_MoCoGAN.pdf"
              ]
            ],
            "resource": "storage/i1142.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MoCoGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion. Our code is available at https://github.com/sergeytulyakov/mocogan."
          ],
          [
            "Access Date",
            "2021-10-24 02:05:41"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1526-1535"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MoCoGAN"
          ],
          [
            "Title",
            "MoCoGAN: Decomposing Motion and Content for Video Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i1142.pdf",
        "selectable": false
      },
      {
        "text": "Multimodal Image Synthesis and Editing",
        "item-id": "i1303",
        "nodes": [
          {
            "text": "Comment: 20 pages, 19 figures",
            "item-id": "n1359",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 20 pages, 19 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 20 pages, 19 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf",
            "item-id": "i1358",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf"
              ]
            ],
            "resource": "storage/i1358.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multimodal Image Synthesis and Editing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modelling the interaction among multimodal information, multimodal image synthesis and editing have become a hot research topic in recent years. Different from traditional visual guidance which provides explicit clues, multimodal guidance offers intuitive and flexible means in image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of features with inherent modality gaps, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis \\& editing and formulate taxonomies according to data modality and model architectures. We start with an introduction to different types of guidance modalities in image synthesis and editing. We then describe multimodal image synthesis and editing approaches extensively with detailed frameworks including Generative Adversarial Networks (GANs), GAN Inversion, Transformers, and other methods such as NeRF and Diffusion models. This is followed by a comprehensive description of benchmark datasets and corresponding evaluation metrics as widely adopted in multimodal image synthesis and editing, as well as detailed comparisons of different synthesis methods with analysis of respective advantages and limitations. Finally, we provide insights into the current research challenges and possible future research directions. A project associated with this survey is available at https://github.com/fnzhan/MISE"
          ],
          [
            "Access Date",
            "2022-02-18 00:42:09"
          ],
          [
            "Creators",
            "Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu"
          ],
          [
            "Date",
            "2021-12-27 2021-12-27"
          ],
          [
            "Extra",
            "arXiv: 2112.13592"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2112.13592 [cs]"
          ],
          [
            "Short Title",
            "Multimodal Image Synthesis and Editing"
          ],
          [
            "Title",
            "Multimodal Image Synthesis and Editing: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2112.13592"
          ]
        ],
        "resource": "storage/i1358.pdf",
        "selectable": false
      },
      {
        "text": "NWT",
        "item-id": "i1317",
        "nodes": [
          {
            "text": "Mama et al_2021_NWT.pdf",
            "item-id": "i1383",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mama et al_2021_NWT.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZCK7ED9U/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/2\">2 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/3\">2.1 dVAE-Adv model</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/3\">2.1.1 Attention as a multinomial discretization function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/4\">2.1.2 Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/5\">2.1.3 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/5\">2.1.4 Attention as a memory-augmentation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/6\">2.2 Prior autoregressive model</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/6\">2.2.1 Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/6\">2.2.2 Architecture</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/6\">3 Experiments and results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/6\">3.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/7\">3.2 Model details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/7\">3.3 Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/8\">3.4 Episode control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/8\">3.5 Style control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/9\">3.6 Video Compression</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/9\">4 Applications and broader impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/9\">5 Future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/9\">6 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/14\">A Exploratory Memcode analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/14\">A.1 Interpretable representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/15\">A.2 Categorical distance</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B Model architectures</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B.1 dVAE-Adv</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B.1.1 Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B.1.2 Hard attention discretization layer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B.1.3 Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B.1.4 Critics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/17\">B.1.5 Gradient approximation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/18\">B.2 Prior autoregressive model</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/18\">B.2.1 Audio encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/18\">B.2.2 Style encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/19\">B.2.3 Video Memcode decoder</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/19\">B.3 Episode embedding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/19\">C Model training and parameters</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/19\">C.1 Training algorithm</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/20\">C.2 Parameters</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/20\">C.2.1 dVAE-Adv</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/20\">C.2.2 Prior autoregressive model</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/21\">C.3 FAR vs MAR</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/24\">D Data preparation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZCK7ED9U/25\">E Experimental setup of MOS</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mama et al_2021_NWT.pdf"
              ]
            ],
            "resource": "storage/i1383.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "NWT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work we introduce NWT, an expressive speech-to-video model. Unlike approaches that use domain-specific intermediate representations such as pose keypoints, NWT learns its own latent representations, with minimal assumptions about the audio and video content. To this end, we propose a novel discrete variational autoencoder with adversarial loss, dVAE-Adv, which learns a new discrete latent representation we call Memcodes. Memcodes are straightforward to implement, require no additional loss terms, are stable to train compared with other approaches, and show evidence of interpretability. To predict on the Memcode space, we use an autoregressive encoder-decoder model conditioned on audio. Additionally, our model can control latent attributes in the generated video that are not annotated in the data. We train NWT on clips from HBO's Last Week Tonight with John Oliver. NWT consistently scores above other approaches in Mean Opinion Score (MOS) on tests of overall video naturalness, facial naturalness and expressiveness, and lipsync quality. This work sets a strong baseline for generalized audio-to-video synthesis. Samples are available at https://next-week-tonight.github.io/NWT/."
          ],
          [
            "Access Date",
            "2022-02-14 07:11:16"
          ],
          [
            "Archiveid",
            "arXiv:2106.04283"
          ],
          [
            "Creators",
            "Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, Cole Clifford, Ragavan Thurairatnam"
          ],
          [
            "Date",
            "2021-06-08 2021-06-08"
          ],
          [
            "Extra",
            "arXiv:2106.04283 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "NWT"
          ],
          [
            "Title",
            "NWT: Towards natural audio-to-video generation with representation learning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2106.04283"
          ]
        ],
        "resource": "storage/i1383.pdf",
        "selectable": false
      },
      {
        "text": "Neural Head Avatars From Monocular RGB Videos",
        "item-id": "i2148",
        "nodes": [
          {
            "text": "Grassal et al_2022_Neural Head Avatars From Monocular RGB Videos.pdf",
            "item-id": "i2163",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Grassal et al_2022_Neural Head Avatars From Monocular RGB Videos.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Grassal et al_2022_Neural Head Avatars From Monocular RGB Videos.pdf"
              ]
            ],
            "resource": "storage/i2163.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Head Avatars From Monocular RGB Videos",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in AR/VR or other applications in the movie or games industry that rely on a digital human. Our representation can be learned from a monocular RGB portrait video that features a range of different expressions and views. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture. We demonstrate that this representation is able to accurately extrapolate to unseen poses and view points, and generates natural expressions while providing sharp texture details. Compared to previous works on head avatars, our method provides a disentangled shape and appearance model of the complete human head (including hair) that is compatible with the standard graphics pipeline. Moreover, it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novel-view synthesis."
          ],
          [
            "Access Date",
            "2022-12-12 11:07:04"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Philip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias Nie\u00dfner, Justus Thies"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "18653-18664"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Neural Head Avatars From Monocular RGB Videos"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2163.pdf",
        "selectable": false
      },
      {
        "text": "Neural Head Reenactment with Latent Pose Descriptors",
        "item-id": "i1426",
        "nodes": [
          {
            "text": "Burkov et al_2020_Neural Head Reenactment with Latent Pose Descriptors.pdf",
            "item-id": "i1469",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Burkov et al_2020_Neural Head Reenactment with Latent Pose Descriptors.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Burkov et al_2020_Neural Head Reenactment with Latent Pose Descriptors.pdf"
              ]
            ],
            "resource": "storage/i1469.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Head Reenactment with Latent Pose Descriptors",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a neural head reenactment system, which is driven by a latent pose representation and is capable of predicting the foreground segmentation alongside the RGB image. The latent pose representation is learned as a part of the entire reenactment system, and the learning process is based solely on image reconstruction losses. We show that despite its simplicity, with a large and diverse enough training dataset, such learning successfully decomposes pose from identity. The resulting system can then reproduce mimics of the driving person and, furthermore, can perform cross-person reenactment. Additionally, we show that the learned descriptors are useful for other pose-related tasks, such as keypoint prediction and pose-based retrieval."
          ],
          [
            "Access Date",
            "2022-04-03 14:46:20"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Egor Burkov, Igor Pasechnik, Artur Grigorev, Victor Lempitsky"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "13786-13795"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Neural Head Reenactment with Latent Pose Descriptors"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Burkov_Neural_Head_Reenactment_with_Latent_Pose_Descriptors_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i1469.pdf",
        "selectable": false
      },
      {
        "text": "Neural Voice Puppetry",
        "item-id": "i1054",
        "nodes": [
          {
            "text": "Thies et al_2020_Neural Voice Puppetry.pdf",
            "item-id": "i1081",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Thies et al_2020_Neural Voice Puppetry.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_N3362UAE/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/3\">2 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/5\">3 Overview</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_N3362UAE/6\">4 Data</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/6\">4.1 Preprocessing:</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_N3362UAE/7\">5 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/7\">5.1 Audio2ExpressionNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/9\">5.2 Neural Face Rendering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/9\">5.3 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/10\">5.4 Inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_N3362UAE/10\">6 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/11\">6.1 Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/12\">6.2 Comparisons to State-of-the-art Methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/14\">7 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/14\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_N3362UAE/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Thies et al_2020_Neural Voice Puppetry.pdf"
              ]
            ],
            "resource": "storage/i1081.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Voice Puppetry",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis (Video, Code and Demo: https://justusthies.github.io/posts/neural-voice-puppetry/). Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, Matthias Nie\u00dfner, Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58517-4_42"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58517-4"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "716-731"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "Neural Voice Puppetry"
          ],
          [
            "Title",
            "Neural Voice Puppetry: Audio-Driven Facial Reenactment"
          ]
        ],
        "resource": "storage/i1081.pdf",
        "selectable": false
      },
      {
        "text": "ObamaNet",
        "item-id": "i1428",
        "nodes": [
          {
            "text": "Kumar et al_2017_ObamaNet.pdf",
            "item-id": "i1471",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kumar et al_2017_ObamaNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kumar et al_2017_ObamaNet.pdf"
              ]
            ],
            "resource": "storage/i1471.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ObamaNet",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present ObamaNet, the first architecture that generates both audio and synchronized photo-realistic lip-sync videos from any new text. Contrary to other published lip-sync approaches, ours is only composed of fully trainable neural modules and does not rely on any traditional computer graphics methods. More precisely, we use three main modules: a text-to-speech network based on Char2Wav, a time-delayed LSTM to generate mouth-keypoints synced to the audio, and a network based on Pix2Pix to generate the video frames conditioned on the keypoints."
          ],
          [
            "Access Date",
            "2022-04-03 14:45:18"
          ],
          [
            "Archiveid",
            "arXiv:1801.01442"
          ],
          [
            "Creators",
            "Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexandre de Brebisson, Yoshua Bengio"
          ],
          [
            "Date",
            "2017-12-06 2017-12-06"
          ],
          [
            "Extra",
            "arXiv:1801.01442 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ObamaNet"
          ],
          [
            "Title",
            "ObamaNet: Photo-realistic lip-sync from text"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1801.01442"
          ]
        ],
        "resource": "storage/i1471.pdf",
        "selectable": false
      },
      {
        "text": "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing",
        "item-id": "i1515",
        "nodes": [
          {
            "text": "Wang et al_2021_One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.pdf",
            "item-id": "i1517",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2021_One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2021_One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.pdf"
              ]
            ],
            "resource": "storage/i1517.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a neural talking-head video synthesis model and demonstrate its application to video conferencing. Our model learns to synthesize a talking-head video using a source image containing the target person's appearance and a driving video that dictates the motion in the output. Our motion is encoded based on a novel keypoint representation, where the identity-specific and motion-related information is decomposed unsupervisedly. Extensive experimental validation shows that our model outperforms competing methods on benchmark datasets. Moreover, our compact keypoint representation enables a video conferencing system that achieves the same visual quality as the commercial H.264 standard while only using one-tenth of the bandwidth. Besides, we show our keypoint representation allows the user to rotate the head during synthesis, which is useful for simulating face-to-face video conferencing experiences."
          ],
          [
            "Access Date",
            "2022-05-02 07:46:11"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ting-Chun Wang, Arun Mallya, Ming-Yu Liu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10039-10049"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1517.pdf",
        "selectable": false
      },
      {
        "text": "One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation Learning",
        "item-id": "i2154",
        "nodes": [
          {
            "text": "Wang et al_2022_One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation.pdf",
            "item-id": "i2171",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation.pdf"
              ]
            ],
            "resource": "storage/i2171.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio-driven one-shot talking face generation methods are usually trained on video resources of various persons. However, their created videos often suffer unnatural mouth shapes and asynchronous lips because those methods struggle to learn a consistent speech style from different speakers. We observe that it would be much easier to learn a consistent speech style from a specific speaker, which leads to authentic mouth movements. Hence, we propose a novel one-shot talking face generation framework by exploring consistent correlations between audio and visual motions from a specific speaker and then transferring audio-driven motion fields to a reference image. Specifically, we develop an Audio-Visual Correlation Transformer (AVCT) that aims to infer talking motions represented by keypoint based dense motion fields from an input audio. In particular, considering audio may come from different identities in deployment, we incorporate phonemes to represent audio signals. In this manner, our AVCT can inherently generalize to audio spoken by other identities. Moreover, as face keypoints are used to represent speakers, AVCT is agnostic against appearances of the training speaker, and thus allows us to manipulate face images of different identities readily. Considering different face shapes lead to different motions, a motion field transfer module is exploited to reduce the audio-driven dense motion field gap between the training identity and the one-shot reference. Once we obtained the dense motion field of the reference image, we employ an image renderer to generate its talking face videos from an audio clip. Thanks to our learned consistent speaking style, our method generates authentic mouth shapes and vivid movements. Extensive experiments demonstrate that our synthesized videos outperform the state-of-the-art in terms of visual quality and lip-sync."
          ],
          [
            "Access Date",
            "2022-12-12 10:54:33"
          ],
          [
            "Conference Name",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Creators",
            "Suzhen Wang, Lincheng Li, Yu Ding, Xin Yu"
          ],
          [
            "DOI",
            "10.1609/aaai.v36i3.20154"
          ],
          [
            "Date",
            "2022-06-28 2022-06-28"
          ],
          [
            "Extra",
            "Number: 3"
          ],
          [
            "ISBN",
            "2374-3468"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "2531-2539"
          ],
          [
            "Proceedings Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c) 2022 Association for the Advancement of Artificial Intelligence"
          ],
          [
            "Title",
            "One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation Learning"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/20154"
          ],
          [
            "Volume",
            "36(3)"
          ]
        ],
        "resource": "storage/i2171.pdf",
        "selectable": false
      },
      {
        "text": "PIRenderer",
        "item-id": "i1646",
        "nodes": [
          {
            "text": "Ren et al_2021_PIRenderer.pdf",
            "item-id": "i1663",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ren et al_2021_PIRenderer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ren et al_2021_PIRenderer.pdf"
              ]
            ],
            "resource": "storage/i1663.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PIRenderer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive control, semantically meaningful and fully disentangled parameters should be used as modifications. However, many existing techniques do not provide such fine-grained controls or use indirect editing methods i.e. mimic motions of other individuals. In this paper, a Portrait Image Neural Renderer (PIRenderer) is proposed to control the face motions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superiority of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by extracting sequential motions from audio inputs. We show that our model can generate coherent videos with convincing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender."
          ],
          [
            "Access Date",
            "2022-06-25 01:34:57"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Yurui Ren, Ge Li, Yuanqi Chen, Thomas H. Li, Shan Liu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "13759-13768"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "PIRenderer"
          ],
          [
            "Title",
            "PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Ren_PIRenderer_Controllable_Portrait_Image_Generation_via_Semantic_Neural_Rendering_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1663.pdf",
        "selectable": false
      },
      {
        "text": "Photorealistic Audio-driven Video Portraits",
        "item-id": "i1323",
        "nodes": [
          {
            "text": "Wen et al_2020_Photorealistic Audio-driven Video Portraits.pdf",
            "item-id": "i1395",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wen et al_2020_Photorealistic Audio-driven Video Portraits.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wen et al_2020_Photorealistic Audio-driven Video Portraits.pdf"
              ]
            ],
            "resource": "storage/i1395.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Photorealistic Audio-driven Video Portraits",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Video portraits are common in a variety of applications, such as videoconferencing, news broadcasting, and virtual education and training. We present a novel method to synthesize photorealistic video portraits for an input portrait video, automatically driven by a person's voice. The main challenge in this task is the hallucination of plausible, photorealistic facial expressions from input speech audio. To address this challenge, we employ a parametric 3D face model represented by geometry, facial expression, illumination, etc., and learn a mapping from audio features to model parameters. The input source audio is first represented as a high-dimensional feature, which is used to predict facial expression parameters of the 3D face model. We then replace the expression parameters computed from the original target video with the predicted one, and rerender the reenacted face. Finally, we generate a photorealistic video portrait from the reenacted synthetic face sequence via a neural face renderer. One appealing feature of our approach is the generalization capability for various input speech audio, including synthetic speech audio from text-to-speech software. Extensive experimental results show that our approach outperforms previous general-purpose audio-driven video portrait methods. This includes a user study demonstrating that our results are rated as more realistic than previous methods."
          ],
          [
            "Creators",
            "Xin Wen, Miao Wang, Christian Richardt, Ze-Yin Chen, Shi-Min Hu"
          ],
          [
            "DOI",
            "10.1109/TVCG.2020.3023573"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Visualization and Computer Graphics"
          ],
          [
            "ISSN",
            "1941-0506"
          ],
          [
            "Issue",
            "12"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "3457-3466"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Visualization and Computer Graphics"
          ],
          [
            "Title",
            "Photorealistic Audio-driven Video Portraits"
          ],
          [
            "Volume",
            "26"
          ]
        ],
        "resource": "storage/i1395.pdf",
        "selectable": false
      },
      {
        "text": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation",
        "item-id": "i914",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1639",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>PC-AVS</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Zhou et al_2021_Pose-Controllable Talking Face Generation by Implicitly Modularized.pdf",
            "item-id": "i923",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhou et al_2021_Pose-Controllable Talking Face Generation by Implicitly Modularized.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhou et al_2021_Pose-Controllable Talking Face Generation by Implicitly Modularized.pdf"
              ]
            ],
            "resource": "storage/i923.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on non-aligned raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework. Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are controllable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization."
          ],
          [
            "Access Date",
            "2021-08-24 04:36:57"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, Ziwei Liu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4176-4186"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Representation_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i923.pdf",
        "selectable": false
      },
      {
        "text": "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis",
        "item-id": "i2766",
        "nodes": [
          {
            "text": "Wang et al_2023_Progressive Disentangled Representation Learning for Fine-Grained Controllable.pdf",
            "item-id": "i2768",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_Progressive Disentangled Representation Learning for Fine-Grained Controllable.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_Progressive Disentangled Representation Learning for Fine-Grained Controllable.pdf"
              ]
            ],
            "resource": "storage/i2768.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them. To effectively disentangle each motion factor, we propose a progressive disentangled representation learning strategy by separating the factors in a coarse-to-fine manner, where we first extract unified motion feature from the driving signal, and then isolate each fine-grained motion from the unified feature. We introduce motion-specific contrastive learning and regressing for non-emotional motions, and feature-level decorrelation and self-reconstruction for emotional expression, to fully utilize the inherent properties of each motion factor in unstructured video data to achieve disentanglement. Experiments show that our method provides high quality speech&lip-motion synchronization along with precise and disentangled control over multiple extra facial motions, which can hardly be achieved by previous methods."
          ],
          [
            "Access Date",
            "2023-06-22 21:20:37"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "17979-17989"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2768.pdf",
        "selectable": false
      },
      {
        "text": "Real-Time Lip Sync for Live 2D Animation",
        "item-id": "i51",
        "nodes": [
          {
            "text": "Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf",
            "item-id": "i197",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LC7Q9KII/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/2\">Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/3\">Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/4\">Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/5\">Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/6\">Model Latency</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/6\">Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/6\">Differences in Style</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/6\">Accuracy and Convergence Behavior</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/7\">Impact of Lookahead</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/7\">Impact of LSTM context</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/7\">Impact of Data Augmentation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/7\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/7\">Comparisons with Competing Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/8\">Comparisons with Groundtruth</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/8\">Comparisons with Different Model Variations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/9\">Matching Animator Styles</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/9\">Impact on Performers</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/9\">Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/9\">Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/10\">Conclusions and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LC7Q9KII/10\">References </a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf"
              ]
            ],
            "resource": "storage/i197.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Real-Time Lip Sync for Live 2D Animation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The emergence of commercial tools for real-time performance-based 2D animation has enabled 2D characters to appear on live broadcasts and streaming platforms. A key requirement for live animation is fast and accurate lip sync that allows characters to respond naturally to other actors or the audience through the voice of a human performer. In this work, we present a deep learning based interactive system that automatically generates live lip sync for layered 2D characters using a Long Short Term Memory (LSTM) model. Our system takes streaming audio as input and produces viseme sequences with less than 200ms of latency (including processing time). Our contributions include specific design decisions for our feature definition and LSTM configuration that provide a small but useful amount of lookahead to produce accurate lip sync. We also describe a data augmentation procedure that allows us to achieve good results with a very small amount of hand-animated training data (13-20 minutes). Extensive human judgement experiments show that our results are preferred over several competing methods, including those that only support offline (non-live) processing. Video summary and supplementary results at GitHub link: https://github.com/deepalianeja/CharacterLipSync2D"
          ],
          [
            "Access Date",
            "2021-04-22 10:34:19"
          ],
          [
            "Archiveid",
            "arXiv:1910.08685"
          ],
          [
            "Creators",
            "Deepali Aneja, Wilmot Li"
          ],
          [
            "Date",
            "2019-10-18 2019-10-18"
          ],
          [
            "Extra",
            "arXiv: 1910.08685"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Real-Time Lip Sync for Live 2D Animation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1910.08685"
          ]
        ],
        "resource": "storage/i197.pdf",
        "selectable": false
      },
      {
        "text": "Realistic Speech-Driven Facial Animation with GANs",
        "item-id": "i1246",
        "nodes": [
          {
            "text": "Vougioukas et al_2020_Realistic Speech-Driven Facial Animation with GANs.pdf",
            "item-id": "i1248",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Vougioukas et al_2020_Realistic Speech-Driven Facial Animation with GANs.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ESDQB62A/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ESDQB62A/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/3\">2.1 Visual Feature Selection and Blending</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/3\">2.2 Synthesis Based on Hidden Markov Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/3\">2.3 Synthesis Based on Deep Neural Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/3\">2.4 GAN-Based Video Synthesis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ESDQB62A/4\">3 Speech-Driven Facial Synthesis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ESDQB62A/4\">3.1 Generator</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/4\">3.1.1 Identity Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/5\">3.1.2 Content Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/5\">3.1.3 Noise Generator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/5\">3.1.4 Frame Decoder</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ESDQB62A/5\">3.2 Discriminators</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/5\">3.2.1 Frame Discriminator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/5\">3.2.2 Sequence Discriminator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/6\">3.2.3 Synchronization Discriminator</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/6\">3.3 Training</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/7\">4 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/7\">5 Metrics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ESDQB62A/9\">6 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/9\">6.1 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/11\">6.2 Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/12\">6.3 Quantitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/13\">6.4 User Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/14\">7 Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/15\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ESDQB62A/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Vougioukas et al_2020_Realistic Speech-Driven Facial Animation with GANs.pdf"
              ]
            ],
            "resource": "storage/i1248.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Realistic Speech-Driven Facial Animation with GANs",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Speech-driven facial animation is the process that automatically synthesizes talking characters based on speech signals. The majority of work in this domain creates a mapping from audio features to visual features. This approach often requires post-processing using computer graphics techniques to produce realistic albeit subject dependent results. We present an end-to-end system that generates videos of a talking head, using only a still image of a person and an audio clip containing speech, without relying on handcrafted intermediate features. Our method generates videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements. Our temporal GAN uses 3 discriminators focused on achieving detailed frames, audio-visual synchronization, and realistic expressions. We quantify the contribution of each component in our model using an ablation study and we provide insights into the latent representation of the model. The generated videos are evaluated based on sharpness, reconstruction quality, lip-reading accuracy, synchronization as well as their ability to generate natural blinks."
          ],
          [
            "Access Date",
            "2021-12-07 14:35:12"
          ],
          [
            "Creators",
            "Konstantinos Vougioukas, Stavros Petridis, Maja Pantic"
          ],
          [
            "DOI",
            "10.1007/s11263-019-01251-8"
          ],
          [
            "Date",
            "2020-05-01 2020-05-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "1398-1413"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Title",
            "Realistic Speech-Driven Facial Animation with GANs"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-019-01251-8"
          ],
          [
            "Volume",
            "128"
          ]
        ],
        "resource": "storage/i1248.pdf",
        "selectable": false
      },
      {
        "text": "ReenactGAN",
        "item-id": "i1821",
        "nodes": [
          {
            "text": "Wu et al_2018_ReenactGAN.pdf",
            "item-id": "i2057",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2018_ReenactGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2018_ReenactGAN.pdf"
              ]
            ],
            "resource": "storage/i2057.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ReenactGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a novel learning-based framework for face reenactment. The proposed method, known as ReenactGAN, is capable of transferring facial movements and expressions from an arbitrary person\u2019s monocular video input to a target person\u2019s video. Instead of performing a direct transfer in the pixel space, which could result in structural artifacts, we first map the source face onto a boundary latent space. A transformer is subsequently used to adapt the source face\u2019s boundary to the target\u2019s boundary. Finally, a target-specific decoder is used to generate the reenacted target face. Thanks to the effective and reliable boundary-based transfer, our method can perform photo-realistic face reenactment. In addition, ReenactGAN is appealing in that the whole reenactment process is purely feed-forward, and thus the reenactment process can run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model are publicly available on our project page."
          ],
          [
            "Access Date",
            "2022-10-22 08:40:35"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, Chen Change Loy"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "603-619"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "ReenactGAN"
          ],
          [
            "Title",
            "ReenactGAN: Learning to Reenact Faces via Boundary Transfer"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Wayne_Wu_Learning_to_Reenact_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i2057.pdf",
        "selectable": false
      },
      {
        "text": "SPACEx",
        "item-id": "i2160",
        "nodes": [
          {
            "text": "Gururani et al_2022_SPACEx.pdf",
            "item-id": "i2178",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gururani et al_2022_SPACEx.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RBXTUSDR/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/2\">2 . Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/3\">3 . Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/3\">3.1 . Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/5\">3.2 . Speech2Landmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/5\">3.3 . Pose generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/6\">3.4 . Landmarks2Latents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/6\">3.5 . Emotion control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/6\">3.6 . Video synthesis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/6\">4 . Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/8\">5 . Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/11\">A . Network architecture and training</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/11\">A.1 . Speech2Landmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/11\">A.2 . Pose Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/11\">A.3 . Landmarks2Latents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/11\">A.4 . Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/11\">A.5 . Training details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/12\">B . Data preprocessing</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/12\">B.1 . Test Set details</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBXTUSDR/12\">C . Evaluation</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gururani et al_2022_SPACEx.pdf"
              ]
            ],
            "resource": "storage/i2178.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "SPACEx",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Animating portraits using speech has received growing attention in recent years, with various creative and practical use cases. An ideal generated video should have good lip sync with the audio, natural facial expressions and head motions, and high frame quality. In this work, we present SPACEx, which uses speech and a single image to generate high-resolution, and expressive videos with realistic head pose, without requiring a driving video. It uses a multi-stage approach, combining the controllability of facial landmarks with the high-quality synthesis power of a pretrained face generator. SPACEx also allows for the control of emotions and their intensities. Our method outperforms prior methods in objective metrics for image quality and facial motions and is strongly preferred by users in pair-wise comparisons. The project website is available at https://deepimagination.cc/SPACEx/"
          ],
          [
            "Access Date",
            "2022-11-30 14:25:27"
          ],
          [
            "Archiveid",
            "arXiv:2211.09809"
          ],
          [
            "Creators",
            "Siddharth Gururani, Arun Mallya, Ting-Chun Wang, Rafael Valle, Ming-Yu Liu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2211.09809"
          ],
          [
            "Date",
            "2022-11-17 2022-11-17"
          ],
          [
            "Extra",
            "arXiv:2211.09809 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "SPACEx"
          ],
          [
            "Title",
            "SPACEx: Speech-driven Portrait Animation with Controllable Expression"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2211.09809"
          ]
        ],
        "resource": "storage/i2178.pdf",
        "selectable": false
      },
      {
        "text": "SadTalker",
        "item-id": "i3224",
        "nodes": [
          {
            "text": "Zhang et al_2023_SadTalker.pdf",
            "item-id": "i3314",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_SadTalker.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_SadTalker.pdf"
              ]
            ],
            "resource": "storage/i3314.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SadTalker",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experiments to show the superior of our method in terms of motion and video quality."
          ],
          [
            "Access Date",
            "2024-01-03 01:58:37"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8652-8661"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "SadTalker"
          ],
          [
            "Title",
            "SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i3314.pdf",
        "selectable": false
      },
      {
        "text": "Seeing What You Said",
        "item-id": "i2746",
        "nodes": [
          {
            "text": "Wang et al_2023_Seeing What You Said.pdf",
            "item-id": "i2748",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_Seeing What You Said.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_Seeing What You Said.pdf"
              ]
            ],
            "resource": "storage/i2748.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Seeing What You Said",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking face generation, also known as speech-to-lip generation, reconstructs facial motions concerning lips given coherent speech input. The previous studies revealed the importance of lip-speech synchronization and visual quality. Despite much progress, they hardly focus on the content of lip movements i.e., the visual intelligibility of the spoken words, which is an important aspect of generation quality. To address the problem, we propose using a lip-reading expert to improve the intelligibility of the generated lip regions by penalizing the incorrect generation results. Moreover, to compensate for data scarcity, we train the lip-reading expert in an audio-visual self-supervised manner. With a lip-reading expert, we propose a novel contrastive learning to enhance lip-speech synchronization, and a transformer to encode audio synchronically with video, while considering global temporal dependency of audio. For evaluation, we propose a new strategy with two different lip-reading experts to measure intelligibility of the generated videos. Rigorous experiments show that our proposal is superior to other State-of-the-art (SOTA) methods, such as Wav2Lip, in reading intelligibility i.e., over 38% Word Error Rate (WER) on LRS2 dataset and 27.8% accuracy on LRW dataset. We also achieve the SOTA performance in lip-speech synchronization and comparable performances in visual quality."
          ],
          [
            "Access Date",
            "2023-06-22 04:13:59"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby T. Tan, Haizhou Li"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14653-14662"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Seeing What You Said"
          ],
          [
            "Title",
            "Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2748.pdf",
        "selectable": false
      },
      {
        "text": "Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation",
        "item-id": "i1648",
        "nodes": [
          {
            "text": "Liu et al_2022_Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation.pdf",
            "item-id": "i1667",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2022_Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PEA36RU5/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/2\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PEA36RU5/2\">3 . Our Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/3\">3.1 . Preliminaries and Problem Setting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/3\">3.2 . Semantic-Aware Dynamic Ray Sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/4\">3.3 . Torso Deformation Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/4\">3.4 . Volume Rendering and Network Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PEA36RU5/5\">4 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/5\">4.1 . Dataset and Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/5\">4.2 . Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/6\">4.3 . Quantitative Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/7\">4.4 . Qualitative Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/8\">4.5 . Ablation Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/8\">5 . Broader Impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PEA36RU5/8\">6 . Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2022_Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation.pdf"
              ]
            ],
            "resource": "storage/i1667.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Animating high-fidelity video portrait with speech audio is crucial for virtual reality and digital entertainment. While most previous studies rely on accurate explicit structural information, recent works explore the implicit scene representation of Neural Radiance Fields (NeRF) for realistic generation. In order to capture the inconsistent motions as well as the semantic difference between human head and torso, some work models them via two individual sets of NeRF, leading to unnatural results. In this work, we propose Semantic-aware Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven portraits using one unified set of NeRF. The proposed model can handle the detailed local facial semantics and the global head-torso relationship through two semantic-aware modules. Specifically, we first propose a Semantic-Aware Dynamic Ray Sampling module with an additional parsing branch that facilitates audio-driven volume rendering. Moreover, to enable portrait rendering in one unified neural radiance field, a Torso Deformation module is designed to stabilize the large-scale non-rigid torso motions. Extensive evaluations demonstrate that our proposed approach renders more realistic video portraits compared to previous methods. Project page: https://alvinliu0.github.io/projects/SSP-NeRF"
          ],
          [
            "Access Date",
            "2022-06-25 01:07:47"
          ],
          [
            "Archiveid",
            "arXiv:2201.07786"
          ],
          [
            "Creators",
            "Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, Bolei Zhou"
          ],
          [
            "DOI",
            "10.48550/arXiv.2201.07786"
          ],
          [
            "Date",
            "2022-01-19 2022/01/19"
          ],
          [
            "Extra",
            "arXiv:2201.07786 [cs]"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "arxiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation"
          ],
          [
            "URL",
            "https://arxiv.org/abs/2201.07786v1"
          ]
        ],
        "resource": "storage/i1667.pdf",
        "selectable": false
      },
      {
        "text": "Show Me What and Tell Me How",
        "item-id": "i2158",
        "nodes": [
          {
            "text": "Han et al_2022_Show Me What and Tell Me How.pdf",
            "item-id": "i2175",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Han et al_2022_Show Me What and Tell Me How.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Han et al_2022_Show Me What and Tell Me How.pdf"
              ]
            ],
            "resource": "storage/i2175.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Show Me What and Tell Me How",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most methods for conditional video synthesis use a single modality as the condition. This comes with major limitations. For example, it is problematic for a model conditioned on an image to generate a specific motion trajectory desired by the user since there is no means to provide motion information. Conversely, language information can describe the desired motion, while not precisely defining the content of the video. This work presents a multimodal video generation framework that benefits from text and images provided jointly or separately. We leverage the recent progress in quantized representations for videos and apply a bidirectional transformer with multiple modalities as inputs to predict a discrete video representation. To improve video quality and consistency, we propose a new video token trained with self-learning and an improved mask-prediction algorithm for sampling video tokens. We introduce text augmentation to improve the robustness of the textual representation and diversity of generated videos. Our framework can incorporate various visual modalities, such as segmentation masks, drawings, and partially occluded images. It can generate much longer sequences than the one used for training. In addition, our model can extract visual information as suggested by the text prompt, e.g., \"an object in image one is moving northeast\", and generate corresponding videos. We run evaluations on three public datasets and a newly collected dataset labeled with facial attributes, achieving state-of-the-art generation results on all four. [Code](https://github.com/snap-research/MMVID) and [webpage](https://snap-research.github.io/MMVID/)."
          ],
          [
            "Access Date",
            "2022-12-12 09:08:13"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, Sergey Tulyakov"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3615-3625"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Show Me What and Tell Me How"
          ],
          [
            "Title",
            "Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Han_Show_Me_What_and_Tell_Me_How_Video_Synthesis_via_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2175.pdf",
        "selectable": false
      },
      {
        "text": "SimSwap",
        "item-id": "i1124",
        "nodes": [
          {
            "text": "Chen et al_2020_SimSwap.pdf",
            "item-id": "i1155",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_SimSwap.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GZ8N4LBV/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/2\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/3\">3.1 Limitation of the DeepFakes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/3\">3.2 Generalization to Arbitrary Identity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/4\">3.3 Preserving the Attributes of the Target</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/4\">3.4 Overall Loss Function</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/4\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/4\">4.1 Qualitative Face Swapping Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/5\">4.2 Comparison with Other Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/6\">4.3 Analysis of SimSwap</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/8\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZ8N4LBV/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_SimSwap.pdf"
              ]
            ],
            "resource": "storage/i1155.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SimSwap",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose an efficient framework, called Simple Swap (SimSwap), aiming for generalized and high fidelity face swapping. In contrast to previous approaches that either lack the ability to generalize to arbitrary identity or fail to preserve attributes like facial expression and gaze direction, our framework is capable of transferring the identity of an arbitrary source face into an arbitrary target face while preserving the attributes of the target face. We overcome the above defects in the following two ways. First, we present the ID Injection Module (IIM) which transfers the identity information of the source face into the target face at feature level. By using this module, we extend the architecture of an identity-specific face swapping algorithm to a framework for arbitrary face swapping. Second, we propose the Weak Feature Matching Loss which efficiently helps our framework to preserve the facial attributes in an implicit way. Extensive experiments on wild faces demonstrate that our SimSwap is able to achieve competitive identity performance while preserving attributes better than previous state-of-the-art methods."
          ],
          [
            "Access Date",
            "2021-10-20"
          ],
          [
            "Conference Name",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Creators",
            "Renwang Chen, Xuanhong Chen, Bingbing Ni, Yanhao Ge"
          ],
          [
            "DOI",
            "10.1145/3394171.3413630"
          ],
          [
            "Date",
            "2020-10-12 October 12, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7988-5"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2003\u20132011"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '20"
          ],
          [
            "Short Title",
            "SimSwap"
          ],
          [
            "Title",
            "SimSwap: An Efficient Framework For High Fidelity Face Swapping"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3394171.3413630"
          ]
        ],
        "resource": "storage/i1155.pdf",
        "selectable": false
      },
      {
        "text": "Speech Drives Templates",
        "item-id": "i2153",
        "nodes": [
          {
            "text": "Qian et al_2021_Speech Drives Templates.pdf",
            "item-id": "i2170",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Qian et al_2021_Speech Drives Templates.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Qian et al_2021_Speech Drives Templates.pdf"
              ]
            ],
            "resource": "storage/i2170.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Speech Drives Templates",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we design a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity. For our method, the template vector determines the general appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both indispensable for synthesizing a realistic gesture sequence. Due to the intractability of an objective metric for gesture-speech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization ability of our model. Extensive experiments show the superiority of our method in both objective and subjective evaluations on fidelity and synchronization."
          ],
          [
            "Access Date",
            "2022-12-12 10:57:27"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Shenhan Qian, Zhi Tu, Yihao Zhi, Wen Liu, Shenghua Gao"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "11077-11086"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "Speech Drives Templates"
          ],
          [
            "Title",
            "Speech Drives Templates: Co-Speech Gesture Synthesis With Learned Templates"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Qian_Speech_Drives_Templates_Co-Speech_Gesture_Synthesis_With_Learned_Templates_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i2170.pdf",
        "selectable": false
      },
      {
        "text": "Speech-driven Face Reenactment for a Video Sequence",
        "item-id": "i1322",
        "nodes": [
          {
            "text": "Nakashima et al_2020_Speech-driven Face Reenactment for a Video Sequence.pdf",
            "item-id": "i1394",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Nakashima et al_2020_Speech-driven Face Reenactment for a Video Sequence.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Nakashima et al_2020_Speech-driven Face Reenactment for a Video Sequence.pdf"
              ]
            ],
            "resource": "storage/i1394.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Speech-driven Face Reenactment for a Video Sequence",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a system for reenacting a person's face driven by speech. Given a video sequence with the corresponding audio track of a person giving a speech and another audio track containing different speech from the same person, we reconstruct a 3D mesh of the face in each frame of the video sequence to match the speech in the second audio track. Audio features are extracted from such two audio tracks. Assuming that the appearance of the mouth is highly correlated to these speech features, we extract the mouth region of the face's 3D mesh from the video sequence when speech features from the second audio track are close to those of the video's audio track. While retaining temporal consistency, these extracted mouth regions then replace the original mouth regions in the video sequence, synthesizing a reenactment video where the person seemingly gives the speech from the second audio track. Our system, coined S2TH (speech to talking head), does not require any special hardware to capture the 3D geometry of faces but uses the state-of-the-art method for facial geometry regression. We visually and subjectively demonstrate reenactment quality."
          ],
          [
            "Creators",
            "Yuta Nakashima, Takaaki Yasui, Leon Nguyen, Noboru Babaguchi"
          ],
          [
            "DOI",
            "10.3169/mta.8.60"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Library Catalog",
            "J-Stage"
          ],
          [
            "Pages",
            "60-68"
          ],
          [
            "Publication Title",
            "ITE Transactions on Media Technology and Applications"
          ],
          [
            "Title",
            "Speech-driven Face Reenactment for a Video Sequence"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i1394.pdf",
        "selectable": false
      },
      {
        "text": "Speech2Video",
        "item-id": "i2152",
        "nodes": [
          {
            "text": "ResearchGate Link",
            "item-id": "i2169",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "ResearchGate Link",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-12-12 10:59:55"
              ],
              [
                "Title",
                "ResearchGate Link"
              ],
              [
                "URL",
                "https://www.researchgate.net/publication/354221832_Speech2Video_Cross-Modal_Distillation_for_Speech_to_Video_Generation"
              ]
            ]
          },
          {
            "text": "Si et al_2021_Speech2Video.pdf",
            "item-id": "i2168",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Si et al_2021_Speech2Video.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LZ4Y3YTG/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/1\">2  Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/2\">2.1  Speech Representation Disentangling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/2\">2.2  Adversarial Video Composition</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/3\">3  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/3\">3.1  Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/3\">3.2  Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/4\">3.3  Quantitative Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/4\">4  Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/4\">5  Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LZ4Y3YTG/5\">6  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Si et al_2021_Speech2Video.pdf"
              ]
            ],
            "resource": "storage/i2168.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Speech2Video",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Conference Name",
            "INTERSPEECH 2021"
          ],
          [
            "Creators",
            "Shijing Si, Jianzong Wang, Xiaoyang Qu, Ning Cheng, Wenqi Wei, Xinghua Zhu, Jing Xiao"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-1996"
          ],
          [
            "Date",
            "2021-08-30 2021-08-30"
          ],
          [
            "Library Catalog",
            "ResearchGate"
          ],
          [
            "Pages",
            "1629-1633"
          ],
          [
            "Proceedings Title",
            "INTERSPEECH 2021"
          ],
          [
            "Short Title",
            "Speech2Video"
          ],
          [
            "Title",
            "Speech2Video: Cross-Modal Distillation for Speech to Video Generation"
          ]
        ],
        "resource": "storage/i2168.pdf",
        "selectable": false
      },
      {
        "text": "Stitch it in Time",
        "item-id": "i2159",
        "nodes": [
          {
            "text": "Tzaban et al_2022_Stitch it in Time.pdf",
            "item-id": "i2176",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tzaban et al_2022_Stitch it in Time.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_88MPMBMV/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/3\">2 Background and Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_88MPMBMV/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/4\">3.1 Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/4\">3.2 Inversion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/5\">3.3 Editing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/5\">3.4 Stitching-Tuning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_88MPMBMV/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/5\">4.1 Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/6\">4.2 Quantitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/7\">4.3 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/7\">4.4 Limitations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/7\">5 CONCLUSIONS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_88MPMBMV/8\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tzaban et al_2022_Stitch it in Time.pdf"
              ]
            ],
            "resource": "storage/i2176.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Stitch it in Time",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Applying StyleGAN editing to real videos introduces two main challenges: (i) StyleGAN operates over aligned crops. When editing videos, these crops need to be pasted back into the frame, resulting in a spatial inconsistency. (ii) Videos introduce a fundamental barrier to overcome \u2014 temporal coherency. To address the first challenge, we propose a novel stitching-tuning procedure. The generator is carefully tuned to overcome the spatial artifacts at crop borders, resulting in smooth transitions even when difficult backgrounds are involved. Turning to temporal coherence, we propose that this challenge is largely artificial. The source video is already temporally coherent, and deviations arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low-frequency functions, and demonstrate that they provide a strongly consistent prior. These components are combined in an end-to-end framework for semantic editing of facial videos. We compare our pipeline to the current state-of-the-art and demonstrate significant improvements. Our method produces meaningful manipulations and maintains greater spatial and temporal consistency, even on challenging talking head videos which current methods struggle with. Our code and videos are available at https://stitch-time.github.io/."
          ],
          [
            "Access Date",
            "2022-12-12"
          ],
          [
            "Creators",
            "Rotem Tzaban, Ron Mokady, Rinon Gal, Amit Bermano, Daniel Cohen-Or"
          ],
          [
            "DOI",
            "10.1145/3550469.3555382"
          ],
          [
            "Date",
            "2022-00-30 \u5341\u4e00\u6708 30, 2022"
          ],
          [
            "ISBN",
            "978-1-4503-9470-3"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1\u20139"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "SIGGRAPH Asia 2022 Conference Papers"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "SA '22"
          ],
          [
            "Short Title",
            "Stitch it in Time"
          ],
          [
            "Title",
            "Stitch it in Time: GAN-Based Facial Editing of Real Videos"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3550469.3555382"
          ]
        ],
        "resource": "storage/i2176.pdf",
        "selectable": false
      },
      {
        "text": "StyleHEAT",
        "item-id": "i1644",
        "nodes": [
          {
            "text": "Comment: Project Page is at http://feiiyin.github.io/StyleHEAT/",
            "item-id": "n1659",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project Page is at http://feiiyin.github.io/StyleHEAT/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project Page is at http://feiiyin.github.io/StyleHEAT/</div>",
            "node_type": "note"
          },
          {
            "text": "Yin et al_2022_StyleHEAT.pdf",
            "item-id": "i1658",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yin et al_2022_StyleHEAT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yin et al_2022_StyleHEAT.pdf"
              ]
            ],
            "resource": "storage/i1658.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "StyleHEAT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "One-shot talking face generation aims at synthesizing a high-quality talking face video from an arbitrary portrait image, driven by a video or an audio segment. One challenging quality factor is the resolution of the output video: higher resolution conveys more details. In this work, we investigate the latent feature space of a pre-trained StyleGAN and discover some excellent spatial transformation properties. Upon the observation, we explore the possibility of using a pre-trained StyleGAN to break through the resolution limit of training datasets. We propose a novel unified framework based on a pre-trained StyleGAN that enables a set of powerful functionalities, i.e., high-resolution video generation, disentangled control by driving video or audio, and flexible face editing. Our framework elevates the resolution of the synthesized talking face to 1024*1024 for the first time, even though the training dataset has a lower resolution. We design a video-based motion generation module and an audio-based one, which can be plugged into the framework either individually or jointly to drive the video generation. The predicted motion is used to transform the latent features of StyleGAN for visual animation. To compensate for the transformation distortion, we propose a calibration network as well as a domain loss to refine the features. Moreover, our framework allows two types of facial editing, i.e., global editing via GAN inversion and intuitive editing based on 3D morphable models. Comprehensive experiments show superior video quality, flexible controllability, and editability over state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-06-27 09:15:31"
          ],
          [
            "Archiveid",
            "arXiv:2203.04036"
          ],
          [
            "Creators",
            "Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, Yujiu Yang"
          ],
          [
            "DOI",
            "10.48550/arXiv.2203.04036"
          ],
          [
            "Date",
            "2022-03-16 2022-03-16"
          ],
          [
            "Extra",
            "arXiv:2203.04036 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "StyleHEAT"
          ],
          [
            "Title",
            "StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2203.04036"
          ]
        ],
        "resource": "storage/i1658.pdf",
        "selectable": false
      },
      {
        "text": "TACR-Net",
        "item-id": "i1331",
        "nodes": [
          {
            "text": "Song et al_2021_TACR-Net.pdf",
            "item-id": "i1410",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2021_TACR-Net.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JBLAKR94/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JBLAKR94/2\">2 RELATED WORK</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/2\">2.1 Audio-Based Face Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/3\">2.2 Monocular 3D Face Reconstruction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/3\">2.3 Speech Recognition and Conversion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JBLAKR94/3\">3 TACR-Net</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/3\">3.1 Temporal-Refinement Expression Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/4\">3.2 Autoregressive Cascade Render</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/5\">3.3 Voice Conversion Network</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JBLAKR94/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/5\">4.1 Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/6\">4.2 Comparison Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/7\">4.3 Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/8\">4.4 User Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/8\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JBLAKR94/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2021_TACR-Net.pdf"
              ]
            ],
            "resource": "storage/i1410.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-book",
        "item_title": "TACR-Net",
        "item_type": "bookSection",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Utilizing an arbitrary speech clip to edit the mouth of the portrait in the target video is a novel yet challenging task. Despite impressive results have been achieved, there are still three limitations in the existing methods: 1) since the acoustic features are not completely decoupled from person identity, there is no global speech to facial features (i.e., landmarks, expression blendshape) mapping method. 2) the audio-driven talking face sequences generated by simple cascade structure usually lack of temporal consistency and spatial correlation, which leads to defects in the consistency of changes in details. 3) the operation of forgery is always at the video level, without considering the forgery of the voice, especially the synchronization of the converted voice and the mouth. To address these distortion problems, we propose a novel deep learning framework, named Temporal-Refinement Autoregressive-Cascade Rendering Network (TACR-Net) for audio-driven dynamic talking face editing. The proposed TACR-Net encodes facial expression blendshape based on the given acoustic features without separately training for special video. Then TACR-Net also involves a novel autoregressive cascade structure generator for video re-rendering. Finally, we transform the in-the-wild speech to the target portrait and obtain a photo-realistic and audio-realistic video."
          ],
          [
            "Access Date",
            "2022-02-01"
          ],
          [
            "Book Title",
            "Proceedings of the 29th ACM International Conference on Multimedia"
          ],
          [
            "Creators",
            "Luchuan Song, Bin Liu, Guojun Yin, Xiaoyi Dong, Yufei Zhang, Jia-Xuan Bai"
          ],
          [
            "Date",
            "2021-10-17 2021-10-17"
          ],
          [
            "ISBN",
            "978-1-4503-8651-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "478\u2013486"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Short Title",
            "TACR-Net"
          ],
          [
            "Title",
            "TACR-Net: Editing on Deep Video and Voice Portraits"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3474085.3475196"
          ]
        ],
        "resource": "storage/i1410.pdf",
        "selectable": false
      },
      {
        "text": "Talking Face Generation Based on Information Bottleneck and Complementary Representations",
        "item-id": "i1327",
        "nodes": [
          {
            "text": "Tang et al_2021_Talking Face Generation Based on Information Bottleneck and Complementary.pdf",
            "item-id": "i1403",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tang et al_2021_Talking Face Generation Based on Information Bottleneck and Complementary.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LSXRSXIC/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/2\">2 Our Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/2\">2.1 3D face reconstruction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/2\">2.2 Mapping from audio to expression</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/3\">2.3 Predicting lip landmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/3\">2.4 Neural Renderer Network</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/3\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/4\">3.1 Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/4\">3.2 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/4\">3.3 User Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/4\">4 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LSXRSXIC/4\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tang et al_2021_Talking Face Generation Based on Information Bottleneck and Complementary.pdf"
              ]
            ],
            "resource": "storage/i1403.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-book",
        "item_title": "Talking Face Generation Based on Information Bottleneck and Complementary Representations",
        "item_type": "bookSection",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio-driven talking face generation is an active research direction in the field of virtual reality. The main challenge is that the generated lip shape of the speaker is out of sync with the input audio. To address this challenge, we propose a novel solution to synthesize lip-synchronized, high-quality, realistic video given input audio. We first decompose the target person's video frames into 3D face model parameters, and the information bottleneck is inserted into the audio-to-expression network to learn the mapping between audio features and expression parameters. Then, we replace the expression parameters in the target video frame with the extracted expression parameters from audio and re-render the face. Finally, we add high-level audio embedding extracted from the raw audio and lip landmarks embedding in the neural rendering network. The 3D face shapes, 2D landmarks, and audio embedding provide complementary information for the neural rendering network which guarantees the generation of lip-synchronized high-quality video portraits from the synthesized rendered faces. Experimental results show that compared with other talking face generation methods, our method is the best concerning lip synchronization with high video definition."
          ],
          [
            "Access Date",
            "2022-02-12"
          ],
          [
            "Book Title",
            "Proceedings of the 30th ACM International Conference on Information & Knowledge Management"
          ],
          [
            "Creators",
            "Jie Tang, Yiling Wu, Minglei Li, Zhu Wang"
          ],
          [
            "Date",
            "2021-10-26 2021-10-26"
          ],
          [
            "ISBN",
            "978-1-4503-8446-9"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "3443\u20133447"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Title",
            "Talking Face Generation Based on Information Bottleneck and Complementary Representations"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3459637.3482198"
          ]
        ],
        "resource": "storage/i1403.pdf",
        "selectable": false
      },
      {
        "text": "Talking Face Generation by Conditional Recurrent Adversarial Network",
        "item-id": "i1299",
        "nodes": [
          {
            "text": "Song et al_2019_Talking Face Generation by Conditional Recurrent Adversarial Network.pdf",
            "item-id": "i1348",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2019_Talking Face Generation by Conditional Recurrent Adversarial Network.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2019_Talking Face Generation by Conditional Recurrent Adversarial Network.pdf"
              ]
            ],
            "resource": "storage/i1348.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Talking Face Generation by Conditional Recurrent Adversarial Network",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Given an arbitrary face image and an arbitrary speech clip, the proposed work attempts to generate the talking face video with accurate lip synchronization. Existing works either do not consider temporal dependency across video frames thus yielding abrupt facial and lip movement or are limited to the generation of talking face video for a specific person thus lacking generalization capacity. We propose a novel conditional recurrent generation network that incorporates both image and audio features in the recurrent unit for  temporal dependency. To achieve both image- and video-realism, a pair of spatial-temporal discriminators are included in the network for better image/video quality. Since accurate lip synchronization is essential to the success of talking face video generation, we also construct a lip-reading discriminator to boost the accuracy of lip synchronization. We also extend the network to model the natural pose and expression of talking face on the Obama Dataset. Extensive experimental results demonstrate the superiority of our framework over the state-of-the-arts in terms of visual quality, lip sync accuracy, and smooth transition pertaining to both lip and facial movement."
          ],
          [
            "Access Date",
            "2022-02-20 07:08:43"
          ],
          [
            "Conference Name",
            "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence"
          ],
          [
            "Creators",
            "Yang Song, Jingwen Zhu, Dawei Li, Andy Wang, Hairong Qi"
          ],
          [
            "DOI",
            "https://doi.org/10.24963/ijcai.2019/129"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "www.ijcai.org"
          ],
          [
            "Pages",
            "919-925"
          ],
          [
            "Proceedings Title",
            "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence"
          ],
          [
            "Title",
            "Talking Face Generation by Conditional Recurrent Adversarial Network"
          ],
          [
            "URL",
            "https://www.ijcai.org/proceedings/2019/129"
          ]
        ],
        "resource": "storage/i1348.pdf",
        "selectable": false
      },
      {
        "text": "Talking-Head Generation with Rhythmic Head Motion",
        "item-id": "i1678",
        "nodes": [
          {
            "text": "Chen et al_2020_Talking-Head Generation with Rhythmic Head Motion.pdf",
            "item-id": "i1680",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Talking-Head Generation with Rhythmic Head Motion.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5UTPKSSR/1\">Talking-head Generation with Rhythmic Head Motion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Talking-Head Generation with Rhythmic Head Motion.pdf"
              ]
            ],
            "resource": "storage/i1680.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Talking-Head Generation with Rhythmic Head Motion",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "When people deliver a speech, they naturally move heads, and this rhythmic head motion conveys prosodic information. However, generating a lip-synced video while moving head naturally is challenging. While remarkably successful, existing works either generate still talking-face videos or rely on landmark/video frames as sparse/dense mapping guidance to generate head movements, which leads to unrealistic or uncontrollable video synthesis. To overcome the limitations, we propose a 3D-aware generative network along with a hybrid embedding module and a non-linear composition module. Through modeling the head motion and facial expressions (In our setting, facial expression means facial movement (e.g., blinks, and lip & chin movements).) explicitly, manipulating 3D animation carefully, and embedding reference images dynamically, our approach achieves controllable, photo-realistic, and temporally coherent talking-head videos with natural head movements. Thoughtful experiments on several standard benchmarks demonstrate that our method achieves significantly better results than the state-of-the-art methods in both quantitative and qualitative comparisons. The code is available on https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, Chenliang Xu, Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58545-7_3"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58545-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "35-51"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Talking-Head Generation with Rhythmic Head Motion"
          ]
        ],
        "resource": "storage/i1680.pdf",
        "selectable": false
      },
      {
        "text": "The Creation and Detection of Deepfakes",
        "item-id": "i958",
        "nodes": [
          {
            "text": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item-id": "i969",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf"
              ]
            ],
            "resource": "storage/i969.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The Creation and Detection of Deepfakes",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these \u201cdeepfakes\u201d have advanced significantly. In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention."
          ],
          [
            "Access Date",
            "2021-10-05 09:05:27"
          ],
          [
            "Creators",
            "Yisroel Mirsky, Wenke Lee"
          ],
          [
            "DOI",
            "10.1145/3425780"
          ],
          [
            "Date",
            "2021-01-02 2021-01-02"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "April 2021"
          ],
          [
            "Pages",
            "7:1\u20137:41"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "The Creation and Detection of Deepfakes"
          ],
          [
            "Title",
            "The Creation and Detection of Deepfakes: A Survey"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3425780"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i969.pdf",
        "selectable": false
      },
      {
        "text": "Towards Automatic Face-to-Face Translation",
        "item-id": "i1025",
        "nodes": [
          {
            "text": "K R et al_2019_Towards Automatic Face-to-Face Translation.pdf",
            "item-id": "i1032",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "K R et al_2019_Towards Automatic Face-to-Face Translation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RDSQ3HLU/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/2\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/2\">2 Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/2\">2.1 Automatic Speech Recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/2\">2.2 Neural Machine Translation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/3\">2.3 Text to Speech</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/3\">2.4 Voice Transfer in Audio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/3\">2.5 Talking Face Synthesis from Audio</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/4\">3 Speech-to-Speech Translation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/4\">3.1 Recognizing speech in source language LA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/4\">3.2 Translating to target language LB</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/4\">3.3 Generating Speech in language LB</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/4\">3.4 Personalizing speaker voice</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/5\">4 Talking face generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/5\">4.1 Model Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/5\">4.2 Generator network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/5\">4.3 Discriminator network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/5\">4.4 Joint training of the GAN framework</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/6\">4.5 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/6\">4.6 Results and Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/7\">4.7 Evaluating the complete pipeline</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/8\">5 Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/8\">5.1 Movie dubbing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/8\">5.2 Educational videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/8\">5.3 Television news and interviews</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RDSQ3HLU/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "K R et al_2019_Towards Automatic Face-to-Face Translation.pdf"
              ]
            ],
            "resource": "storage/i1032.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Towards Automatic Face-to-Face Translation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as \"Face-to-Face Translation\". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards \"Face-to-Face Translation\" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available."
          ],
          [
            "Access Date",
            "2021-10-17"
          ],
          [
            "Creators",
            "Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, C V Jawahar"
          ],
          [
            "DOI",
            "10.1145/3343031.3351066"
          ],
          [
            "Date",
            "2019-10-15 October 15, 2019"
          ],
          [
            "ISBN",
            "978-1-4503-6889-6"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1428\u20131436"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 27th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '19"
          ],
          [
            "Title",
            "Towards Automatic Face-to-Face Translation"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3343031.3351066"
          ]
        ],
        "resource": "storage/i1032.pdf",
        "selectable": false
      },
      {
        "text": "You Said That?",
        "item-id": "i1024",
        "nodes": [
          {
            "text": "Jamaludin et al_2019_You Said That.pdf",
            "item-id": "i1031",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jamaludin et al_2019_You Said That.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_TJ42FLHW/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/3\">3 The Speech2Vid Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/3\">3.1 The Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/3\">3.2 Loss Function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/4\">3.3 Post-processing: Image Sharpening</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/4\">3.4 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/6\">3.5 Discussion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/6\">4 Video Dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/7\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/7\">5.1 Quantitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/9\">5.2 Preserving Identity with Skip Connections</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/9\">5.3 Preserving Identity Using Multiple Still Images</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/9\">6 Re-dubbing Videos</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/9\">6.1 Baseline: Poisson Editing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/9\">6.2 Network-Based Blending</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/12\">7 Summary and Extensions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/12\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TJ42FLHW/12\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jamaludin et al_2019_You Said That.pdf"
              ]
            ],
            "resource": "storage/i1031.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "You Said That?",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We describe a method for generating a video of a talking face. The method takes still images of the target face and an audio speech segment as inputs, and generates a video of the target face lip synched with the audio. The method runs in real time and is applicable to faces and audio not seen at training time. To achieve this we develop an encoder\u2013decoder convolutional neural network (CNN) model that uses a joint embedding of the face and audio to generate synthesised talking face video frames. The model is trained on unlabelled videos using cross-modal self-supervision. We also propose methods to re-dub videos by visually blending the generated face into the source video frame using a multi-stream CNN model."
          ],
          [
            "Access Date",
            "2021-10-17 15:28:16"
          ],
          [
            "Creators",
            "Amir Jamaludin, Joon Son Chung, Andrew Zisserman"
          ],
          [
            "DOI",
            "10.1007/s11263-019-01150-y"
          ],
          [
            "Date",
            "2019-12-01 2019-12-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "1767-1779"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Short Title",
            "You Said That?"
          ],
          [
            "Title",
            "You Said That?: Synthesising Talking Faces from Audio"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-019-01150-y"
          ],
          [
            "Volume",
            "127"
          ]
        ],
        "resource": "storage/i1031.pdf",
        "selectable": false
      }
    ],
    "item_title": "Deepfake Generation",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Diffusion",
    "item-id": "c24,i3664",
    "nodes": [
      {
        "text": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "item-id": "i2217",
        "nodes": [
          {
            "text": "Comment: 33 pages",
            "item-id": "n2258",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 33 pages",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 33 pages</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang_Agrawala_2023_Adding Conditional Control to Text-to-Image Diffusion Models.pdf",
            "item-id": "i2257",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang_Agrawala_2023_Adding Conditional Control to Text-to-Image Diffusion Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YWL8CMTZ/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/3\">2.1 HyperNetwork and Neural Network Structure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/3\">2.2 Diffusion Probabilistic Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/3\">2.3 Text-to-Image Diffusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/3\">2.4 Personalization,Customization, and Control of Pretrained Diffusion Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/3\">2.5 Image-to-Image Translation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/4\">3.1 ControlNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/6\">3.2 ControlNet in Image Diffusion Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/7\">3.3 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/7\">3.4 Improved Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/8\">3.5 Implementation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/9\">4 Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/9\">4.1 Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/9\">4.2 Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/9\">4.3 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/9\">4.4 Comparison to previous methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/10\">4.5 Comparison of pre-trained models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/10\">4.6 More Applications</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YWL8CMTZ/10\">5 Limitation</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang_Agrawala_2023_Adding Conditional Control to Text-to-Image Diffusion Models.pdf"
              ]
            ],
            "resource": "storage/i2257.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications."
          ],
          [
            "Access Date",
            "2023-02-16 21:26:58"
          ],
          [
            "Archiveid",
            "arXiv:2302.05543"
          ],
          [
            "Creators",
            "Lvmin Zhang, Maneesh Agrawala"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.05543"
          ],
          [
            "Date",
            "2023-02-10 2023-02-10"
          ],
          [
            "Extra",
            "arXiv:2302.05543 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Adding Conditional Control to Text-to-Image Diffusion Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.05543"
          ]
        ],
        "resource": "storage/i2257.pdf",
        "selectable": false
      },
      {
        "text": "Align Your Latents",
        "item-id": "i2816",
        "nodes": [
          {
            "text": "Blattmann et al_2023_Align Your Latents.pdf",
            "item-id": "i2912",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Blattmann et al_2023_Align Your Latents.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Blattmann et al_2023_Align Your Latents.pdf"
              ]
            ],
            "resource": "storage/i2912.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Align Your Latents",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512x1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280x2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/"
          ],
          [
            "Access Date",
            "2023-07-01 07:14:49"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "22563-22575"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Align Your Latents"
          ],
          [
            "Title",
            "Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2912.pdf",
        "selectable": false
      },
      {
        "text": "DPM-Solver",
        "item-id": "i2181",
        "nodes": [
          {
            "text": "Lu et al_2022_DPM-Solver.pdf",
            "item-id": "i2191",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lu et al_2022_DPM-Solver.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/2\">Diffusion Probabilistic Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/3\">Forward Process and Diffusion SDEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/3\">Diffusion (Probability Flow) ODEs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/4\">Customized Fast Solvers for Diffusion ODEs </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/4\">Simplified Formulation of Exact Solutions of Diffusion ODEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/5\">High-Order Solvers for Diffusion ODEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/7\">Step Size Schedule</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/7\">Sampling from Discrete-Time DPMs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/7\">Comparison with Existing Fast Sampling Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/7\">DDIM as DPM-Solver-1</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/7\">Comparison with Traditional Runge-Kutta Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/8\">Training-based Fast Sampling Methods for DPMs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/8\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/9\">Comparison with Continuous-Time Sampling Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/9\">Comparison with Discrete-Time Sampling Methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/10\">Conclusions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Sampling with Invariance to the Noise Schedule</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Decoupling the Sampling Solution from the Noise Schedule</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Choosing Time Steps for  is Invariant to the Noise Schedule</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Relationship with the Maximum Likelihood Training of Diffusion Models</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Proof of Theorem 3.2</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Assumptions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">General Expansion of the Exponentially Weighted Integral</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Proof of Theorem 3.2 when k=1</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Proof of Theorem 3.2 when k=2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Proof of Theorem 3.2 when k=3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Connections to Explicit Exponential Runge-Kutta (expRK) Methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Algorithms of DPM-Solvers</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Implementation Details of DPM-Solver</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">End Time of Sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Sampling from Discrete-Time DPMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">DPM-Solver in 20 Function Evaluations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Analytical Formulation of the function t() (the inverse function of (t))</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Conditional Sampling by DPM-Solver</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Numerical Stability</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Experiment Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Diffusion ODEs w.r.t. </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Code Implementation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Sample Quality Comparison with Continuous-Time Sampling Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Sample Quality Comparison with RK Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Sample Quality Comparison with Discrete-Time Sampling Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Comparing Different Orders of DPM-Solver</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Runtime Comparison between DPM-Solver and DDIM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Conditional Sampling on ImageNet 256x256</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U4ILQYJY/1\">Additional Samples</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lu et al_2022_DPM-Solver.pdf"
              ]
            ],
            "resource": "storage/i2191.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DPM-Solver",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a 4~16x speedup compared with previous state-of-the-art training-free samplers on various datasets."
          ],
          [
            "Access Date",
            "2023-01-07 09:30:05"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu"
          ],
          [
            "Date",
            "2022-10-31 2022/10/31"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Short Title",
            "DPM-Solver"
          ],
          [
            "Title",
            "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=2uAaGwlP_V"
          ]
        ],
        "resource": "storage/i2191.pdf",
        "selectable": false
      },
      {
        "text": "DPM-Solver++",
        "item-id": "i2098",
        "nodes": [
          {
            "text": "Lu et al_2022_DPM-Solver++.pdf",
            "item-id": "i2132",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lu et al_2022_DPM-Solver++.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EZ5AFR29/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/3\">2 Diffusion Probabilistic Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/3\">2.1 Fast Sampling for DPMs by Diffusion ODEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/3\">2.2 Guided Sampling for DPMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/4\">2.3 Exponential Integrators and High-Order ODE Solvers</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/4\">3 Challenges of High-Order Solvers for Guided Sampling</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/4\">4 Designing Training-Free Fast Samplers for Guided Sampling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/5\">4.1 Designing Solvers by Data Prediction Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/6\">4.2 From Singlestep to Multistep</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/6\">4.3 Combining Thresholding with DPM-Solver++</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/7\">5 Relationship with Other Fast Sampling Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/7\">5.1 Comparison with Solvers based on Exponential Integrators</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/7\">5.2 Other Fast Sampling Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/8\">6 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/9\">6.1 Pixel-Space DPMs with Guidance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/9\">6.2 Latent-Space DPMs with Guidance</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/10\">7 Conclusions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/13\">A Additional Proofs</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/13\">A.1 Proof of Proposition 4.1</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/13\">A.2 Convergence of Algorithms</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/13\">A.2.1 Convergence of Algorithm 1</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/14\">A.3 Convergence of Algorithm 2</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/14\">B Implementation Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/14\">B.1 Converting Discrete-Time DPMs to Continuous-Time</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/15\">B.2 Ablating Time Steps</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/15\">B.3 Experiment Settings</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EZ5AFR29/16\">C Experiment Details</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lu et al_2022_DPM-Solver++.pdf"
              ]
            ],
            "resource": "storage/i2132.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "DPM-Solver++",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solves the diffusion ODE with the data prediction model and adopts thresholding methods to keep the solution matches training data distribution. We further propose a multistep variant of DPM-Solver++ to address the instability issue by reducing the effective step size. Experiments show that DPM-Solver++ can generate high-quality samples within only 15 to 20 steps for guided sampling by pixel-space and latent-space DPMs."
          ],
          [
            "Access Date",
            "2022-11-07 15:02:04"
          ],
          [
            "Archiveid",
            "arXiv:2211.01095"
          ],
          [
            "Creators",
            "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2211.01095"
          ],
          [
            "Date",
            "2022-11-02 2022-11-02"
          ],
          [
            "Extra",
            "arXiv:2211.01095 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "DPM-Solver++"
          ],
          [
            "Title",
            "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2211.01095"
          ]
        ],
        "resource": "storage/i2132.pdf",
        "selectable": false
      },
      {
        "text": "Denoising Diffusion Implicit Models",
        "item-id": "i2187",
        "nodes": [
          {
            "text": "Song et al_2022_Denoising Diffusion Implicit Models.pdf",
            "item-id": "i2197",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2022_Denoising Diffusion Implicit Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZRGXCQWG/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/2\">Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/3\">Variational Inference for non-Markovian Forward Processes</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/3\">Non-Markovian forward processes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/4\">Generative process and unified variational inference objective</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/4\">Sampling from Generalized Generative Processes</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/5\">Denoising Diffusion Implicit Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/5\">Accelerated generation processes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/6\">Relevance to Neural ODEs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/6\">Sample quality and efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/7\">Sample consistency in DDIMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/8\">Interpolation in deterministic generative processes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/8\">Reconstruction from Latent Space</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/9\">Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/9\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/13\">Non-Markovian Forward Processes for a Discrete Case</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/13\">Proofs</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/15\">Additional Derivations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/15\">Accelerated sampling processes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/15\">Derivation of denoising objectives for DDPMs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/16\">Experimental Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/16\">Datasets and architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/17\">Reverse process sub-sequence selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/17\">Closed form equations for each sampling step</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/17\">Samples and Consistency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZRGXCQWG/17\">Interpolation</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2022_Denoising Diffusion Implicit Models.pdf"
              ]
            ],
            "resource": "storage/i2197.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Denoising Diffusion Implicit Models",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error."
          ],
          [
            "Access Date",
            "2022-12-19 17:01:21"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Jiaming Song, Chenlin Meng, Stefano Ermon"
          ],
          [
            "Date",
            "2022-02-10 2022/02/10"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Denoising Diffusion Implicit Models"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=St1giarCHLP"
          ]
        ],
        "resource": "storage/i2197.pdf",
        "selectable": false
      },
      {
        "text": "Denoising Diffusion Probabilistic Models",
        "item-id": "i1807",
        "nodes": [
          {
            "text": "Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf",
            "item-id": "i1809",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf"
              ]
            ],
            "resource": "storage/i1809.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Denoising Diffusion Probabilistic Models",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN."
          ],
          [
            "Access Date",
            "2022-10-10 11:38:57"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Jonathan Ho, Ajay Jain, Pieter Abbeel"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "6840\u20136851"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Denoising Diffusion Probabilistic Models"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1809.pdf",
        "selectable": false
      },
      {
        "text": "DiffTalk",
        "item-id": "i2743",
        "nodes": [
          {
            "text": "Shen et al_2023_DiffTalk.pdf",
            "item-id": "i2745",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shen et al_2023_DiffTalk.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SWXUWEGE/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/3\">. Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/3\">. Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/3\">. Conditional Diffusion Model for Talking Head</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/5\">. Progressive Inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/5\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/5\">. Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/6\">. Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/7\">. Method Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/8\">. Expand to Higher Resolution</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWXUWEGE/8\">. Conclusion and Discussion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shen et al_2023_DiffTalk.pdf"
              ]
            ],
            "resource": "storage/i2745.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DiffTalk",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the generation quality or enhance the model generalization. However, there are few works able to address both issues simultaneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturally generalized across different identities without any further fine-tuning. Additionally, our DiffTalk can be gracefully tailored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identities. For more video results, please refer to https://sstzal.github.io/DiffTalk/."
          ],
          [
            "Access Date",
            "2023-06-22 01:58:40"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1982-1991"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DiffTalk"
          ],
          [
            "Title",
            "DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper"
          ]
        ],
        "resource": "storage/i2745.pdf",
        "selectable": false
      },
      {
        "text": "Diffusion Models Beat GANs on Image Synthesis",
        "item-id": "i2161",
        "nodes": [
          {
            "text": "Dhariwal_Nichol_2021_Diffusion Models Beat GANs on Image Synthesis.pdf",
            "item-id": "i2179",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dhariwal_Nichol_2021_Diffusion Models Beat GANs on Image Synthesis.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/2\">Background</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/3\">Architecture Improvements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/4\">Classifier Guidance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/6\">Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/7\">Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/9\">Limitations and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/9\">Societal Impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/10\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Additional Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Architecture Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Guidance</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Computational Requirements</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Throughput</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Early Stopping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Training Compute Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Detailed Formulation of DDPM</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Derivations for Classifier Guidance</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Conditional Diffusion Process</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Deriving Algorithm 1: Conditional Sampling for DDPM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Deriving Algorithm 2: Conditional Sampling for DDIM</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Nearest Neighbors for Samples</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Effect of Varying the Classifier Scale</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">LSUN Diversity Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Interpolating Between Dataset Images Using DDIM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Reduced Temperature Sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Sample Quality Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Using Fewer Sampling Steps on LSUN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Samples from ImageNet 512x512</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Samples from ImageNet 256x256</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_A9DSDZD9/1\">Samples from LSUN</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dhariwal_Nichol_2021_Diffusion Models Beat GANs on Image Synthesis.pdf"
              ]
            ],
            "resource": "storage/i2179.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Diffusion Models Beat GANs on Image Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2022-11-26 01:08:25"
          ],
          [
            "Creators",
            "Prafulla Dhariwal, Alexander Nichol"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "8780\u20138794"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Diffusion Models Beat GANs on Image Synthesis"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i2179.pdf",
        "selectable": false
      },
      {
        "text": "Diffusion-GAN",
        "item-id": "i1846",
        "nodes": [
          {
            "text": "Comment: Project homepage: https://github.com/Zhendong-Wang/Diffusion-GAN",
            "item-id": "n1936",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project homepage: https://github.com/Zhendong-Wang/Diffusion-GAN",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project homepage: https://github.com/Zhendong-Wang/Diffusion-GAN</div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2022_Diffusion-GAN.pdf",
            "item-id": "i1935",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_Diffusion-GAN.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_QR4P2BTF/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/3\">2 Preliminaries: GANs and diffusion-based generative models</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/3\">3 Diffusion-GAN: Method and Theoretical Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/4\">3.1 Instance noise injection via diffusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/4\">3.2 Adversarial Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/5\">3.3 Adaptive diffusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/6\">3.4 Theoretical analysis with Examples</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/7\">3.5 Related work</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/7\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/8\">4.1 Comparison to state-of-the-art GANs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/10\">4.2 Effectiveness of Diffusion-GAN for domain-agnostic augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/11\">4.3 Effectiveness of Diffusion-GAN for limited data</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/11\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/16\">A Related work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/17\">B Proof</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/19\">C Derivations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/19\">D Details of toy example</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/21\">E Dataset descriptions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/22\">F Algorithm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/22\">G Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/23\">H Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/23\">I Ablation on the mixing procedure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/24\">J More GAN variants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QR4P2BTF/25\">K More generated images</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_Diffusion-GAN.pdf"
              ]
            ],
            "resource": "storage/i1935.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Diffusion-GAN",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator's timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs."
          ],
          [
            "Access Date",
            "2022-11-02 09:02:28"
          ],
          [
            "Archiveid",
            "arXiv:2206.02262"
          ],
          [
            "Creators",
            "Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou"
          ],
          [
            "DOI",
            "10.48550/arXiv.2206.02262"
          ],
          [
            "Date",
            "2022-10-08 2022-10-08"
          ],
          [
            "Extra",
            "arXiv:2206.02262 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Diffusion-GAN"
          ],
          [
            "Title",
            "Diffusion-GAN: Training GANs with Diffusion"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2206.02262"
          ]
        ],
        "resource": "storage/i1935.pdf",
        "selectable": false
      },
      {
        "text": "DreamBooth",
        "item-id": "i2817",
        "nodes": [
          {
            "text": "Ruiz et al_2023_DreamBooth.pdf",
            "item-id": "i2913",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ruiz et al_2023_DreamBooth.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ruiz et al_2023_DreamBooth.pdf"
              ]
            ],
            "resource": "storage/i2913.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DreamBooth",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
          ],
          [
            "Access Date",
            "2023-07-01 07:13:27"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "22500-22510"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "DreamBooth"
          ],
          [
            "Title",
            "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2913.pdf",
        "selectable": false
      },
      {
        "text": "Elucidating the Design Space of Diffusion-Based Generative Models",
        "item-id": "i1852",
        "nodes": [
          {
            "text": "Comment: NeurIPS 2022",
            "item-id": "n1947",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NeurIPS 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NeurIPS 2022</div>",
            "node_type": "note"
          },
          {
            "text": "Karras et al_2022_Elucidating the Design Space of Diffusion-Based Generative Models.pdf",
            "item-id": "i1946",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Karras et al_2022_Elucidating the Design Space of Diffusion-Based Generative Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XN2VGUMP/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/2\">2 Expressing diffusion models in a common framework</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/4\">3 Improvements to deterministic sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/6\">4 Stochastic sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/8\">5 Preconditioning and training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/10\">6 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/13\">A Additional results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/13\">B Derivation of formulas</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/13\">B.1 Original ODE / SDE formulation from previous work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/13\">B.2 Our ODE formulation (Eq. 1 and Eq. 4)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/22\">B.3 Denoising score matching (Eq. 2 and Eq. 3)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/24\">B.4 Evaluating our ODE in practice (Algorithm 1)</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/25\">B.5 Our SDE formulation (Eq. 6)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/25\">B.5.1 Generating the marginals by heat diffusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/26\">B.5.2 Derivation of our SDE</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/27\">B.6 Our preconditioning and training (Eq. 8)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/29\">C Reframing previous methods in our framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/29\">C.1 Variance preserving formulation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/29\">C.1.1 VP sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/31\">C.1.2 VP preconditioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/31\">C.1.3 VP training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/32\">C.1.4 VP practical considerations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/32\">C.2 Variance exploding formulation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/32\">C.2.1 VE sampling in theory</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/33\">C.2.2 VE sampling in practice</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/34\">C.2.3 VE preconditioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/34\">C.2.4 VE training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/35\">C.2.5 VE practical considerations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/35\">C.3 Improved DDPM and DDIM</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/35\">C.3.1 DDIM ODE formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/35\">C.3.2 iDDPM time step discretization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/38\">C.3.3 iDDPM preconditioning and training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/38\">C.3.4 iDDPM practical considerations</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/38\">D Further analysis of deterministic sampling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/38\">D.1 Truncation error analysis and choice of discretization parameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/40\">D.2 General family of 2nd order Runge-Kutta variants</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/41\">E Further results with stochastic sampling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/41\">E.1 Image degradation due to excessive stochastic iteration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/43\">E.2 Stochastic sampling parameters</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/43\">F Implementation details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/44\">F.1 FID calculation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/44\">F.2 Augmentation regularization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/45\">F.3 Training configurations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/46\">F.4 Network architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XN2VGUMP/47\">F.5 Licenses</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Karras et al_2022_Elucidating the Design Space of Diffusion-Based Generative Models.pdf"
              ]
            ],
            "resource": "storage/i1946.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Elucidating the Design Space of Diffusion-Based Generative Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36."
          ],
          [
            "Access Date",
            "2022-11-02 08:00:21"
          ],
          [
            "Archiveid",
            "arXiv:2206.00364"
          ],
          [
            "Creators",
            "Tero Karras, Miika Aittala, Timo Aila, Samuli Laine"
          ],
          [
            "DOI",
            "10.48550/arXiv.2206.00364"
          ],
          [
            "Date",
            "2022-10-11 2022-10-11"
          ],
          [
            "Extra",
            "arXiv:2206.00364 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Elucidating the Design Space of Diffusion-Based Generative Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2206.00364"
          ]
        ],
        "resource": "storage/i1946.pdf",
        "selectable": false
      },
      {
        "text": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
        "item-id": "i1848",
        "nodes": [
          {
            "text": "Ramesh et al_2022_Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf",
            "item-id": "i1940",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ramesh et al_2022_Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_S8AYHS3E/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/3\">2 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/4\">2.1 Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/4\">2.2 Prior</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/6\">3 Image Manipulations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/6\">3.1 Variations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/7\">3.2 Interpolations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/7\">3.3 Text Diffs</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/8\">4 Probing the CLIP Latent Space</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/9\">5 Text-to-Image Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/9\">5.1 Importance of the Prior</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/10\">5.2 Human Evaluations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/12\">5.3 Improved Diversity-Fidelity Trade-off with Guidance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/13\">5.4 Comparison on MS-COCO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/13\">5.5 Aesthetic Quality Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/15\">6 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/16\">7 Limitations and Risks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/18\">8 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/23\">A Linear Probes for Evaluations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/23\">B Error Bars for Human Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/23\">C Training Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S8AYHS3E/25\">D Random samples</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ramesh et al_2022_Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf"
              ]
            ],
            "resource": "storage/i1940.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."
          ],
          [
            "Access Date",
            "2022-11-02 08:14:12"
          ],
          [
            "Archiveid",
            "arXiv:2204.06125"
          ],
          [
            "Creators",
            "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.06125"
          ],
          [
            "Date",
            "2022-04-12 2022-04-12"
          ],
          [
            "Extra",
            "arXiv:2204.06125 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Hierarchical Text-Conditional Image Generation with CLIP Latents"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.06125"
          ]
        ],
        "resource": "storage/i1940.pdf",
        "selectable": false
      },
      {
        "text": "High-Resolution Image Synthesis With Latent Diffusion Models",
        "item-id": "i1785",
        "nodes": [
          {
            "text": "Rombach et al_2022_High-Resolution Image Synthesis With Latent Diffusion Models.pdf",
            "item-id": "i1794",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rombach et al_2022_High-Resolution Image Synthesis With Latent Diffusion Models.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rombach et al_2022_High-Resolution Image Synthesis With Latent Diffusion Models.pdf"
              ]
            ],
            "resource": "storage/i1794.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "High-Resolution Image Synthesis With Latent Diffusion Models",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2022-10-08 06:07:52"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10684-10695"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "High-Resolution Image Synthesis With Latent Diffusion Models"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1794.pdf",
        "selectable": false
      },
      {
        "text": "Image Super-Resolution Via Iterative Refinement",
        "item-id": "i1850",
        "nodes": [
          {
            "text": "Saharia et al_2022_Image Super-Resolution Via Iterative Refinement.pdf",
            "item-id": "i1943",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Saharia et al_2022_Image Super-Resolution Via Iterative Refinement.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Saharia et al_2022_Image Super-Resolution Via Iterative Refinement.pdf"
              ]
            ],
            "resource": "storage/i1943.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Image Super-Resolution Via Iterative Refinement",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models [1], [2] to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8\u00d7 face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34%. We evaluate SR3 on a 4\u00d7 super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256\u00d7256 ImageNet generation challenge."
          ],
          [
            "Creators",
            "Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2022.3204461"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-14"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Title",
            "Image Super-Resolution Via Iterative Refinement"
          ]
        ],
        "resource": "storage/i1943.pdf",
        "selectable": false
      },
      {
        "text": "Imagic",
        "item-id": "i2097",
        "nodes": [
          {
            "text": "Kawar et al_2022_Imagic.pdf",
            "item-id": "i2130",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kawar et al_2022_Imagic.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_3EXBJFVB/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/3\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/4\">3 . Imagic: Diffusion-Based Real Image Editing</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/4\">3.1 . Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/5\">3.2 . Our Method</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/5\">3.3 . Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/6\">4 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/6\">4.1 . Qualitative Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/6\">4.2 . Comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/8\">4.3 . Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/8\">4.4 . Limitations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/9\">5 . Conclusions and Future Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/9\">5.1 . Societal Impact</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3EXBJFVB/12\">A . Ablation Study</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kawar et al_2022_Imagic.pdf"
              ]
            ],
            "resource": "storage/i2130.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Imagic",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently either limited to specific editing types (e.g., object overlay, style transfer), or apply to synthetically generated images, or require multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-guided semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down or jump, cause a bird to spread its wings, etc. -- each within its single high-resolution natural image provided by the user. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, which we call \"Imagic\", leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of our method on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework."
          ],
          [
            "Access Date",
            "2022-11-07 17:24:50"
          ],
          [
            "Archiveid",
            "arXiv:2210.09276"
          ],
          [
            "Creators",
            "Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani"
          ],
          [
            "DOI",
            "10.48550/arXiv.2210.09276"
          ],
          [
            "Date",
            "2022-10-17 2022-10-17"
          ],
          [
            "Extra",
            "arXiv:2210.09276 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Imagic"
          ],
          [
            "Title",
            "Imagic: Text-Based Real Image Editing with Diffusion Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2210.09276"
          ]
        ],
        "resource": "storage/i2130.pdf",
        "selectable": false
      },
      {
        "text": "Improved Denoising Diffusion Probabilistic Models",
        "item-id": "i1828",
        "nodes": [
          {
            "text": "Nichol_Dhariwal_2021_Improved Denoising Diffusion Probabilistic Models.pdf",
            "item-id": "i2075",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Nichol_Dhariwal_2021_Improved Denoising Diffusion Probabilistic Models.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Nichol_Dhariwal_2021_Improved Denoising Diffusion Probabilistic Models.pdf"
              ]
            ],
            "resource": "storage/i2075.pdf"
          },
          {
            "text": "Supplementary PDF",
            "item-id": "i2074",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-10-13 13:50:57"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v139/nichol21a/nichol21a-supp.pdf"
              ]
            ],
            "resource": "storage/i2074.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Improved Denoising Diffusion Probabilistic Models",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion."
          ],
          [
            "Access Date",
            "2022-10-13 13:50:55"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Alexander Quinn Nichol, Prafulla Dhariwal"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "8162-8171"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 38th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Improved Denoising Diffusion Probabilistic Models"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v139/nichol21a.html"
          ]
        ],
        "selectable": false
      },
      {
        "text": "Improving image generation with better captions",
        "item-id": "i3230",
        "nodes": [
          {
            "text": "Betker et al_2023_Improving image generation with better captions.pdf",
            "item-id": "i3318",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Betker et al_2023_Improving image generation with better captions.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Betker et al_2023_Improving image generation with better captions.pdf"
              ]
            ],
            "resource": "storage/i3318.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Improving image generation with better captions",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems."
          ],
          [
            "Creators",
            "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Library Catalog",
            "Google Scholar"
          ],
          [
            "Publication Title",
            "Computer Science"
          ],
          [
            "Title",
            "Improving image generation with better captions"
          ],
          [
            "URL",
            "https://cdn.openai.com/papers/dall-e-3.pdf"
          ]
        ],
        "resource": "storage/i3318.pdf",
        "selectable": false
      },
      {
        "text": "LoRA",
        "item-id": "i2215",
        "nodes": [
          {
            "text": "Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency",
            "item-id": "n2254",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency</div>",
            "node_type": "note"
          },
          {
            "text": "Hu et al_2021_LoRA.pdf",
            "item-id": "i2253",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hu et al_2021_LoRA.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_VF86H357/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/2\">2 Problem Statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/3\">3 Aren't Existing Solutions Good Enough?</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VF86H357/4\">4 Our Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/4\">4.1 Low-Rank-Parametrized Update Matrices</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/5\">4.2 Applying LoRA to Transformer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VF86H357/5\">5 Empirical Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/5\">5.1 Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/7\">5.2 RoBERTa base/large</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/7\">5.3 DeBERTa XXL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/7\">5.4 GPT-2 medium/large</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/8\">5.5 Scaling up to GPT-3 175B</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/8\">6 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VF86H357/9\">7 Understanding the Low-Rank Updates</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/10\">7.1 Which Weight Matrices in Transformer Should We Apply LoRA to?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/10\">7.2 What is the Optimal Rank r for LoRA?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/11\">7.3 How Does the Adaptation Matrix W Compare to W?</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/12\">8 Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/16\">A Large Language Models Still Need Parameter Updates</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/17\">B Inference Latency Introduced by Adapter Layers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/17\">C Dataset Details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VF86H357/18\">D Hyperparameters Used in Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/18\">D.1 RoBERTa</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/18\">D.2 DeBERTa</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/19\">D.3 GPT-2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/19\">D.4 GPT-3</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/20\">E Combining LoRA with Prefix Tuning</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VF86H357/21\">F Additional Empirical Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/21\">F.1 Additional Experiments on GPT-2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/22\">F.2 Additional Experiments on GPT-3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/22\">F.3 Low-Data Regime</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/22\">G Measuring Similarity Between Subspaces</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VF86H357/24\">H Additional Experiments on Low-Rank Matrices</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/24\">H.1 Correlation between LoRA Modules</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/24\">H.2 Effect of r on GPT-2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/24\">H.3 Correlation between W and W</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VF86H357/25\">H.4 Amplification Factor</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hu et al_2021_LoRA.pdf"
              ]
            ],
            "resource": "storage/i2253.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "LoRA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
          ],
          [
            "Access Date",
            "2023-02-17 00:12:27"
          ],
          [
            "Archiveid",
            "arXiv:2106.09685"
          ],
          [
            "Creators",
            "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen"
          ],
          [
            "DOI",
            "10.48550/arXiv.2106.09685"
          ],
          [
            "Date",
            "2021-10-16 2021-10-16"
          ],
          [
            "Extra",
            "arXiv:2106.09685 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "LoRA"
          ],
          [
            "Title",
            "LoRA: Low-Rank Adaptation of Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2106.09685"
          ]
        ],
        "resource": "storage/i2253.pdf",
        "selectable": false
      },
      {
        "text": "Navigating Text-To-Image Customization",
        "item-id": "i3649",
        "nodes": [
          {
            "text": "Comment: 77 pages, 54 figures, 6 tables",
            "item-id": "n3696",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 77 pages, 54 figures, 6 tables",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 77 pages, 54 figures, 6 tables</div>",
            "node_type": "note"
          },
          {
            "text": "Yeh et al_2023_Navigating Text-To-Image Customization.pdf",
            "item-id": "i3695",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yeh et al_2023_Navigating Text-To-Image Customization.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WQ6MTG5P/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/2\">2 Preliminary</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/2\">2.1 Stable Diffusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/3\">2.2 Model Customization With LoRA</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/3\">3 The LyCORIS Library</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/3\">3.1 Design and Objectives</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/3\">3.2 Implemented Algorithms</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/5\">4 Evaluating Fine-Tuned Text-To-Image Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/5\">4.1 Classification of Prompts for Image Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/6\">4.2 Evaluation Criteria</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/6\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/6\">5.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/7\">5.2 Algorithm Configuration and Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/8\">5.3 Experimental Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/10\">6 Concluding Remarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/19\">A Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/21\">B Further Discussion on Implemented Algorithms</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/22\">B.1 Effect of Merge Ratio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/23\">B.2 Tucker Decomposition of Convolutional Layers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/24\">B.3 LoKr as Consecutive Linear Layers</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/25\">C Challenges in Algorithm Evaluation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/26\">D Experimental Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/26\">D.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/29\">D.2 Algorithm Configuration and Hardware</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/29\">D.3 Evaluation Prompts</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/30\">D.4 Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/33\">D.5 Encoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/33\">D.6 Metric Value Processing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/34\">E Correlation Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/34\">E.1 Influence of Encoders, Resizing Methods, and Prompt Types</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/35\">E.2 Relation between Different Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/36\">F Supporting Plots</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/37\">F.1 Plots for Category ``Movie Characters''</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/39\">F.2 Plots for Category ``Scenes''</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/41\">F.3 Plots for Category ``Stuffed Toys''</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/43\">F.4 Plots for Category ``Anime Characters''</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/45\">F.5 Plots for Category ``Styles''</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/47\">G Further Qualitative Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/47\">G.1 Discrepancy of Results Across Classes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/47\">G.2 Unreliability of Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/47\">G.3 Illustrating the Impact of Different Algorithm Components</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/56\">G.4 Violations of the General Principles</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/64\">G.5 Example Generations for Category ``Styles''</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/69\">H Additional Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/69\">H.1 Investigating the Relevance of Image Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/71\">H.2 Image Quality Assessment with Pretrained Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/73\">H.3 Impact of Captioning Strategies on Model Performance</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WQ6MTG5P/77\">I Author Contributions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yeh et al_2023_Navigating Text-To-Image Customization.pdf"
              ]
            ],
            "resource": "storage/i3695.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3694",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 10:00:41"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2309.14859"
              ]
            ],
            "resource": "storage/i3694.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Navigating Text-To-Image Customization",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application."
          ],
          [
            "Access Date",
            "2024-01-14 09:58:37"
          ],
          [
            "Archiveid",
            "arXiv:2309.14859"
          ],
          [
            "Creators",
            "Shin-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard B. W. Yang, Giyeong Oh, Yanmin Gong"
          ],
          [
            "DOI",
            "10.48550/arXiv.2309.14859"
          ],
          [
            "Date",
            "2023-09-26 2023-09-26"
          ],
          [
            "Extra",
            "arXiv:2309.14859 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Navigating Text-To-Image Customization"
          ],
          [
            "Title",
            "Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2309.14859"
          ]
        ],
        "resource": "storage/i3695.pdf",
        "selectable": false
      },
      {
        "text": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "item-id": "i1851",
        "nodes": [
          {
            "text": "Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf",
            "item-id": "i2073",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BF5HN5U5/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/3\">2 Imagen</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/3\">2.1 Pretrained text encoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/4\">2.2 Diffusion models and classifier-free guidance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/4\">2.3 Large guidance weight samplers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/5\">2.4 Robust cascaded diffusion models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/5\">2.5 Neural network architecture</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/5\">3 Evaluating Text-to-Image Models</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/6\">4.1 Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/7\">4.2 Results on COCO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/7\">4.3 Results on DrawBench</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/7\">4.4 Analysis of Imagen</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/8\">5 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/9\">6 Conclusions, Limitations and Societal Impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/10\">7 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/20\">A Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/20\">B Architecture Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/20\">B.1 Efficient U-Net</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/21\">C DrawBench</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/21\">D Imagen Detailed Abalations and Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/21\">D.1 Pre-trained Text Encoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/22\">D.2 Classifier-free Guidance and the Alignment-Fidelity Trade-off</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/25\">D.3 Impact of Model Size</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/25\">D.3.1 Impact of Text Conditioning Schemas</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/26\">D.3.2 Comparison of U-Net vs Efficient U-Net</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/26\">E Comparison to GLIDE and DALL-E 2</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/44\">F Implementation Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/44\">F.1 6464</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/45\">F.2 6464 256256</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/45\">F.3 256256 10241024</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf"
              ]
            ],
            "resource": "storage/i2073.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."
          ],
          [
            "Access Date",
            "2022-10-13 21:03:34"
          ],
          [
            "Archiveid",
            "arXiv:2205.11487"
          ],
          [
            "Creators",
            "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, Mohammad Norouzi"
          ],
          [
            "DOI",
            "10.48550/arXiv.2205.11487"
          ],
          [
            "Date",
            "2022-05-23 2022-05-23"
          ],
          [
            "Extra",
            "arXiv:2205.11487 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2205.11487"
          ]
        ],
        "resource": "storage/i2073.pdf",
        "selectable": false
      },
      {
        "text": "PriorGrad",
        "item-id": "i1847",
        "nodes": [
          {
            "text": "Lee et al_2022_PriorGrad.pdf",
            "item-id": "i1938",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lee et al_2022_PriorGrad.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZQDZBE3I/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/2\">Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/4\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/4\">General Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/5\">Theoretical Analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/5\">Application to Vocoder</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/5\">PriorGrad for Vocoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/6\">Experimental Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/6\">Experimental Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/7\">Application to Acoustic Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/8\">PriorGrad for Acoustic Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/8\">Experimental Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/8\">Experimental Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/9\">Discussion and Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/13\">Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/13\">Derivation of the ELBO Loss</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/13\">Theoretical Benefits of PriorGrad</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/14\">Exploration on the Conditional Information Source of PriorGrad Vocoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/15\">Description of objective metrics for PriorGrad vocoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/16\">Additional Details of PriorGrad Acoustic Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/17\">Comparison to trainable diffusion prior</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/18\">Comparison to state-of-the-art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/19\">MOS evaluation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZQDZBE3I/19\">Audio samples demo page</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lee et al_2022_PriorGrad.pdf"
              ]
            ],
            "resource": "storage/i1938.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PriorGrad",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework assumes the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the audio domain, we consider the recently proposed diffusion-based audio generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and superior performance, leading to an improved perceptual quality and tolerance to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior."
          ],
          [
            "Access Date",
            "2022-11-02 08:18:41"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, Tie-Yan Liu"
          ],
          [
            "Date",
            "2022-02-20 2022/02/20"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "PriorGrad"
          ],
          [
            "Title",
            "PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=_BNiN4IjC5"
          ]
        ],
        "resource": "storage/i1938.pdf",
        "selectable": false
      },
      {
        "text": "SDXL",
        "item-id": "i3664",
        "nodes": [
          {
            "text": "Podell et al_2023_SDXL.pdf",
            "item-id": "i3698",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Podell et al_2023_SDXL.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DB5G7GTQ/2\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/2\">Improving Stable Diffusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/2\">Architecture &amp;amp; Scale</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/3\">Micro-Conditioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/5\">Multi-Aspect Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/7\">Improved Autoencoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/7\">Putting Everything Together</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/8\">Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/9\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/9\">Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/11\">Diffusion Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/12\">Comparison to the State of the Art</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/13\">Comparison to Midjourney v5.1</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/13\">Overall Votes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/13\">Category &amp;amp; challenge comparisons on PartiPrompts (P2)</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/14\">On FID Assessment of Generative Text-Image Foundation Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/15\">Additional Comparison between Single- and Two-Stage SDXL pipeline</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/16\">Comparison between SD 1.5 vs. SD 2.1 vs. SDXL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/17\">Multi-Aspect Training Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DB5G7GTQ/18\">Pseudo-code for Conditioning Concatenation along the Channel Axis</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Podell et al_2023_SDXL.pdf"
              ]
            ],
            "resource": "storage/i3698.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3697",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-14 08:34:56"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2307.01952"
              ]
            ],
            "resource": "storage/i3697.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "SDXL",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models"
          ],
          [
            "Access Date",
            "2024-01-14 08:34:25"
          ],
          [
            "Archiveid",
            "arXiv:2307.01952"
          ],
          [
            "Creators",
            "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, Robin Rombach"
          ],
          [
            "DOI",
            "10.48550/arXiv.2307.01952"
          ],
          [
            "Date",
            "2023-07-04 2023-07-04"
          ],
          [
            "Extra",
            "arXiv:2307.01952 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "SDXL"
          ],
          [
            "Title",
            "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2307.01952"
          ]
        ],
        "resource": "storage/i3698.pdf",
        "selectable": false
      },
      {
        "text": "Safe Latent Diffusion",
        "item-id": "i2818",
        "nodes": [
          {
            "text": "Schramowski et al_2023_Safe Latent Diffusion.pdf",
            "item-id": "i2914",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Schramowski et al_2023_Safe Latent Diffusion.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Schramowski et al_2023_Safe Latent Diffusion.pdf"
              ]
            ],
            "resource": "storage/i2914.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Safe Latent Diffusion",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed - inappropriate image prompts (I2P) - containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment."
          ],
          [
            "Access Date",
            "2023-07-01 07:12:29"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, Kristian Kersting"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "22522-22531"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Safe Latent Diffusion"
          ],
          [
            "Title",
            "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2914.pdf",
        "selectable": false
      },
      {
        "text": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "item-id": "i1853",
        "nodes": [
          {
            "text": "Song et al_2022_Score-Based Generative Modeling through Stochastic Differential Equations.pdf",
            "item-id": "i1949",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2022_Score-Based Generative Modeling through Stochastic Differential Equations.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BPHP92RL/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPHP92RL/2\">Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/2\">Denoising score matching with Langevin dynamics (SMLD)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/3\">Denoising diffusion probabilistic models (DDPM)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPHP92RL/3\">Score-based generative modeling with SDEs</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/3\">Perturbing data with SDEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/4\">Generating samples by reversing the SDE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/4\">Estimating scores for the SDE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/5\">Examples: VE, VP SDEs and beyond</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPHP92RL/5\">Solving the reverse SDE</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/6\">General-purpose numerical SDE solvers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/6\">Predictor-corrector samplers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/7\">Probability flow and connection to neural ODEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/8\">Architecture improvements</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/8\">Controllable generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/9\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/13\">The framework for more general SDEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/14\">VE, VP and sub-VP SDEs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/15\">SDEs in the wild</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPHP92RL/16\">Probability flow ODE</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/16\">Derivation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/18\">Likelihood computation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/18\">Probability flow sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/19\">Sampling with black-box ODE solvers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/19\">Uniquely identifiable encoding</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/19\">Reverse diffusion sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/21\">Ancestral sampling for SMLD models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/22\">Predictor-Corrector samplers</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPHP92RL/24\">Architecture improvements</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/25\">Settings for architecture exploration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/26\">Results on CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/29\">High resolution images</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPHP92RL/29\">Controllable generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/29\">Class-conditional sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/29\">Imputation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/31\">Colorization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPHP92RL/31\">Solving general inverse problems</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2022_Score-Based Generative Modeling through Stochastic Differential Equations.pdf"
              ]
            ],
            "resource": "storage/i1949.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model."
          ],
          [
            "Access Date",
            "2022-11-02 07:55:20"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole"
          ],
          [
            "Date",
            "2022-02-10 2022/02/10"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Score-Based Generative Modeling through Stochastic Differential Equations"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=PxTIG12RRHS"
          ]
        ],
        "resource": "storage/i1949.pdf",
        "selectable": false
      },
      {
        "text": "The Chosen One",
        "item-id": "i3648",
        "nodes": [
          {
            "text": "Avrahami et al_2023_The Chosen One.pdf",
            "item-id": "i3677",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Avrahami et al_2023_The Chosen One.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_W8JAXCYS/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/3\">. Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/4\">. Identity Clustering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/5\">. Identity Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/5\">. Convergence</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/5\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/5\">. Qualitative and Quantitative Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/7\">. User Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/7\">. Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/7\">. Applications</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/8\">. Limitations and Conclusions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Additional Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Additional Comparisons and Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Non-determinism of Our Method</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Na\u00efve Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Additional Feature Extractors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Additional Clustering Visualization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/9\">. Dataset Non-Memorization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/11\">. Stable Diffusion 2 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/12\">. Implementation Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/12\">. Method Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/12\">. Automatic Metrics Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/13\">. User Study Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/13\">. Applications Implementation Details</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W8JAXCYS/14\">. Societal Impact</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Avrahami et al_2023_The Chosen One.pdf"
              ]
            ],
            "resource": "storage/i3677.pdf"
          },
          {
            "text": "Comment: Project page is available at https://omriavrahami.com/the-chosen-one",
            "item-id": "n3678",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page is available at https://omriavrahami.com/the-chosen-one",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page is available at https://omriavrahami.com/the-chosen-one</div>",
            "node_type": "note"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3676",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-15 09:53:00"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2311.10093"
              ]
            ],
            "resource": "storage/i3676.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "The Chosen One",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, these models struggle with generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt. We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set. Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study. To conclude, we showcase several practical applications of our approach. Project page is available at https://omriavrahami.com/the-chosen-one"
          ],
          [
            "Access Date",
            "2024-01-15 09:52:16"
          ],
          [
            "Archiveid",
            "arXiv:2311.10093"
          ],
          [
            "Creators",
            "Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski"
          ],
          [
            "DOI",
            "10.48550/arXiv.2311.10093"
          ],
          [
            "Date",
            "2023-11-27 2023-11-27"
          ],
          [
            "Extra",
            "arXiv:2311.10093 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "The Chosen One"
          ],
          [
            "Title",
            "The Chosen One: Consistent Characters in Text-to-Image Diffusion Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2311.10093"
          ]
        ],
        "resource": "storage/i3677.pdf",
        "selectable": false
      },
      {
        "text": "Tune-A-Video",
        "item-id": "i3204",
        "nodes": [
          {
            "text": "Wu et al_2023_Tune-A-Video.pdf",
            "item-id": "i3273",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2023_Tune-A-Video.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2023_Tune-A-Video.pdf"
              ]
            ],
            "resource": "storage/i3273.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Tune-A-Video",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting--One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications."
          ],
          [
            "Access Date",
            "2023-11-14 09:32:49"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7623-7633"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "Tune-A-Video"
          ],
          [
            "Title",
            "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.html"
          ]
        ],
        "resource": "storage/i3273.pdf",
        "selectable": false
      },
      {
        "text": "eDiffi",
        "item-id": "i2099",
        "nodes": [
          {
            "text": "Balaji et al_2022_eDiffi.pdf",
            "item-id": "i2134",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Balaji et al_2022_eDiffi.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MN9B9REZ/2\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/4\">2 . Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/5\">3 . Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/5\">4 . Ensemble of Expert Denoisers</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/6\">4.1 . Efficient Training of Expert Denoisers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/7\">4.2 . Multiple Conditional Inputs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/7\">4.3 . Paint-with-words</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/7\">5 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/9\">5.1 . Main Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/10\">5.2 . CLIP Text and T5 Text</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/10\">5.3 . Style transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/14\">5.4 . Paint-with-words</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/14\">6 . Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/23\">A . Network Architecture</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/24\">B . Ensemble training schedule</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN9B9REZ/24\">B.1 . Hyper-parameters</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Balaji et al_2022_eDiffi.pdf"
              ]
            ],
            "resource": "storage/i2134.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "eDiffi",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiffi, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiffi's \"paint-with-words\" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiffi/"
          ],
          [
            "Access Date",
            "2022-11-07 14:29:41"
          ],
          [
            "Archiveid",
            "arXiv:2211.01324"
          ],
          [
            "Creators",
            "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2211.01324"
          ],
          [
            "Date",
            "2022-11-02 2022-11-02"
          ],
          [
            "Extra",
            "arXiv:2211.01324 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "eDiffi"
          ],
          [
            "Title",
            "eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2211.01324"
          ]
        ],
        "resource": "storage/i2134.pdf",
        "selectable": false
      }
    ],
    "item_title": "Diffusion",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "EEG",
    "item-id": "c42,i2927",
    "nodes": [
      {
        "text": "The eyes know it",
        "item-id": "i2927",
        "nodes": [
          {
            "text": "Gupta et al_2020_The eyes know it.pdf",
            "item-id": "i2934",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gupta et al_2020_The eyes know it.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gupta et al_2020_The eyes know it.pdf"
              ]
            ],
            "resource": "storage/i2934.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The eyes know it",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present FakeET -- an eye-tracking database to understand human visual perception of deepfake videos. Given that the principal purpose of deepfakes is to deceive human observers, FakeET is designed to understand and evaluate the ability of viewers to detect synthetic video artifacts. FakeET contains viewing patterns compiled from 40 users via the Tobii desktop eye-tracker for 811 videos from the Google Deepfake dataset, with a minimum of two viewings per video. Additionally, EEG responses acquired via the Emotiv sensor are also available. The compiled data confirms (a) distinct eye movement characteristics for real vs fake videos; (b) utility of the eye-track saliency maps for spatial forgery localization and detection, and (c) Error Related Negativity (ERN) triggers in the EEG responses, and the ability of the raw EEG signal to distinguish between real and fake videos."
          ],
          [
            "Access Date",
            "2023-07-21"
          ],
          [
            "Creators",
            "Parul Gupta, Komal Chugh, Abhinav Dhall, Ramanathan Subramanian"
          ],
          [
            "DOI",
            "10.1145/3382507.3418857"
          ],
          [
            "Date",
            "2020-00-22 \u5341\u6708 22, 2020"
          ],
          [
            "ISBN",
            "978-1-4503-7581-8"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "519\u2013527"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2020 International Conference on Multimodal Interaction"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "ICMI '20"
          ],
          [
            "Short Title",
            "The eyes know it"
          ],
          [
            "Title",
            "The eyes know it: FakeET- An Eye-tracking Database to Understand Deepfake Perception"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3382507.3418857"
          ]
        ],
        "resource": "storage/i2934.pdf",
        "selectable": false
      }
    ],
    "item_title": "EEG",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Emotion Recognition",
    "item-id": "c19,i2374",
    "nodes": [
      {
        "text": "A Self-Fusion Network Based on Contrastive Learning for Group Emotion Recognition",
        "item-id": "i2358",
        "nodes": [
          {
            "text": "Wang et al_2023_A Self-Fusion Network Based on Contrastive Learning for Group Emotion.pdf",
            "item-id": "i2459",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_A Self-Fusion Network Based on Contrastive Learning for Group Emotion.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_A Self-Fusion Network Based on Contrastive Learning for Group Emotion.pdf"
              ]
            ],
            "resource": "storage/i2459.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Self-Fusion Network Based on Contrastive Learning for Group Emotion Recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Group emotion recognition (GER) from image has attracted much attention in recent years. Networks using attention mechanism for GER have shown great potential. However, the performance of the current attention-based GER networks suffers from the indistinctive features of individuals in the group, poor feature fusion weights, and the lack of semantic information of the objects in the image. We present a new framework that is composed of three networks, FacesNet, SceneNet, and ObjectsNet, to address these shortcomings. This new framework is designed to recognize group emotion by exploiting the information from the faces, scene, and objects in image. In FacesNet, we use contrastive learning to help the network extract distinctive emotion features and a new attention mechanism named self-fusion module to generate precise fusion weights for aggregation of individual facial features. We design SceneNet to capture the multiscale scene features to exploit the emotion cues from the scene. We construct a fully connected network named ObjectsNet to classify the semantic features of the objects. Finally, we linearly integrate the outputs of these three networks as the final output of this unique framework for GER. Experiment results on three datasets for GER show that our proposed framework achieved better performance in terms of recognition accuracy compared with the state-of-the-art methods."
          ],
          [
            "Creators",
            "Xingzhi Wang, Dong Zhang, Hong-Zhou Tan, Dah-Jye Lee"
          ],
          [
            "DOI",
            "10.1109/TCSS.2022.3202249"
          ],
          [
            "Date",
            "2023-04-00 2023-04"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Computational Social Systems"
          ],
          [
            "ISSN",
            "2329-924X"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "458-469"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Computational Social Systems"
          ],
          [
            "Title",
            "A Self-Fusion Network Based on Contrastive Learning for Group Emotion Recognition"
          ],
          [
            "Volume",
            "10"
          ]
        ],
        "resource": "storage/i2459.pdf",
        "selectable": false
      },
      {
        "text": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
        "item-id": "i1422",
        "nodes": [
          {
            "text": "Comment: Winner of the ACL20: Second Grand-Challenge on Multimodal Language",
            "item-id": "n1462",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Winner of the ACL20: Second Grand-Challenge on Multimodal Language",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Winner of the ACL20: Second Grand-Challenge on Multimodal Language</div>",
            "node_type": "note"
          },
          {
            "text": "Delbrouck et al_2020_A Transformer-based joint-encoding for Emotion Recognition and Sentiment.pdf",
            "item-id": "i1461",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Delbrouck et al_2020_A Transformer-based joint-encoding for Emotion Recognition and Sentiment.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Delbrouck et al_2020_A Transformer-based joint-encoding for Emotion Recognition and Sentiment.pdf"
              ]
            ],
            "resource": "storage/i1461.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source: https://github.com/jbdel/MOSEI_UMONS."
          ],
          [
            "Access Date",
            "2022-04-04 08:08:48"
          ],
          [
            "Creators",
            "Jean-Benoit Delbrouck, No\u00e9 Tits, Mathilde Brousmiche, St\u00e9phane Dupont"
          ],
          [
            "DOI",
            "10.18653/v1/2020.challengehml-1.1"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "arXiv: 2006.15955"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "1-7"
          ],
          [
            "Publication Title",
            "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)"
          ],
          [
            "Title",
            "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2006.15955"
          ]
        ],
        "resource": "storage/i1461.pdf",
        "selectable": false
      },
      {
        "text": "Affective Computing",
        "item-id": "i2365",
        "icon": "glyphicon glyphicon-book",
        "item_title": "Affective Computing",
        "item_type": "book",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "According to Rosalind Picard, if we want computers to be genuinely intelligent and to interact naturally with us, we must give computers the ability to recognize, understand, even to have and express emotions.The latest scientific findings indicate that emotions play an essential role in decision making, perception, learning, and more\u2014that is, they influence the very mechanisms of rational thinking. Not only too much, but too little emotion can impair decision making. According to Rosalind Picard, if we want computers to be genuinely intelligent and to interact naturally with us, we must give computers the ability to recognize, understand, even to have and express emotions.Part 1 of this book provides the intellectual framework for affective computing. It includes background on human emotions, requirements for emotionally intelligent computers, applications of affective computing, and moral and social questions raised by the technology. Part 2 discusses the design and construction of affective computers. Although this material is more technical than that in Part 1, the author has kept it less technical than typical scientific publications in order to make it accessible to newcomers. Topics in Part 2 include signal-based representations of emotions, human affect recognition as a pattern recognition and learning problem, recent and ongoing efforts to build models of emotion for synthesizing emotions in computers, and the new application area of affective wearable computers."
          ],
          [
            "Creators",
            "Rosalind W. Picard"
          ],
          [
            "Date",
            "2000-07-24 2000-07-24"
          ],
          [
            "Extra",
            "Google-Books-ID: GaVncRTcb1gC"
          ],
          [
            "ISBN",
            "978-0-262-66115-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Google Books"
          ],
          [
            "Num Pages",
            "308"
          ],
          [
            "Publisher",
            "MIT Press"
          ],
          [
            "Title",
            "Affective Computing"
          ]
        ]
      },
      {
        "text": "Automatic Group Happiness Intensity Analysis",
        "item-id": "i2374",
        "nodes": [
          {
            "text": "Dhall et al_2015_Automatic Group Happiness Intensity Analysis.pdf",
            "item-id": "i2410",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dhall et al_2015_Automatic Group Happiness Intensity Analysis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dhall et al_2015_Automatic Group Happiness Intensity Analysis.pdf"
              ]
            ],
            "resource": "storage/i2410.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Automatic Group Happiness Intensity Analysis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The recent advancement of social media has given users a platform to socially engage and interact with a larger population. Millions of images and videos are being uploaded everyday by users on the web from different events and social gatherings. There is an increasing interest in designing systems capable of understanding human manifestations of emotional attributes and affective displays. As images and videos from social events generally contain multiple subjects, it is an essential step to study these groups of people. In this paper, we study the problem of happiness intensity analysis of a group of people in an image using facial expression analysis. A user perception study is conducted to understand various attributes, which affect a person's perception of the happiness intensity of a group. We identify the challenges in developing an automatic mood analysis system and propose three models based on the attributes in the study. An `in the wild' image-based database is collected. To validate the methods, both quantitative and qualitative experiments are performed and applied to the problem of shot selection, event summarisation and album creation. The experiments show that the global and local attributes defined in the paper provide useful information for theme expression analysis, with results close to human perception results."
          ],
          [
            "Creators",
            "Abhinav Dhall, Roland Goecke, Tom Gedeon"
          ],
          [
            "DOI",
            "10.1109/TAFFC.2015.2397456"
          ],
          [
            "Date",
            "2015-01-00 2015-01"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Affective Computing"
          ],
          [
            "ISSN",
            "1949-3045"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "13-26"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Affective Computing"
          ],
          [
            "Title",
            "Automatic Group Happiness Intensity Analysis"
          ],
          [
            "Volume",
            "6"
          ]
        ],
        "resource": "storage/i2410.pdf",
        "selectable": false
      },
      {
        "text": "BOLD",
        "item-id": "i2330",
        "nodes": [
          {
            "text": "Dhamala et al_2021_BOLD.pdf",
            "item-id": "i2340",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dhamala et al_2021_BOLD.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_VPEVAP7F/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/2\">3 BOLD: Bias in Open-ended Language Generation Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/3\">3.1 BOLD statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/3\">3.2 BOLD collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/3\">3.3 BOLD post-processing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/3\">4 Evaluation Metrics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/3\">4.1 Sentiment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/4\">4.2 Toxicity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/4\">4.3 Regard</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/4\">4.4 Psycholinguistic norms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/4\">4.5 Gender polarity</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/5\">5 Generating with Language Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/5\">5.1 BERT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/5\">5.2 GPT-2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/5\">5.3 CTRL</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/5\">6 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/5\">6.1 Bias across groups in each domain</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/7\">6.2 Comparison of language generation models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/8\">6.3 Validation with human annotated metrics</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/10\">7 Limitations and Discussions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/10\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/10\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/11\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/12\">A Appendix A</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/12\">A.1 Data Collection Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/12\">A.2 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/12\">A.3 AMT experiment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VPEVAP7F/13\">A.4 Detailed Results</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dhamala et al_2021_BOLD.pdf"
              ]
            ],
            "resource": "storage/i2340.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BOLD",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices."
          ],
          [
            "Access Date",
            "2023-05-03"
          ],
          [
            "Creators",
            "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta"
          ],
          [
            "DOI",
            "10.1145/3442188.3445924"
          ],
          [
            "Date",
            "2021-00-01 \u4e09\u6708 1, 2021"
          ],
          [
            "ISBN",
            "978-1-4503-8309-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "862\u2013872"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "FAccT '21"
          ],
          [
            "Short Title",
            "BOLD"
          ],
          [
            "Title",
            "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3442188.3445924"
          ]
        ],
        "resource": "storage/i2340.pdf",
        "selectable": false
      },
      {
        "text": "Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network",
        "item-id": "i2331",
        "nodes": [
          {
            "text": "Savchenko et al_2022_Classifying Emotions and Engagement in Online Learning Based on a Single Facial.pdf",
            "item-id": "i2341",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Savchenko et al_2022_Classifying Emotions and Engagement in Online Learning Based on a Single Facial.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Savchenko et al_2022_Classifying Emotions and Engagement in Online Learning Based on a Single Facial.pdf"
              ]
            ],
            "resource": "storage/i2341.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this article, behaviour of students in the e-learning environment is analyzed. The novel pipeline is proposed based on video facial processing. At first, face detection, tracking and clustering techniques are applied to extract the sequences of faces of each student. Next, a single efficient neural network is used to extract emotional features in each frame. This network is pre-trained on face identification and fine-tuned for facial expression recognition on static images from AffectNet using a specially developed robust optimization technique. It is shown that the resulting facial features can be used for fast simultaneous prediction of students\u2019 engagement levels (from disengaged to highly engaged), individual emotions (happy, sad, etc.,) and group-level affect (positive, neutral or negative). This model can be used for real-time video processing even on a mobile device of each student without the need for sending their facial video to the remote server or teacher's PC. In addition, the possibility to prepare a summary of a lesson is demonstrated by saving short clips of different emotions and engagement of all students. The experimental study on the datasets from EmotiW (Emotion Recognition in the Wild) challenges showed that the proposed network significantly outperforms existing single models."
          ],
          [
            "Creators",
            "Andrey V. Savchenko, Lyudmila V. Savchenko, Ilya Makarov"
          ],
          [
            "DOI",
            "10.1109/TAFFC.2022.3188390"
          ],
          [
            "Date",
            "2022-10-00 2022-10"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Affective Computing"
          ],
          [
            "ISSN",
            "1949-3045"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2132-2143"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Affective Computing"
          ],
          [
            "Title",
            "Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "resource": "storage/i2341.pdf",
        "selectable": false
      },
      {
        "text": "Deep Facial Expression Recognition",
        "item-id": "i1907",
        "nodes": [
          {
            "text": "Li_Deng_2022_Deep Facial Expression Recognition.pdf",
            "item-id": "i2042",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li_Deng_2022_Deep Facial Expression Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li_Deng_2022_Deep Facial Expression Recognition.pdf"
              ]
            ],
            "resource": "storage/i2042.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Facial Expression Recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose, and identity bias. In this survey, we provide a comprehensive review of deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions for applicable implementations for each stage. For the state-of-the-art in deep FER, we introduce existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems."
          ],
          [
            "Creators",
            "Shan Li, Weihong Deng"
          ],
          [
            "DOI",
            "10.1109/TAFFC.2020.2981446"
          ],
          [
            "Date",
            "2022-07-00 2022-07"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Affective Computing"
          ],
          [
            "ISSN",
            "1949-3045"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1195-1215"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Affective Computing"
          ],
          [
            "Short Title",
            "Deep Facial Expression Recognition"
          ],
          [
            "Title",
            "Deep Facial Expression Recognition: A Survey"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "resource": "storage/i2042.pdf",
        "selectable": false
      },
      {
        "text": "How you feelin'?",
        "item-id": "i2335",
        "nodes": [
          {
            "text": "Comment: CVPR 2023. Project Page: https://katha-ai.github.io/projects/emotx/",
            "item-id": "n2350",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: CVPR 2023. Project Page: https://katha-ai.github.io/projects/emotx/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: CVPR 2023. Project Page: https://katha-ai.github.io/projects/emotx/</div>",
            "node_type": "note"
          },
          {
            "text": "arXiv Fulltext PDF",
            "item-id": "i2349",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv Fulltext PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-05-02 07:15:28"
              ],
              [
                "Title",
                "arXiv Fulltext PDF"
              ],
              [
                "URL",
                "https://arxiv.org/pdf/2304.05634.pdf"
              ]
            ],
            "resource": "storage/i2349.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "How you feelin'?",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Movie story analysis requires understanding characters' emotions and mental states. Towards this goal, we formulate emotion understanding as predicting a diverse and multi-label set of emotions at the level of a movie scene and for each character. We propose EmoTx, a multimodal Transformer-based architecture that ingests videos, multiple characters, and dialog utterances to make joint predictions. By leveraging annotations from the MovieGraphs dataset, we aim to predict classic emotions (e.g. happy, angry) and other mental states (e.g. honest, helpful). We conduct experiments on the most frequently occurring 10 and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies and comparison against adapted state-of-the-art emotion recognition approaches shows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scores reveals that expressive emotions often look at character tokens while other mental states rely on video and dialog cues."
          ],
          [
            "Access Date",
            "2023-05-02 07:15:18"
          ],
          [
            "Archiveid",
            "arXiv:2304.05634"
          ],
          [
            "Creators",
            "Dhruv Srivastava, Aditya Kumar Singh, Makarand Tapaswi"
          ],
          [
            "DOI",
            "10.48550/arXiv.2304.05634"
          ],
          [
            "Date",
            "2023-04-12 2023-04-12"
          ],
          [
            "Extra",
            "arXiv:2304.05634 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "How you feelin'?"
          ],
          [
            "Title",
            "How you feelin'? Learning Emotions and Mental States in Movie Scenes"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2304.05634"
          ]
        ],
        "resource": "storage/i2349.pdf",
        "selectable": false
      },
      {
        "text": "IEMOCAP",
        "item-id": "i2202",
        "nodes": [
          {
            "text": "Busso et al_2008_IEMOCAP.pdf",
            "item-id": "i2233",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Busso et al_2008_IEMOCAP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Busso et al_2008_IEMOCAP.pdf"
              ]
            ],
            "resource": "storage/i2233.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "IEMOCAP",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the \u201cinteractive emotional dyadic motion capture database\u201d (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12\u00a0h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication."
          ],
          [
            "Access Date",
            "2023-03-12 10:55:14"
          ],
          [
            "Creators",
            "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, Shrikanth S. Narayanan"
          ],
          [
            "DOI",
            "10.1007/s10579-008-9076-6"
          ],
          [
            "Date",
            "2008-12-01 2008-12-01"
          ],
          [
            "ISSN",
            "1574-0218"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "Lang Resources & Evaluation"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "335-359"
          ],
          [
            "Publication Title",
            "Language Resources and Evaluation"
          ],
          [
            "Short Title",
            "IEMOCAP"
          ],
          [
            "Title",
            "IEMOCAP: interactive emotional dyadic motion capture database"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s10579-008-9076-6"
          ],
          [
            "Volume",
            "42"
          ]
        ],
        "resource": "storage/i2233.pdf",
        "selectable": false
      },
      {
        "text": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
        "item-id": "i82",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n331",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>RAF-DB</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf",
            "item-id": "i330",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf"
              ]
            ],
            "resource": "storage/i330.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30 000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI, and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning-based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community."
          ],
          [
            "Creators",
            "S. Li, W. Deng"
          ],
          [
            "DOI",
            "10.1109/TIP.2018.2868382"
          ],
          [
            "Date",
            "2019-01-00 January 2019"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "356-370"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Title",
            "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition"
          ],
          [
            "Volume",
            "28"
          ]
        ],
        "resource": "storage/i330.pdf",
        "selectable": false
      },
      {
        "text": "SEWA DB",
        "item-id": "i1417",
        "nodes": [
          {
            "text": "Kossaifi et al_2021_SEWA DB.pdf",
            "item-id": "i1450",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kossaifi et al_2021_SEWA DB.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YS322WTS/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2 State-of-the-art in audio-visual emotion databases</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2.1 Elicitation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2.1.1 Posed behaviour</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/2\">2.1.2 Induced behaviour</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.1.3 Spontaneous behaviour</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.2 Representation of emotion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.3 Data Annotation and generation of the Gold Standard</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.4 The Existing Corpora</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/3\">2.4.1 Elicitation using Conversational Context</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">2.4.2 Elicitation using Human-Machine Interfaces</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">2.4.3 Elicitation through Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">2.4.4 Corpus collected by segmenting existing recordings</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">3 SEWA database</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/5\">3.1 Data collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/6\">3.2 The Data Statistics and subject demographics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/6\">3.3 Data annotation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/6\">3.3.1 Facial landmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/7\">3.3.2 Hand Gesture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/7\">3.3.3 Head Gesture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/7\">3.3.4 Transcript</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/8\">3.3.5 Facial Action Units annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/8\">3.3.6 Valence, Arousal, and Liking/Disliking annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/9\">3.3.7 Behaviour Templates</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/9\">3.3.8 Mimicry Episodes</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/9\">3.4 Database availability</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4 Baseline experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4.0.1 Methods</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4.1 Feature extraction</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/10\">4.1.1 Appearance features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/11\">4.1.2 Geometric features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/11\">4.1.3 Audio features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/11\">4.1.4 Feature fusion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.2 Experimental setting</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.2.1 Performance measure</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.3 Experimental results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/12\">4.3.1 Action unit detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/13\">4.3.2 Estimation of valence, arousal and liking/disliking</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/13\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/16\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Jean Kossaifi</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Robert Walecki</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Yannis Panagakis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/18\">Jie Shen</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Maximilian Schmitt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Fabien Ringeval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Jing Han</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Vedhas Pandit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Antoine Toisoul</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Bj\u00f6rn Schuller</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/19\">Kam Star</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/20\">Elnar Hajiyev</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YS322WTS/20\">Maja Pantic</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kossaifi et al_2021_SEWA DB.pdf"
              ]
            ],
            "resource": "storage/i1450.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SEWA DB",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal and (dis)liking intensity estimation."
          ],
          [
            "Access Date",
            "2022-04-06 08:28:51"
          ],
          [
            "Creators",
            "Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn Schuller, Kam Star, Elnar Hajiyev, Maja Pantic"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2019.2944808"
          ],
          [
            "Date",
            "2021-03-01 2021-3-1"
          ],
          [
            "Extra",
            "arXiv: 1901.02839"
          ],
          [
            "ISSN",
            "0162-8828, 2160-9292, 1939-3539"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Journal Abbreviation",
            "IEEE Trans. Pattern Anal. Mach. Intell."
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "1022-1040"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "SEWA DB"
          ],
          [
            "Title",
            "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1901.02839"
          ],
          [
            "Volume",
            "43"
          ]
        ],
        "resource": "storage/i1450.pdf",
        "selectable": false
      },
      {
        "text": "The Expression of the Emotions in Man and Animals",
        "item-id": "i1906",
        "icon": "glyphicon glyphicon-book",
        "item_title": "The Expression of the Emotions in Man and Animals",
        "item_type": "book",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In his study of infants and children (including observations of his own baby's smiles and pouts), of the insane, of painting and sculpture, of cats and dogs and monkeys, and of the ways that people in different cultures express their feelings, Darwin's insights have not been surpassed by modern science. This definitive edition of Darwin's masterpiece contains a substantial new Introduction and Afterword by Paul Ekman. Ekman also provides commentaries that use the latest scientific knowledge to elaborate, support, and occasionally challenge Darwin's study. For this edition, Ekman has returned to Darwin's original notes in order to produce for the first time a corrected, authoritative text illustrated by drawings and photographs positioned exactly as its author intended. \"This new edition of Darwin's extraordinary book is a major event in the human sciences.\"-Steven Pinker \"This new comprehensive edition of Expression will introduce a new generation of readers to Darwin's masterpiece, undiminished and intensely relevant even 125 years after publication.\"-Oliver Sacks \"Ekman's contribution to his edition of Darwin's 1872 monograph can count as a book in its own right.\"-Ian Hacking, Times Literary Supplement"
          ],
          [
            "Creators",
            "Charles Darwin, Phillip Prodger"
          ],
          [
            "Date",
            "1998-00-00 1998"
          ],
          [
            "Extra",
            "Google-Books-ID: TFRtLZSHMcYC"
          ],
          [
            "ISBN",
            "978-0-19-515806-9"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Google Books"
          ],
          [
            "Num Pages",
            "516"
          ],
          [
            "Publisher",
            "Oxford University Press"
          ],
          [
            "Title",
            "The Expression of the Emotions in Man and Animals"
          ]
        ]
      }
    ],
    "item_title": "Emotion Recognition",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Facial Recognition",
    "item-id": "c30,i2220",
    "nodes": [
      {
        "text": "Deep face recognition",
        "item-id": "i1824",
        "nodes": [
          {
            "text": "Parkhi et al_2015_Deep face recognition.pdf",
            "item-id": "i2065",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Parkhi et al_2015_Deep face recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Parkhi et al_2015_Deep face recognition.pdf"
              ]
            ],
            "resource": "storage/i2065.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep face recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The goal of this paper is face recognition \u2013 from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks."
          ],
          [
            "Access Date",
            "2022-10-22 08:32:27"
          ],
          [
            "Creators",
            "O. Parkhi, A. Vedaldi, A. Zisserman"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Extra",
            "Publisher: British Machine Vision Association"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ora.ox.ac.uk"
          ],
          [
            "Publication Title",
            "BMVC 2015 - Proceedings of the British Machine Vision Conference 2015"
          ],
          [
            "Title",
            "Deep face recognition"
          ],
          [
            "URL",
            "https://ora.ox.ac.uk/objects/uuid:a5f2e93f-2768-45bb-8508-74747f85cad1"
          ]
        ],
        "resource": "storage/i2065.pdf",
        "selectable": false
      },
      {
        "text": "Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion Convolutional Network",
        "item-id": "i2219",
        "nodes": [
          {
            "text": "Singh et al_2017_Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion.pdf",
            "item-id": "i2261",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Singh et al_2017_Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Singh et al_2017_Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion.pdf"
              ]
            ],
            "resource": "storage/i2261.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion Convolutional Network",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods."
          ],
          [
            "Access Date",
            "2023-02-10 00:16:06"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision Workshops"
          ],
          [
            "Creators",
            "Amarjot Singh, Devendra Patil, Meghana Reddy, S. N. Omkar"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1648-1655"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision Workshops"
          ],
          [
            "Title",
            "Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion Convolutional Network"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2017_workshops/w23/html/Singh_Disguised_Face_Identification_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i2261.pdf",
        "selectable": false
      },
      {
        "text": "FaceNet",
        "item-id": "i1825",
        "nodes": [
          {
            "text": "Schroff et al_2015_FaceNet.pdf",
            "item-id": "i2067",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Schroff et al_2015_FaceNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Schroff et al_2015_FaceNet.pdf"
              ]
            ],
            "resource": "storage/i2067.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FaceNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Despite significant recent advances in the field of face recognition [DeepFace, DeepId2], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128 bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [DeepId2+] by 30% on both datasets."
          ],
          [
            "Access Date",
            "2022-10-22 08:31:33"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Florian Schroff, Dmitry Kalenichenko, James Philbin"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Library Catalog",
            "www.cv-foundation.org"
          ],
          [
            "Pages",
            "815-823"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "FaceNet"
          ],
          [
            "Title",
            "FaceNet: A Unified Embedding for Face Recognition and Clustering"
          ],
          [
            "URL",
            "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html"
          ]
        ],
        "resource": "storage/i2067.pdf",
        "selectable": false
      },
      {
        "text": "HyperExtended LightFace",
        "item-id": "i2103",
        "nodes": [
          {
            "text": "Serengil_Ozpinar_2021_HyperExtended LightFace.pdf",
            "item-id": "i2141",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Serengil_Ozpinar_2021_HyperExtended LightFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Serengil_Ozpinar_2021_HyperExtended LightFace.pdf"
              ]
            ],
            "resource": "storage/i2141.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HyperExtended LightFace",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Facial attribute analysis from facial images has always been a challenging task. Its practical use cases are very different. This paper mentioned how to build machine learning models with facial image data for age, gender, facial expression, and race/ethnicity prediction tasks. A fully coded open-source software framework was also developed and published with those functionalities."
          ],
          [
            "Conference Name",
            "2021 International Conference on Engineering and Emerging Technologies (ICEET)"
          ],
          [
            "Creators",
            "Sefik Ilkin Serengil, Alper Ozpinar"
          ],
          [
            "DOI",
            "10.1109/ICEET53442.2021.9659697"
          ],
          [
            "Date",
            "2021-10-00 2021-10"
          ],
          [
            "Extra",
            "ISSN: 2409-2983"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-4"
          ],
          [
            "Proceedings Title",
            "2021 International Conference on Engineering and Emerging Technologies (ICEET)"
          ],
          [
            "Short Title",
            "HyperExtended LightFace"
          ],
          [
            "Title",
            "HyperExtended LightFace: A Facial Attribute Analysis Framework"
          ]
        ],
        "resource": "storage/i2141.pdf",
        "selectable": false
      },
      {
        "text": "LightFace",
        "item-id": "i2102",
        "nodes": [
          {
            "text": "Serengil_Ozpinar_2020_LightFace.pdf",
            "item-id": "i2139",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Serengil_Ozpinar_2020_LightFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Serengil_Ozpinar_2020_LightFace.pdf"
              ]
            ],
            "resource": "storage/i2139.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "LightFace",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face recognition constitutes a relatively a popular area which has emerged from the rulers of the social media to top universities in the world. Those frontiers and rule makers recently designed deep learning based custom face recognition models. A modern face recognition pipeline consists of four common stages: detecting, alignment, representation and verification. However, face recognition studies mainly mention the representation stage of a pipeline. In this paper, first of all a review face recognition has been done and then the description of the developed lightweight hybrid high performance face recognition framework has been made. Its hybrid feature enables to switch face recognition models among state-of-the-art ones."
          ],
          [
            "Conference Name",
            "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
          ],
          [
            "Creators",
            "Sefik Ilkin Serengil, Alper Ozpinar"
          ],
          [
            "DOI",
            "10.1109/ASYU50717.2020.9259802"
          ],
          [
            "Date",
            "2020-10-00 2020-10"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-5"
          ],
          [
            "Proceedings Title",
            "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
          ],
          [
            "Short Title",
            "LightFace"
          ],
          [
            "Title",
            "LightFace: A Hybrid Deep Face Recognition Framework"
          ]
        ],
        "resource": "storage/i2139.pdf",
        "selectable": false
      },
      {
        "text": "MS-Celeb-1M",
        "item-id": "i1892",
        "nodes": [
          {
            "text": "Guo et al_2016_MS-Celeb-1M.pdf",
            "item-id": "i2011",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Guo et al_2016_MS-Celeb-1M.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_622H4S7N/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/4\">2 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_622H4S7N/6\">3 Benchmark Construction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/6\">3.1 One Million Celebrity List</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/7\">3.2 Celebrity Selection for Measurement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/8\">3.3 Labeling for Measurement</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_622H4S7N/10\">4 Celebrity Recognition</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/10\">4.1 Evaluation Protocol</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/11\">4.2 Training Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/13\">4.3 Baseline</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/14\">5 Discussion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_622H4S7N/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Guo et al_2016_MS-Celeb-1M.pdf"
              ]
            ],
            "resource": "storage/i2011.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MS-Celeb-1M",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46487-9_6"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46487-9"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "87-102"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "MS-Celeb-1M"
          ],
          [
            "Title",
            "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition"
          ]
        ],
        "resource": "storage/i2011.pdf",
        "selectable": false
      },
      {
        "text": "Neural Aggregation Network for Video Face Recognition",
        "item-id": "i2220",
        "nodes": [
          {
            "text": "Yang et al_2017_Neural Aggregation Network for Video Face Recognition.pdf",
            "item-id": "i2262",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2017_Neural Aggregation Network for Video Face Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2017_Neural Aggregation Network for Video Face Recognition.pdf"
              ]
            ],
            "resource": "storage/i2262.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Aggregation Network for Video Face Recognition",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy."
          ],
          [
            "Access Date",
            "2023-02-10 00:15:35"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, Gang Hua"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4362-4371"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Neural Aggregation Network for Video Face Recognition"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Neural_Aggregation_Network_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i2262.pdf",
        "selectable": false
      }
    ],
    "item_title": "Facial Recognition",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Generative Adversarial Networks",
    "item-id": "c14,i3208",
    "nodes": [
      {
        "text": "3D-Aware Video Generation",
        "item-id": "i1669",
        "nodes": [
          {
            "text": "Bahmani et al_2022_3D-Aware Video Generation.pdf",
            "item-id": "i1672",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bahmani et al_2022_3D-Aware Video Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bahmani et al_2022_3D-Aware Video Generation.pdf"
              ]
            ],
            "resource": "storage/i1672.pdf"
          },
          {
            "text": "Comment: Project page: https://sherwinbahmani.github.io/3dvidgen",
            "item-id": "n1673",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page: https://sherwinbahmani.github.io/3dvidgen",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page: https://sherwinbahmani.github.io/3dvidgen</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "3D-Aware Video Generation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs."
          ],
          [
            "Access Date",
            "2022-07-04 01:31:40"
          ],
          [
            "Archiveid",
            "arXiv:2206.14797"
          ],
          [
            "Creators",
            "Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Hao Tang, Gordon Wetzstein, Leonidas Guibas, Luc Van Gool, Radu Timofte"
          ],
          [
            "DOI",
            "10.48550/arXiv.2206.14797"
          ],
          [
            "Date",
            "2022-06-29 2022-06-29"
          ],
          [
            "Extra",
            "arXiv:2206.14797 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "3D-Aware Video Generation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2206.14797"
          ]
        ],
        "resource": "storage/i1672.pdf",
        "selectable": false
      },
      {
        "text": "A Style-Based Generator Architecture for Generative Adversarial Networks",
        "item-id": "i98",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n581",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>StyleGAN</p>\n<p>The understanding of the process of image synthesis lacks and there is no quantitative way to measure the latent space interpolations.</p>\n<p>The architecture inspired from style transfer can control the image synthesis process. With the noise and style info injected to the networks, it can separate the high-level attributes and stochastic variations unsupervisedly. &nbsp;Also, using a mapping network to map the noise input to intermediate latent space can free the restriction and allow to be disentangled.</p>\n<p></p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf",
            "item-id": "i225",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf"
              ]
            ],
            "resource": "storage/i225.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-29 04:41:33"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tero Karras, Samuli Laine, Timo Aila"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4401-4410"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "A Style-Based Generator Architecture for Generative Adversarial Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i225.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Generative Adversarial Networks",
        "item-id": "i1222",
        "nodes": [
          {
            "text": "Jabbar et al_2021_A Survey on Generative Adversarial Networks.pdf",
            "item-id": "i1224",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jabbar et al_2021_A Survey on Generative Adversarial Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jabbar et al_2021_A Survey on Generative Adversarial Networks.pdf"
              ]
            ],
            "resource": "storage/i1224.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Generative Adversarial Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic."
          ],
          [
            "Access Date",
            "2021-12-06 11:48:21"
          ],
          [
            "Creators",
            "Abdul Jabbar, Xi Li, Bourahla Omar"
          ],
          [
            "DOI",
            "10.1145/3463475"
          ],
          [
            "Date",
            "2021-10-04 2021-10-04"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "November 2022"
          ],
          [
            "Pages",
            "157:1\u2013157:49"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "A Survey on Generative Adversarial Networks"
          ],
          [
            "Title",
            "A Survey on Generative Adversarial Networks: Variants, Applications, and Training"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3463475"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i1224.pdf",
        "selectable": false
      },
      {
        "text": "Adversarial Latent Autoencoders",
        "item-id": "i74",
        "nodes": [
          {
            "text": "Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf",
            "item-id": "i299",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf"
              ]
            ],
            "resource": "storage/i299.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Adversarial Latent Autoencoders",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture."
          ],
          [
            "Access Date",
            "2021-03-16 14:44:03"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Stanislav Pidhorskyi, Donald A. Adjeroh, Gianfranco Doretto"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14104-14113"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Adversarial Latent Autoencoders"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i299.pdf",
        "selectable": false
      },
      {
        "text": "Analyzing and Improving the Image Quality of StyleGAN",
        "item-id": "i566",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n570",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>StyleGAN2</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf",
            "item-id": "i569",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf"
              ]
            ],
            "resource": "storage/i569.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Analyzing and Improving the Image Quality of StyleGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."
          ],
          [
            "Access Date",
            "2021-05-10 09:08:54"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8110-8119"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Analyzing and Improving the Image Quality of StyleGAN"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i569.pdf",
        "selectable": false
      },
      {
        "text": "Banach Wasserstein GAN",
        "item-id": "i1564",
        "nodes": [
          {
            "text": "Adler_Lunz_2018_Banach Wasserstein GAN.pdf",
            "item-id": "i1599",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Adler_Lunz_2018_Banach Wasserstein GAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Adler_Lunz_2018_Banach Wasserstein GAN.pdf"
              ]
            ],
            "resource": "storage/i1599.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Banach Wasserstein GAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered \u21132 as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA."
          ],
          [
            "Access Date",
            "2022-05-20 07:09:52"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Jonas Adler, Sebastian Lunz"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Banach Wasserstein GAN"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2018/hash/91d0dbfd38d950cb716c4dd26c5da08a-Abstract.html"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i1599.pdf",
        "selectable": false
      },
      {
        "text": "CVAE-GAN",
        "item-id": "i88",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n251",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Intoducing CVAE-GAN (Conditional VAE-GAN) by using label to improve the performance of VAE-GAN.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Bao et al_2017_CVAE-GAN.pdf",
            "item-id": "i252",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bao et al_2017_CVAE-GAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bao et al_2017_CVAE-GAN.pdf"
              ]
            ],
            "resource": "storage/i252.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CVAE-GAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-22 07:12:39"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, Gang Hua"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2745-2754"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "CVAE-GAN"
          ],
          [
            "Title",
            "CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i252.pdf",
        "selectable": false
      },
      {
        "text": "DFA-NeRF",
        "item-id": "i1319",
        "nodes": [
          {
            "text": "Yao et al_2022_DFA-NeRF.pdf",
            "item-id": "i1385",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yao et al_2022_DFA-NeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yao et al_2022_DFA-NeRF.pdf"
              ]
            ],
            "resource": "storage/i1385.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "DFA-NeRF",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While recent advances in deep neural networks have made it possible to render high-quality images, generating photo-realistic and personalized talking head remains challenging. With given audio, the key to tackling this task is synchronizing lip movement and simultaneously generating personalized attributes like head movement and eye blink. In this work, we observe that the input audio is highly correlated to lip motion while less correlated to other personalized attributes (e.g., head movements). Inspired by this, we propose a novel framework based on neural radiance field to pursue high-fidelity and personalized talking head generation. Specifically, neural radiance field takes lip movements features and personalized attributes as two disentangled conditions, where lip movements are directly predicted from the audio inputs to achieve lip-synchronized generation. In the meanwhile, personalized attributes are sampled from a probabilistic model, where we design a Transformer-based variational autoencoder sampled from Gaussian Process to learn plausible and natural-looking head pose and eye blink. Experiments on several benchmarks demonstrate that our method achieves significantly better results than state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-02-14 07:04:37"
          ],
          [
            "Archiveid",
            "arXiv:2201.00791"
          ],
          [
            "Creators",
            "Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai, Xiaokang Yang"
          ],
          [
            "Date",
            "2022-01-03 2022-01-03"
          ],
          [
            "Extra",
            "arXiv:2201.00791 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "DFA-NeRF"
          ],
          [
            "Title",
            "DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2201.00791"
          ]
        ],
        "resource": "storage/i1385.pdf",
        "selectable": false
      },
      {
        "text": "Deep Person Generation",
        "item-id": "i2147",
        "nodes": [
          {
            "text": "Sha et al_2022_Deep Person Generation.pdf",
            "item-id": "i2162",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sha et al_2022_Deep Person Generation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NYKSXY4R/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/4\">2 Talking-head Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/5\">2.1 Motion-driven Talking-Head Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/7\">2.2 Audio-driven Talking-Head Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/9\">2.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/10\">3 Pose-guided Person Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/10\">3.1 Pose-guided Person Image Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/14\">3.2 Pose-guided Person Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/14\">3.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/15\">4 Garment-Oriented Person Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/15\">4.1 Virtual try-on</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">4.2 Garment Manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">4.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">5 Benchmarks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">5.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/20\">5.2 Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/21\">5.3 Performance Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/22\">6 Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7 Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7.1 Generative Data Augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7.2 Virtual Fitting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/26\">7.3 Digital Human</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/27\">8 Future Directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/28\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/28\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sha et al_2022_Deep Person Generation.pdf"
              ]
            ],
            "resource": "storage/i2162.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Person Generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep person generation has attracted extensive research attention due to its wide applications in virtual agents, video conferencing, online shopping and art/movie production. With the advancement of deep learning, visual appearances (face, pose, cloth) of a person image can be easily generated on demand. In this survey, we first summarize the scope of person generation, and then systematically review recent progress and technical trends in identity-preserving deep person generation, covering three major tasks: talking-head generation (face), pose-guided person generation (pose) and garment-oriented person generation (cloth). More than two hundred papers are covered for a thorough overview, and the milestone works are highlighted to witness the major technical breakthrough. Based on these fundamental tasks, many applications are investigated, e.g., virtual fitting, digital human, generative data augmentation. We hope this survey could shed some light on the future prospects of identity-preserving deep person generation, and provide a helpful foundation for full applications towards the digital human."
          ],
          [
            "Access Date",
            "2022-12-12 11:08:18"
          ],
          [
            "Creators",
            "Tong Sha, Wei Zhang, Tong Shen, Zhoujun Li, Tao Mei"
          ],
          [
            "DOI",
            "10.1145/3575656"
          ],
          [
            "Date",
            "2022-00-07 \u5341\u4e8c\u6708 7, 2022"
          ],
          [
            "Extra",
            "Just Accepted"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Deep Person Generation"
          ],
          [
            "Title",
            "Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3575656"
          ]
        ],
        "resource": "storage/i2162.pdf",
        "selectable": false
      },
      {
        "text": "Drag Your GAN",
        "item-id": "i2378",
        "nodes": [
          {
            "text": "Comment: Accepted to SIGGRAPH 2023. Project page: https://vcai.mpi-inf.mpg.de/projects/DragGAN/",
            "item-id": "n2440",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted to SIGGRAPH 2023. Project page: https://vcai.mpi-inf.mpg.de/projects/DragGAN/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted to SIGGRAPH 2023. Project page: https://vcai.mpi-inf.mpg.de/projects/DragGAN/</div>",
            "node_type": "note"
          },
          {
            "text": "Pan et al_2023_Drag Your GAN.pdf",
            "item-id": "i2439",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Pan et al_2023_Drag Your GAN.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G9DTP6L3/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/2\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/2\">2.1 Generative Models for Interactive Content Creation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/3\">2.2 Point Tracking</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/3\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/3\">3.1 Interactive Point-based Manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/4\">3.2 Motion Supervision</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/5\">3.3 Point Tracking</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/5\">3.4 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/6\">4.1 Qualitative Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/6\">4.2 Quantitative Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/7\">4.3 Discussions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/8\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G9DTP6L3/8\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Pan et al_2023_Drag Your GAN.pdf"
              ]
            ],
            "resource": "storage/i2439.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Drag Your GAN",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to \"drag\" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion."
          ],
          [
            "Access Date",
            "2023-05-23 06:47:17"
          ],
          [
            "Archiveid",
            "arXiv:2305.10973"
          ],
          [
            "Creators",
            "Xingang Pan, Ayush Tewari, Thomas Leimk\u00fchler, Lingjie Liu, Abhimitra Meka, Christian Theobalt"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.10973"
          ],
          [
            "Date",
            "2023-05-18 2023-05-18"
          ],
          [
            "Extra",
            "arXiv:2305.10973 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Drag Your GAN"
          ],
          [
            "Title",
            "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.10973"
          ]
        ],
        "resource": "storage/i2439.pdf",
        "selectable": false
      },
      {
        "text": "EigenGAN",
        "item-id": "i1566",
        "nodes": [
          {
            "text": "He et al_2021_EigenGAN.pdf",
            "item-id": "i1600",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2021_EigenGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2021_EigenGAN.pdf"
              ]
            ],
            "resource": "storage/i1600.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "EigenGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via generative adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of \"eigen-dimensions\" at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, in the linear case, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow."
          ],
          [
            "Access Date",
            "2022-05-20 06:58:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Zhenliang He, Meina Kan, Shiguang Shan"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14408-14417"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "EigenGAN"
          ],
          [
            "Title",
            "EigenGAN: Layer-Wise Eigen-Learning for GANs"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/He_EigenGAN_Layer-Wise_Eigen-Learning_for_GANs_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1600.pdf",
        "selectable": false
      },
      {
        "text": "GANcraft",
        "item-id": "i895",
        "nodes": [
          {
            "text": "Hao et al_2021_GANcraft.pdf",
            "item-id": "i897",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hao et al_2021_GANcraft.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hao et al_2021_GANcraft.pdf"
              ]
            ],
            "resource": "storage/i897.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GANcraft",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ ."
          ],
          [
            "Access Date",
            "2021-08-11 08:06:22"
          ],
          [
            "Creators",
            "Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu"
          ],
          [
            "Date",
            "2021-04-15 2021-04-15"
          ],
          [
            "Extra",
            "arXiv: 2104.07659"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2104.07659 [cs]"
          ],
          [
            "Short Title",
            "GANcraft"
          ],
          [
            "Title",
            "GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2104.07659"
          ]
        ],
        "resource": "storage/i897.pdf",
        "selectable": false
      },
      {
        "text": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "item-id": "i1820",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2055",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>FID Metrics</p>\n<p>Fr\u00e9chet Inception Distance</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf",
            "item-id": "i2056",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf"
              ]
            ],
            "resource": "storage/i2056.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fr\u00e9chet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark."
          ],
          [
            "Access Date",
            "2022-10-22 08:43:54"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i2056.pdf",
        "selectable": false
      },
      {
        "text": "GIRAFFE",
        "item-id": "i1309",
        "nodes": [
          {
            "text": "Niemeyer_Geiger_2021_GIRAFFE.pdf",
            "item-id": "i1372",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Niemeyer_Geiger_2021_GIRAFFE.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Niemeyer_Geiger_2021_GIRAFFE.pdf"
              ]
            ],
            "resource": "storage/i1372.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GIRAFFE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose."
          ],
          [
            "Access Date",
            "2022-02-17 23:38:22"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Michael Niemeyer, Andreas Geiger"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "11453-11464"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "GIRAFFE"
          ],
          [
            "Title",
            "GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1372.pdf",
        "selectable": false
      },
      {
        "text": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
        "item-id": "i1310",
        "nodes": [
          {
            "text": "Schwarz et al_2020_GRAF.pdf",
            "item-id": "i1373",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Schwarz et al_2020_GRAF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Schwarz et al_2020_GRAF.pdf"
              ]
            ],
            "resource": "storage/i1373.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity."
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, H. Lin"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Pages",
            "20154\u201320166"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/file/e92e1b476bb5262d793fd40931e0ed53-Paper.pdf"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1373.pdf",
        "selectable": false
      },
      {
        "text": "Generative Adversarial Nets",
        "item-id": "i1562",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n220",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Generative Adversarial Networks (GAN)</p>\n<p>It is difficult to approximate because the difficulty of utilizing the benefits of picewise linear units in the generative context.</p>\n<p>It is possible to use a adversarial structures to overcome the difficulties.</p>\n<p>The GAN framework has two models, generative model and discriminative model. The generator input a noise vector try to confuse the discriminator, and the discriminator can classify the image is from real dataset or generator.</p>\n<p>MNIST, Toronto Face Database (TFD), CIFAR-10</p>\n<p>The metric is window-based log-likelihood etimates. The adversarial nets is better than other generative networks.</p>\n<p>+ve: Has potential in the future. No inference is needed during learning. A wide variety of functions can be incorporated into the model. The adversarial models does not directly receive any information from dataset. GAN can represent very sharp distributions.</p>\n<p>-ve: The discriminator must be synchronized well with generator during training (Hard to train). </p>\n<p>The development of GAN broaden the area of image synthesis and translation area. GAN has the advantages of generating very realistic data, so will be widely used in the future generation tasks. However, GAN is hard to train, but there will be future works to improve that.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Goodfellow et al_2014_Generative Adversarial Nets.pdf",
            "item-id": "i1627",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Goodfellow et al_2014_Generative Adversarial Nets.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Goodfellow et al_2014_Generative Adversarial Nets.pdf"
              ]
            ],
            "resource": "storage/i1627.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative Adversarial Nets",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples."
          ],
          [
            "Access Date",
            "2022-05-20 06:27:52"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
          ],
          [
            "Date",
            "2014-00-00 2014"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Generative Adversarial Nets"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html"
          ],
          [
            "Volume",
            "27"
          ]
        ],
        "resource": "storage/i1627.pdf",
        "selectable": false
      },
      {
        "text": "Generative Adversarial Networks in Computer Vision",
        "item-id": "i1220",
        "nodes": [
          {
            "text": "Wang et al_2021_Generative Adversarial Networks in Computer Vision.pdf",
            "item-id": "i1221",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2021_Generative Adversarial Networks in Computer Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2021_Generative Adversarial Networks in Computer Vision.pdf"
              ]
            ],
            "resource": "storage/i1221.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative Adversarial Networks in Computer Vision",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review."
          ],
          [
            "Access Date",
            "2021-12-06 11:31:04"
          ],
          [
            "Creators",
            "Zhengwei Wang, Qi She, Tom\u00e1s E. Ward"
          ],
          [
            "DOI",
            "10.1145/3439723"
          ],
          [
            "Date",
            "2021-02-09 2021-02-09"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "March 2022"
          ],
          [
            "Pages",
            "37:1\u201337:38"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Generative Adversarial Networks in Computer Vision"
          ],
          [
            "Title",
            "Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3439723"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i1221.pdf",
        "selectable": false
      },
      {
        "text": "Generative adversarial networks",
        "item-id": "i1049",
        "nodes": [
          {
            "text": "Goodfellow et al_2020_Generative adversarial networks.pdf",
            "item-id": "i1050",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Goodfellow et al_2020_Generative adversarial networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Goodfellow et al_2020_Generative adversarial networks.pdf"
              ]
            ],
            "resource": "storage/i1050.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative adversarial networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization."
          ],
          [
            "Access Date",
            "2021-10-18 12:08:36"
          ],
          [
            "Creators",
            "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
          ],
          [
            "DOI",
            "10.1145/3422622"
          ],
          [
            "Date",
            "2020-10-22 October 22, 2020"
          ],
          [
            "ISSN",
            "0001-0782"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Journal Abbreviation",
            "Commun. ACM"
          ],
          [
            "Library Catalog",
            "November 2020"
          ],
          [
            "Pages",
            "139\u2013144"
          ],
          [
            "Publication Title",
            "Communications of the ACM"
          ],
          [
            "Title",
            "Generative adversarial networks"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3422622"
          ],
          [
            "Volume",
            "63"
          ]
        ],
        "resource": "storage/i1050.pdf",
        "selectable": false
      },
      {
        "text": "High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs",
        "item-id": "i36",
        "nodes": [
          {
            "text": "Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf",
            "item-id": "i180",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf"
              ]
            ],
            "resource": "storage/i180.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing."
          ],
          [
            "Access Date",
            "2021-04-22 13:32:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8798-8807"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i180.pdf",
        "selectable": false
      },
      {
        "text": "Improved Training of Wasserstein GANs",
        "item-id": "i1563",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n1597",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotation</p>\n<p>WGAN-GP</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Gulrajani et al_2017_Improved Training of Wasserstein GANs.pdf",
            "item-id": "i1598",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gulrajani et al_2017_Improved Training of Wasserstein GANs.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gulrajani et al_2017_Improved Training of Wasserstein GANs.pdf"
              ]
            ],
            "resource": "storage/i1598.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Improved Training of Wasserstein GANs",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms."
          ],
          [
            "Access Date",
            "2022-05-20 07:11:06"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron C Courville"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Improved Training of Wasserstein GANs"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i1598.pdf",
        "selectable": false
      },
      {
        "text": "InfoGAN",
        "item-id": "i1834",
        "nodes": [
          {
            "text": "Chen et al_2016_InfoGAN.pdf",
            "item-id": "i1916",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2016_InfoGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2016_InfoGAN.pdf"
              ]
            ],
            "resource": "storage/i1916.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "InfoGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods."
          ],
          [
            "Access Date",
            "2022-11-03 10:58:01"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "InfoGAN"
          ],
          [
            "Title",
            "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html"
          ],
          [
            "Volume",
            "29"
          ]
        ],
        "resource": "storage/i1916.pdf",
        "selectable": false
      },
      {
        "text": "Large Scale Adversarial Representation Learning",
        "item-id": "i1833",
        "nodes": [
          {
            "text": "Donahue_Simonyan_2019_Large Scale Adversarial Representation Learning.pdf",
            "item-id": "i1915",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Donahue_Simonyan_2019_Large Scale Adversarial Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Donahue_Simonyan_2019_Large Scale Adversarial Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i1915.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Large Scale Adversarial Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as compelling results in unconditional image generation."
          ],
          [
            "Access Date",
            "2022-11-03 11:06:36"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Jeff Donahue, Karen Simonyan"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Large Scale Adversarial Representation Learning"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/18cdf49ea54eec029238fcc95f76ce41-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i1915.pdf",
        "selectable": false
      },
      {
        "text": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
        "item-id": "i101",
        "nodes": [
          {
            "text": "Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf",
            "item-id": "i229",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_22J4NRNH/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/2\">2 Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22J4NRNH/3\">3 Scaling Up GANs</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/4\">3.1 Trading off variety and fidelity with the Truncation Trick</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/5\">3.2 Summary</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22J4NRNH/5\">4 Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/5\">4.1 Characterizing Instability: The Generator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/6\">4.2 Characterizing Instability: The Discriminator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/6\">4.3 Summary</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22J4NRNH/7\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/7\">5.1 Evaluation on ImageNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/8\">5.2 Additional evaluation on JFT-300M</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/9\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/12\">Appendix A Additional Samples, Interpolations, and Nearest Neighbors from ImageNet models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/17\">Appendix B Architectural details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22J4NRNH/23\">Appendix C Experimental Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/23\">C.1 BatchNorm Statistics and Sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/23\">C.2 CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/23\">C.3 Inception Scores of ImageNet Images</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/24\">Appendix D Additional Plots</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/26\">Appendix E Choosing Latent Spaces</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/27\">Appendix F Monitored Training Statistics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22J4NRNH/32\">Appendix G Additional Discussion: Stability and Collapse</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/32\">G.1 Intervening Before Collapse</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/33\">G.2 Spikes in the Discriminator's Spectra</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/34\">Appendix H Negative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22J4NRNH/35\">Appendix I Hyperparameters</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf"
              ]
            ],
            "resource": "storage/i229.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6."
          ],
          [
            "Access Date",
            "2021-03-29 04:21:24"
          ],
          [
            "Creators",
            "Andrew Brock, Jeff Donahue, Karen Simonyan"
          ],
          [
            "Date",
            "2019-02-25 2019-02-25"
          ],
          [
            "Extra",
            "arXiv: 1809.11096"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1809.11096 [cs, stat]"
          ],
          [
            "Title",
            "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1809.11096"
          ]
        ],
        "resource": "storage/i229.pdf",
        "selectable": false
      },
      {
        "text": "Lifelong GAN",
        "item-id": "i79",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n249",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>Lifelong GAN. Using knowledge distillation to perform continous training BicycleGAN for image translation avoiding forgeting.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Zhai et al_2019_Lifelong GAN.pdf",
            "item-id": "i260",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhai et al_2019_Lifelong GAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhai et al_2019_Lifelong GAN.pdf"
              ]
            ],
            "resource": "storage/i260.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Lifelong GAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-22 05:51:57"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Mengyao Zhai, Lei Chen, Frederick Tung, Jiawei He, Megha Nawhal, Greg Mori"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2759-2768"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "Lifelong GAN"
          ],
          [
            "Title",
            "Lifelong GAN: Continual Learning for Conditional Image Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i260.pdf",
        "selectable": false
      },
      {
        "text": "Long Video Generation with\u00a0Time-Agnostic VQGAN and\u00a0Time-Sensitive Transformer",
        "item-id": "i3208",
        "nodes": [
          {
            "text": "Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf",
            "item-id": "i3277",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_63TA6BXK/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_63TA6BXK/4\">2 Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/4\">2.1 Extending the VQGAN Framework for Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/7\">2.2 Time-Agnostic VQGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/8\">2.3 Time-Sensitive Transformer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_63TA6BXK/9\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/9\">3.1 Experimental Setups</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/9\">3.2 Quantitative Evaluation on Short Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/11\">3.3 Quantitative Evaluation on Long Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/12\">3.4 Qualitative Evaluation on Long Video Generation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/14\">4 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/15\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf"
              ]
            ],
            "resource": "storage/i3277.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Long Video Generation with\u00a0Time-Agnostic VQGAN and\u00a0Time-Sensitive Transformer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Videos are created to express emotion, exchange information, and share experiences. Video synthesis has intrigued researchers for a long time. Despite the rapid progress driven by advances in visual synthesis, most existing studies focus on improving the frames\u2019 quality and the transitions between them, while little progress has been made in generating longer videos. In this paper, we present a method that builds on 3D-VQGAN and transformers to generate videos with thousands of frames. Our evaluation shows that our model trained on 16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse, and Taichi-HD datasets can generate diverse, coherent, and high-quality long videos. We also showcase conditional extensions of our approach for generating meaningful long videos by incorporating temporal information with text and audio. Videos and code can be found at https://songweige.github.io/projects/tats."
          ],
          [
            "Conference Name",
            "Computer Vision \u2013 ECCV 2022"
          ],
          [
            "Creators",
            "Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, Devi Parikh, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19790-1_7"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19790-1"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "102-118"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Computer Vision \u2013 ECCV 2022"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Long Video Generation with\u00a0Time-Agnostic VQGAN and\u00a0Time-Sensitive Transformer"
          ]
        ],
        "resource": "storage/i3277.pdf",
        "selectable": false
      },
      {
        "text": "MoCoGAN",
        "item-id": "i1118",
        "nodes": [
          {
            "text": "Tulyakov et al_2018_MoCoGAN.pdf",
            "item-id": "i1142",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tulyakov et al_2018_MoCoGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tulyakov et al_2018_MoCoGAN.pdf"
              ]
            ],
            "resource": "storage/i1142.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MoCoGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion. Our code is available at https://github.com/sergeytulyakov/mocogan."
          ],
          [
            "Access Date",
            "2021-10-24 02:05:41"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1526-1535"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MoCoGAN"
          ],
          [
            "Title",
            "MoCoGAN: Decomposing Motion and Content for Video Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i1142.pdf",
        "selectable": false
      },
      {
        "text": "Multimodal Image Synthesis and Editing",
        "item-id": "i1303",
        "nodes": [
          {
            "text": "Comment: 20 pages, 19 figures",
            "item-id": "n1359",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 20 pages, 19 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 20 pages, 19 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf",
            "item-id": "i1358",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf"
              ]
            ],
            "resource": "storage/i1358.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multimodal Image Synthesis and Editing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modelling the interaction among multimodal information, multimodal image synthesis and editing have become a hot research topic in recent years. Different from traditional visual guidance which provides explicit clues, multimodal guidance offers intuitive and flexible means in image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of features with inherent modality gaps, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis \\& editing and formulate taxonomies according to data modality and model architectures. We start with an introduction to different types of guidance modalities in image synthesis and editing. We then describe multimodal image synthesis and editing approaches extensively with detailed frameworks including Generative Adversarial Networks (GANs), GAN Inversion, Transformers, and other methods such as NeRF and Diffusion models. This is followed by a comprehensive description of benchmark datasets and corresponding evaluation metrics as widely adopted in multimodal image synthesis and editing, as well as detailed comparisons of different synthesis methods with analysis of respective advantages and limitations. Finally, we provide insights into the current research challenges and possible future research directions. A project associated with this survey is available at https://github.com/fnzhan/MISE"
          ],
          [
            "Access Date",
            "2022-02-18 00:42:09"
          ],
          [
            "Creators",
            "Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu"
          ],
          [
            "Date",
            "2021-12-27 2021-12-27"
          ],
          [
            "Extra",
            "arXiv: 2112.13592"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2112.13592 [cs]"
          ],
          [
            "Short Title",
            "Multimodal Image Synthesis and Editing"
          ],
          [
            "Title",
            "Multimodal Image Synthesis and Editing: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2112.13592"
          ]
        ],
        "resource": "storage/i1358.pdf",
        "selectable": false
      },
      {
        "text": "Pi-GAN",
        "item-id": "i1308",
        "nodes": [
          {
            "text": "Chan et al_2021_Pi-GAN.pdf",
            "item-id": "i1370",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chan et al_2021_Pi-GAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chan et al_2021_Pi-GAN.pdf"
              ]
            ],
            "resource": "storage/i1370.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pi-GAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (p-GAN or pi-GAN), for high-quality 3D-aware image synthesis. p-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets."
          ],
          [
            "Access Date",
            "2022-02-17 23:39:20"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5799-5809"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Pi-GAN"
          ],
          [
            "Title",
            "Pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Chan_Pi-GAN_Periodic_Implicit_Generative_Adversarial_Networks_for_3D-Aware_Image_Synthesis_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1370.pdf",
        "selectable": false
      },
      {
        "text": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
        "item-id": "i625",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n629",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>ProGAN, Progressive Growing GAN</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf",
            "item-id": "i628",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf"
              ]
            ],
            "resource": "storage/i628.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality."
          ],
          [
            "Access Date",
            "2021-05-14 03:02:29"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen"
          ],
          [
            "Date",
            "2018-02-15 2018/02/15"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Progressive Growing of GANs for Improved Quality, Stability, and Variation"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=Hk99zCeAb"
          ]
        ],
        "resource": "storage/i628.pdf",
        "selectable": false
      },
      {
        "text": "SinGAN",
        "item-id": "i1225",
        "nodes": [
          {
            "text": "Comment: ICCV 2019",
            "item-id": "n1226",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ICCV 2019",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ICCV 2019</div>",
            "node_type": "note"
          },
          {
            "text": "Shaham et al_2019_SinGAN.pdf",
            "item-id": "i1228",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shaham et al_2019_SinGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shaham et al_2019_SinGAN.pdf"
              ]
            ],
            "resource": "storage/i1228.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SinGAN",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks."
          ],
          [
            "Access Date",
            "2021-12-06 12:21:33"
          ],
          [
            "Creators",
            "Tamar Rott Shaham, Tali Dekel, Tomer Michaeli"
          ],
          [
            "Date",
            "2019-09-04 2019-09-04"
          ],
          [
            "Extra",
            "arXiv: 1905.01164"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1905.01164 [cs]"
          ],
          [
            "Short Title",
            "SinGAN"
          ],
          [
            "Title",
            "SinGAN: Learning a Generative Model from a Single Natural Image"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1905.01164"
          ]
        ],
        "resource": "storage/i1228.pdf",
        "selectable": false
      },
      {
        "text": "StarGAN v2",
        "item-id": "i2088",
        "nodes": [
          {
            "text": "Choi et al_2020_StarGAN v2.pdf",
            "item-id": "i2113",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Choi et al_2020_StarGAN v2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Choi et al_2020_StarGAN v2.pdf"
              ]
            ],
            "resource": "storage/i2113.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "StarGAN v2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset are available at https://github.com/clovaai/stargan-v2."
          ],
          [
            "Access Date",
            "2022-11-11 06:35:25"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8188-8197"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "StarGAN v2"
          ],
          [
            "Title",
            "StarGAN v2: Diverse Image Synthesis for Multiple Domains"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Choi_StarGAN_v2_Diverse_Image_Synthesis_for_Multiple_Domains_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i2113.pdf",
        "selectable": false
      },
      {
        "text": "StyleGAN-V",
        "item-id": "i2089",
        "nodes": [
          {
            "text": "Skorokhodov et al_2022_StyleGAN-V.pdf",
            "item-id": "i2114",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Skorokhodov et al_2022_StyleGAN-V.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Skorokhodov et al_2022_StyleGAN-V.pdf"
              ]
            ],
            "resource": "storage/i2114.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "StyleGAN-V",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Videos show continuous events, yet most -- if not all -- video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be -- time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image + video discriminators pair and design a holistic discriminator that aggregates temporal information by simply concatenating frames' features. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024x1024 videos for the first time. We build our model on top of StyleGAN2 and it is just 5% more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model is tested on four modern 256x256 and one 1024x1024-resolution video synthesis benchmarks. In terms of sheer metrics, it performs on average 30% better than the closest runner-up. Project website: https://universome.github.io/stylegan-v."
          ],
          [
            "Access Date",
            "2022-11-11 06:34:28"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3626-3636"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "StyleGAN-V"
          ],
          [
            "Title",
            "StyleGAN-V: A Continuous Video Generator With the Price, Image Quality and Perks of StyleGAN2"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2114.pdf",
        "selectable": false
      },
      {
        "text": "Taming Transformers for High-Resolution Image Synthesis",
        "item-id": "i1528",
        "nodes": [
          {
            "text": "Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf",
            "item-id": "i1549",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf"
              ]
            ],
            "resource": "storage/i1549.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Taming Transformers for High-Resolution Image Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers."
          ],
          [
            "Access Date",
            "2022-05-12 15:37:07"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Patrick Esser, Robin Rombach, Bjorn Ommer"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12873-12883"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Taming Transformers for High-Resolution Image Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1549.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Generative Adversarial Nets With Singular Value Clipping",
        "item-id": "i37",
        "nodes": [
          {
            "text": "Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf",
            "item-id": "i187",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf"
              ]
            ],
            "resource": "storage/i187.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Generative Adversarial Nets With Singular Value Clipping",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods."
          ],
          [
            "Access Date",
            "2021-04-22 11:57:13"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Masaki Saito, Eiichi Matsumoto, Shunta Saito"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2830-2839"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Temporal Generative Adversarial Nets With Singular Value Clipping"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i187.pdf",
        "selectable": false
      },
      {
        "text": "Towards Real-World Blind Face Restoration With Generative Facial Prior",
        "item-id": "i1318",
        "nodes": [
          {
            "text": "Wang et al_2021_Towards Real-World Blind Face Restoration With Generative Facial Prior.pdf",
            "item-id": "i1387",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2021_Towards Real-World Blind Face Restoration With Generative Facial Prior.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2021_Towards Real-World Blind Face Restoration With Generative Facial Prior.pdf"
              ]
            ],
            "resource": "storage/i1387.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Towards Real-World Blind Face Restoration With Generative Facial Prior",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Blind face restoration usually relies on facial priors, such as facial geometry prior or reference prior, to restore realistic and faithful details. However, very low-quality inputs cannot offer accurate geometric prior while high-quality references are inaccessible, limiting the applicability in real-world scenarios. In this work, we propose GFP-GAN that leverages rich and diverse priors encapsulated in a pretrained face GAN for blind face restoration. This Generative Facial Prior (GFP) is incorporated into the face restoration process via spatial feature transform layers, which allow our method to achieve a good balance of realness and fidelity. Thanks to the powerful generative facial prior and delicate designs, our GFP-GAN could jointly restore facial details and enhance colors with just a single forward pass, while GAN inversion methods require image-specific optimization at inference. Extensive experiments show that our method achieves superior performance to prior art on both synthetic and real-world datasets."
          ],
          [
            "Access Date",
            "2022-02-14 07:05:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Xintao Wang, Yu Li, Honglun Zhang, Ying Shan"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9168-9178"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Towards Real-World Blind Face Restoration With Generative Facial Prior"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Towards_Real-World_Blind_Face_Restoration_With_Generative_Facial_Prior_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1387.pdf",
        "selectable": false
      },
      {
        "text": "Training data-efficient image transformers & distillation through attention",
        "item-id": "i740",
        "nodes": [
          {
            "text": "Supplementary PDF",
            "item-id": "i741",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2021-07-21 16:18:50"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v139/touvron21a/touvron21a-supp.pdf"
              ]
            ],
            "resource": "storage/i741.pdf"
          },
          {
            "text": "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf",
            "item-id": "i742",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf"
              ]
            ],
            "resource": "storage/i742.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Training data-efficient image transformers & distillation through attention",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models."
          ],
          [
            "Access Date",
            "2021-07-21 16:18:49"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "10347-10357"
          ],
          [
            "Proceedings Title",
            "International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Training data-efficient image transformers & distillation through attention"
          ],
          [
            "URL",
            "http://proceedings.mlr.press/v139/touvron21a.html"
          ]
        ],
        "selectable": false
      },
      {
        "text": "Unconstrained Scene Generation With Locally Conditioned Radiance Fields",
        "item-id": "i1307",
        "nodes": [
          {
            "text": "DeVries et al_2021_Unconstrained Scene Generation With Locally Conditioned Radiance Fields.pdf",
            "item-id": "i1368",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "DeVries et al_2021_Unconstrained Scene Generation With Locally Conditioned Radiance Fields.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "DeVries et al_2021_Unconstrained Scene Generation With Locally Conditioned Radiance Fields.pdf"
              ]
            ],
            "resource": "storage/i1368.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unconstrained Scene Generation With Locally Conditioned Radiance Fields",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from view-points that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher quality scene renderings across several different scene datasets."
          ],
          [
            "Access Date",
            "2022-02-17 23:40:07"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14304-14313"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Unconstrained Scene Generation With Locally Conditioned Radiance Fields"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/DeVries_Unconstrained_Scene_Generation_With_Locally_Conditioned_Radiance_Fields_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1368.pdf",
        "selectable": false
      },
      {
        "text": "Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks",
        "item-id": "i2222",
        "nodes": [
          {
            "text": "Zhu et al_2017_Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks.pdf",
            "item-id": "i2265",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2017_Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2017_Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks.pdf"
              ]
            ],
            "resource": "storage/i2265.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X -> Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y -> X and introduce a cycle consistency loss to push F(G(X)) ~ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach."
          ],
          [
            "Access Date",
            "2023-02-09 23:50:50"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2223-2232"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i2265.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "item-id": "i90",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n292",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>DCGAN</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Comment: Under review as a conference paper at ICLR 2016",
            "item-id": "n293",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Under review as a conference paper at ICLR 2016",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Under review as a conference paper at ICLR 2016</div>",
            "node_type": "note"
          },
          {
            "text": "Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf",
            "item-id": "i291",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XULW8HJK/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2.1 Representation Learning from unlabeled data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2.2 Generating natural images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2.3 Visualizing the internals of CNNs</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">3 Approach and Model Architecture</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/3\">4 Details of adversarial training</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/4\">4.1 LSUN</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/4\">4.1.1 Deduplication</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/4\">4.2 Faces</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/5\">4.3 Imagenet-1k</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">5 Empirical Validation of DCGANs capabilities</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">5.1 Classifying CIFAR-10 using GANs as a feature extractor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">5.2 Classifying SVHN digits using GANs as a feature extractor</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">6 Investigating and visualizing the internals of the networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.1 Walking in the latent space</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.2 Visualizing the Discriminator Features</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.3 Manipulating the Generator Representation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.3.1 Forgetting to draw certain objects</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/8\">6.3.2 Vector arithmetic on face samples</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/9\">7 Conclusion and Future Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/14\">8 Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/14\">8.1 Evaluating DCGANs capability to capture data distributions</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf"
              ]
            ],
            "resource": "storage/i291.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
          ],
          [
            "Access Date",
            "2021-03-16 14:49:34"
          ],
          [
            "Conference Name",
            "4th International Conference on Learning Representations, ICLR 2016"
          ],
          [
            "Creators",
            "Alec Radford, Luke Metz, Soumith Chintala"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Proceedings Title",
            "4th International Conference on Learning Representations, ICLR 2016"
          ],
          [
            "Title",
            "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
          ],
          [
            "URL",
            "https://arxiv.org/abs/1511.06434"
          ]
        ],
        "resource": "storage/i291.pdf",
        "selectable": false
      },
      {
        "text": "Wasserstein Generative Adversarial Networks",
        "item-id": "i1565",
        "nodes": [
          {
            "text": "Arjovsky et al_2017_Wasserstein Generative Adversarial Networks.pdf",
            "item-id": "i1596",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Arjovsky et al_2017_Wasserstein Generative Adversarial Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Arjovsky et al_2017_Wasserstein Generative Adversarial Networks.pdf"
              ]
            ],
            "resource": "storage/i1596.pdf"
          },
          {
            "text": "Supplementary PDF",
            "item-id": "i1595",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-05-20 07:13:22"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf"
              ]
            ],
            "resource": "storage/i1595.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Wasserstein Generative Adversarial Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions."
          ],
          [
            "Access Date",
            "2022-05-20 07:13:19"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou"
          ],
          [
            "Date",
            "2017-07-17 2017-07-17"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "214-223"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 34th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Wasserstein Generative Adversarial Networks"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v70/arjovsky17a.html"
          ]
        ],
        "selectable": false
      }
    ],
    "item_title": "Generative Adversarial Networks",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Knowledge Distillation",
    "item-id": "c12,i1813",
    "nodes": [
      {
        "text": "CKD",
        "item-id": "i71",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n246",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Using knowledge distillation to perform text to image synthesis task.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Yuan_Peng_2020_CKD.pdf",
            "item-id": "i247",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yuan_Peng_2020_CKD.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yuan_Peng_2020_CKD.pdf"
              ]
            ],
            "resource": "storage/i247.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CKD",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach."
          ],
          [
            "Creators",
            "M. Yuan, Y. Peng"
          ],
          [
            "DOI",
            "10.1109/TMM.2019.2951463"
          ],
          [
            "Date",
            "2020-08-00 August 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Multimedia"
          ],
          [
            "ISSN",
            "1941-0077"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1955-1968"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Multimedia"
          ],
          [
            "Short Title",
            "CKD"
          ],
          [
            "Title",
            "CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis"
          ],
          [
            "Volume",
            "22"
          ]
        ],
        "resource": "storage/i247.pdf",
        "selectable": false
      },
      {
        "text": "Collaborative Distillation for Ultra-Resolution Universal Style Transfer",
        "item-id": "i100",
        "nodes": [
          {
            "text": "Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf",
            "item-id": "i227",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf"
              ]
            ],
            "resource": "storage/i227.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Collaborative Distillation for Ultra-Resolution Universal Style Transfer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation."
          ],
          [
            "Access Date",
            "2021-03-29 04:33:48"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Huan Wang, Yijun Li, Yuehai Wang, Haoji Hu, Ming-Hsuan Yang"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1860-1869"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Collaborative Distillation for Ultra-Resolution Universal Style Transfer"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i227.pdf",
        "selectable": false
      },
      {
        "text": "Compressing GANs using Knowledge Distillation",
        "item-id": "i84",
        "nodes": [
          {
            "text": "Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf",
            "item-id": "i242",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf"
              ]
            ],
            "resource": "storage/i242.pdf"
          },
          {
            "text": "Annotation",
            "item-id": "n240",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Using knowledge distillation to compress the generator of GAN.</p></div></div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Compressing GANs using Knowledge Distillation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super resolution, and synthetic data generation. State-of-the-art GANs often use tens of millions of parameters, making them expensive to deploy for applications in low SWAP (size, weight, and power) hardware, such as mobile devices, and for applications with real time capabilities. There has been no work found to reduce the number of parameters used in GANs. Therefore, we propose a method to compress GANs using knowledge distillation techniques, in which a smaller \"student\" GAN learns to mimic a larger \"teacher\" GAN. We show that the distillation methods used on MNIST, CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1, 58:1, and 87:1, respectively, while retaining the quality of the generated image. From our experiments, we observe a qualitative limit for GAN's compression. Moreover, we observe that, with a fixed parameter budget, compressed GANs outperform GANs trained using standard training methods. We conjecture that this is partially owing to the optimization landscape of over-parameterized GANs which allows efficient training using alternating gradient descent. Thus, training an over-parameterized GAN followed by our proposed compression scheme provides a high quality generative model with a small number of parameters."
          ],
          [
            "Access Date",
            "2021-03-26 07:04:30"
          ],
          [
            "Creators",
            "Angeline Aguinaldo, Ping-Yeh Chiang, Alex Gain, Ameya Patil, Kolten Pearson, Soheil Feizi"
          ],
          [
            "Date",
            "2019-01-31 2019-01-31"
          ],
          [
            "Extra",
            "arXiv: 1902.00159"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1902.00159 [cs, stat]"
          ],
          [
            "Title",
            "Compressing GANs using Knowledge Distillation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1902.00159"
          ]
        ],
        "resource": "storage/i242.pdf",
        "selectable": false
      },
      {
        "text": "DeepVID",
        "item-id": "i80",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n278",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>Using VAE and knowledge distillation to generate neighbours of interesting points.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2019_DeepVID.pdf",
            "item-id": "i279",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2019_DeepVID.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2019_DeepVID.pdf"
              ]
            ],
            "resource": "storage/i279.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DeepVID",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID."
          ],
          [
            "Creators",
            "J. Wang, L. Gou, W. Zhang, H. Yang, H. Shen"
          ],
          [
            "DOI",
            "10.1109/TVCG.2019.2903943"
          ],
          [
            "Date",
            "2019-06-00 June 2019"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Visualization and Computer Graphics"
          ],
          [
            "ISSN",
            "1941-0506"
          ],
          [
            "Issue",
            "6"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2168-2180"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Visualization and Computer Graphics"
          ],
          [
            "Short Title",
            "DeepVID"
          ],
          [
            "Title",
            "DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation"
          ],
          [
            "Volume",
            "25"
          ]
        ],
        "resource": "storage/i279.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake Detection Scheme Based on Vision Transformer and Distillation",
        "item-id": "i936",
        "nodes": [
          {
            "text": "Comment: 7 pages, 5 figures",
            "item-id": "n942",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 7 pages, 5 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 7 pages, 5 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf",
            "item-id": "i941",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf"
              ]
            ],
            "resource": "storage/i941.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Deepfake Detection Scheme Based on Vision Transformer and Distillation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition."
          ],
          [
            "Access Date",
            "2021-09-15 22:04:42"
          ],
          [
            "Archiveid",
            "arXiv:2104.01353"
          ],
          [
            "Creators",
            "Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim"
          ],
          [
            "Date",
            "2021-04-03 2021-04-03"
          ],
          [
            "Extra",
            "arXiv:2104.01353 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Deepfake Detection Scheme Based on Vision Transformer and Distillation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2104.01353"
          ]
        ],
        "resource": "storage/i941.pdf",
        "selectable": false
      },
      {
        "text": "Distilling Image Dehazing With Heterogeneous Task Imitation",
        "item-id": "i77",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n265",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Using knowledge distillation and an autoencoder teacher to train a dehazing network with representation mimicking loss.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf",
            "item-id": "i266",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf"
              ]
            ],
            "resource": "storage/i266.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Distilling Image Dehazing With Heterogeneous Task Imitation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-18 05:07:28"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ming Hong, Yuan Xie, Cuihua Li, Yanyun Qu"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3462-3471"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Distilling Image Dehazing With Heterogeneous Task Imitation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i266.pdf",
        "selectable": false
      },
      {
        "text": "Distilling the Knowledge in a Neural Network",
        "item-id": "i93",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n281",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>Knowledge Distillation</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Comment: NIPS 2014 Deep Learning Workshop",
            "item-id": "n284",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NIPS 2014 Deep Learning Workshop",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NIPS 2014 Deep Learning Workshop</div>",
            "node_type": "note"
          },
          {
            "text": "Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf",
            "item-id": "i283",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_B6YTT2Q4/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/2\">2 Distillation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/3\">2.1 Matching logits is a special case of distillation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/3\">3 Preliminary experiments on MNIST</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/4\">4 Experiments on speech recognition</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/5\">4.1 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/5\">5 Training ensembles of specialists on very big datasets</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/5\">5.1 The JFT dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/6\">5.2 Specialist Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/6\">5.3 Assigning classes to specialists</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/6\">5.4 Performing inference with ensembles of specialists</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/7\">5.5 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/7\">6 Soft Targets as Regularizers</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/8\">6.1 Using soft targets to prevent specialists from overfitting</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/8\">7 Relationship to Mixtures of Experts</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B6YTT2Q4/8\">8 Discussion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf"
              ]
            ],
            "resource": "storage/i283.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Distilling the Knowledge in a Neural Network",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
          ],
          [
            "Access Date",
            "2021-03-17 12:52:08"
          ],
          [
            "Creators",
            "Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
          ],
          [
            "Date",
            "2015-03-09 2015-03-09"
          ],
          [
            "Extra",
            "arXiv: 1503.02531"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1503.02531 [cs, stat]"
          ],
          [
            "Title",
            "Distilling the Knowledge in a Neural Network"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1503.02531"
          ]
        ],
        "resource": "storage/i283.pdf",
        "selectable": false
      },
      {
        "text": "Dreaming to Distill",
        "item-id": "i103",
        "nodes": [
          {
            "text": "Yin et al_2020_Dreaming to Distill.pdf",
            "item-id": "i233",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yin et al_2020_Dreaming to Distill.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yin et al_2020_Dreaming to Distill.pdf"
              ]
            ],
            "resource": "storage/i233.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Dreaming to Distill",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We \"invert\" a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning."
          ],
          [
            "Access Date",
            "2021-03-29 04:15:29"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Hongxu Yin, Pavlo Molchanov, Jose M. Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K. Jha, Jan Kautz"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8715-8724"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Dreaming to Distill"
          ],
          [
            "Title",
            "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i233.pdf",
        "selectable": false
      },
      {
        "text": "Image Super-Resolution Using Knowledge Distillation",
        "item-id": "i83",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n268",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Perform super resolution with knowledge distillation.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf",
            "item-id": "i269",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_C3G4QF6R/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/3\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/3\">2.1 Image Super Resolution via Deep Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/4\">2.2 Knowledge Distillation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/5\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/5\">3.1 Structure of Teacher Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/7\">3.2 Structure of Student Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/8\">3.3 Knowledge Distillation and Propagation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.1 Datasets and Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.2 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.3 Important of Different Level of Feature Maps</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.4 Comparison of Different Ways for Knowledge Distillation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/12\">4.5 Improvement of Student SR Network</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/14\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/14\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf"
              ]
            ],
            "resource": "storage/i269.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Image Super-Resolution Using Knowledge Distillation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The significant improvements in image super-resolution (SR) in recent years is majorly resulted from the use of deeper and deeper convolutional neural networks (CNN). However, both computational time and memory consumption simultaneously increase with the utilization of very deep CNN models, posing challenges to deploy SR models in realtime on computationally limited devices. In this work, we propose a novel strategy that uses a teacher-student network to improve the image SR performance. The training of a small but efficient student network is guided by a deep and powerful teacher network. We have evaluated the performance using different ways of knowledge distillation. Through the validations on four datasets, the proposed method significantly improves the SR performance of a student network without changing its structure. This means that the computational time and the memory consumption do not increase during the testing stage while the SR performance is significantly improved."
          ],
          [
            "Conference Name",
            "Computer Vision - ACCV 2018"
          ],
          [
            "Creators",
            "Qinquan Gao, Yan Zhao, Gen Li, Tong Tong, C. V. Jawahar, Hongdong Li, Greg Mori, Konrad Schindler"
          ],
          [
            "DOI",
            "10.1007/978-3-030-20890-5_34"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "ISBN",
            "978-3-030-20890-5"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "527-541"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Computer Vision - ACCV 2018"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Image Super-Resolution Using Knowledge Distillation"
          ]
        ],
        "resource": "storage/i269.pdf",
        "selectable": false
      },
      {
        "text": "Knowledge Distillation",
        "item-id": "i1703",
        "nodes": [
          {
            "text": "Beyer et al_2022_Knowledge Distillation.pdf",
            "item-id": "i1751",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Beyer et al_2022_Knowledge Distillation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Beyer et al_2022_Knowledge Distillation.pdf"
              ]
            ],
            "resource": "storage/i1751.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Knowledge Distillation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8% top-1 accuracy."
          ],
          [
            "Access Date",
            "2022-07-20 19:31:47"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Lucas Beyer, Xiaohua Zhai, Am\u00e9lie Royer, Larisa Markeeva, Rohan Anil, Alexander Kolesnikov"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10925-10934"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Knowledge Distillation"
          ],
          [
            "Title",
            "Knowledge Distillation: A Good Teacher Is Patient and Consistent"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1751.pdf",
        "selectable": false
      },
      {
        "text": "Knowledge Distillation for Face Photo-Sketch Synthesis",
        "item-id": "i69",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n237",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>Using Knowledge Distillation and GAN to translate images between face photo and sketch.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf",
            "item-id": "i238",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf"
              ]
            ],
            "resource": "storage/i238.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Knowledge Distillation for Face Photo-Sketch Synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Significant progress has been made with face photo-sketch synthesis in recent years due to the development of deep convolutional neural networks, particularly generative adversarial networks (GANs). However, the performance of existing methods is still limited because of the lack of training data (photo-sketch pairs). To address this challenge, we investigate the effect of knowledge distillation (KD) on training neural networks for the face photo-sketch synthesis task and propose an effective KD model to improve the performance of synthetic images. In particular, we utilize a teacher network trained on a large amount of data in a related task to separately learn knowledge of the face photo and knowledge of the face sketch and simultaneously transfer this knowledge to two student networks designed for the face photo-sketch synthesis task. In addition to assimilating the knowledge from the teacher network, the two student networks can mutually transfer their own knowledge to further enhance their learning. To further enhance the perception quality of the synthetic image, we propose a KD+ model that combines GANs with KD. The generator can produce images with more realistic textures and less noise under the guide of knowledge. Extensive experiments and a user study demonstrate the superiority of our models over the state-of-the-art methods."
          ],
          [
            "Creators",
            "M. Zhu, J. Li, N. Wang, X. Gao"
          ],
          [
            "DOI",
            "10.1109/TNNLS.2020.3030536"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Neural Networks and Learning Systems"
          ],
          [
            "ISSN",
            "2162-2388"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-14"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Neural Networks and Learning Systems"
          ],
          [
            "Title",
            "Knowledge Distillation for Face Photo-Sketch Synthesis"
          ]
        ],
        "resource": "storage/i238.pdf",
        "selectable": false
      },
      {
        "text": "Self-supervised Knowledge Distillation for Few-shot Learning",
        "item-id": "i1813",
        "nodes": [
          {
            "text": "Rajasegaran et al_2020_Self-supervised Knowledge Distillation for Few-shot Learning.pdf",
            "item-id": "i1816",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rajasegaran et al_2020_Self-supervised Knowledge Distillation for Few-shot Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rajasegaran et al_2020_Self-supervised Knowledge Distillation for Few-shot Learning.pdf"
              ]
            ],
            "resource": "storage/i1816.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Self-supervised Knowledge Distillation for Few-shot Learning",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works [7, 41] show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: https://github.com/brjathu/SKD."
          ],
          [
            "Access Date",
            "2022-10-12 07:00:54"
          ],
          [
            "Archiveid",
            "arXiv:2006.09785"
          ],
          [
            "Creators",
            "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah"
          ],
          [
            "DOI",
            "10.48550/arXiv.2006.09785"
          ],
          [
            "Date",
            "2020-08-04 2020-08-04"
          ],
          [
            "Extra",
            "arXiv:2006.09785 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Self-supervised Knowledge Distillation for Few-shot Learning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2006.09785"
          ]
        ],
        "resource": "storage/i1816.pdf",
        "selectable": false
      },
      {
        "text": "Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation",
        "item-id": "i75",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n262",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Using knowledge distillation to perform image-image translation with proposed semantic relation preserving matrix and semantic preserving distillation loss.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf",
            "item-id": "i205",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_QPV3DNRT/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/3\">2.1 GANs for Image-to-Image Translation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/4\">2.2 Semantic Relation Preserving Knowledge Distillation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/4\">2.3 Model Compression on GANs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/5\">3 Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/6\">3.1 Vanilla Knowledge Distillation on GANs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/7\">3.2 Semantic Preserving Loss</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/8\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/8\">4.1 Different Image-to-Image Translation Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/14\">4.2 Different Architectures</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/14\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QPV3DNRT/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf"
              ]
            ],
            "resource": "storage/i205.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher\u2019s feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Zeqi Li, Ruowei Jiang, Parham Aarabi, Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58574-7_39"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58574-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "648-663"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation"
          ]
        ],
        "resource": "storage/i205.pdf",
        "selectable": false
      },
      {
        "text": "Similarity-Preserving Knowledge Distillation",
        "item-id": "i81",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n243",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Using corsine similarity matrix to represent the relation in each batch, to replace the feature map to be learned in knowledge distillation.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf",
            "item-id": "i245",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf"
              ]
            ],
            "resource": "storage/i245.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Similarity-Preserving Knowledge Distillation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-22 08:16:09"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Frederick Tung, Greg Mori"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1365-1374"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Similarity-Preserving Knowledge Distillation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i245.pdf",
        "selectable": false
      },
      {
        "text": "StyleGAN2 Distillation for Feed-Forward Image Manipulation",
        "item-id": "i72",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n277",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>Using StyleGAN to generate pairs for pix2pixHD to be trained as image translation task.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf",
            "item-id": "i276",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YPQZMXUB/1\">StyleGAN2 Distillation for Feed-forward Image Manipulation</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf"
              ]
            ],
            "resource": "storage/i276.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "StyleGAN2 Distillation for Feed-Forward Image Manipulation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces\u2019 transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin, Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58542-6_11"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58542-6"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "170-186"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "StyleGAN2 Distillation for Feed-Forward Image Manipulation"
          ]
        ],
        "resource": "storage/i276.pdf",
        "selectable": false
      },
      {
        "text": "Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation",
        "item-id": "i76",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n234",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"1\"><p>Annotation</p>\n<p>Using knowledge distillation to improve the performance of image segmentation.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf",
            "item-id": "i236",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf"
              ]
            ],
            "resource": "storage/i236.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-03-29 00:40:26"
          ],
          [
            "Creators",
            "Kang Li, Lequan Yu, Shujun Wang, Pheng-Ann Heng"
          ],
          [
            "DOI",
            "10.1609/aaai.v34i01.5421"
          ],
          [
            "Date",
            "2020-04-03 2020/04/03"
          ],
          [
            "Extra",
            "Number: 01"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "01"
          ],
          [
            "Journal Abbreviation",
            "AAAI"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "775-783"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c) 2020 Association for the Advancement of Artificial Intelligence"
          ],
          [
            "Title",
            "Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/5421"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i236.pdf",
        "selectable": false
      },
      {
        "text": "Training data-efficient image transformers & distillation through attention",
        "item-id": "i740",
        "nodes": [
          {
            "text": "Supplementary PDF",
            "item-id": "i741",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2021-07-21 16:18:50"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v139/touvron21a/touvron21a-supp.pdf"
              ]
            ],
            "resource": "storage/i741.pdf"
          },
          {
            "text": "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf",
            "item-id": "i742",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf"
              ]
            ],
            "resource": "storage/i742.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Training data-efficient image transformers & distillation through attention",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models."
          ],
          [
            "Access Date",
            "2021-07-21 16:18:49"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "10347-10357"
          ],
          [
            "Proceedings Title",
            "International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Training data-efficient image transformers & distillation through attention"
          ],
          [
            "URL",
            "http://proceedings.mlr.press/v139/touvron21a.html"
          ]
        ],
        "selectable": false
      }
    ],
    "item_title": "Knowledge Distillation",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Large Language Model",
    "item-id": "c40,i3705",
    "nodes": [
      {
        "text": "A Survey on Evaluation of Large Language Models",
        "item-id": "i3259",
        "nodes": [
          {
            "text": "Chang et al_2023_A Survey on Evaluation of Large Language Models.pdf",
            "item-id": "i3389",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chang et al_2023_A Survey on Evaluation of Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DR8B2745/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/2\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DR8B2745/4\">2 Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/4\">2.1 Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/6\">2.2 AI Model Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DR8B2745/6\">3 What to Evaluate</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/7\">3.1 Natural Language Processing Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/13\">3.2 Robustness, Ethic, Bias, and Trustworthiness</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/15\">3.3 Social Science</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/16\">3.4 Natural Science and Engineering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/17\">3.5 Medical Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/19\">3.6 Agent Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/19\">3.7 Other Applications</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DR8B2745/23\">4 Where to Evaluate: Datasets and Benchmarks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/23\">4.1 Benchmarks for General Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/24\">4.2 Benchmarks for Specific Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/26\">4.3 Benchmarks for Multi-modal task</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DR8B2745/26\">5 How to Evaluate</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/26\">5.1 Automatic Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/27\">5.2 Human Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DR8B2745/27\">6 Summary</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/28\">6.1 Task: Success and Failure Cases of LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/29\">6.2 Benchmark and Evaluation Protocol</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DR8B2745/29\">7 Grand Challenges and Opportunities for Future Research</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/29\">7.1 Designing AGI Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/29\">7.2 Complete Behavioral Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/30\">7.3 Robustness Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/30\">7.4 Dynamic and Evolving Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/30\">7.5 Principled and Trustworthy Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/30\">7.6 Unified Evaluation that Supports All LLMs Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/30\">7.7 Beyond Evaluation: LLMs Enhancement</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/31\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DR8B2745/31\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chang et al_2023_A Survey on Evaluation of Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i3389.pdf"
          },
          {
            "text": "Comment: 31 pages; a major update to include more recent works; https://llm-eval.github.io/",
            "item-id": "n3390",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 31 pages; a major update to include more recent works; https://llm-eval.github.io/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 31 pages; a major update to include more recent works; https://llm-eval.github.io/</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "A Survey on Evaluation of Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey."
          ],
          [
            "Access Date",
            "2023-12-09 06:45:08"
          ],
          [
            "Archiveid",
            "arXiv:2307.03109"
          ],
          [
            "Creators",
            "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie"
          ],
          [
            "DOI",
            "10.48550/arXiv.2307.03109"
          ],
          [
            "Date",
            "2023-10-17 2023-10-17"
          ],
          [
            "Extra",
            "arXiv:2307.03109 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "A Survey on Evaluation of Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2307.03109"
          ]
        ],
        "resource": "storage/i3389.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Multimodal Large Language Models",
        "item-id": "i2791",
        "nodes": [
          {
            "text": "Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
            "item-id": "n2837",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</div>",
            "node_type": "note"
          },
          {
            "text": "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf",
            "item-id": "i2836",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BKHSHM5R/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Overview</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Multimodal Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/3\">Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/4\">Modality Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/4\">Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/5\">Modality Bridging</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/5\">Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/6\">. Multimodal In-Context Learning</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">. Multimodal Chain of Thought</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">Modality bridging</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">Learning Paradigms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/8\">Chain Configuration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/8\">Generation Patterns</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">. LLM-Aided Visual Reasoning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">Training Paradigms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">Evaluation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">. Challenges and Future Directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/11\">. Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i2836.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "A Survey on Multimodal Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models."
          ],
          [
            "Access Date",
            "2023-07-18 06:46:19"
          ],
          [
            "Archiveid",
            "arXiv:2306.13549"
          ],
          [
            "Creators",
            "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.13549"
          ],
          [
            "Date",
            "2023-06-23 2023-06-23"
          ],
          [
            "Extra",
            "arXiv:2306.13549 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "A Survey on Multimodal Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.13549"
          ]
        ],
        "resource": "storage/i2836.pdf",
        "selectable": false
      },
      {
        "text": "An In-depth Look at Gemini's Language Abilities",
        "item-id": "i3244",
        "nodes": [
          {
            "text": "Akter et al_2023_An In-depth Look at Gemini's Language Abilities.pdf",
            "item-id": "i3358",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Akter et al_2023_An In-depth Look at Gemini's Language Abilities.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Akter et al_2023_An In-depth Look at Gemini's Language Abilities.pdf"
              ]
            ],
            "resource": "storage/i3358.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "An In-depth Look at Gemini's Language Abilities",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark"
          ],
          [
            "Access Date",
            "2023-12-21 10:02:34"
          ],
          [
            "Archiveid",
            "arXiv:2312.11444"
          ],
          [
            "Creators",
            "Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex B\u00e4uerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig"
          ],
          [
            "DOI",
            "10.48550/arXiv.2312.11444"
          ],
          [
            "Date",
            "2023-12-18 2023-12-18"
          ],
          [
            "Extra",
            "arXiv:2312.11444 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "An In-depth Look at Gemini's Language Abilities"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2312.11444"
          ]
        ],
        "resource": "storage/i3358.pdf",
        "selectable": false
      },
      {
        "text": "ChatGPT",
        "item-id": "i3236",
        "nodes": [
          {
            "text": "Emsley_2023_ChatGPT.pdf",
            "item-id": "i3350",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Emsley_2023_ChatGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Emsley_2023_ChatGPT.pdf"
              ]
            ],
            "resource": "storage/i3350.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ChatGPT",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-12-27 09:16:04"
          ],
          [
            "Creators",
            "Robin Emsley"
          ],
          [
            "DOI",
            "10.1038/s41537-023-00379-4"
          ],
          [
            "Date",
            "2023-08-19 2023-08-19"
          ],
          [
            "Extra",
            "Number: 1\nPublisher: Nature Publishing Group"
          ],
          [
            "ISSN",
            "2754-6993"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "Schizophr"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.nature.com"
          ],
          [
            "Pages",
            "1-2"
          ],
          [
            "Publication Title",
            "Schizophrenia"
          ],
          [
            "Rights",
            "2023 The Author(s)"
          ],
          [
            "Short Title",
            "ChatGPT"
          ],
          [
            "Title",
            "ChatGPT: these are not hallucinations \u2013 they\u2019re fabrications and falsifications"
          ],
          [
            "URL",
            "https://www.nature.com/articles/s41537-023-00379-4"
          ],
          [
            "Volume",
            "9"
          ]
        ],
        "resource": "storage/i3350.pdf",
        "selectable": false
      },
      {
        "text": "Foundation Transformers",
        "item-id": "i3705",
        "nodes": [
          {
            "text": "Comment: Work in progress",
            "item-id": "n3706",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Work in progress",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Work in progress</div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2022_Foundation Transformers.pdf",
            "item-id": "i3708",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_Foundation Transformers.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8FL3QTSC/2\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/2\">2 TL;DR for Practitioners</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/3\">3 Magneto: A Foundation Transformer</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/3\">3.1 Architecture: Sub-LayerNorm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/4\">3.2 Initialization: Theoretical Derivation from DeepNet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/6\">4 Experiments on Language Tasks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/6\">4.1 Causal Language Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/7\">4.2 Masked Language Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/8\">4.3 Neural Machine Translation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/8\">5 Experiments on Vision Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/9\">6 Experiments on Speech Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/9\">7 Experiments on Vision-Language Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/10\">8 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/14\">A Model update for Encoder-only Transformers</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/14\">A.1 Pre-LN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/16\">A.2 Magneto</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/17\">B Model update for Encoder-decoder Transformers</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/17\">B.1 Pre-LN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/17\">B.2 Magneto</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8FL3QTSC/19\">C Hyperparameters</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_Foundation Transformers.pdf"
              ]
            ],
            "resource": "storage/i3708.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3709",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-17 03:53:13"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2210.06423"
              ]
            ],
            "resource": "storage/i3709.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Foundation Transformers",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name \"Transformers\", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3)."
          ],
          [
            "Access Date",
            "2024-01-17 03:53:06"
          ],
          [
            "Archiveid",
            "arXiv:2210.06423"
          ],
          [
            "Creators",
            "Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2210.06423"
          ],
          [
            "Date",
            "2022-10-19 2022-10-19"
          ],
          [
            "Extra",
            "arXiv:2210.06423 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Foundation Transformers"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2210.06423"
          ]
        ],
        "resource": "storage/i3708.pdf",
        "selectable": false
      },
      {
        "text": "Gemini",
        "item-id": "i3242",
        "nodes": [
          {
            "text": "Gemini Team et al_2023_Gemini.pdf",
            "item-id": "i3352",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gemini Team et al_2023_Gemini.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gemini Team et al_2023_Gemini.pdf"
              ]
            ],
            "resource": "storage/i3352.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Gemini",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users."
          ],
          [
            "Access Date",
            "2023-12-27 08:48:34"
          ],
          [
            "Archiveid",
            "arXiv:2312.11805"
          ],
          [
            "Creators",
            " Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S\u00e9bastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u0144ska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u010di\u0107, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha\u00ebl Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L\u00e9onard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri\u00e0 Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u011flar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u0107evi\u0107, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G. Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Casta\u00f1o, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj Rybi\u0144ski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R\u00e9mi Leblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim P\u00f5der, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, T. J. Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi\u00e8re, Alanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci\u0144ska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, M. K. Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, Oriol Vinyals"
          ],
          [
            "DOI",
            "10.48550/arXiv.2312.11805"
          ],
          [
            "Date",
            "2023-12-18 2023-12-18"
          ],
          [
            "Extra",
            "arXiv:2312.11805 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Gemini"
          ],
          [
            "Title",
            "Gemini: A Family of Highly Capable Multimodal Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2312.11805"
          ]
        ],
        "resource": "storage/i3352.pdf",
        "selectable": false
      },
      {
        "text": "InstructBLIP",
        "item-id": "i3031",
        "nodes": [
          {
            "text": "Comment: preprint",
            "item-id": "n3086",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: preprint",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: preprint</div>",
            "node_type": "note"
          },
          {
            "text": "Dai et al_2023_InstructBLIP.pdf",
            "item-id": "i3085",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dai et al_2023_InstructBLIP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dai et al_2023_InstructBLIP.pdf"
              ]
            ],
            "resource": "storage/i3085.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "InstructBLIP",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip."
          ],
          [
            "Access Date",
            "2023-10-05 07:18:05"
          ],
          [
            "Archiveid",
            "arXiv:2305.06500"
          ],
          [
            "Creators",
            "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.06500"
          ],
          [
            "Date",
            "2023-06-15 2023-06-15"
          ],
          [
            "Extra",
            "arXiv:2305.06500 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "InstructBLIP"
          ],
          [
            "Title",
            "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.06500"
          ]
        ],
        "resource": "storage/i3085.pdf",
        "selectable": false
      },
      {
        "text": "Instruction Tuning for Large Language Models",
        "item-id": "i3030",
        "nodes": [
          {
            "text": "Comment: A Survey paper, Pre-print",
            "item-id": "n3092",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: A Survey paper, Pre-print",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: A Survey paper, Pre-print</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang et al_2023_Instruction Tuning for Large Language Models.pdf",
            "item-id": "i3077",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_Instruction Tuning for Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_Instruction Tuning for Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i3077.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Instruction Tuning for Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey"
          ],
          [
            "Access Date",
            "2023-10-05 07:13:54"
          ],
          [
            "Archiveid",
            "arXiv:2308.10792"
          ],
          [
            "Creators",
            "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang"
          ],
          [
            "DOI",
            "10.48550/arXiv.2308.10792"
          ],
          [
            "Date",
            "2023-10-04 2023-10-04"
          ],
          [
            "Extra",
            "arXiv:2308.10792 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Instruction Tuning for Large Language Models"
          ],
          [
            "Title",
            "Instruction Tuning for Large Language Models: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2308.10792"
          ]
        ],
        "resource": "storage/i3077.pdf",
        "selectable": false
      },
      {
        "text": "Kosmos-2",
        "item-id": "i2995",
        "nodes": [
          {
            "text": "Comment: 20 pages",
            "item-id": "n3018",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 20 pages",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 20 pages</div>",
            "node_type": "note"
          },
          {
            "text": "Peng et al_2023_Kosmos-2.pdf",
            "item-id": "i3017",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Peng et al_2023_Kosmos-2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DEBGT5VC/3\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/3\">Construction of Web-Scale Grounded Image-Text Pairs (GrIT)</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/4\">Kosmos-2: A Grounded Multimodal Large Language Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/5\">Grounded Input Representations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/5\">Grounded Multimodal Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/6\">Model Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/6\">Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/7\">Multimodal Grounding</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/7\">Phrase Grounding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/8\">Referring Expression Comprehension</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/9\">Multimodal Referring</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/9\">Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/9\">Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/10\">Perception-Language Tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/10\">Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/10\">Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/10\">Language Tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/11\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/14\">Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/14\">Templates for Grounded Instruction Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/15\">Examples of GrIT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DEBGT5VC/16\">More Examples of Kosmos-2</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Peng et al_2023_Kosmos-2.pdf"
              ]
            ],
            "resource": "storage/i3017.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Kosmos-2",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2."
          ],
          [
            "Access Date",
            "2023-08-23 19:20:08"
          ],
          [
            "Archiveid",
            "arXiv:2306.14824"
          ],
          [
            "Creators",
            "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.14824"
          ],
          [
            "Date",
            "2023-07-13 2023-07-13"
          ],
          [
            "Extra",
            "arXiv:2306.14824 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Kosmos-2"
          ],
          [
            "Title",
            "Kosmos-2: Grounding Multimodal Large Language Models to the World"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.14824"
          ]
        ],
        "resource": "storage/i3017.pdf",
        "selectable": false
      },
      {
        "text": "Kosmos-2.5",
        "item-id": "i3645",
        "nodes": [
          {
            "text": "Lv et al_2023_Kosmos-2.pdf",
            "item-id": "i3673",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lv et al_2023_Kosmos-2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BPDEMYJ2/2\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/3\">Kosmos-2.5</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/3\">Model Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/3\">Image and Text Representations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/4\">Pre-training Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/4\">Data Processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/5\">Filtering and Quality Control</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/6\">Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/7\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/7\">Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/7\">Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/9\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/9\">Multimodal Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/10\">Text Image Understanding</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/10\">Conclusion and Future Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/16\">Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/16\">Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/16\">Data Samples</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BPDEMYJ2/16\">Examples of Model Inference</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lv et al_2023_Kosmos-2.pdf"
              ]
            ],
            "resource": "storage/i3673.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3674",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-16 11:43:08"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2309.11419"
              ]
            ],
            "resource": "storage/i3674.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Kosmos-2.5",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models."
          ],
          [
            "Access Date",
            "2024-01-16 11:42:45"
          ],
          [
            "Archiveid",
            "arXiv:2309.11419"
          ],
          [
            "Creators",
            "Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2309.11419"
          ],
          [
            "Date",
            "2023-09-20 2023-09-20"
          ],
          [
            "Extra",
            "arXiv:2309.11419 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Kosmos-2.5"
          ],
          [
            "Title",
            "Kosmos-2.5: A Multimodal Literate Model"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2309.11419"
          ]
        ],
        "resource": "storage/i3673.pdf",
        "selectable": false
      },
      {
        "text": "Kosmos-G",
        "item-id": "i3644",
        "nodes": [
          {
            "text": "Comment: Code: https://aka.ms/Kosmos-G Project Page: https://xichenpan.github.io/kosmosg",
            "item-id": "n3670",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Code: https://aka.ms/Kosmos-G Project Page: https://xichenpan.github.io/kosmosg",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Code: https://aka.ms/Kosmos-G Project Page: https://xichenpan.github.io/kosmosg</div>",
            "node_type": "note"
          },
          {
            "text": "Pan et al_2023_Kosmos-G.pdf",
            "item-id": "i3669",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Pan et al_2023_Kosmos-G.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WY2E9CTI/2\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/4\">Kosmos-G: Image as a Foreign Language in Image Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/4\">Multimodal Language Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/4\">Image Decoder Aligning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/6\">Instruction Tuning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/7\">Model Training</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/7\">Multimodal Training Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/7\">Training Setup</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/8\">Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/8\">Main Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/9\">Quantitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/9\">Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/9\">Applications</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/10\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WY2E9CTI/15\">Images and Prompts for DreamBench Evaluation</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Pan et al_2023_Kosmos-G.pdf"
              ]
            ],
            "resource": "storage/i3669.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3668",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-16 12:41:40"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2310.02992"
              ]
            ],
            "resource": "storage/i3668.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Kosmos-G",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\""
          ],
          [
            "Access Date",
            "2024-01-16 12:41:20"
          ],
          [
            "Archiveid",
            "arXiv:2310.02992"
          ],
          [
            "Creators",
            "Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2310.02992"
          ],
          [
            "Date",
            "2023-10-04 2023-10-04"
          ],
          [
            "Extra",
            "arXiv:2310.02992 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Kosmos-G"
          ],
          [
            "Title",
            "Kosmos-G: Generating Images in Context with Multimodal Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2310.02992"
          ]
        ],
        "resource": "storage/i3669.pdf",
        "selectable": false
      },
      {
        "text": "LLaMA",
        "item-id": "i2523",
        "nodes": [
          {
            "text": "Touvron et al_2023_LLaMA.pdf",
            "item-id": "i2728",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Touvron et al_2023_LLaMA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Touvron et al_2023_LLaMA.pdf"
              ]
            ],
            "resource": "storage/i2728.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "LLaMA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
          ],
          [
            "Access Date",
            "2023-06-08 04:08:55"
          ],
          [
            "Archiveid",
            "arXiv:2302.13971"
          ],
          [
            "Creators",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.13971"
          ],
          [
            "Date",
            "2023-02-27 2023-02-27"
          ],
          [
            "Extra",
            "arXiv:2302.13971 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "LLaMA"
          ],
          [
            "Title",
            "LLaMA: Open and Efficient Foundation Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.13971"
          ]
        ],
        "resource": "storage/i2728.pdf",
        "selectable": false
      },
      {
        "text": "Language Is Not All You Need",
        "item-id": "i2996",
        "nodes": [
          {
            "text": "Huang et al_2023_Language Is Not All You Need.pdf",
            "item-id": "i3015",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2023_Language Is Not All You Need.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2023_Language Is Not All You Need.pdf"
              ]
            ],
            "resource": "storage/i3015.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Language Is Not All You Need",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs."
          ],
          [
            "Access Date",
            "2023-08-23 19:30:08"
          ],
          [
            "Archiveid",
            "arXiv:2302.14045"
          ],
          [
            "Creators",
            "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.14045"
          ],
          [
            "Date",
            "2023-03-01 2023-03-01"
          ],
          [
            "Extra",
            "arXiv:2302.14045 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Language Is Not All You Need"
          ],
          [
            "Title",
            "Language Is Not All You Need: Aligning Perception with Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.14045"
          ]
        ],
        "resource": "storage/i3015.pdf",
        "selectable": false
      },
      {
        "text": "Language Models are Few-Shot Learners",
        "item-id": "i3205",
        "nodes": [
          {
            "text": "Brown et al_2020_Language Models are Few-Shot Learners.pdf",
            "item-id": "i3282",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Brown et al_2020_Language Models are Few-Shot Learners.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_D2TMCKC9/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/3\">Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/3\">Model and Architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/3\">Training Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/4\">Training Process</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/4\">Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/4\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/4\">Language Modeling, Cloze, and Completion Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/5\">Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/6\">Translation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/6\">SuperGLUE</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/7\">Measuring and Preventing Memorization Of Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/8\">Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/8\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/9\">Conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/9\">Misuse of Language Models</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/9\">Potential Misuse Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/10\">Threat Actor Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/10\">External Incentive Structures</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/10\">Fairness, Bias, and Representation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/11\">Gender</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/12\">Race</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/12\">Religion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/13\">Future Bias and Fairness Challenges</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/14\">Energy Usage</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D2TMCKC9/15\">News Generation</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Brown et al_2020_Language Models are Few-Shot Learners.pdf"
              ]
            ],
            "resource": "storage/i3282.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Language Models are Few-Shot Learners",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora."
          ],
          [
            "Access Date",
            "2023-11-14 04:46:23"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "1877\u20131901"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Language Models are Few-Shot Learners"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i3282.pdf",
        "selectable": false
      },
      {
        "text": "Language Models are General-Purpose Interfaces",
        "item-id": "i3646",
        "nodes": [
          {
            "text": "Comment: 32 pages. The first three authors contribute equally",
            "item-id": "n3667",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 32 pages. The first three authors contribute equally",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 32 pages. The first three authors contribute equally</div>",
            "node_type": "note"
          },
          {
            "text": "Hao et al_2022_Language Models are General-Purpose Interfaces.pdf",
            "item-id": "i3666",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hao et al_2022_Language Models are General-Purpose Interfaces.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZBF2T72R/4\">1 Introduction: Design Principles</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/5\">2 MetaLM: Meta Language Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/5\">2.1 Input Representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/6\">2.2 Model Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/6\">2.3 Proposed Objective: Semi-Causal Language Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/7\">2.4 Capabilities on Downstream Tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/7\">3 Experiments on Language-Only Tasks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/7\">3.1 Evaluation Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/8\">3.2 Pretraining Setup</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/9\">3.3 Multitask Finetuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/9\">3.3.1 Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/10\">3.3.2 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/11\">3.4 Single-Task Finetuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/11\">3.4.1 Finetuning Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/11\">3.4.2 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/11\">3.5 Instruction-Tuned Zero-Shot Generalization</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/11\">3.5.1 Instruction-Tuning Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/12\">3.5.2 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/13\">3.6 In-Context Learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/13\">3.6.1 Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/13\">3.6.2 Results</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/14\">4 Experiments on Vision-Language Tasks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/14\">4.1 Evaluation Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/14\">4.2 Pretraining Setup</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/15\">4.3 Zero-Shot Generalization</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/15\">4.3.1 Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/16\">4.3.2 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/16\">4.4 In-Context Learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/16\">4.4.1 Evaluation Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/17\">4.4.2 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/17\">4.5 Finetuning on Downstream Tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/17\">4.5.1 Finetuning Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/18\">4.5.2 Results: Visual Question Answering and Visual Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/20\">4.5.3 Results: Visually Grounded Language Generation</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/20\">5 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/20\">5.1 Language Model Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/21\">5.2 General-Purpose Modeling</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/21\">6 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/29\">A Hyperparameters of Language-Only Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/29\">A.1 Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/29\">A.2 Multitask Finetuning and Instruction Tuning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/29\">B Datasets Used for Language-Only Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/29\">B.1 Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/30\">B.2 Multitask Finetuning and Instruction Tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/30\">B.3 In-Context Learning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/30\">C Detailed Results of Multitask Finetuning in Section 3.3</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/32\">D Hyperparameters of Vision-Language Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/32\">D.1 Hyperparameters of Vision-Language Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZBF2T72R/32\">D.2 Hyperparameters in Vision-Language Finetuning</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hao et al_2022_Language Models are General-Purpose Interfaces.pdf"
              ]
            ],
            "resource": "storage/i3666.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3665",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-16 12:44:00"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2206.06336"
              ]
            ],
            "resource": "storage/i3665.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Language Models are General-Purpose Interfaces",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning."
          ],
          [
            "Access Date",
            "2024-01-16 12:43:46"
          ],
          [
            "Archiveid",
            "arXiv:2206.06336"
          ],
          [
            "Creators",
            "Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2206.06336"
          ],
          [
            "Date",
            "2022-06-13 2022-06-13"
          ],
          [
            "Extra",
            "arXiv:2206.06336 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Language Models are General-Purpose Interfaces"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2206.06336"
          ]
        ],
        "resource": "storage/i3666.pdf",
        "selectable": false
      },
      {
        "text": "Llama 2",
        "item-id": "i2940",
        "nodes": [
          {
            "text": "Touvron et al_2023_Llama 2.pdf",
            "item-id": "i2966",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Touvron et al_2023_Llama 2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_W27CMVB8/3\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W27CMVB8/5\">Pretraining</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/5\">Pretraining Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/5\">Training Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/7\">Llama 2 Pretrained Model Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W27CMVB8/8\">Fine-tuning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/9\">Supervised Fine-Tuning (SFT)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/9\">Reinforcement Learning with Human Feedback (RLHF)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/16\">System Message for Multi-Turn Consistency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/17\">RLHF Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W27CMVB8/20\">Safety</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/20\">Safety in Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/23\">Safety Fine-Tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/28\">Red Teaming</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/29\">Safety Evaluation of Llama 2-Chat</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W27CMVB8/32\">Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/32\">Learnings and Observations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/34\">Limitations and Ethical Considerations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/35\">Responsible Release Strategy</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/35\">Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/36\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_W27CMVB8/46\">Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/46\">Contributions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/47\">Additional Details for Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/51\">Additional Details for Fine-tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/58\">Additional Details for Safety</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/72\">Data Annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/75\">Dataset Contamination</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_W27CMVB8/77\">Model Card</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Touvron et al_2023_Llama 2.pdf"
              ]
            ],
            "resource": "storage/i2966.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Llama 2",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
          ],
          [
            "Access Date",
            "2023-07-27 01:23:00"
          ],
          [
            "Archiveid",
            "arXiv:2307.09288"
          ],
          [
            "Creators",
            "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom"
          ],
          [
            "DOI",
            "10.48550/arXiv.2307.09288"
          ],
          [
            "Date",
            "2023-07-19 2023-07-19"
          ],
          [
            "Extra",
            "arXiv:2307.09288 [cs]\nCitation Key: touvronLlama2023a"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Llama 2"
          ],
          [
            "Title",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2307.09288"
          ]
        ],
        "resource": "storage/i2966.pdf",
        "selectable": false
      },
      {
        "text": "MiniGPT-4",
        "item-id": "i2792",
        "nodes": [
          {
            "text": "Comment: Project Website: https://minigpt-4.github.io/; Code, Pretrained Model, and Dataset: https://github.com/Vision-C",
            "item-id": "n2847",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project Website: https://minigpt-4.github.io/; Code, Pretrained Model, and Dataset: https://github.com/Vision-C",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project Website: https://minigpt-4.github.io/; Code, Pretrained Model, and Dataset: https://github.com/Vision-CAIR/MiniGPT-4; Deyao Zhu and Jun Chen contributed equally to this work</div>",
            "node_type": "note"
          },
          {
            "text": "Zhu et al_2023_MiniGPT-4.pdf",
            "item-id": "i2846",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2023_MiniGPT-4.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Z5TH7WU3/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/2\">2 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/4\">3.1 First pretraining stage</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/4\">3.2 Curating a high-quality alignment dataset for vision-language domain.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/5\">3.3 Second-stage finetuning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/5\">4 Demonstrations:</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/5\">5 Limitations</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2023_MiniGPT-4.pdf"
              ]
            ],
            "resource": "storage/i2846.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MiniGPT-4",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. We believe the primary reason for GPT-4's advanced multi-modal generation capabilities lies in the utilization of a more advanced large language model (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer. Our findings reveal that MiniGPT-4 possesses many capabilities similar to those exhibited by GPT-4 like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, providing solutions to problems shown in images, teaching users how to cook based on food photos, etc. In our experiment, we found that only performing the pretraining on raw image-text pairs could produce unnatural language outputs that lack coherency including repetition and fragmented sentences. To address this problem, we curate a high-quality, well-aligned dataset in the second stage to finetune our model using a conversational template. This step proved crucial for augmenting the model's generation reliability and overall usability. Notably, our model is highly computationally efficient, as we only train a projection layer utilizing approximately 5 million aligned image-text pairs. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/."
          ],
          [
            "Access Date",
            "2023-07-18 01:15:35"
          ],
          [
            "Archiveid",
            "arXiv:2304.10592"
          ],
          [
            "Creators",
            "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny"
          ],
          [
            "DOI",
            "10.48550/arXiv.2304.10592"
          ],
          [
            "Date",
            "2023-04-20 2023-04-20"
          ],
          [
            "Extra",
            "arXiv:2304.10592 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MiniGPT-4"
          ],
          [
            "Title",
            "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2304.10592"
          ]
        ],
        "resource": "storage/i2846.pdf",
        "selectable": false
      },
      {
        "text": "MultiModal-GPT",
        "item-id": "i3034",
        "nodes": [
          {
            "text": "Comment: 10 pages, 8 figures",
            "item-id": "n3084",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 10 pages, 8 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 10 pages, 8 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Gong et al_2023_MultiModal-GPT.pdf",
            "item-id": "i3082",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gong et al_2023_MultiModal-GPT.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SVBXUUSE/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/2\">Unified Instruction Template</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/2\">Language-only Instruction Template</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/3\">Vision and Language Instruction Template</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/4\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/4\">Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/4\">Joint Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/4\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/4\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/5\">The Quality of Data Matters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SVBXUUSE/5\">Demos</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gong et al_2023_MultiModal-GPT.pdf"
              ]
            ],
            "resource": "storage/i3082.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MultiModal-GPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint training of language-only and visual-language instructions with the \\emph{same} instruction template effectively improves dialogue performance. Various demos show the ability of continuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are at https://github.com/open-mmlab/Multimodal-GPT"
          ],
          [
            "Access Date",
            "2023-10-05 07:19:00"
          ],
          [
            "Archiveid",
            "arXiv:2305.04790"
          ],
          [
            "Creators",
            "Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.04790"
          ],
          [
            "Date",
            "2023-06-13 2023-06-13"
          ],
          [
            "Extra",
            "arXiv:2305.04790 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MultiModal-GPT"
          ],
          [
            "Title",
            "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.04790"
          ]
        ],
        "resource": "storage/i3082.pdf",
        "selectable": false
      },
      {
        "text": "NExT-GPT",
        "item-id": "i3022",
        "nodes": [
          {
            "text": "Comment: work in progress",
            "item-id": "n3057",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: work in progress",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: work in progress</div>",
            "node_type": "note"
          },
          {
            "text": "Wu et al_2023_NExT-GPT.pdf",
            "item-id": "i3056",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2023_NExT-GPT.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G6L659VK/2\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/3\">Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/4\">Overall Architecture</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G6L659VK/5\">Lightweight Multimodal Alignment Learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/5\">Encoding-side LLM-centric Multimodal Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/6\">Decoding-side Instruction-following Alignment</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G6L659VK/6\">Modality-switching Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/6\">Instruction Tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/7\">Instruction Dataset</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G6L659VK/9\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/9\">Any-to-any Multimodal Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/10\">Example Demonstrations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G6L659VK/10\">Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2023_NExT-GPT.pdf"
              ]
            ],
            "resource": "storage/i3056.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "NExT-GPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/"
          ],
          [
            "Access Date",
            "2023-10-06 06:18:50"
          ],
          [
            "Archiveid",
            "arXiv:2309.05519"
          ],
          [
            "Creators",
            "Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua"
          ],
          [
            "DOI",
            "10.48550/arXiv.2309.05519"
          ],
          [
            "Date",
            "2023-09-13 2023-09-13"
          ],
          [
            "Extra",
            "arXiv:2309.05519 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "NExT-GPT"
          ],
          [
            "Title",
            "NExT-GPT: Any-to-Any Multimodal LLM"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2309.05519"
          ]
        ],
        "resource": "storage/i3056.pdf",
        "selectable": false
      },
      {
        "text": "Otter",
        "item-id": "i3032",
        "nodes": [
          {
            "text": "Comment: Technical Report",
            "item-id": "n3081",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical Report",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical Report</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2023_Otter.pdf",
            "item-id": "i3079",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2023_Otter.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2023_Otter.pdf"
              ]
            ],
            "resource": "storage/i3079.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Otter",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines."
          ],
          [
            "Access Date",
            "2023-10-05 07:19:17"
          ],
          [
            "Archiveid",
            "arXiv:2305.03726"
          ],
          [
            "Creators",
            "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.03726"
          ],
          [
            "Date",
            "2023-05-05 2023-05-05"
          ],
          [
            "Extra",
            "arXiv:2305.03726 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Otter"
          ],
          [
            "Title",
            "Otter: A Multi-Modal Model with In-Context Instruction Tuning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.03726"
          ]
        ],
        "resource": "storage/i3079.pdf",
        "selectable": false
      },
      {
        "text": "Reasoning with Language Model is Planning with World Model",
        "item-id": "i3452",
        "nodes": [
          {
            "text": "Comment: EMNLP 2023. Code is available at https://github.com/Ber666/llm-reasoners",
            "item-id": "n3513",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: EMNLP 2023. Code is available at https://github.com/Ber666/llm-reasoners",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: EMNLP 2023. Code is available at https://github.com/Ber666/llm-reasoners</div>",
            "node_type": "note"
          },
          {
            "text": "Hao et al_2023_Reasoning with Language Model is Planning with World Model.pdf",
            "item-id": "i3512",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hao et al_2023_Reasoning with Language Model is Planning with World Model.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hao et al_2023_Reasoning with Language Model is Planning with World Model.pdf"
              ]
            ],
            "resource": "storage/i3512.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Reasoning with Language Model is Planning with World Model",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting."
          ],
          [
            "Access Date",
            "2024-01-08 11:17:47"
          ],
          [
            "Archiveid",
            "arXiv:2305.14992"
          ],
          [
            "Creators",
            "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.14992"
          ],
          [
            "Date",
            "2023-10-23 2023-10-23"
          ],
          [
            "Extra",
            "arXiv:2305.14992 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Reasoning with Language Model is Planning with World Model"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.14992"
          ]
        ],
        "resource": "storage/i3512.pdf",
        "selectable": false
      },
      {
        "text": "Valley",
        "item-id": "i2942",
        "nodes": [
          {
            "text": "Luo et al_2023_Valley.pdf",
            "item-id": "i2970",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Luo et al_2023_Valley.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UQ8IRX9V/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/3\">Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/3\">Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/4\">Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/5\">Instruction tuning data collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/5\">Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/6\">Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/7\">Qualitative Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/7\">Limitation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQ8IRX9V/8\">Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Luo et al_2023_Valley.pdf"
              ]
            ],
            "resource": "storage/i2970.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Valley",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, several multi-modal models have been developed for joint image and language understanding, which have demonstrated impressive chat abilities by utilizing advanced large language models (LLMs). The process of developing such models is straightforward yet effective. It involves pre-training an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on the instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of perceiving video, image, and language within a general framework. To achieve this goal, we introduce Valley: Video Assistant with Large Language model Enhanced ability. Specifically, our proposed Valley model is designed with a simple projection module that bridges video, image, and language modalities, and is further unified with a multi-lingual LLM. We also collect multi-source vision-text pairs and adopt a spatio-temporal pooling strategy to obtain a unified vision encoding of video and image input for pre-training. Furthermore, we generate multi-task instruction-following video data, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. To obtain the instruction-following data, we design diverse rounds of task-oriented conversations between humans and videos, facilitated by ChatGPT. Qualitative examples demonstrate that our proposed model has the potential to function as a highly effective multilingual video assistant that can make complex video understanding scenarios easy. Code, data, and models will be available at https://github.com/RupertLuo/Valley."
          ],
          [
            "Access Date",
            "2023-07-27 01:16:39"
          ],
          [
            "Archiveid",
            "arXiv:2306.07207"
          ],
          [
            "Creators",
            "Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, Zhongyu Wei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.07207"
          ],
          [
            "Date",
            "2023-06-12 2023-06-12"
          ],
          [
            "Extra",
            "arXiv:2306.07207 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Valley"
          ],
          [
            "Title",
            "Valley: Video Assistant with Large Language model Enhanced abilitY"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.07207"
          ]
        ],
        "resource": "storage/i2970.pdf",
        "selectable": false
      },
      {
        "text": "Video Understanding with Large Language Models",
        "item-id": "i3435",
        "nodes": [
          {
            "text": "Tang et al_2024_Video Understanding with Large Language Models.pdf",
            "item-id": "i3490",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tang et al_2024_Video Understanding with Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Q7V8RBBF/3\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/5\">Foundations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/7\">Vision Integration with LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/7\">Language's Roles in Video Understanding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">Other Modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">Training Strategies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">Vid-LLMs: Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">LLM-based Video Agents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/11\">Vid-LLM Pretraining</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/11\">Vid-LLM Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/12\">Fine-tuning with Connective Adapters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/14\">Fine-tuning with Insertive Adapters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/14\">Fine-tuning with Hybrid Adapters</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/15\">Hybrid Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/15\">Tasks, Datasets, and Benchmarks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Recognition and Anticipation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Dataset Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Captioning and Description</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/17\">Dataset Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/18\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/18\">Grounding and Retrieval</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/19\">Dataset Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/19\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/19\">Question Answering</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/20\">Datasets Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/20\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/21\">Video Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/21\">Pretraining Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/21\">Fine-tuning Dataset</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Media and Entertainment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Interactive and User-Centric Technologies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Healthcare and Security Applications</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/23\">Future Directions and Conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/23\">Limitations and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/24\">Conclusion</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tang et al_2024_Video Understanding with Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i3490.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video Understanding with Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding."
          ],
          [
            "Access Date",
            "2024-01-08 15:56:30"
          ],
          [
            "Archiveid",
            "arXiv:2312.17432"
          ],
          [
            "Creators",
            "Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2312.17432"
          ],
          [
            "Date",
            "2024-01-03 2024-01-03"
          ],
          [
            "Extra",
            "arXiv:2312.17432 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video Understanding with Large Language Models"
          ],
          [
            "Title",
            "Video Understanding with Large Language Models: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2312.17432"
          ]
        ],
        "resource": "storage/i3490.pdf",
        "selectable": false
      },
      {
        "text": "Video-ChatGPT",
        "item-id": "i2567",
        "nodes": [
          {
            "text": "Maaz et al_2023_Video-ChatGPT.pdf",
            "item-id": "i2667",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Maaz et al_2023_Video-ChatGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Maaz et al_2023_Video-ChatGPT.pdf"
              ]
            ],
            "resource": "storage/i2667.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video-ChatGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the underexplored field of video-based conversation by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with a LLM. The model is capable of understanding and generating human-like conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantiative evaluation framework for video-based dialogue models to objectively analyse the strengths and weaknesses of proposed models. Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT."
          ],
          [
            "Access Date",
            "2023-06-10 06:20:11"
          ],
          [
            "Archiveid",
            "arXiv:2306.05424"
          ],
          [
            "Creators",
            "Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.05424"
          ],
          [
            "Date",
            "2023-06-08 2023-06-08"
          ],
          [
            "Extra",
            "arXiv:2306.05424 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video-ChatGPT"
          ],
          [
            "Title",
            "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.05424"
          ]
        ],
        "resource": "storage/i2667.pdf",
        "selectable": false
      },
      {
        "text": "Video-LLaMA",
        "item-id": "i2524",
        "nodes": [
          {
            "text": "Comment: Technical Report; Code, Pretrained Model, and Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA",
            "item-id": "n2731",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical Report; Code, Pretrained Model, and Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical Report; Code, Pretrained Model, and Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang et al_2023_Video-LLaMA.pdf",
            "item-id": "i2730",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_Video-LLaMA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_Video-LLaMA.pdf"
              ]
            ],
            "resource": "storage/i2730.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video-LLaMA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble the pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM's embedding space, we train Video-LLaMA on massive video/image-caption pairs as well as visual-instruction-tuning datasets of moderate amount but higher quality. We found Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos. This highlights the potential of Video-LLaMA as a promising prototype for audio-visual AI assistants."
          ],
          [
            "Access Date",
            "2023-06-08 03:55:52"
          ],
          [
            "Archiveid",
            "arXiv:2306.02858"
          ],
          [
            "Creators",
            "Hang Zhang, Xin Li, Lidong Bing"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.02858"
          ],
          [
            "Date",
            "2023-06-06 2023-06-06"
          ],
          [
            "Extra",
            "arXiv:2306.02858 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video-LLaMA"
          ],
          [
            "Title",
            "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.02858"
          ]
        ],
        "resource": "storage/i2730.pdf",
        "selectable": false
      },
      {
        "text": "VideoChat",
        "item-id": "i2566",
        "nodes": [
          {
            "text": "Comment: Technical report",
            "item-id": "n2651",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical report",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical report</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2023_VideoChat.pdf",
            "item-id": "i2650",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2023_VideoChat.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2023_VideoChat.pdf"
              ]
            ],
            "resource": "storage/i2650.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "VideoChat",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything"
          ],
          [
            "Access Date",
            "2023-06-12 13:08:24"
          ],
          [
            "Archiveid",
            "arXiv:2305.06355"
          ],
          [
            "Creators",
            "KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.06355"
          ],
          [
            "Date",
            "2023-05-10 2023-05-10"
          ],
          [
            "Extra",
            "arXiv:2305.06355 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "VideoChat"
          ],
          [
            "Title",
            "VideoChat: Chat-Centric Video Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.06355"
          ]
        ],
        "resource": "storage/i2650.pdf",
        "selectable": false
      },
      {
        "text": "ViperGPT",
        "item-id": "i3035",
        "nodes": [
          {
            "text": "Comment: Website: https://viper.cs.columbia.edu/",
            "item-id": "n3068",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Website: https://viper.cs.columbia.edu/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Website: https://viper.cs.columbia.edu/</div>",
            "node_type": "note"
          },
          {
            "text": "Sur\u00eds et al_2023_ViperGPT.pdf",
            "item-id": "i3067",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sur\u00eds et al_2023_ViperGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sur\u00eds et al_2023_ViperGPT.pdf"
              ]
            ],
            "resource": "storage/i3067.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ViperGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks."
          ],
          [
            "Access Date",
            "2023-10-05 07:40:57"
          ],
          [
            "Archiveid",
            "arXiv:2303.08128"
          ],
          [
            "Creators",
            "D\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.08128"
          ],
          [
            "Date",
            "2023-03-14 2023-03-14"
          ],
          [
            "Extra",
            "arXiv:2303.08128 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ViperGPT"
          ],
          [
            "Title",
            "ViperGPT: Visual Inference via Python Execution for Reasoning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.08128"
          ]
        ],
        "resource": "storage/i3067.pdf",
        "selectable": false
      },
      {
        "text": "VisionLLM",
        "item-id": "i3029",
        "nodes": [
          {
            "text": "Comment: Technical Report",
            "item-id": "n3091",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical Report",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical Report</div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2023_VisionLLM.pdf",
            "item-id": "i3089",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_VisionLLM.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_VisionLLM.pdf"
              ]
            ],
            "resource": "storage/i3089.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "VisionLLM",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM."
          ],
          [
            "Access Date",
            "2023-10-05 07:14:00"
          ],
          [
            "Archiveid",
            "arXiv:2305.11175"
          ],
          [
            "Creators",
            "Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.11175"
          ],
          [
            "Date",
            "2023-05-25 2023-05-25"
          ],
          [
            "Extra",
            "arXiv:2305.11175 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "VisionLLM"
          ],
          [
            "Title",
            "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.11175"
          ]
        ],
        "resource": "storage/i3089.pdf",
        "selectable": false
      },
      {
        "text": "Visual ChatGPT",
        "item-id": "i2941",
        "nodes": [
          {
            "text": "Wu et al_2023_Visual ChatGPT.pdf",
            "item-id": "i2968",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2023_Visual ChatGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2023_Visual ChatGPT.pdf"
              ]
            ],
            "resource": "storage/i2968.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Visual ChatGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}."
          ],
          [
            "Access Date",
            "2023-07-27 01:21:19"
          ],
          [
            "Archiveid",
            "arXiv:2303.04671"
          ],
          [
            "Creators",
            "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.04671"
          ],
          [
            "Date",
            "2023-03-08 2023-03-08"
          ],
          [
            "Extra",
            "arXiv:2303.04671 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Visual ChatGPT"
          ],
          [
            "Title",
            "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.04671"
          ]
        ],
        "resource": "storage/i2968.pdf",
        "selectable": false
      },
      {
        "text": "Visual Instruction Tuning",
        "item-id": "i3033",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n3090",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>LLaVA</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Comment: project page: https://llava-vl.github.io/",
            "item-id": "n3096",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: project page: https://llava-vl.github.io/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: project page: https://llava-vl.github.io/</div>",
            "node_type": "note"
          },
          {
            "text": "Liu et al_2023_Visual Instruction Tuning.pdf",
            "item-id": "i3095",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2023_Visual Instruction Tuning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8Q3KEQCN/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/2\">2 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/2\">3 GPT-assisted Visual Instruction Data Generation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/4\">4 Visual Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/4\">4.1 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/5\">4.2 Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/6\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/6\">5.1 Multimodal Chatbot</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/7\">5.2 ScienceQA</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/9\">6 Discussions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/14\">A Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Q3KEQCN/15\">B Prompts</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2023_Visual Instruction Tuning.pdf"
              ]
            ],
            "resource": "storage/i3095.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Visual Instruction Tuning",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
          ],
          [
            "Access Date",
            "2023-10-05 07:11:27"
          ],
          [
            "Archiveid",
            "arXiv:2304.08485"
          ],
          [
            "Creators",
            "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee"
          ],
          [
            "DOI",
            "10.48550/arXiv.2304.08485"
          ],
          [
            "Date",
            "2023-04-17 2023-04-17"
          ],
          [
            "Extra",
            "arXiv:2304.08485 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Visual Instruction Tuning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2304.08485"
          ]
        ],
        "resource": "storage/i3095.pdf",
        "selectable": false
      },
      {
        "text": "Visual Programming",
        "item-id": "i3036",
        "nodes": [
          {
            "text": "Gupta_Kembhavi_2023_Visual Programming.pdf",
            "item-id": "i3070",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gupta_Kembhavi_2023_Visual Programming.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gupta_Kembhavi_2023_Visual Programming.pdf"
              ]
            ],
            "resource": "storage/i3070.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Visual Programming",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform."
          ],
          [
            "Access Date",
            "2023-10-05 07:33:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tanmay Gupta, Aniruddha Kembhavi"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14953-14962"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Visual Programming"
          ],
          [
            "Title",
            "Visual Programming: Compositional Visual Reasoning Without Training"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i3070.pdf",
        "selectable": false
      }
    ],
    "item_title": "Large Language Model",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Lip Reading",
    "item-id": "c28,i1855",
    "nodes": [
      {
        "text": "Lip reading in profile",
        "item-id": "i1855",
        "nodes": [
          {
            "text": "Chung_Zisserman_2017_Lip reading in profile.pdf",
            "item-id": "i2053",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chung_Zisserman_2017_Lip reading in profile.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chung_Zisserman_2017_Lip reading in profile.pdf"
              ]
            ],
            "resource": "storage/i2053.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Lip reading in profile",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "There has been a quantum leap in the performance of automated lip reading recently due to the application of neural network sequence models trained on a very large corpus of aligned text and face videos. However, this advance has only been demonstrated for frontal or near frontal faces, and so the question remains: can lips be read in profile to the same standard? The objective of this paper is to answer that question. We make three contributions: first, we obtain a new large aligned training corpus that contains profile faces, and select these using a face pose regressor network; second, we propose a curriculum learning procedure that is able to extend SyncNet [10] (a network to synchronize face movements and speech) progressively from frontal to profile faces; third, we demonstrate lip reading in profile for unseen videos. The trained model is evaluated on a held out test set, and is also shown to far surpass the state of the art on the OuluVS2 multi-view benchmark."
          ],
          [
            "Access Date",
            "2022-10-22 08:45:58"
          ],
          [
            "Creators",
            "J. Chung, A. Zisserman"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Extra",
            "Publisher: British Machine Vision Association and Society for Pattern Recognition"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ora.ox.ac.uk"
          ],
          [
            "Publication Title",
            "British Machine Vision Conference, 2017"
          ],
          [
            "Title",
            "Lip reading in profile"
          ],
          [
            "URL",
            "https://ora.ox.ac.uk/objects/uuid:9f06858c-349c-416f-8ace-87751cd401fc"
          ]
        ],
        "resource": "storage/i2053.pdf",
        "selectable": false
      },
      {
        "text": "Out of Time",
        "item-id": "i33",
        "nodes": [
          {
            "text": "Chung_Zisserman_2017_Out of Time.pdf",
            "item-id": "i167",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chung_Zisserman_2017_Out of Time.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RBYFKY3S/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/2\">1.1 Related Works</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/2\">2 Representations and Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/3\">2.1 Audio Stream</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/3\">2.2 Visual Stream</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/4\">2.3 Loss Function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/4\">2.4 Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/5\">3 Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/6\">3.1 Compiling the Training Data</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/7\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/8\">4.1 Determining the Lip-Sync Error</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/8\">4.2 Application: Active Speaker Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/10\">4.3 Application: Lip Reading</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/11\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RBYFKY3S/12\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chung_Zisserman_2017_Out of Time.pdf"
              ]
            ],
            "resource": "storage/i167.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Out of Time",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The goal of this work is to determine the audio-video synchronisation between mouth motion and speech in a video.We propose a two-stream ConvNet architecture that enables the mapping between the sound and the mouth images to be trained end-to-end from unlabelled data. The trained network is used to determine the lip-sync error in a video.We apply the network to two further tasks: active speaker detection and lip reading. On both tasks we set a new state-of-the-art on standard benchmark datasets."
          ],
          [
            "Creators",
            "Joon Son Chung, Andrew Zisserman, Chu-Song Chen, Jiwen Lu, Kai-Kuang Ma"
          ],
          [
            "DOI",
            "10.1007/978-3-319-54427-4_19"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "ISBN",
            "978-3-319-54427-4"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "251-263"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Computer Vision \u2013 ACCV 2016 Workshops"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "Out of Time"
          ],
          [
            "Title",
            "Out of Time: Automated Lip Sync in the Wild"
          ]
        ],
        "resource": "storage/i167.pdf",
        "selectable": false
      }
    ],
    "item_title": "Lip Reading",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Machine Translation",
    "item-id": "c17,i1270",
    "nodes": [
      {
        "text": "Attention is all you need",
        "item-id": "i24",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n107",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Self-attention and transformer</p>\n<p>The previous recurrent model structures cannot compute parallelly.</p>\n<p>The attention mechanisms can allow the dependencies without regard to their distances in the sequences. The transformer structure can parallel the computation, and reach a better quarlity.</p>\n<p>They proposed the Transformer with Multi-Head Attention blocks. The tansformer is a encoder-decoder structure, and the attention layers are the main part of this structure. The Multi-head attention layer has 3 imput, Query, Key and Value. The Q, K and V will be used to calculate the output of attention layer.</p>\n<p>The standard WMT 2014 English-German dataset and WMT 2014 English-French dataset.</p>\n<p>The results shows the performance is better than other models and the training cost is less than others.</p>\n<p>+ve: Faster train. Better performance.</p>\n<p>-ve: Only used for text data. No locality focus.</p>\n<p>This paper is the origin of self-attention and transformer, which replaces the traditional RNN based structures for NLP tasks, with faster training speed and better perforance. Based on this method, lots of work also focus on the transformer for computer vision.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Vaswani et al_2017_Attention is all you need.pdf",
            "item-id": "i209",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Vaswani et al_2017_Attention is all you need.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Vaswani et al_2017_Attention is all you need.pdf"
              ]
            ],
            "resource": "storage/i209.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attention is all you need",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature."
          ],
          [
            "Conference Name",
            "Proceedings of the 31st International Conference on Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin"
          ],
          [
            "Date",
            "2017-12-04 December 4, 2017"
          ],
          [
            "ISBN",
            "978-1-5108-6096-4"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "6000\u20136010"
          ],
          [
            "Place",
            "Red Hook, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 31st International Conference on Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates Inc."
          ],
          [
            "Series",
            "NIPS'17"
          ],
          [
            "Title",
            "Attention is all you need"
          ]
        ],
        "resource": "storage/i209.pdf",
        "selectable": false
      },
      {
        "text": "Effective Approaches to Attention-based Neural Machine Translation",
        "item-id": "i1265",
        "nodes": [
          {
            "text": "Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details",
            "item-id": "n1266",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details</div>",
            "node_type": "note"
          },
          {
            "text": "Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf",
            "item-id": "i1268",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G8JVIWG8/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/2\">2 Neural Machine Translation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/3\">3 Attention-based Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/3\">3.1 Global Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/4\">3.2 Local Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/5\">3.3 Input-feeding Approach</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/5\">4.1 Training Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/6\">4.2 English-German Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/7\">4.3 German-English Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/7\">5 Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/7\">5.1 Learning curves</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/7\">5.2 Effects of Translating Long Sentences</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/7\">5.3 Choices of Attentional Architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/8\">5.4 Alignment Quality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/8\">5.5 Sample Translations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/9\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G8JVIWG8/10\">A Alignment Visualization</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf"
              ]
            ],
            "resource": "storage/i1268.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Effective Approaches to Attention-based Neural Machine Translation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker."
          ],
          [
            "Access Date",
            "2021-12-15 00:26:51"
          ],
          [
            "Creators",
            "Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
          ],
          [
            "Date",
            "2015-09-20 2015-09-20"
          ],
          [
            "Extra",
            "arXiv: 1508.04025"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1508.04025 [cs]"
          ],
          [
            "Title",
            "Effective Approaches to Attention-based Neural Machine Translation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1508.04025"
          ]
        ],
        "resource": "storage/i1268.pdf",
        "selectable": false
      },
      {
        "text": "Massive Exploration of Neural Machine Translation Architectures",
        "item-id": "i1270",
        "nodes": [
          {
            "text": "Britz et al_2017_Massive Exploration of Neural Machine Translation Architectures.pdf",
            "item-id": "i1273",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Britz et al_2017_Massive Exploration of Neural Machine Translation Architectures.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Britz et al_2017_Massive Exploration of Neural Machine Translation Architectures.pdf"
              ]
            ],
            "resource": "storage/i1273.pdf"
          },
          {
            "text": "Comment: 9 pages, 2 figures, 8 tables, submitted to ACL 2017, open source code at https://github.com/google/seq2seq/",
            "item-id": "n1271",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 9 pages, 2 figures, 8 tables, submitted to ACL 2017, open source code at https://github.com/google/seq2seq/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 9 pages, 2 figures, 8 tables, submitted to ACL 2017, open source code at https://github.com/google/seq2seq/</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Massive Exploration of Neural Machine Translation Architectures",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results."
          ],
          [
            "Access Date",
            "2021-12-15 00:28:32"
          ],
          [
            "Creators",
            "Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le"
          ],
          [
            "Date",
            "2017-03-21 2017-03-21"
          ],
          [
            "Extra",
            "arXiv: 1703.03906"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1703.03906 [cs]"
          ],
          [
            "Title",
            "Massive Exploration of Neural Machine Translation Architectures"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1703.03906"
          ]
        ],
        "resource": "storage/i1273.pdf",
        "selectable": false
      },
      {
        "text": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "item-id": "i1116",
        "nodes": [
          {
            "text": "Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf",
            "item-id": "i1137",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DFEFWITS/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/2\">2 Background: Neural Machine Translation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/2\">2.1 RNN Encoder\u2013Decoder</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/3\">3 Learning to Align and Translate</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/3\">3.1 Decoder: General Description</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/4\">3.2 Encoder: Bidirectional RNN for Annotating Sequences</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/4\">4 Experiment Settings</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/4\">4.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/5\">4.2 Models</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/5\">5 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/5\">5.1 Quantitative Results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/7\">5.2 Qualitative Analysis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/7\">5.2.1 Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/7\">5.2.2 Long Sentences</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/8\">6 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/8\">6.1 Learning to Align</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/9\">6.2 Neural Networks for Machine Translation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/9\">7 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/12\">A Model Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/12\">A.1 Architectural Choices</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/12\">A.1.1 Recurrent Neural Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/12\">A.1.2 Alignment Model</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/13\">A.2 Detailed Description of the Model</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/13\">A.2.1 Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/13\">A.2.2 Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/14\">A.2.3 Model Size</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DFEFWITS/14\">B Training Procedure</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/14\">B.1 Parameter Initialization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/14\">B.2 Training</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DFEFWITS/15\">C Translations of Long Sentences</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf"
              ]
            ],
            "resource": "storage/i1137.pdf"
          },
          {
            "text": "Comment: Accepted at ICLR 2015 as oral presentation",
            "item-id": "n1138",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted at ICLR 2015 as oral presentation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted at ICLR 2015 as oral presentation</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
          ],
          [
            "Access Date",
            "2021-10-24 10:15:29"
          ],
          [
            "Creators",
            "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
          ],
          [
            "Date",
            "2016-05-19 2016-05-19"
          ],
          [
            "Extra",
            "arXiv: 1409.0473"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1409.0473 [cs, stat]"
          ],
          [
            "Title",
            "Neural Machine Translation by Jointly Learning to Align and Translate"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1409.0473"
          ]
        ],
        "resource": "storage/i1137.pdf",
        "selectable": false
      }
    ],
    "item_title": "Machine Translation",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Metrics",
    "item-id": "c44,i3217",
    "nodes": [
      {
        "text": "Fr\\'echet Audio Distance",
        "item-id": "i3213",
        "nodes": [
          {
            "text": "Kilgour et al_2019_Fr-'echet Audio Distance.pdf",
            "item-id": "i3287",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kilgour et al_2019_Fr-'echet Audio Distance.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8XH24N66/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8XH24N66/2\">3 Fr\u00e9chet Audio Distance (FAD)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/3\">3.1 Definition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/3\">3.2 FAD Embedding Model</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8XH24N66/4\">4 Experimental Setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/4\">4.1 Artificial Distortions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/5\">4.2 Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/5\">4.3 Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/5\">4.4 Human Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8XH24N66/5\">5 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/6\">5.1 Comparison to Signal Based Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/9\">5.2 Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/10\">5.3 Human Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/11\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/11\">7 Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/11\">8 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/17\">A Window Step Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/18\">B Evaluation Set Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XH24N66/19\">C Evaluated Distortion Parameter Configurations</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kilgour et al_2019_Fr-'echet Audio Distance.pdf"
              ]
            ],
            "resource": "storage/i3287.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Fr\\'echet Audio Distance",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose the Fr\\'echet Audio Distance (FAD), a novel, reference-free evaluation metric for music enhancement algorithms. We demonstrate how typical evaluation metrics for speech enhancement and blind source separation can fail to accurately measure the perceived effect of a wide variety of distortions. As an alternative, we propose adapting the Fr\\'echet Inception Distance (FID) metric used to evaluate generative image models to the audio domain. FAD is validated using a wide variety of artificial distortions and is compared to the signal based metrics signal to distortion ratio (SDR), cosine distance and magnitude L2 distance. We show that, with a correlation coefficient of 0.52, FAD correlates more closely with human perception than either SDR, cosine distance or magnitude L2 distance, with correlation coefficients of 0.39, -0.15 and -0.01 respectively."
          ],
          [
            "Access Date",
            "2023-11-12 14:42:27"
          ],
          [
            "Archiveid",
            "arXiv:1812.08466"
          ],
          [
            "Creators",
            "Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, Matthew Sharifi"
          ],
          [
            "DOI",
            "10.48550/arXiv.1812.08466"
          ],
          [
            "Date",
            "2019-01-17 2019-01-17"
          ],
          [
            "Extra",
            "arXiv:1812.08466 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Fr\\'echet Audio Distance"
          ],
          [
            "Title",
            "Fr\\'echet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1812.08466"
          ]
        ],
        "resource": "storage/i3287.pdf",
        "selectable": false
      },
      {
        "text": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "item-id": "i1820",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2055",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>FID Metrics</p>\n<p>Fr\u00e9chet Inception Distance</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf",
            "item-id": "i2056",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf"
              ]
            ],
            "resource": "storage/i2056.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fr\u00e9chet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark."
          ],
          [
            "Access Date",
            "2022-10-22 08:43:54"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i2056.pdf",
        "selectable": false
      },
      {
        "text": "Image quality assessment",
        "item-id": "i3217",
        "nodes": [
          {
            "text": "Wang et al_2004_Image quality assessment.pdf",
            "item-id": "i3298",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2004_Image quality assessment.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2004_Image quality assessment.pdf"
              ]
            ],
            "resource": "storage/i3298.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Image quality assessment",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/."
          ],
          [
            "Access Date",
            "2023-11-10 14:08:17"
          ],
          [
            "Creators",
            "Zhou Wang, A.C. Bovik, H.R. Sheikh, E.P. Simoncelli"
          ],
          [
            "DOI",
            "10.1109/TIP.2003.819861"
          ],
          [
            "Date",
            "2004-04-00 2004-04"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "600-612"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Short Title",
            "Image quality assessment"
          ],
          [
            "Title",
            "Image quality assessment: from error visibility to structural similarity"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/1284395"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "resource": "storage/i3298.pdf",
        "selectable": false
      }
    ],
    "item_title": "Metrics",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Multi-Instance Learning",
    "item-id": "c8,i772",
    "nodes": [
      {
        "text": "Multiple instance learning",
        "item-id": "i772",
        "nodes": [
          {
            "text": "ScienceDirect Full Text PDF",
            "item-id": "i773",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "ScienceDirect Full Text PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2021-08-02 04:40:25"
              ],
              [
                "Title",
                "ScienceDirect Full Text PDF"
              ],
              [
                "URL",
                "https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320317X00137/1-s2.0-S0031320317304065/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICHXdlFh%2FlRHFxME%2FfAt%2FOTex0L0AeClnnyLNsPd1KqvAiEA%2FcYmu%2Fel3URXJWtfabJnNVPcEv%2Bl7DYrOCe24NlnS%2FEqgwQI5f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwwNTkwMDM1NDY4NjUiDJP3MXKRuFycge5chirXA3GL8H0gkFgQs4FNuhUOqKJ41LRqBarOzQiflwVtWTrnmfzHcwEuCLRxGYz0l5oxH1kCsfFDmVPOwPbdkK9ov8GlEiM5w1BmeIGuEf7FcpMtgIw4sMhIBXq4WvM0xA%2BJIIRJ%2BA4D56qFznjiHSuzByALQAII078LRQaG9trmAjckjqA3Jx7snQd16jt6CdETGHbUxUIjhO4E9fhT%2BLZZKIySQO7URbx0aUR%2Fp7aOUNoUCB2fal77pcamwP6WUdj6XaxR5yaZzxyisG13mMGD97jcyN92d1VofGHOnu4GVQs3ZcTbifzBb7he%2BFgtPQ96WTEpFvk9ZToix1yz9ukQiBzkfrYY2kx3tj2KTeeU6hCiTbGuVSIgIKICMrWUFNPMnUAkwmUqOrfEqx%2FZLBSGeme7P7L2rEyZB5pFZhaCX0cdhVQe5FxH5sjgOGXHsxXe%2FULa3W1MzBqyP2UAYtvplEMIuJzmTCBQuV9j0dqoOhfQzXqk6tDaW%2F4XR78JtvDU22VM8cEs5GbMLpzUaZgoYawO%2FbM4PcXOJhghJkrxUM745nBW91EIuT%2FvECYMAJCDaeIBjHwfuavXPzN3eWDxjAqapio5jaHY%2BoA5e2JcYTkvr7QVqcLSTDDn252IBjqlAS17Gj%2Bzk4A%2F0lQ8HtbmkkeZHbJWk2CGncTgnYr7eL8j2TvQ89uTMaKMAGDGcEdODrjoZt7V%2BtYhFzGm%2Fxnt13NaTA0YOXtPwldSaSWVJRNzPEGHV1UQ93NrAg41kUs8Yfl3hzMy8A5v8lJLODjNQWM5td8QbTl7ll4L8fofckjpHzZVg%2FbzSoqOF5dInykbV0DmQiNpZ0StwYv2XxnPW0Upcq1zzQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210802T044021Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY2ZZLB35Z%2F20210802%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d96803183c93f8d732960434181282b5471d0579873e379035df94b17d8902b1&hash=064895336992de430eb7ea53f24ad1a0a790bb367c53bd17e24499e5bda75442&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320317304065&tid=spdf-eb40bcb1-c55c-4157-9b90-bc21479586fa&sid=27c1532b5f3cb74da52acee558e2b6ba6518gxrqa&type=client"
              ]
            ],
            "resource": "storage/i773.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multiple instance learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research. Code is available on-line at https://github.com/macarbonneau/MILSurvey."
          ],
          [
            "Access Date",
            "2021-08-02 04:40:21"
          ],
          [
            "Creators",
            "Marc-Andr\u00e9 Carbonneau, Veronika Cheplygina, Eric Granger, Ghyslain Gagnon"
          ],
          [
            "DOI",
            "10.1016/j.patcog.2017.10.009"
          ],
          [
            "Date",
            "2018-05-01 May 1, 2018"
          ],
          [
            "ISSN",
            "0031-3203"
          ],
          [
            "Journal Abbreviation",
            "Pattern Recognition"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "329-353"
          ],
          [
            "Publication Title",
            "Pattern Recognition"
          ],
          [
            "Short Title",
            "Multiple instance learning"
          ],
          [
            "Title",
            "Multiple instance learning: A survey of problem characteristics and applications"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0031320317304065"
          ],
          [
            "Volume",
            "77"
          ]
        ],
        "resource": "storage/i773.pdf",
        "selectable": false
      }
    ],
    "item_title": "Multi-Instance Learning",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Natural Language Processing",
    "item-id": "c26,i3464",
    "nodes": [
      {
        "text": "A Review of Text Style Transfer using Deep Learning",
        "item-id": "i1256",
        "nodes": [
          {
            "text": "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf",
            "item-id": "i1259",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i1259.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Review of Text Style Transfer using Deep Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves, however, they adjust their speaking and writing style to a social context, an audience, an interlocutor or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence. A systematic review of text style transfer methodologies using deep learning is presented in this paper. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field."
          ],
          [
            "Creators",
            "Martina Toshevska, Sonja Gievska"
          ],
          [
            "DOI",
            "10.1109/TAI.2021.3115992"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Artificial Intelligence"
          ],
          [
            "ISSN",
            "2691-4581"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Artificial Intelligence"
          ],
          [
            "Title",
            "A Review of Text Style Transfer using Deep Learning"
          ]
        ],
        "resource": "storage/i1259.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Multimodal Large Language Models",
        "item-id": "i2791",
        "nodes": [
          {
            "text": "Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
            "item-id": "n2837",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</div>",
            "node_type": "note"
          },
          {
            "text": "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf",
            "item-id": "i2836",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BKHSHM5R/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Overview</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Multimodal Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/3\">Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/4\">Modality Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/4\">Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/5\">Modality Bridging</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/5\">Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/6\">. Multimodal In-Context Learning</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">. Multimodal Chain of Thought</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">Modality bridging</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">Learning Paradigms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/8\">Chain Configuration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/8\">Generation Patterns</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">. LLM-Aided Visual Reasoning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">Training Paradigms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">Evaluation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">. Challenges and Future Directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/11\">. Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i2836.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "A Survey on Multimodal Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models."
          ],
          [
            "Access Date",
            "2023-07-18 06:46:19"
          ],
          [
            "Archiveid",
            "arXiv:2306.13549"
          ],
          [
            "Creators",
            "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.13549"
          ],
          [
            "Date",
            "2023-06-23 2023-06-23"
          ],
          [
            "Extra",
            "arXiv:2306.13549 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "A Survey on Multimodal Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.13549"
          ]
        ],
        "resource": "storage/i2836.pdf",
        "selectable": false
      },
      {
        "text": "ActivityNet-QA",
        "item-id": "i2793",
        "nodes": [
          {
            "text": "Yu et al_2019_ActivityNet-QA.pdf",
            "item-id": "i2830",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yu et al_2019_ActivityNet-QA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yu et al_2019_ActivityNet-QA.pdf"
              ]
            ],
            "resource": "storage/i2830.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ActivityNet-QA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos."
          ],
          [
            "Access Date",
            "2023-07-18 07:07:32"
          ],
          [
            "Creators",
            "Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, Dacheng Tao"
          ],
          [
            "DOI",
            "10.1609/aaai.v33i01.33019127"
          ],
          [
            "Date",
            "2019-07-17 2019-07-17"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "9127-9134"
          ],
          [
            "Proceedings Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Short Title",
            "ActivityNet-QA"
          ],
          [
            "Title",
            "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/4946"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i2830.pdf",
        "selectable": false
      },
      {
        "text": "Adversarial Multi-task Learning for Text Classification",
        "item-id": "i2947",
        "nodes": [
          {
            "text": "Liu et al_2017_Adversarial Multi-task Learning for Text Classification.pdf",
            "item-id": "i2952",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2017_Adversarial Multi-task Learning for Text Classification.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_FMB4M3ZS/1\">Adversarial Multi-task Learning for Text Classification</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2017_Adversarial Multi-task Learning for Text Classification.pdf"
              ]
            ],
            "resource": "storage/i2952.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Adversarial Multi-task Learning for Text Classification",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at http://nlp.fudan.edu.cn/data/."
          ],
          [
            "Access Date",
            "2023-07-27 17:31:33"
          ],
          [
            "Conference Name",
            "ACL 2017"
          ],
          [
            "Creators",
            "Pengfei Liu, Xipeng Qiu, Xuanjing Huang"
          ],
          [
            "DOI",
            "10.18653/v1/P17-1001"
          ],
          [
            "Date",
            "2017-07-00 2017-07"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "1\u201310"
          ],
          [
            "Place",
            "Vancouver, Canada"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Title",
            "Adversarial Multi-task Learning for Text Classification"
          ],
          [
            "URL",
            "https://aclanthology.org/P17-1001"
          ]
        ],
        "resource": "storage/i2952.pdf",
        "selectable": false
      },
      {
        "text": "Attention is all you need",
        "item-id": "i24",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n107",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Self-attention and transformer</p>\n<p>The previous recurrent model structures cannot compute parallelly.</p>\n<p>The attention mechanisms can allow the dependencies without regard to their distances in the sequences. The transformer structure can parallel the computation, and reach a better quarlity.</p>\n<p>They proposed the Transformer with Multi-Head Attention blocks. The tansformer is a encoder-decoder structure, and the attention layers are the main part of this structure. The Multi-head attention layer has 3 imput, Query, Key and Value. The Q, K and V will be used to calculate the output of attention layer.</p>\n<p>The standard WMT 2014 English-German dataset and WMT 2014 English-French dataset.</p>\n<p>The results shows the performance is better than other models and the training cost is less than others.</p>\n<p>+ve: Faster train. Better performance.</p>\n<p>-ve: Only used for text data. No locality focus.</p>\n<p>This paper is the origin of self-attention and transformer, which replaces the traditional RNN based structures for NLP tasks, with faster training speed and better perforance. Based on this method, lots of work also focus on the transformer for computer vision.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Vaswani et al_2017_Attention is all you need.pdf",
            "item-id": "i209",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Vaswani et al_2017_Attention is all you need.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Vaswani et al_2017_Attention is all you need.pdf"
              ]
            ],
            "resource": "storage/i209.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attention is all you need",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature."
          ],
          [
            "Conference Name",
            "Proceedings of the 31st International Conference on Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin"
          ],
          [
            "Date",
            "2017-12-04 December 4, 2017"
          ],
          [
            "ISBN",
            "978-1-5108-6096-4"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "6000\u20136010"
          ],
          [
            "Place",
            "Red Hook, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 31st International Conference on Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates Inc."
          ],
          [
            "Series",
            "NIPS'17"
          ],
          [
            "Title",
            "Attention is all you need"
          ]
        ],
        "resource": "storage/i209.pdf",
        "selectable": false
      },
      {
        "text": "BERT",
        "item-id": "i27",
        "nodes": [
          {
            "text": "Devlin et al_2019_BERT.pdf",
            "item-id": "i151",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Devlin et al_2019_BERT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Devlin et al_2019_BERT.pdf"
              ]
            ],
            "resource": "storage/i151.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BERT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
          ],
          [
            "Access Date",
            "2021-04-27 14:50:52"
          ],
          [
            "Conference Name",
            "NAACL-HLT 2019"
          ],
          [
            "Creators",
            "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
          ],
          [
            "DOI",
            "10.18653/v1/N19-1423"
          ],
          [
            "Date",
            "2019-06-00 2019-06"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "4171\u20134186"
          ],
          [
            "Place",
            "Minneapolis, Minnesota"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "BERT"
          ],
          [
            "Title",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
          ],
          [
            "URL",
            "https://www.aclweb.org/anthology/N19-1423"
          ]
        ],
        "resource": "storage/i151.pdf",
        "selectable": false
      },
      {
        "text": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer",
        "item-id": "i1286",
        "nodes": [
          {
            "text": "Comment: COLING 2020",
            "item-id": "n1291",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: COLING 2020",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: COLING 2020</div>",
            "node_type": "note"
          },
          {
            "text": "Huang et al_2020_Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer.pdf",
            "item-id": "i1290",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2020_Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_K7PWPWPC/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/2\">2 Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/4\">3 CAE: Cycle-consistent Adversarial Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/4\">3.1 LSTM autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/4\">3.2 Adversarial style transfer networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/5\">3.3 Cycle-consistent constraint</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/5\">3.4 Training and inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1 Experimental setup</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.2 Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.3 Hyper-parameter settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.4 Evaluation metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/7\">4.2 Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/7\">4.2.1 Yelp restaurant reviews sentiment transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/7\">4.2.2 Yahoo questions topic transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.2.3 Human evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.3 Ablation study</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.4 Analyses</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.4.1 Style-transferred sentences</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/9\">4.4.2 Comparison with the nearest neighbour sequences from training data</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/9\">5 Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2020_Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer.pdf"
              ]
            ],
            "resource": "storage/i1290.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Unsupervised text style transfer is full of challenges due to the lack of parallel data and difficulties in content preservation. In this paper, we propose a novel neural approach to unsupervised text style transfer, which we refer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from non-parallel data. CAE consists of three essential components: (1) LSTM autoencoders that encode a text in one style into its latent representation and decode an encoded representation into its original text or a transferred representation into a style-transferred text, (2) adversarial style transfer networks that use an adversarially trained generator to transform a latent representation in one style into a representation in another style, and (3) a cycle-consistent constraint that enhances the capacity of the adversarial style transfer networks in content preservation. The entire CAE with these three components can be trained end-to-end. Extensive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation."
          ],
          [
            "Access Date",
            "2022-01-16 11:58:46"
          ],
          [
            "Creators",
            "Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, Feiyu Xu"
          ],
          [
            "Date",
            "2020-10-01 2020-10-01"
          ],
          [
            "Extra",
            "arXiv: 2010.00735"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2010.00735 [cs]"
          ],
          [
            "Title",
            "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2010.00735"
          ]
        ],
        "resource": "storage/i1290.pdf",
        "selectable": false
      },
      {
        "text": "Dear Sir or Madam, May I introduce the GYAFC Dataset",
        "item-id": "i1432",
        "nodes": [
          {
            "text": "Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human ",
            "item-id": "n1482",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human ",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf",
            "item-id": "i1481",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf"
              ]
            ],
            "resource": "storage/i1481.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Dear Sir or Madam, May I introduce the GYAFC Dataset",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics."
          ],
          [
            "Access Date",
            "2022-03-29 13:28:17"
          ],
          [
            "Creators",
            "Sudha Rao, Joel Tetreault"
          ],
          [
            "Date",
            "2018-04-16 2018-04-16"
          ],
          [
            "Extra",
            "arXiv: 1803.06535"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1803.06535 [cs]"
          ],
          [
            "Short Title",
            "Dear Sir or Madam, May I introduce the GYAFC Dataset"
          ],
          [
            "Title",
            "Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1803.06535"
          ]
        ],
        "resource": "storage/i1481.pdf",
        "selectable": false
      },
      {
        "text": "Delete, Retrieve, Generate",
        "item-id": "i1277",
        "nodes": [
          {
            "text": "Comment: NAACL 2018",
            "item-id": "n1284",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NAACL 2018",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NAACL 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2018_Delete, Retrieve, Generate.pdf",
            "item-id": "i1283",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2018_Delete, Retrieve, Generate.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2018_Delete, Retrieve, Generate.pdf"
              ]
            ],
            "resource": "storage/i1283.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Delete, Retrieve, Generate",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing \"screen is just the right size\" to \"screen is too small\"). Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., \"too small\"). Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous."
          ],
          [
            "Access Date",
            "2021-12-21 04:47:12"
          ],
          [
            "Creators",
            "Juncen Li, Robin Jia, He He, Percy Liang"
          ],
          [
            "Date",
            "2018-04-17 2018-04-17"
          ],
          [
            "Extra",
            "arXiv: 1804.06437"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1804.06437 [cs]"
          ],
          [
            "Short Title",
            "Delete, Retrieve, Generate"
          ],
          [
            "Title",
            "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1804.06437"
          ]
        ],
        "resource": "storage/i1283.pdf",
        "selectable": false
      },
      {
        "text": "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer",
        "item-id": "i1430",
        "nodes": [
          {
            "text": "Comment: ACL 2018",
            "item-id": "n1476",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ACL 2018",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ACL 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Santos et al_2018_Fighting Offensive Language on Social Media with Unsupervised Text Style.pdf",
            "item-id": "i1475",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Santos et al_2018_Fighting Offensive Language on Social Media with Unsupervised Text Style.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Santos et al_2018_Fighting Offensive Language on Social Media with Unsupervised Text Style.pdf"
              ]
            ],
            "resource": "storage/i1475.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences."
          ],
          [
            "Access Date",
            "2022-03-29 13:30:14"
          ],
          [
            "Creators",
            "Cicero Nogueira dos Santos, Igor Melnyk, Inkit Padhi"
          ],
          [
            "Date",
            "2018-05-19 2018-05-19"
          ],
          [
            "Extra",
            "arXiv: 1805.07685"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1805.07685 [cs]"
          ],
          [
            "Title",
            "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1805.07685"
          ]
        ],
        "resource": "storage/i1475.pdf",
        "selectable": false
      },
      {
        "text": "GPT-3",
        "item-id": "i2101",
        "nodes": [
          {
            "text": "Floridi_Chiriatti_2020_GPT-3.pdf",
            "item-id": "i2137",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Floridi_Chiriatti_2020_GPT-3.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_69AWW6U9/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_69AWW6U9/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_69AWW6U9/4\">2 GPT-3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_69AWW6U9/8\">3 Three Tests: Mathematics, Semantics, and\u00a0Ethics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_69AWW6U9/10\">4 Some Consequences</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_69AWW6U9/13\">4.1 Warning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_69AWW6U9/13\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_69AWW6U9/13\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Floridi_Chiriatti_2020_GPT-3.pdf"
              ]
            ],
            "resource": "storage/i2137.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GPT-3",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts."
          ],
          [
            "Access Date",
            "2022-11-07 09:04:46"
          ],
          [
            "Creators",
            "Luciano Floridi, Massimo Chiriatti"
          ],
          [
            "DOI",
            "10.1007/s11023-020-09548-1"
          ],
          [
            "Date",
            "2020-12-01 2020-12-01"
          ],
          [
            "ISSN",
            "1572-8641"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "Minds & Machines"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "681-694"
          ],
          [
            "Publication Title",
            "Minds and Machines"
          ],
          [
            "Short Title",
            "GPT-3"
          ],
          [
            "Title",
            "GPT-3: Its Nature, Scope, Limits, and Consequences"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11023-020-09548-1"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i2137.pdf",
        "selectable": false
      },
      {
        "text": "How Positive Are You",
        "item-id": "i1287",
        "nodes": [
          {
            "text": "Kim_Sohn_2020_How Positive Are You.pdf",
            "item-id": "i1292",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kim_Sohn_2020_How Positive Are You.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kim_Sohn_2020_How Positive Are You.pdf"
              ]
            ],
            "resource": "storage/i1292.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "How Positive Are You",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The prevalent approach for unsupervised text style transfer is disentanglement between content and style. However, it is difficult to completely separate style information from the content. Other approaches allow the latent text representation to contain style and the target style to affect the generated output more than the latent representation does. In both approaches, however, it is impossible to adjust the strength of the style in the generated output. Moreover, those previous approaches typically perform both the sentence reconstruction and style control tasks in a single model, which complicates the overall architecture. In this paper, we address these issues by separating the model into a sentence reconstruction module and a style module. We use the Transformer-based autoencoder model for sentence reconstruction and the adaptive style embedding is learned directly in the style module. Because of this separation, each module can better focus on its own task. Moreover, we can vary the style strength of the generated sentence by changing the style of the embedding expression. Therefore, our approach not only controls the strength of the style, but also simplifies the model architecture. Experimental results show that our approach achieves better style transfer performance and content preservation than previous approaches."
          ],
          [
            "Access Date",
            "2022-01-16 11:44:50"
          ],
          [
            "Conference Name",
            "COLING 2020"
          ],
          [
            "Creators",
            "Heejin Kim, Kyung-Ah Sohn"
          ],
          [
            "DOI",
            "10.18653/v1/2020.coling-main.191"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "2115\u20132125"
          ],
          [
            "Place",
            "Barcelona, Spain (Online)"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th International Conference on Computational Linguistics"
          ],
          [
            "Publisher",
            "International Committee on Computational Linguistics"
          ],
          [
            "Short Title",
            "How Positive Are You"
          ],
          [
            "Title",
            "How Positive Are You: Text Style Transfer using Adaptive Style Embedding"
          ],
          [
            "URL",
            "https://aclanthology.org/2020.coling-main.191"
          ]
        ],
        "resource": "storage/i1292.pdf",
        "selectable": false
      },
      {
        "text": "LLaMA",
        "item-id": "i2523",
        "nodes": [
          {
            "text": "Touvron et al_2023_LLaMA.pdf",
            "item-id": "i2728",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Touvron et al_2023_LLaMA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Touvron et al_2023_LLaMA.pdf"
              ]
            ],
            "resource": "storage/i2728.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "LLaMA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
          ],
          [
            "Access Date",
            "2023-06-08 04:08:55"
          ],
          [
            "Archiveid",
            "arXiv:2302.13971"
          ],
          [
            "Creators",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.13971"
          ],
          [
            "Date",
            "2023-02-27 2023-02-27"
          ],
          [
            "Extra",
            "arXiv:2302.13971 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "LLaMA"
          ],
          [
            "Title",
            "LLaMA: Open and Efficient Foundation Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.13971"
          ]
        ],
        "resource": "storage/i2728.pdf",
        "selectable": false
      },
      {
        "text": "Learning Transferable Visual Models From Natural Language Supervision",
        "item-id": "i1829",
        "nodes": [
          {
            "text": "Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf",
            "item-id": "i2076",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf"
              ]
            ],
            "resource": "storage/i2076.pdf"
          },
          {
            "text": "Supplementary PDF",
            "item-id": "i2077",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-10-12 16:26:23"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v139/radford21a/radford21a-supp.pdf"
              ]
            ],
            "resource": "storage/i2077.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Transferable Visual Models From Natural Language Supervision",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on."
          ],
          [
            "Access Date",
            "2022-10-12 16:26:21"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "8748-8763"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 38th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Learning Transferable Visual Models From Natural Language Supervision"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v139/radford21a.html"
          ]
        ],
        "selectable": false
      },
      {
        "text": "MiniGPT-4",
        "item-id": "i2792",
        "nodes": [
          {
            "text": "Comment: Project Website: https://minigpt-4.github.io/; Code, Pretrained Model, and Dataset: https://github.com/Vision-C",
            "item-id": "n2847",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project Website: https://minigpt-4.github.io/; Code, Pretrained Model, and Dataset: https://github.com/Vision-C",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project Website: https://minigpt-4.github.io/; Code, Pretrained Model, and Dataset: https://github.com/Vision-CAIR/MiniGPT-4; Deyao Zhu and Jun Chen contributed equally to this work</div>",
            "node_type": "note"
          },
          {
            "text": "Zhu et al_2023_MiniGPT-4.pdf",
            "item-id": "i2846",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2023_MiniGPT-4.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Z5TH7WU3/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/2\">2 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/4\">3.1 First pretraining stage</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/4\">3.2 Curating a high-quality alignment dataset for vision-language domain.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/5\">3.3 Second-stage finetuning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/5\">4 Demonstrations:</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Z5TH7WU3/5\">5 Limitations</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2023_MiniGPT-4.pdf"
              ]
            ],
            "resource": "storage/i2846.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MiniGPT-4",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. We believe the primary reason for GPT-4's advanced multi-modal generation capabilities lies in the utilization of a more advanced large language model (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer. Our findings reveal that MiniGPT-4 possesses many capabilities similar to those exhibited by GPT-4 like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, providing solutions to problems shown in images, teaching users how to cook based on food photos, etc. In our experiment, we found that only performing the pretraining on raw image-text pairs could produce unnatural language outputs that lack coherency including repetition and fragmented sentences. To address this problem, we curate a high-quality, well-aligned dataset in the second stage to finetune our model using a conversational template. This step proved crucial for augmenting the model's generation reliability and overall usability. Notably, our model is highly computationally efficient, as we only train a projection layer utilizing approximately 5 million aligned image-text pairs. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/."
          ],
          [
            "Access Date",
            "2023-07-18 01:15:35"
          ],
          [
            "Archiveid",
            "arXiv:2304.10592"
          ],
          [
            "Creators",
            "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny"
          ],
          [
            "DOI",
            "10.48550/arXiv.2304.10592"
          ],
          [
            "Date",
            "2023-04-20 2023-04-20"
          ],
          [
            "Extra",
            "arXiv:2304.10592 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MiniGPT-4"
          ],
          [
            "Title",
            "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2304.10592"
          ]
        ],
        "resource": "storage/i2846.pdf",
        "selectable": false
      },
      {
        "text": "Multiple-Attribute Text Rewriting",
        "item-id": "i1275",
        "nodes": [
          {
            "text": "Lample et al_2018_Multiple-Attribute Text Rewriting.pdf",
            "item-id": "i1279",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lample et al_2018_Multiple-Attribute Text Rewriting.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CG3FYHA9/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/3\">Controllable Text Rewriting</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/3\">Are Adversarial Models really Doing Disentanglement?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/4\">Our Approach</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/5\">Implementation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/6\">Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/7\">Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/7\">Model selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/8\">Comparisons to Prior Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/9\">Evaluating Multiple Attribute Control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/9\">Ablation study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/10\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/14\">Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/14\">Training Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/14\">Dataset Creation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/16\">Additional Qualitative Examples</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lample et al_2018_Multiple-Attribute Text Rewriting.pdf"
              ]
            ],
            "resource": "storage/i1279.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multiple-Attribute Text Rewriting",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A system for rewriting text conditioned on multiple controllable attributes"
          ],
          [
            "Access Date",
            "2021-12-21 04:54:30"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Guillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc'Aurelio Ranzato, Y.-Lan Boureau"
          ],
          [
            "Date",
            "2018-09-27 2018/09/27"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Title",
            "Multiple-Attribute Text Rewriting"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=H1g2NhC5KQ"
          ]
        ],
        "resource": "storage/i1279.pdf",
        "selectable": false
      },
      {
        "text": "Politeness Transfer",
        "item-id": "i1431",
        "nodes": [
          {
            "text": "Comment: To appear at ACL 2020",
            "item-id": "n1479",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: To appear at ACL 2020",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: To appear at ACL 2020</div>",
            "node_type": "note"
          },
          {
            "text": "Madaan et al_2020_Politeness Transfer.pdf",
            "item-id": "i1478",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Madaan et al_2020_Politeness Transfer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Madaan et al_2020_Politeness Transfer.pdf"
              ]
            ],
            "resource": "storage/i1478.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Politeness Transfer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate."
          ],
          [
            "Access Date",
            "2022-03-29 13:29:34"
          ],
          [
            "Creators",
            "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black, Shrimai Prabhumoye"
          ],
          [
            "Date",
            "2020-05-01 2020-05-01"
          ],
          [
            "Extra",
            "arXiv: 2004.14257"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2004.14257 [cs]"
          ],
          [
            "Short Title",
            "Politeness Transfer"
          ],
          [
            "Title",
            "Politeness Transfer: A Tag and Generate Approach"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2004.14257"
          ]
        ],
        "resource": "storage/i1478.pdf",
        "selectable": false
      },
      {
        "text": "RoBERTa",
        "item-id": "i2977",
        "nodes": [
          {
            "text": "Liu et al_2019_RoBERTa.pdf",
            "item-id": "i2984",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2019_RoBERTa.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2019_RoBERTa.pdf"
              ]
            ],
            "resource": "storage/i2984.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "RoBERTa",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
          ],
          [
            "Access Date",
            "2023-08-01 10:03:12"
          ],
          [
            "Archiveid",
            "arXiv:1907.11692"
          ],
          [
            "Creators",
            "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
          ],
          [
            "DOI",
            "10.48550/arXiv.1907.11692"
          ],
          [
            "Date",
            "2019-07-26 2019-07-26"
          ],
          [
            "Extra",
            "arXiv:1907.11692 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "RoBERTa"
          ],
          [
            "Title",
            "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1907.11692"
          ]
        ],
        "resource": "storage/i2984.pdf",
        "selectable": false
      },
      {
        "text": "Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis",
        "item-id": "i1252",
        "nodes": [
          {
            "text": "Lee et al_2018_Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance.pdf",
            "item-id": "i1254",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lee et al_2018_Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lee et al_2018_Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance.pdf"
              ]
            ],
            "resource": "storage/i1254.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Conventional seq2seq chatbot models only try to find the sentences with the highest probabilities conditioned on the input sequences, without considering the sentiment of the output sentences. Some research works trying to modify the sentiment of the output sequences were reported. In this paper, we propose five models to scale or adjust the sentiment of the chatbot response: persona-based model, reinforcement learning, plug and play model, sentiment transformation network and cycleGAN, all based on the conventional seq2seq model. We also develop two evaluation metrics to estimate if the responses are reasonable given the input. These metrics together with other two popularly used metrics were used to analyze the performance of the five proposed models on different aspects, and reinforcement learning and cycleGAN were shown to be very attractive. The evaluation metrics were also found to be well correlated with human evaluation."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Chih-Wei Lee, Yau-Shian Wang, Tsung-Yuan Hsu, Kuan-Yu Chen, Hung-Yi Lee, Lin-Shan Lee"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8461377"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6164-6168"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis"
          ]
        ],
        "resource": "storage/i1254.pdf",
        "selectable": false
      },
      {
        "text": "Sentence-BERT",
        "item-id": "i3464",
        "nodes": [
          {
            "text": "Reimers_Gurevych_2019_Sentence-BERT.pdf",
            "item-id": "i3531",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Reimers_Gurevych_2019_Sentence-BERT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Reimers_Gurevych_2019_Sentence-BERT.pdf"
              ]
            ],
            "resource": "storage/i3531.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Sentence-BERT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\\textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
          ],
          [
            "Access Date",
            "2024-01-08 06:33:52"
          ],
          [
            "Conference Name",
            "EMNLP-IJCNLP 2019"
          ],
          [
            "Creators",
            "Nils Reimers, Iryna Gurevych, Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan"
          ],
          [
            "DOI",
            "10.18653/v1/D19-1410"
          ],
          [
            "Date",
            "2019-11-00 2019-11"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "3982\u20133992"
          ],
          [
            "Place",
            "Hong Kong, China"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "Sentence-BERT"
          ],
          [
            "Title",
            "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
          ],
          [
            "URL",
            "https://aclanthology.org/D19-1410"
          ]
        ],
        "resource": "storage/i3531.pdf",
        "selectable": false
      },
      {
        "text": "Style Transformer",
        "item-id": "i1276",
        "nodes": [
          {
            "text": "Dai et al_2019_Style Transformer.pdf",
            "item-id": "i1281",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dai et al_2019_Style Transformer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dai et al_2019_Style Transformer.pdf"
              ]
            ],
            "resource": "storage/i1281.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Style Transformer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation."
          ],
          [
            "Access Date",
            "2021-12-21 04:52:37"
          ],
          [
            "Creators",
            "Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang"
          ],
          [
            "Date",
            "2019-08-20 2019-08-20"
          ],
          [
            "Extra",
            "arXiv: 1905.05621"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1905.05621 [cs]"
          ],
          [
            "Short Title",
            "Style Transformer"
          ],
          [
            "Title",
            "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1905.05621"
          ]
        ],
        "resource": "storage/i1281.pdf",
        "selectable": false
      },
      {
        "text": "TGIF-QA",
        "item-id": "i2795",
        "nodes": [
          {
            "text": "Jang et al_2017_TGIF-QA.pdf",
            "item-id": "i2834",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jang et al_2017_TGIF-QA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jang et al_2017_TGIF-QA.pdf"
              ]
            ],
            "resource": "storage/i2834.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TGIF-QA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations."
          ],
          [
            "Access Date",
            "2023-07-18 06:59:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2758-2766"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "TGIF-QA"
          ],
          [
            "Title",
            "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i2834.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Text Style Transfer using Language Models as Discriminators",
        "item-id": "i1418",
        "nodes": [
          {
            "text": "Yang et al_2018_Unsupervised Text Style Transfer using Language Models as Discriminators.pdf",
            "item-id": "i1451",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2018_Unsupervised Text Style Transfer using Language Models as Discriminators.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2018_Unsupervised Text Style Transfer using Language Models as Discriminators.pdf"
              ]
            ],
            "resource": "storage/i1451.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Text Style Transfer using Language Models as Discriminators",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2022-04-04 08:33:01"
          ],
          [
            "Creators",
            "Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, Taylor Berg-Kirkpatrick"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Unsupervised Text Style Transfer using Language Models as Discriminators"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2018/hash/398475c83b47075e8897a083e97eb9f0-Abstract.html"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i1451.pdf",
        "selectable": false
      },
      {
        "text": "Video Question Answering via Gradually Refined Attention over Appearance and Motion",
        "item-id": "i2794",
        "nodes": [
          {
            "text": "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf",
            "item-id": "i2833",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2QNGQXBI/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2.1 Video Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2.2 Image Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/3\">2.3 Video Question Answering</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/3\">3 Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/4\">3.1 Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/4\">3.2 Attention Memory Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/5\">3.3 Answer Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.1 Data Preparation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.2 Model details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.3 Baseline methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/7\">4.4 Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/7\">4.5 Results and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/8\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf"
              ]
            ],
            "resource": "storage/i2833.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently image question answering (ImageQA) has gained lots of attention in the research community. However, as its natural extension, video question answering (VideoQA) is less explored. Although both tasks look similar, VideoQA is more challenging mainly because of the complexity and diversity of videos. As such, simply extending the ImageQA methods to videos is insufficient and suboptimal. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this paper, we consider exploiting the appearance and motion information resided in the video with a novel attention mechanism. More specifically, we propose an end-to-end model which gradually refines its attention over the appearance and motion features of the video using the question as guidance. The question is processed word by word until the model generates the final optimized attention. The weighted representation of the video, as well as other contextual information, are used to generate the answer. Extensive experiments show the advantages of our model compared to other baseline models. We also demonstrate the effectiveness of our model by analyzing the refined attention weights during the question answering procedure."
          ],
          [
            "Access Date",
            "2023-07-17"
          ],
          [
            "Creators",
            "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, Yueting Zhuang"
          ],
          [
            "DOI",
            "10.1145/3123266.3123427"
          ],
          [
            "Date",
            "2017-00-23 \u5341\u6708 23, 2017"
          ],
          [
            "ISBN",
            "978-1-4503-4906-2"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "1645\u20131653"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 25th ACM international conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '17"
          ],
          [
            "Title",
            "Video Question Answering via Gradually Refined Attention over Appearance and Motion"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3123266.3123427"
          ]
        ],
        "resource": "storage/i2833.pdf",
        "selectable": false
      },
      {
        "text": "Video-ChatGPT",
        "item-id": "i2567",
        "nodes": [
          {
            "text": "Maaz et al_2023_Video-ChatGPT.pdf",
            "item-id": "i2667",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Maaz et al_2023_Video-ChatGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Maaz et al_2023_Video-ChatGPT.pdf"
              ]
            ],
            "resource": "storage/i2667.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video-ChatGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the underexplored field of video-based conversation by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with a LLM. The model is capable of understanding and generating human-like conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantiative evaluation framework for video-based dialogue models to objectively analyse the strengths and weaknesses of proposed models. Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT."
          ],
          [
            "Access Date",
            "2023-06-10 06:20:11"
          ],
          [
            "Archiveid",
            "arXiv:2306.05424"
          ],
          [
            "Creators",
            "Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.05424"
          ],
          [
            "Date",
            "2023-06-08 2023-06-08"
          ],
          [
            "Extra",
            "arXiv:2306.05424 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video-ChatGPT"
          ],
          [
            "Title",
            "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.05424"
          ]
        ],
        "resource": "storage/i2667.pdf",
        "selectable": false
      },
      {
        "text": "Video-LLaMA",
        "item-id": "i2524",
        "nodes": [
          {
            "text": "Comment: Technical Report; Code, Pretrained Model, and Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA",
            "item-id": "n2731",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical Report; Code, Pretrained Model, and Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical Report; Code, Pretrained Model, and Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang et al_2023_Video-LLaMA.pdf",
            "item-id": "i2730",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_Video-LLaMA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_Video-LLaMA.pdf"
              ]
            ],
            "resource": "storage/i2730.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video-LLaMA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble the pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM's embedding space, we train Video-LLaMA on massive video/image-caption pairs as well as visual-instruction-tuning datasets of moderate amount but higher quality. We found Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos. This highlights the potential of Video-LLaMA as a promising prototype for audio-visual AI assistants."
          ],
          [
            "Access Date",
            "2023-06-08 03:55:52"
          ],
          [
            "Archiveid",
            "arXiv:2306.02858"
          ],
          [
            "Creators",
            "Hang Zhang, Xin Li, Lidong Bing"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.02858"
          ],
          [
            "Date",
            "2023-06-06 2023-06-06"
          ],
          [
            "Extra",
            "arXiv:2306.02858 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video-LLaMA"
          ],
          [
            "Title",
            "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.02858"
          ]
        ],
        "resource": "storage/i2730.pdf",
        "selectable": false
      },
      {
        "text": "VideoCLIP",
        "item-id": "i2086",
        "nodes": [
          {
            "text": "Xu et al_2021_VideoCLIP.pdf",
            "item-id": "i2109",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2021_VideoCLIP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2021_VideoCLIP.pdf"
              ]
            ],
            "resource": "storage/i2109.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VideoCLIP",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT."
          ],
          [
            "Access Date",
            "2022-11-14 09:37:49"
          ],
          [
            "Conference Name",
            "EMNLP 2021"
          ],
          [
            "Creators",
            "Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer"
          ],
          [
            "DOI",
            "10.18653/v1/2021.emnlp-main.544"
          ],
          [
            "Date",
            "2021-11-00 2021-11"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "6787\u20136800"
          ],
          [
            "Place",
            "Online and Punta Cana, Dominican Republic"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "VideoCLIP"
          ],
          [
            "Title",
            "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"
          ],
          [
            "URL",
            "https://aclanthology.org/2021.emnlp-main.544"
          ]
        ],
        "resource": "storage/i2109.pdf",
        "selectable": false
      },
      {
        "text": "VideoChat",
        "item-id": "i2566",
        "nodes": [
          {
            "text": "Comment: Technical report",
            "item-id": "n2651",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical report",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical report</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2023_VideoChat.pdf",
            "item-id": "i2650",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2023_VideoChat.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2023_VideoChat.pdf"
              ]
            ],
            "resource": "storage/i2650.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "VideoChat",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything"
          ],
          [
            "Access Date",
            "2023-06-12 13:08:24"
          ],
          [
            "Archiveid",
            "arXiv:2305.06355"
          ],
          [
            "Creators",
            "KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.06355"
          ],
          [
            "Date",
            "2023-05-10 2023-05-10"
          ],
          [
            "Extra",
            "arXiv:2305.06355 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "VideoChat"
          ],
          [
            "Title",
            "VideoChat: Chat-Centric Video Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.06355"
          ]
        ],
        "resource": "storage/i2650.pdf",
        "selectable": false
      },
      {
        "text": "XLNet",
        "item-id": "i2975",
        "nodes": [
          {
            "text": "Yang et al_2019_XLNet.pdf",
            "item-id": "i2985",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2019_XLNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2019_XLNet.pdf"
              ]
            ],
            "resource": "storage/i2985.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "XLNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling.\nHowever, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy.\nIn light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation.\nFurthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.\nEmpirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."
          ],
          [
            "Creators",
            "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch\u00e9-Buc, E. Fox, R. Garnett"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "XLNet"
          ],
          [
            "Title",
            "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i2985.pdf",
        "selectable": false
      },
      {
        "text": "Yelp Dataset Challenge",
        "item-id": "i1429",
        "nodes": [
          {
            "text": "Asghar_2016_Yelp Dataset Challenge.pdf",
            "item-id": "i1473",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Asghar_2016_Yelp Dataset Challenge.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Asghar_2016_Yelp Dataset Challenge.pdf"
              ]
            ],
            "resource": "storage/i1473.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Yelp Dataset Challenge",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models."
          ],
          [
            "Access Date",
            "2022-03-29 13:31:08"
          ],
          [
            "Creators",
            "Nabiha Asghar"
          ],
          [
            "Date",
            "2016-05-17 2016-05-17"
          ],
          [
            "Extra",
            "arXiv: 1605.05362"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1605.05362 [cs]"
          ],
          [
            "Short Title",
            "Yelp Dataset Challenge"
          ],
          [
            "Title",
            "Yelp Dataset Challenge: Review Rating Prediction"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1605.05362"
          ]
        ],
        "resource": "storage/i1473.pdf",
        "selectable": false
      }
    ],
    "item_title": "Natural Language Processing",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Neural Radiance Fields",
    "item-id": "c18,i3037",
    "nodes": [
      {
        "text": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
        "item-id": "i3037",
        "nodes": [
          {
            "text": "Kerbl et al_2023_3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf",
            "item-id": "i3105",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kerbl et al_2023_3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_D8TWYPX3/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/2\">2.1 Traditional Scene Reconstruction and Rendering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/2\">2.2 Neural Rendering and Radiance Fields</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/3\">2.3 Point-Based Rendering and Radiance Fields</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/4\">3 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/4\">4 Differentiable 3D Gaussian Splatting</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/4\">5 Optimization with Adaptive Density Control of 3D Gaussians</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/5\">5.1 Optimization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/5\">5.2 Adaptive Control of Gaussians</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/6\">6 Fast Differentiable Rasterizer for Gaussians</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/6\">7 Implementation, results and evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/8\">7.1 Implementation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/8\">7.2 Results and Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/9\">7.3 Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/10\">7.4 Limitations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/11\">8 Discussion and Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/11\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/12\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/13\">A Details of Gradient Computation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/13\">B Optimization and Densification Algorithm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/13\">C Details of the Rasterizer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_D8TWYPX3/14\">D Per-Scene Error Metrics</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kerbl et al_2023_3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf"
              ]
            ],
            "resource": "storage/i3105.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets."
          ],
          [
            "Access Date",
            "2023-09-20 10:56:17"
          ],
          [
            "Creators",
            "Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, George Drettakis"
          ],
          [
            "DOI",
            "10.1145/3592433"
          ],
          [
            "Date",
            "2023-00-26 \u4e03\u6708 26, 2023"
          ],
          [
            "ISSN",
            "0730-0301"
          ],
          [
            "Issue",
            "4"
          ],
          [
            "Journal Abbreviation",
            "ACM Trans. Graph."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "139:1\u2013139:14"
          ],
          [
            "Publication Title",
            "ACM Transactions on Graphics"
          ],
          [
            "Title",
            "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3592433"
          ],
          [
            "Volume",
            "42"
          ]
        ],
        "resource": "storage/i3105.pdf",
        "selectable": false
      },
      {
        "text": "AD-NeRF",
        "item-id": "i1320",
        "nodes": [
          {
            "text": "Guo et al_2021_AD-NeRF.pdf",
            "item-id": "i1389",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Guo et al_2021_AD-NeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Guo et al_2021_AD-NeRF.pdf"
              ]
            ],
            "resource": "storage/i1389.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AD-NeRF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF."
          ],
          [
            "Access Date",
            "2022-02-14 06:56:13"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, Juyong Zhang"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5784-5794"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "AD-NeRF"
          ],
          [
            "Title",
            "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1389.pdf",
        "selectable": false
      },
      {
        "text": "AdaNeRF",
        "item-id": "i1765",
        "nodes": [
          {
            "text": "Comment: ECCV 2022. Project page: https://thomasneff.github.io/adanerf",
            "item-id": "n1772",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ECCV 2022. Project page: https://thomasneff.github.io/adanerf",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ECCV 2022. Project page: https://thomasneff.github.io/adanerf</div>",
            "node_type": "note"
          },
          {
            "text": "Kurz et al_2022_AdaNeRF.pdf",
            "item-id": "i1771",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kurz et al_2022_AdaNeRF.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_E5DX4YQT/1\">AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kurz et al_2022_AdaNeRF.pdf"
              ]
            ],
            "resource": "storage/i1771.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "AdaNeRF",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Novel view synthesis has recently been revolutionized by learning neural radiance fields directly from sparse observations. However, rendering images with this new paradigm is slow due to the fact that an accurate quadrature of the volume rendering equation requires a large number of samples for each ray. Previous work has mainly focused on speeding up the network evaluations that are associated with each sample point, e.g., via caching of radiance values into explicit spatial data structures, but this comes at the expense of model compactness. In this paper, we propose a novel dual-network architecture that takes an orthogonal direction by learning how to best reduce the number of required sample points. To this end, we split our network into a sampling and shading network that are jointly trained. Our training scheme employs fixed sample positions along each ray, and incrementally introduces sparsity throughout training to achieve high quality even at low sample counts. After fine-tuning with the target number of samples, the resulting compact neural representation can be rendered in real-time. Our experiments demonstrate that our approach outperforms concurrent compact neural representations in terms of quality and frame rate and performs on par with highly efficient hybrid representations. Code and supplementary material is available at https://thomasneff.github.io/adanerf."
          ],
          [
            "Access Date",
            "2022-09-02 07:02:03"
          ],
          [
            "Archiveid",
            "arXiv:2207.10312"
          ],
          [
            "Creators",
            "Andreas Kurz, Thomas Neff, Zhaoyang Lv, Michael Zollh\u00f6fer, Markus Steinberger"
          ],
          [
            "DOI",
            "10.48550/arXiv.2207.10312"
          ],
          [
            "Date",
            "2022-07-28 2022-07-28"
          ],
          [
            "Extra",
            "arXiv:2207.10312 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "AdaNeRF"
          ],
          [
            "Title",
            "AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2207.10312"
          ]
        ],
        "resource": "storage/i1771.pdf",
        "selectable": false
      },
      {
        "text": "D-NeRF",
        "item-id": "i1691",
        "nodes": [
          {
            "text": "Pumarola et al_2021_D-NeRF.pdf",
            "item-id": "i1732",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Pumarola et al_2021_D-NeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Pumarola et al_2021_D-NeRF.pdf"
              ]
            ],
            "resource": "storage/i1732.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "D-NeRF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions."
          ],
          [
            "Access Date",
            "2022-07-25 01:56:10"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10318-10327"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "D-NeRF"
          ],
          [
            "Title",
            "D-NeRF: Neural Radiance Fields for Dynamic Scenes"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Pumarola_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1732.pdf",
        "selectable": false
      },
      {
        "text": "DFA-NeRF",
        "item-id": "i1319",
        "nodes": [
          {
            "text": "Yao et al_2022_DFA-NeRF.pdf",
            "item-id": "i1385",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yao et al_2022_DFA-NeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yao et al_2022_DFA-NeRF.pdf"
              ]
            ],
            "resource": "storage/i1385.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "DFA-NeRF",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While recent advances in deep neural networks have made it possible to render high-quality images, generating photo-realistic and personalized talking head remains challenging. With given audio, the key to tackling this task is synchronizing lip movement and simultaneously generating personalized attributes like head movement and eye blink. In this work, we observe that the input audio is highly correlated to lip motion while less correlated to other personalized attributes (e.g., head movements). Inspired by this, we propose a novel framework based on neural radiance field to pursue high-fidelity and personalized talking head generation. Specifically, neural radiance field takes lip movements features and personalized attributes as two disentangled conditions, where lip movements are directly predicted from the audio inputs to achieve lip-synchronized generation. In the meanwhile, personalized attributes are sampled from a probabilistic model, where we design a Transformer-based variational autoencoder sampled from Gaussian Process to learn plausible and natural-looking head pose and eye blink. Experiments on several benchmarks demonstrate that our method achieves significantly better results than state-of-the-art methods."
          ],
          [
            "Access Date",
            "2022-02-14 07:04:37"
          ],
          [
            "Archiveid",
            "arXiv:2201.00791"
          ],
          [
            "Creators",
            "Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai, Xiaokang Yang"
          ],
          [
            "Date",
            "2022-01-03 2022-01-03"
          ],
          [
            "Extra",
            "arXiv:2201.00791 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "DFA-NeRF"
          ],
          [
            "Title",
            "DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2201.00791"
          ]
        ],
        "resource": "storage/i1385.pdf",
        "selectable": false
      },
      {
        "text": "Everybody\u2019s Talkin\u2019",
        "item-id": "i1306",
        "nodes": [
          {
            "text": "Song et al_2022_Everybody\u2019s Talkin\u2019.pdf",
            "item-id": "i1365",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Song et al_2022_Everybody\u2019s Talkin\u2019.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Song et al_2022_Everybody\u2019s Talkin\u2019.pdf"
              ]
            ],
            "resource": "storage/i1365.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Everybody\u2019s Talkin\u2019",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photo-realistic video. This method is unique because it is highly dynamic. It does not assume a person-specific rendering network yet capable of translating one source audio into one random chosen video output within a set of speech videos. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we first factorize each target video frame into orthogonal parameter spaces, i.e., expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely mapped to the source audio. The geometry and pose parameters of the target human portrait are retained, therefore preserving the context of the original video footage. Finally, we introduce a novel video rendering network and a dynamic programming method to construct a temporally coherent and photo-realistic video. Extensive experiments demonstrate the superiority of our method over existing approaches. Our method is end-to-end learnable and robust to voice variations in the source audio."
          ],
          [
            "Creators",
            "Linsen Song, Wayne Wu, Chen Qian, Ran He, Chen Change Loy"
          ],
          [
            "DOI",
            "10.1109/TIFS.2022.3146783"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Information Forensics and Security"
          ],
          [
            "ISSN",
            "1556-6021"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "585-598"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Information Forensics and Security"
          ],
          [
            "Short Title",
            "Everybody\u2019s Talkin\u2019"
          ],
          [
            "Title",
            "Everybody\u2019s Talkin\u2019: Let Me Talk as You Want"
          ],
          [
            "Volume",
            "17"
          ]
        ],
        "resource": "storage/i1365.pdf",
        "selectable": false
      },
      {
        "text": "GIRAFFE",
        "item-id": "i1309",
        "nodes": [
          {
            "text": "Niemeyer_Geiger_2021_GIRAFFE.pdf",
            "item-id": "i1372",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Niemeyer_Geiger_2021_GIRAFFE.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Niemeyer_Geiger_2021_GIRAFFE.pdf"
              ]
            ],
            "resource": "storage/i1372.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GIRAFFE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose."
          ],
          [
            "Access Date",
            "2022-02-17 23:38:22"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Michael Niemeyer, Andreas Geiger"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "11453-11464"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "GIRAFFE"
          ],
          [
            "Title",
            "GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1372.pdf",
        "selectable": false
      },
      {
        "text": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
        "item-id": "i1310",
        "nodes": [
          {
            "text": "Schwarz et al_2020_GRAF.pdf",
            "item-id": "i1373",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Schwarz et al_2020_GRAF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Schwarz et al_2020_GRAF.pdf"
              ]
            ],
            "resource": "storage/i1373.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity."
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, H. Lin"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Pages",
            "20154\u201320166"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/file/e92e1b476bb5262d793fd40931e0ed53-Paper.pdf"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1373.pdf",
        "selectable": false
      },
      {
        "text": "HeadNeRF",
        "item-id": "i1690",
        "nodes": [
          {
            "text": "Hong et al_2022_HeadNeRF.pdf",
            "item-id": "i1730",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hong et al_2022_HeadNeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hong et al_2022_HeadNeRF.pdf"
              ]
            ],
            "resource": "storage/i1730.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HeadNeRF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model that integrates the neural radiance field to the parametric representation of the human head. It can render high fidelity head images in real-time on modern GPUs, and supports directly controlling the generated images' rendering pose and various semantic attributes. Different from existing related parametric models, we use the neural radiance fields as a novel 3D proxy instead of the traditional 3D textured mesh, which makes that HeadNeRF is able to generate high fidelity images. However, the computationally expensive rendering process of the original NeRF hinders the construction of the parametric NeRF model. To address this issue, we adopt the strategy of integrating 2D neural rendering to the rendering process of NeRF and design novel loss terms. As a result, the rendering speed of HeadNeRF can be significantly accelerated, and the rendering time of one frame is reduced from 5s to 25ms. The well designed loss terms also improve the rendering accuracy, and the fine-level details of the human head, such as the gaps between teeth, wrinkles, and beards, can be represented and synthesized by HeadNeRF. Extensive experimental results and several applications demonstrate its effectiveness. The trained parametric model is available at https://github.com/CrisHY1995/headnerf."
          ],
          [
            "Access Date",
            "2022-07-25 02:06:49"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, Juyong Zhang"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "20374-20384"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "HeadNeRF"
          ],
          [
            "Title",
            "HeadNeRF: A Real-Time NeRF-Based Parametric Head Model"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1730.pdf",
        "selectable": false
      },
      {
        "text": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",
        "item-id": "i1294",
        "nodes": [
          {
            "text": "Comment: 13 pages, 13 figures",
            "item-id": "n1339",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 13 pages, 13 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 13 pages, 13 figures</div>",
            "node_type": "note"
          },
          {
            "text": "M\u00fcller et al_2022_Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.pdf",
            "item-id": "i1338",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "M\u00fcller et al_2022_Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_T5KIHT7G/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/2\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/2\">2 Background and Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/4\">3 Multiresolution Hash Encoding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/6\">4 Implementation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/7\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/7\">5.1 Gigapixel Image Approximation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/7\">5.2 Signed Distance Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/7\">5.3 Neural Radiance Caching</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/9\">5.4 Neural Radiance and Density Fields (NeRF)</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/10\">6 Discussion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/11\">7 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/11\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/11\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/12\">A Smooth Interpolation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/12\">B Real-time SDF Training Data Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/12\">B.1 Efficient Sampling of 3D Training Positions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/13\">B.2 Efficient Signed Distances to the Triangle Mesh</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/13\">C Accelerated NeRF Ray Marching</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/13\">C.1 Ray Marching Step Size and Stopping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T5KIHT7G/13\">C.2 Occupancy Grids</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "M\u00fcller et al_2022_Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.pdf"
              ]
            ],
            "resource": "storage/i1338.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$."
          ],
          [
            "Access Date",
            "2022-02-27 18:47:11"
          ],
          [
            "Creators",
            "Thomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller"
          ],
          [
            "Date",
            "2022-01-16 2022-01-16"
          ],
          [
            "Extra",
            "arXiv: 2201.05989"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2201.05989 [cs]"
          ],
          [
            "Title",
            "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2201.05989"
          ]
        ],
        "resource": "storage/i1338.pdf",
        "selectable": false
      },
      {
        "text": "Learning Dynamic Facial Radiance Fields for\u00a0Few-Shot Talking Head Synthesis",
        "item-id": "i2185",
        "nodes": [
          {
            "text": "Full Text PDF",
            "item-id": "i2195",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Full Text PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-12-23 14:30:43"
              ],
              [
                "Title",
                "Full Text PDF"
              ],
              [
                "URL",
                "https://link.springer.com/content/pdf/10.1007%2F978-3-031-19775-8_39.pdf"
              ]
            ],
            "resource": "storage/i2195.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Dynamic Facial Radiance Fields for\u00a0Few-Shot Talking Head Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Talking head synthesis is an emerging technology with wide applications in film dubbing, virtual avatars and online education. Recent NeRF-based methods generate more natural talking videos, as they better capture the 3D structural information of faces. However, a specific model needs to be trained for each identity with a large dataset. In this paper, we propose Dynamic Facial Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly generalize to an unseen identity with few training data. Different from the existing NeRF-based methods which directly encode the 3D geometry and appearance of a specific person into the network, our DFRF conditions face radiance field on 2D appearance images to learn the face prior. Thus the facial radiance field can be flexibly adjusted to the new identity with few reference images. Additionally, for better modeling of the facial deformations, we propose a differentiable face warping module conditioned on audio signals to deform all reference images to the query space. Extensive experiments show that with only tens of seconds of training clip available, our proposed DFRF can synthesize natural and high-quality audio-driven talking head videos for novel identities with only 40k iterations. We highly recommend readers view our supplementary video for intuitive comparisons. Code is available in https://sstzal.github.io/DFRF/."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou, Jiwen Lu, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19775-8_39"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19775-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "666-682"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Learning Dynamic Facial Radiance Fields for\u00a0Few-Shot Talking Head Synthesis"
          ]
        ],
        "resource": "storage/i2195.pdf",
        "selectable": false
      },
      {
        "text": "MobileNeRF",
        "item-id": "i2763",
        "nodes": [
          {
            "text": "Chen et al_2023_MobileNeRF.pdf",
            "item-id": "i2765",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2023_MobileNeRF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2023_MobileNeRF.pdf"
              ]
            ],
            "resource": "storage/i2765.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MobileNeRF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones."
          ],
          [
            "Access Date",
            "2023-06-22 21:16:46"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "16569-16578"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MobileNeRF"
          ],
          [
            "Title",
            "MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2765.pdf",
        "selectable": false
      },
      {
        "text": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
        "item-id": "i1",
        "nodes": [
          {
            "text": "Mildenhall et al_2020_NeRF.pdf",
            "item-id": "i287",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mildenhall et al_2020_NeRF.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UAEM4U64/1\">NeRF: Representing Scenes as  Neural Radiance Fields for View Synthesis</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mildenhall et al_2020_NeRF.pdf"
              ]
            ],
            "resource": "storage/i287.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (\u03b8,\u03d5)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."
          ],
          [
            "Conference Name",
            "European Conference on Computer Vision"
          ],
          [
            "Creators",
            "Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, Ren Ng"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58452-8_24"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58451-1"
          ],
          [
            "Pages",
            "405-421"
          ],
          [
            "Place",
            "Springer, Cham"
          ],
          [
            "Proceedings Title",
            "European Conference on Computer Vision"
          ],
          [
            "Title",
            "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
          ]
        ],
        "resource": "storage/i287.pdf",
        "selectable": false
      },
      {
        "text": "NeuralMarker",
        "item-id": "i1808",
        "nodes": [
          {
            "text": "Comment: Accepted by ToG (SIGGRAPH Asia 2022). Project Page: https://drinkingcoder.github.io/publication/neuralmarker/",
            "item-id": "n1812",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted by ToG (SIGGRAPH Asia 2022). Project Page: https://drinkingcoder.github.io/publication/neuralmarker/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted by ToG (SIGGRAPH Asia 2022). Project Page: https://drinkingcoder.github.io/publication/neuralmarker/</div>",
            "node_type": "note"
          },
          {
            "text": "Huang et al_2022_NeuralMarker.pdf",
            "item-id": "i1811",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2022_NeuralMarker.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PNY7NDLJ/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/2\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/3\">3 NeuralMarker</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/3\">3.1 Marker Correspondence Neural Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/3\">3.2 Supervised Training with FlyingMarkers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/4\">3.3 Weakly Supervised Training with SfM Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/5\">3.4 Training Neural Network</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/5\">4 Marker Correspondence Benchmark</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/5\">4.1 FlyingMarkers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/5\">4.2 DVL-Markers Benchmark</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/6\">4.3 Difficulty Levels in DVL-Markers</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/6\">5 Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/8\">6 Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/8\">7 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/9\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/9\">9 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PNY7NDLJ/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2022_NeuralMarker.pdf"
              ]
            ],
            "resource": "storage/i1811.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "NeuralMarker",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We tackle the problem of estimating correspondences from a general marker, such as a movie poster, to an image that captures such a marker. Conventionally, this problem is addressed by fitting a homography model based on sparse feature matching. However, they are only able to handle plane-like markers and the sparse features do not sufficiently utilize appearance information. In this paper, we propose a novel framework NeuralMarker, training a neural network estimating dense marker correspondences under various challenging conditions, such as marker deformation, harsh lighting, etc. Besides, we also propose a novel marker correspondence evaluation method circumstancing annotations on real marker-image pairs and create a new benchmark. We show that NeuralMarker significantly outperforms previous methods and enables new interesting applications, including Augmented Reality (AR) and video editing."
          ],
          [
            "Access Date",
            "2022-10-10 08:58:22"
          ],
          [
            "Archiveid",
            "arXiv:2209.08896"
          ],
          [
            "Creators",
            "Zhaoyang Huang, Xiaokun Pan, Weihong Pan, Weikang Bian, Yan Xu, Ka Chun Cheung, Guofeng Zhang, Hongsheng Li"
          ],
          [
            "DOI",
            "10.48550/arXiv.2209.08896"
          ],
          [
            "Date",
            "2022-09-19 2022-09-19"
          ],
          [
            "Extra",
            "arXiv:2209.08896 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "NeuralMarker"
          ],
          [
            "Title",
            "NeuralMarker: A Framework for Learning General Marker Correspondence"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2209.08896"
          ]
        ],
        "resource": "storage/i1811.pdf",
        "selectable": false
      },
      {
        "text": "PAniC-3D",
        "item-id": "i2776",
        "nodes": [
          {
            "text": "Chen et al_2023_PAniC-3D.pdf",
            "item-id": "i2916",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2023_PAniC-3D.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2023_PAniC-3D.pdf"
              ]
            ],
            "resource": "storage/i2916.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PAniC-3D",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose PAniC-3D, a system to reconstruct stylized 3D character heads directly from illustrated (p)ortraits of (ani)me (c)haracters. Our anime-style domain poses unique challenges to single-view reconstruction; compared to natural images of human heads, character portrait illustrations have hair and accessories with more complex and diverse geometry, and are shaded with non-photorealistic contour lines. In addition, there is a lack of both 3D model and portrait illustration data suitable to train and evaluate this ambiguous stylized reconstruction task. Facing these challenges, our proposed PAniC-3D architecture crosses the illustration-to-3D domain gap with a line-filling model, and represents sophisticated geometries with a volumetric radiance field. We train our system with two large new datasets (11.2k Vroid 3D models, 1k Vtuber portrait illustrations), and evaluate on a novel AnimeRecon benchmark of illustration-to-3D pairs. PAniC-3D significantly outperforms baseline methods, and provides data to establish the task of stylized reconstruction from portrait illustrations."
          ],
          [
            "Access Date",
            "2023-07-01 07:10:25"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Shuhong Chen, Kevin Zhang, Yichun Shi, Heng Wang, Yiheng Zhu, Guoxian Song, Sizhe An, Janus Kristjansson, Xiao Yang, Matthias Zwicker"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "21068-21077"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "PAniC-3D"
          ],
          [
            "Title",
            "PAniC-3D: Stylized Single-View 3D Reconstruction From Portraits of Anime Characters"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Chen_PAniC-3D_Stylized_Single-View_3D_Reconstruction_From_Portraits_of_Anime_Characters_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2916.pdf",
        "selectable": false
      },
      {
        "text": "Pi-GAN",
        "item-id": "i1308",
        "nodes": [
          {
            "text": "Chan et al_2021_Pi-GAN.pdf",
            "item-id": "i1370",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chan et al_2021_Pi-GAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chan et al_2021_Pi-GAN.pdf"
              ]
            ],
            "resource": "storage/i1370.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pi-GAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (p-GAN or pi-GAN), for high-quality 3D-aware image synthesis. p-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets."
          ],
          [
            "Access Date",
            "2022-02-17 23:39:20"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5799-5809"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Pi-GAN"
          ],
          [
            "Title",
            "Pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Chan_Pi-GAN_Periodic_Implicit_Generative_Adversarial_Networks_for_3D-Aware_Image_Synthesis_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1370.pdf",
        "selectable": false
      },
      {
        "text": "Unconstrained Scene Generation With Locally Conditioned Radiance Fields",
        "item-id": "i1307",
        "nodes": [
          {
            "text": "DeVries et al_2021_Unconstrained Scene Generation With Locally Conditioned Radiance Fields.pdf",
            "item-id": "i1368",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "DeVries et al_2021_Unconstrained Scene Generation With Locally Conditioned Radiance Fields.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "DeVries et al_2021_Unconstrained Scene Generation With Locally Conditioned Radiance Fields.pdf"
              ]
            ],
            "resource": "storage/i1368.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unconstrained Scene Generation With Locally Conditioned Radiance Fields",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from view-points that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher quality scene renderings across several different scene datasets."
          ],
          [
            "Access Date",
            "2022-02-17 23:40:07"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14304-14313"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Unconstrained Scene Generation With Locally Conditioned Radiance Fields"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/DeVries_Unconstrained_Scene_Generation_With_Locally_Conditioned_Radiance_Fields_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1368.pdf",
        "selectable": false
      }
    ],
    "item_title": "Neural Radiance Fields",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Neuro Symbolic",
    "item-id": "c43,i3606",
    "nodes": [
      {
        "text": "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection",
        "item-id": "i3247",
        "nodes": [
          {
            "text": "Yu et al_2022_A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual.pdf",
            "item-id": "i3377",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yu et al_2022_A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yu et al_2022_A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual.pdf"
              ]
            ],
            "resource": "storage/i3377.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper aims to leverage symbolic knowledge to improve the performance and interpretability of the Visual Relationship Detection (VRD) models. Existing VRD methods based on deep learning suffer from the problems of poor performance on insufficient labeled examples and lack of interpretability. To overcome the aforementioned weaknesses, we integrate symbolic knowledge into deep learning models and propose a bi-level probabilistic graphical reasoning framework called BPGR. Specifically, in the high-level structure, we take the objects and relationships detected by the VRD model as hidden variables (reasoning results); In the low-level structure of BPGR, we use Markov Logic Networks (MLNs) to project First-Order Logic (FOL) as observed variables (symbolic knowledge) to correct error reasoning results. We adopt a variational EM algorithm for optimization. Experiments results show that our BPGR improves the performance of the VRD models. In particular, BPGR can also provide easy-to-understand insights for reasoning results to show interpretability."
          ],
          [
            "Access Date",
            "2023-12-09 09:14:49"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Dongran Yu, Bo Yang, Qianhao Wei, Anchen Li, Shirui Pan"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10609-10618"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Yu_A_Probabilistic_Graphical_Model_Based_on_Neural-Symbolic_Reasoning_for_Visual_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i3377.pdf",
        "selectable": false
      },
      {
        "text": "A survey on neural-symbolic learning systems",
        "item-id": "i3254",
        "nodes": [
          {
            "text": "Yu et al_2023_A survey on neural-symbolic learning systems.pdf",
            "item-id": "i3405",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yu et al_2023_A survey on neural-symbolic learning systems.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yu et al_2023_A survey on neural-symbolic learning systems.pdf"
              ]
            ],
            "resource": "storage/i3405.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A survey on neural-symbolic learning systems",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, neural systems have demonstrated highly effective learning ability and superior perception intelligence. However, they have been found to lack effective reasoning and cognitive ability. On the other hand, symbolic systems exhibit exceptional cognitive intelligence but suffer from poor learning capabilities when compared to neural systems. Recognizing the advantages and disadvantages of both methodologies, an ideal solution emerges: combining neural systems and symbolic systems to create neural-symbolic learning systems that possess powerful perception and cognition. The purpose of this paper is to survey the advancements in neural-symbolic learning systems from four distinct perspectives: challenges, methods, applications, and future directions. By doing so, this research aims to propel this emerging field forward, offering researchers a comprehensive and holistic overview. This overview will not only highlight the current state-of-the-art but also identify promising avenues for future research."
          ],
          [
            "Access Date",
            "2023-12-08 21:53:57"
          ],
          [
            "Creators",
            "Dongran Yu, Bo Yang, Dayou Liu, Hui Wang, Shirui Pan"
          ],
          [
            "DOI",
            "10.1016/j.neunet.2023.06.028"
          ],
          [
            "Date",
            "2023-09-01 2023-09-01"
          ],
          [
            "ISSN",
            "0893-6080"
          ],
          [
            "Journal Abbreviation",
            "Neural Networks"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "105-126"
          ],
          [
            "Publication Title",
            "Neural Networks"
          ],
          [
            "Title",
            "A survey on neural-symbolic learning systems"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0893608023003398"
          ],
          [
            "Volume",
            "166"
          ]
        ],
        "resource": "storage/i3405.pdf",
        "selectable": false
      },
      {
        "text": "CLEVRER",
        "item-id": "i3466",
        "nodes": [
          {
            "text": "Yi et al_2019_CLEVRER.pdf",
            "item-id": "i3551",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yi et al_2019_CLEVRER.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DRVMZGMK/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/3\">The CLEVRER Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/3\">Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/4\">Questions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/5\">Baseline Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/5\">Model Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/6\">Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/7\">Neuro-Symbolic Dynamic Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/9\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/14\">Descriptive Question Sub-types and Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/14\">NS-DR Model Details and Training Paradigm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/15\">Extra Examples from CLEVRER</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yi et al_2019_CLEVRER.pdf"
              ]
            ],
            "resource": "storage/i3551.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CLEVRER",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations."
          ],
          [
            "Access Date",
            "2024-01-08 03:17:28"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum"
          ],
          [
            "Date",
            "2019-09-25 2019/09/25"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "CLEVRER"
          ],
          [
            "Title",
            "CLEVRER: Collision Events for Video Representation and Reasoning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=HkxYzANYDB"
          ]
        ],
        "resource": "storage/i3551.pdf",
        "selectable": false
      },
      {
        "text": "Compositional Attention Networks for Machine Reasoning",
        "item-id": "i3262",
        "nodes": [
          {
            "text": "Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf",
            "item-id": "i3394",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JAHGBETI/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/3\">Compositional Attention Networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/3\">The Input Unit</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/3\">The MAC cell</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/4\">The Control Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/5\">The Read Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/6\">The Write Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/8\">Discussion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/8\">The Output Unit</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/9\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/10\">Data Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/11\">CLEVR Humans - Natural Language Questions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/11\">Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/13\">Interpretability</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/14\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/17\">Details of Input Unit</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/17\">The Query Unit</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/17\">The Image Unit</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Implementation and Training details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Further discussion of related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Module Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Augmented Convolutional Neural Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/19\">Memory and Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/20\">Attention vs. Convolution</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf"
              ]
            ],
            "resource": "storage/i3394.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Compositional Attention Networks for Machine Reasoning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results."
          ],
          [
            "Access Date",
            "2023-12-09 01:02:47"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Drew A. Hudson, Christopher D. Manning"
          ],
          [
            "Date",
            "2018-02-15 2018/02/15"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Title",
            "Compositional Attention Networks for Machine Reasoning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=S1Euwz-Rb"
          ]
        ],
        "resource": "storage/i3394.pdf",
        "selectable": false
      },
      {
        "text": "Deep Learning For Symbolic Mathematics",
        "item-id": "i3604",
        "nodes": [
          {
            "text": "Lample_Charton_2019_Deep Learning For Symbolic Mathematics.pdf",
            "item-id": "i3629",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lample_Charton_2019_Deep Learning For Symbolic Mathematics.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lample_Charton_2019_Deep Learning For Symbolic Mathematics.pdf"
              ]
            ],
            "resource": "storage/i3629.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Learning For Symbolic Mathematics",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing these mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica."
          ],
          [
            "Access Date",
            "2024-01-11 11:26:45"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Guillaume Lample, Fran\u00e7ois Charton"
          ],
          [
            "Date",
            "2019-09-25 2019/09/25"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Deep Learning For Symbolic Mathematics"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=S1eZYeHFDS"
          ]
        ],
        "resource": "storage/i3629.pdf",
        "selectable": false
      },
      {
        "text": "DeepProbLog",
        "item-id": "i3255",
        "nodes": [
          {
            "text": "Manhaeve et al_2018_DeepProbLog.pdf",
            "item-id": "i3386",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Manhaeve et al_2018_DeepProbLog.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Manhaeve et al_2018_DeepProbLog.pdf"
              ]
            ],
            "resource": "storage/i3386.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DeepProbLog",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples."
          ],
          [
            "Access Date",
            "2023-12-09 07:25:19"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "DeepProbLog"
          ],
          [
            "Title",
            "DeepProbLog: Neural Probabilistic Logic Programming"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2018/hash/dc5d637ed5e62c36ecb73b654b05ba2a-Abstract.html"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i3386.pdf",
        "selectable": false
      },
      {
        "text": "Embedding Symbolic Knowledge into Deep Networks",
        "item-id": "i3603",
        "nodes": [
          {
            "text": "Xie et al_2019_Embedding Symbolic Knowledge into Deep Networks.pdf",
            "item-id": "i3620",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xie et al_2019_Embedding Symbolic Knowledge into Deep Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xie et al_2019_Embedding Symbolic Knowledge into Deep Networks.pdf"
              ]
            ],
            "resource": "storage/i3620.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Embedding Symbolic Knowledge into Deep Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning."
          ],
          [
            "Access Date",
            "2024-01-11 14:41:24"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Yaqi Xie, Ziwei Xu, Mohan S Kankanhalli, Kuldeep S Meel, Harold Soh"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Embedding Symbolic Knowledge into Deep Networks"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/7b66b4fd401a271a1c7224027ce111bc-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i3620.pdf",
        "selectable": false
      },
      {
        "text": "Explainable Neural Computation via Stack Neural Module Networks",
        "item-id": "i3265",
        "nodes": [
          {
            "text": "Hu et al_2018_Explainable Neural Computation via Stack Neural Module Networks.pdf",
            "item-id": "i3397",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hu et al_2018_Explainable Neural Computation via Stack Neural Module Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hu et al_2018_Explainable Neural Computation via Stack Neural Module Networks.pdf"
              ]
            ],
            "resource": "storage/i3397.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Explainable Neural Computation via Stack Neural Module Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs."
          ],
          [
            "Access Date",
            "2023-12-09 00:35:04"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "53-69"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Title",
            "Explainable Neural Computation via Stack Neural Module Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Ronghang_Hu_Explainable_Neural_Computation_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i3397.pdf",
        "selectable": false
      },
      {
        "text": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
        "item-id": "i3444",
        "nodes": [
          {
            "text": "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf",
            "item-id": "i3549",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_H2CF698R/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2CF698R/3\">Dynamic Concept Learner</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/4\">Model Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/5\">Training and inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Comparisons on Temporal and Causal Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/7\">Evaluation of Object and Event Concept Grounding in Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/7\">Generalization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/9\">Extension to real videos and the new concept</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/9\">Discussion and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Dynamic Predictor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/14\">Program Parser</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/14\">CLEVRER Operations and Program Execution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/15\">Trajectory Performance Evaluation.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/16\">Statistics for CLEVRER-Grounding and CLEVRER-Retrieval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/16\">Training Objectives</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf"
              ]
            ],
            "resource": "storage/i3549.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from video and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity."
          ],
          [
            "Access Date",
            "2024-01-08 03:21:42"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, Chuang Gan"
          ],
          [
            "Date",
            "2020-10-02 2020/10/02"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=bhCDO_cEGCz"
          ]
        ],
        "resource": "storage/i3549.pdf",
        "selectable": false
      },
      {
        "text": "HuggingGPT",
        "item-id": "i3249",
        "nodes": [
          {
            "text": "Shen et al_2023_HuggingGPT.pdf",
            "item-id": "i3376",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shen et al_2023_HuggingGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shen et al_2023_HuggingGPT.pdf"
              ]
            ],
            "resource": "storage/i3376.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "HuggingGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence."
          ],
          [
            "Access Date",
            "2023-12-11 04:47:18"
          ],
          [
            "Archiveid",
            "arXiv:2303.17580"
          ],
          [
            "Creators",
            "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.17580"
          ],
          [
            "Date",
            "2023-03-30 2023-03-30"
          ],
          [
            "Extra",
            "arXiv:2303.17580 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "HuggingGPT"
          ],
          [
            "Title",
            "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.17580"
          ]
        ],
        "resource": "storage/i3376.pdf",
        "selectable": false
      },
      {
        "text": "IdealGPT",
        "item-id": "i3248",
        "nodes": [
          {
            "text": "You et al_2023_IdealGPT.pdf",
            "item-id": "i3391",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "You et al_2023_IdealGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "You et al_2023_IdealGPT.pdf"
              ]
            ],
            "resource": "storage/i3391.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "IdealGPT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT."
          ],
          [
            "Access Date",
            "2023-12-09 03:20:25"
          ],
          [
            "Conference Name",
            "The 2023 Conference on Empirical Methods in Natural Language Processing"
          ],
          [
            "Creators",
            "Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang, Shih-Fu Chang"
          ],
          [
            "Date",
            "2023-12-01 2023/12/01"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "The 2023 Conference on Empirical Methods in Natural Language Processing"
          ],
          [
            "Short Title",
            "IdealGPT"
          ],
          [
            "Title",
            "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=IvwcvJHLpc"
          ]
        ],
        "resource": "storage/i3391.pdf",
        "selectable": false
      },
      {
        "text": "Inferring and Executing Programs for Visual Reasoning",
        "item-id": "i3263",
        "nodes": [
          {
            "text": "Johnson et al_2017_Inferring and Executing Programs for Visual Reasoning.pdf",
            "item-id": "i3395",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Johnson et al_2017_Inferring and Executing Programs for Visual Reasoning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Johnson et al_2017_Inferring and Executing Programs for Visual Reasoning.pdf"
              ]
            ],
            "resource": "storage/i3395.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Inferring and Executing Programs for Visual Reasoning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings."
          ],
          [
            "Access Date",
            "2023-12-09 00:38:13"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2989-2998"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Inferring and Executing Programs for Visual Reasoning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Johnson_Inferring_and_Executing_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i3395.pdf",
        "selectable": false
      },
      {
        "text": "Learning by Abstraction",
        "item-id": "i3261",
        "nodes": [
          {
            "text": "Hudson_Manning_2019_Learning by Abstraction.pdf",
            "item-id": "i3393",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hudson_Manning_2019_Learning by Abstraction.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hudson_Manning_2019_Learning by Abstraction.pdf"
              ]
            ],
            "resource": "storage/i3393.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning by Abstraction",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach."
          ],
          [
            "Access Date",
            "2023-12-09 01:04:30"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Drew Hudson, Christopher D Manning"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "Learning by Abstraction"
          ],
          [
            "Title",
            "Learning by Abstraction: The Neural State Machine"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/c20a7ce2a627ba838cfbff082db35197-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i3393.pdf",
        "selectable": false
      },
      {
        "text": "Learning to Reason",
        "item-id": "i3264",
        "nodes": [
          {
            "text": "Hu et al_2017_Learning to Reason.pdf",
            "item-id": "i3396",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hu et al_2017_Learning to Reason.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hu et al_2017_Learning to Reason.pdf"
              ]
            ],
            "resource": "storage/i3396.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning to Reason",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer \"is there an equal number of balls and boxes?\" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question."
          ],
          [
            "Access Date",
            "2023-12-09 00:36:12"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "804-813"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "Learning to Reason"
          ],
          [
            "Title",
            "Learning to Reason: End-To-End Module Networks for Visual Question Answering"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Hu_Learning_to_Reason_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i3396.pdf",
        "selectable": false
      },
      {
        "text": "Logic Tensor Networks",
        "item-id": "i3252",
        "nodes": [
          {
            "text": "Badreddine et al_2022_Logic Tensor Networks.pdf",
            "item-id": "i3381",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Badreddine et al_2022_Logic Tensor Networks.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_9V5QE5KK/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/3\">2 Real Logic</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/3\">2.1 Syntax</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/3\">2.2 Semantics of Real Logic</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/4\">2.2.1 Grounding domains and the signature</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/4\">2.2.2 Grounding terms and atomic formulas</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/5\">2.2.3 Connectives and quantifiers</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/7\">2.3 Guarded quantifiers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/7\">2.4 Stable Product Real Logic</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/9\">3 Learning, reasoning, and querying in Real Logic</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/9\">3.1 Representing knowledge with Real Logic</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/9\">3.1.1 Knowledge through symbol groundings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/10\">3.1.2 Knowledge through formulas</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/10\">3.1.3 Knowledge through fuzzy semantics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/10\">3.1.4 Satisfiability</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/11\">3.2 Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/11\">3.3 Querying</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/11\">3.4 Reasoning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/11\">3.4.1 Logical consequence in Real Logic</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/12\">3.4.2 Reasoning by optimization</a><ul style=\"list-style-type: none; padding-left:36px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/12\">Reasoning Option 1 (querying after learning)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/12\">Reasoning Option 2 (proof by refutation)</a></li></ul></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/13\">4 The reach of Logic Tensor Networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/13\">4.1 Binary classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/15\">4.2 Multi-class single-label classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/17\">4.3 Multi-class multi-label classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/19\">4.4 Semi-supervised pattern recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/22\">4.5 Regression</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/23\">4.6 Unsupervised learning (clustering)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/24\">4.7 Learning embeddings with LTN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/26\">4.8 Reasoning in LTN</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/28\">5 Related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/29\">5.1 Neural architectures for logical reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/29\">5.2 Logical specification of neural network architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/29\">5.3 Neurosymbolic architectures for the integration of inductive learning and deductive reasoning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/32\">6 Conclusions and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/32\">Declaration of competing interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/32\">Acknowledgement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/33\">Appendix A Implementation details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/33\">Appendix B Fuzzy operators and properties</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/33\">B.1 Negation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/33\">B.2 Conjunction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/34\">B.3 Disjunction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/35\">B.4 Implication</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/35\">B.5 Aggregation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/36\">Appendix C Analyzing gradients of generalized mean aggregators</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9V5QE5KK/38\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Badreddine et al_2022_Logic Tensor Networks.pdf"
              ]
            ],
            "resource": "storage/i3381.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Logic Tensor Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI."
          ],
          [
            "Access Date",
            "2023-12-09 08:00:58"
          ],
          [
            "Creators",
            "Samy Badreddine, Artur d'Avila Garcez, Luciano Serafini, Michael Spranger"
          ],
          [
            "DOI",
            "10.1016/j.artint.2021.103649"
          ],
          [
            "Date",
            "2022-02-01 2022-02-01"
          ],
          [
            "ISSN",
            "0004-3702"
          ],
          [
            "Journal Abbreviation",
            "Artificial Intelligence"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "103649"
          ],
          [
            "Publication Title",
            "Artificial Intelligence"
          ],
          [
            "Title",
            "Logic Tensor Networks"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0004370221002009"
          ],
          [
            "Volume",
            "303"
          ]
        ],
        "resource": "storage/i3381.pdf",
        "selectable": false
      },
      {
        "text": "Neural Module Networks",
        "item-id": "i3251",
        "nodes": [
          {
            "text": "Andreas et al_2016_Neural Module Networks.pdf",
            "item-id": "i3401",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Andreas et al_2016_Neural Module Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Andreas et al_2016_Neural Module Networks.pdf"
              ]
            ],
            "resource": "storage/i3401.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Module Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning _neural module networks_, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes."
          ],
          [
            "Access Date",
            "2023-12-09 00:17:22"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "39-48"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Neural Module Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i3401.pdf",
        "selectable": false
      },
      {
        "text": "Neural-Symbolic VQA",
        "item-id": "i3463",
        "nodes": [
          {
            "text": "Yi et al_2018_Neural-Symbolic VQA.pdf",
            "item-id": "i3484",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yi et al_2018_Neural-Symbolic VQA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yi et al_2018_Neural-Symbolic VQA.pdf"
              ]
            ],
            "resource": "storage/i3484.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural-Symbolic VQA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step."
          ],
          [
            "Access Date",
            "2024-01-08 01:31:59"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "Neural-Symbolic VQA"
          ],
          [
            "Title",
            "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2018/hash/5e388103a391daabe3de1d76a6739ccd-Abstract.html"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i3484.pdf",
        "selectable": false
      },
      {
        "text": "Neuro-Symbolic Artificial Intelligence",
        "item-id": "i3253",
        "nodes": [
          {
            "text": "Sarker et al_2022_Neuro-Symbolic Artificial Intelligence.pdf",
            "item-id": "i3383",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sarker et al_2022_Neuro-Symbolic Artificial Intelligence.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XMMPEGQX/1\">1 What Is Neuro-Symbolic Artificial Intelligence?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XMMPEGQX/2\">2 Categorizing Neuro-Symbolic Artificial Intelligence</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XMMPEGQX/4\">3 Neuro-Symbolic Artificial Intelligence: Trends</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XMMPEGQX/7\">4 Paths Forward</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XMMPEGQX/8\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XMMPEGQX/8\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sarker et al_2022_Neuro-Symbolic Artificial Intelligence.pdf"
              ]
            ],
            "resource": "storage/i3383.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neuro-Symbolic Artificial Intelligence",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Creators",
            "Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, Pascal Hitzler"
          ],
          [
            "DOI",
            "10.3233/AIC-210084"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Publication Title",
            "AI Communications"
          ],
          [
            "Short Title",
            "Neuro-Symbolic Artificial Intelligence"
          ],
          [
            "Title",
            "Neuro-Symbolic Artificial Intelligence: Current Trends"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i3383.pdf",
        "selectable": false
      },
      {
        "text": "Neurosymbolic AI and its Taxonomy",
        "item-id": "i3246",
        "nodes": [
          {
            "text": "Comment: submitted to ACM Computing Surveys",
            "item-id": "n3400",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: submitted to ACM Computing Surveys",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: submitted to ACM Computing Surveys</div>",
            "node_type": "note"
          },
          {
            "text": "Gibaut et al_2023_Neurosymbolic AI and its Taxonomy.pdf",
            "item-id": "i3398",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gibaut et al_2023_Neurosymbolic AI and its Taxonomy.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WJGSXKDS/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/2\">1.1 Symbols</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/3\">1.2 Methodoly and Scope of this Work</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/4\">2 Knowledge Representation in Neurosymbolic Systems</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/6\">3 Learning in Neurosymbolic Systems</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/6\">3.1 Inductive Logic Programming</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/7\">3.2 Hybrid Learning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/8\">4 Reasoning in Neurosymbolic Systems</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/8\">4.1 Forward and Backward chaining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/8\">4.2 Approximate Satisfiability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/9\">4.3 Relationship reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/9\">4.4 Exploration of Practical Reasoning in DFL</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/9\">5 Explainability and Trustworthiness in Neurosymbolic Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/10\">6 Applications of neuro-symbolic Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/15\">7 Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gibaut et al_2023_Neurosymbolic AI and its Taxonomy.pdf"
              ]
            ],
            "resource": "storage/i3398.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Neurosymbolic AI and its Taxonomy",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications."
          ],
          [
            "Access Date",
            "2023-12-09 00:19:51"
          ],
          [
            "Archiveid",
            "arXiv:2305.08876"
          ],
          [
            "Creators",
            "Wandemberg Gibaut, Leonardo Pereira, Fabio Grassiotto, Alexandre Osorio, Eder Gadioli, Amparo Munoz, Sildolfo Gomes, Claudio dos Santos"
          ],
          [
            "DOI",
            "10.48550/arxiv.2305.08876"
          ],
          [
            "Date",
            "2023-05-12 2023-05-12"
          ],
          [
            "Extra",
            "arXiv:2305.08876 [cs]"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Neurosymbolic AI and its Taxonomy"
          ],
          [
            "Title",
            "Neurosymbolic AI and its Taxonomy: a survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.08876"
          ]
        ],
        "resource": "storage/i3398.pdf",
        "selectable": false
      },
      {
        "text": "PEORL",
        "item-id": "i3605",
        "nodes": [
          {
            "text": "Yang et al_2018_PEORL.pdf",
            "item-id": "i3614",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2018_PEORL.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2018_PEORL.pdf"
              ]
            ],
            "resource": "storage/i3614.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PEORL",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework PEORL that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision-making in a dynamic environment with uncertainties. Symbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL."
          ],
          [
            "Access Date",
            "2024-01-11"
          ],
          [
            "Creators",
            "Fangkai Yang, Daoming Lyu, Bo Liu, Steven Gustafson"
          ],
          [
            "Date",
            "2018-00-13 \u4e03\u6708 13, 2018"
          ],
          [
            "ISBN",
            "978-0-9992411-2-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "4860\u20134866"
          ],
          [
            "Place",
            "Stockholm, Sweden"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 27th International Joint Conference on Artificial Intelligence"
          ],
          [
            "Publisher",
            "AAAI Press"
          ],
          [
            "Series",
            "IJCAI'18"
          ],
          [
            "Short Title",
            "PEORL"
          ],
          [
            "Title",
            "PEORL: integrating symbolic planning and hierarchical reinforcement learning for robust decision-making"
          ]
        ],
        "resource": "storage/i3614.pdf",
        "selectable": false
      },
      {
        "text": "Scallop",
        "item-id": "i3228",
        "nodes": [
          {
            "text": "Li et al_2023_Scallop.pdf",
            "item-id": "i3365",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2023_Scallop.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5UDXGW65/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/3\">2 Illustrative Overview</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UDXGW65/6\">3 Language</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/6\">3.1 Data Types</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/7\">3.2 (Horn) Rules</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/8\">3.3 Probabilistic Extensions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UDXGW65/9\">4 Reasoning Framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/9\">4.1 Provenance Framework</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/9\">4.2 SclRam  Intermediate Language</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/10\">4.3 Operational Semantics of SclRam</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/12\">4.4 External Interface and Execution Pipeline</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/13\">4.5 Differentiable Reasoning with Provenance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/14\">4.6 Practical Considerations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/15\">5 Implementation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UDXGW65/15\">6 Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/15\">6.1 Benchmarks and Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/17\">6.2 RQ1: Our Solutions and Expressivity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/18\">6.3 RQ2: Performance and Accuracy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/19\">6.4 RQ3: Runtime Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/19\">6.5 RQ4: Generalizability, Interpretability, and Data-Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/20\">6.6 RQ5: Analysis of Failure Modes</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/21\">7 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/21\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UDXGW65/22\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2023_Scallop.pdf"
              ]
            ],
            "resource": "storage/i3365.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Scallop",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability."
          ],
          [
            "Access Date",
            "2023-12-20 06:29:30"
          ],
          [
            "Creators",
            "Ziyang Li, Jiani Huang, Mayur Naik"
          ],
          [
            "DOI",
            "10.1145/3591280"
          ],
          [
            "Date",
            "2023-00-06 \u516d\u6708 6, 2023"
          ],
          [
            "Issue",
            "PLDI"
          ],
          [
            "Journal Abbreviation",
            "Proc. ACM Program. Lang."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "166:1463\u2013166:1487"
          ],
          [
            "Publication Title",
            "Proceedings of the ACM on Programming Languages"
          ],
          [
            "Short Title",
            "Scallop"
          ],
          [
            "Title",
            "Scallop: A Language for Neurosymbolic Programming"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3591280"
          ],
          [
            "Volume",
            "7"
          ]
        ],
        "resource": "storage/i3365.pdf",
        "selectable": false
      },
      {
        "text": "The Neuro-Symbolic Concept Learner",
        "item-id": "i3260",
        "nodes": [
          {
            "text": "Mao et al_2018_The Neuro-Symbolic Concept Learner.pdf",
            "item-id": "i3392",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mao et al_2018_The Neuro-Symbolic Concept Learner.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mao et al_2018_The Neuro-Symbolic Concept Learner.pdf"
              ]
            ],
            "resource": "storage/i3392.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The Neuro-Symbolic Concept Learner",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval."
          ],
          [
            "Access Date",
            "2023-12-09 01:05:45"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu"
          ],
          [
            "Date",
            "2018-09-27 2018/09/27"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "The Neuro-Symbolic Concept Learner"
          ],
          [
            "Title",
            "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=rJgMlhRctm"
          ]
        ],
        "resource": "storage/i3392.pdf",
        "selectable": false
      },
      {
        "text": "The Third AI Summer",
        "item-id": "i3606",
        "nodes": [
          {
            "text": "Kautz_2022_The Third AI Summer.pdf",
            "item-id": "i3628",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kautz_2022_The Third AI Summer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kautz_2022_The Third AI Summer.pdf"
              ]
            ],
            "resource": "storage/i3628.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The Third AI Summer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This article summarizes the author's Robert S. Englemore Memorial Lecture presented at the Thirty-Fourth AAAI Conference on Artificial Intelligence on February 10, 2020. It explores recurring themes in the history of AI, real and imagined dangers from AI, and the future of the field."
          ],
          [
            "Access Date",
            "2024-01-11 11:39:40"
          ],
          [
            "Creators",
            "Henry Kautz"
          ],
          [
            "DOI",
            "10.1002/aaai.12036"
          ],
          [
            "Date",
            "2022-03-31 2022-03-31"
          ],
          [
            "Extra",
            "Number: 1"
          ],
          [
            "ISSN",
            "2371-9621"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "105-125"
          ],
          [
            "Publication Title",
            "AI Magazine"
          ],
          [
            "Rights",
            "Copyright (c) 2022 AI Magazine"
          ],
          [
            "Short Title",
            "The Third AI Summer"
          ],
          [
            "Title",
            "The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture"
          ],
          [
            "URL",
            "https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/19122"
          ],
          [
            "Volume",
            "43"
          ]
        ],
        "resource": "storage/i3628.pdf",
        "selectable": false
      },
      {
        "text": "Towards Data-and Knowledge-Driven Artificial Intelligence",
        "item-id": "i3602",
        "nodes": [
          {
            "text": "Comment: Ongoing project",
            "item-id": "n3618",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Ongoing project",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Ongoing project</div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2023_Towards Data-and Knowledge-Driven Artificial Intelligence.pdf",
            "item-id": "i3617",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_Towards Data-and Knowledge-Driven Artificial Intelligence.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_Towards Data-and Knowledge-Driven Artificial Intelligence.pdf"
              ]
            ],
            "resource": "storage/i3617.pdf"
          },
          {
            "text": "arXiv.org Snapshot",
            "item-id": "i3616",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "arXiv.org Snapshot",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-11 14:52:32"
              ],
              [
                "Title",
                "arXiv.org Snapshot"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2210.15889"
              ]
            ],
            "resource": "storage/i3616.html"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Towards Data-and Knowledge-Driven Artificial Intelligence",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successful application of modern NeSy approaches in several domains. Then, we benchmark several NeSy methods on three representative application tasks. Finally, we identify the open problems together with potential future research directions. This survey is expected to help new researchers enter this rapidly evolving field and accelerate the progress towards data-and knowledge-driven AI."
          ],
          [
            "Access Date",
            "2024-01-11 14:52:25"
          ],
          [
            "Archiveid",
            "arXiv:2210.15889"
          ],
          [
            "Creators",
            "Wenguan Wang, Yi Yang, Fei Wu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2210.15889"
          ],
          [
            "Date",
            "2023-10-12 2023-10-12"
          ],
          [
            "Extra",
            "arXiv:2210.15889 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Towards Data-and Knowledge-Driven Artificial Intelligence"
          ],
          [
            "Title",
            "Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2210.15889"
          ]
        ],
        "resource": "storage/i3617.pdf",
        "selectable": false
      },
      {
        "text": "ViperGPT",
        "item-id": "i3035",
        "nodes": [
          {
            "text": "Comment: Website: https://viper.cs.columbia.edu/",
            "item-id": "n3068",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Website: https://viper.cs.columbia.edu/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Website: https://viper.cs.columbia.edu/</div>",
            "node_type": "note"
          },
          {
            "text": "Sur\u00eds et al_2023_ViperGPT.pdf",
            "item-id": "i3067",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sur\u00eds et al_2023_ViperGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sur\u00eds et al_2023_ViperGPT.pdf"
              ]
            ],
            "resource": "storage/i3067.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ViperGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks."
          ],
          [
            "Access Date",
            "2023-10-05 07:40:57"
          ],
          [
            "Archiveid",
            "arXiv:2303.08128"
          ],
          [
            "Creators",
            "D\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.08128"
          ],
          [
            "Date",
            "2023-03-14 2023-03-14"
          ],
          [
            "Extra",
            "arXiv:2303.08128 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ViperGPT"
          ],
          [
            "Title",
            "ViperGPT: Visual Inference via Python Execution for Reasoning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.08128"
          ]
        ],
        "resource": "storage/i3067.pdf",
        "selectable": false
      },
      {
        "text": "Visual ChatGPT",
        "item-id": "i2941",
        "nodes": [
          {
            "text": "Wu et al_2023_Visual ChatGPT.pdf",
            "item-id": "i2968",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2023_Visual ChatGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2023_Visual ChatGPT.pdf"
              ]
            ],
            "resource": "storage/i2968.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Visual ChatGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}."
          ],
          [
            "Access Date",
            "2023-07-27 01:21:19"
          ],
          [
            "Archiveid",
            "arXiv:2303.04671"
          ],
          [
            "Creators",
            "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.04671"
          ],
          [
            "Date",
            "2023-03-08 2023-03-08"
          ],
          [
            "Extra",
            "arXiv:2303.04671 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Visual ChatGPT"
          ],
          [
            "Title",
            "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.04671"
          ]
        ],
        "resource": "storage/i2968.pdf",
        "selectable": false
      },
      {
        "text": "Visual Programming",
        "item-id": "i3036",
        "nodes": [
          {
            "text": "Gupta_Kembhavi_2023_Visual Programming.pdf",
            "item-id": "i3070",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gupta_Kembhavi_2023_Visual Programming.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gupta_Kembhavi_2023_Visual Programming.pdf"
              ]
            ],
            "resource": "storage/i3070.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Visual Programming",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform."
          ],
          [
            "Access Date",
            "2023-10-05 07:33:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Tanmay Gupta, Aniruddha Kembhavi"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14953-14962"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Visual Programming"
          ],
          [
            "Title",
            "Visual Programming: Compositional Visual Reasoning Without Training"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i3070.pdf",
        "selectable": false
      },
      {
        "text": "What's Left?",
        "item-id": "i3245",
        "nodes": [
          {
            "text": "Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/lef",
            "item-id": "n3368",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/lef",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/left_neurips_2023</div>",
            "node_type": "note"
          },
          {
            "text": "Hsu et al_2023_What's Left.pdf",
            "item-id": "i3367",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hsu et al_2023_What's Left.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_J5VBVVPX/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/3\">Logic-Enhanced Foundation Model (LEFT)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/3\">Domain-independent LLM interpreter</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/5\">Domain-independent first-order logic executor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/6\">Domain-specific grounding modules</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/7\">Concept learning across domains</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/7\">Accuracy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/8\">Data efficiency</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/9\">Reasoning generalization across tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/9\">Accuracy</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/10\">Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/14\">LEFT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/14\">Function definitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/15\">Broader impact</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Error bars</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Transfer task construction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Compute</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Code</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hsu et al_2023_What's Left.pdf"
              ]
            ],
            "resource": "storage/i3367.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "What's Left?",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like \"left\" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains."
          ],
          [
            "Access Date",
            "2023-12-20 06:23:46"
          ],
          [
            "Archiveid",
            "arXiv:2310.16035"
          ],
          [
            "Creators",
            "Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2310.16035"
          ],
          [
            "Date",
            "2023-10-24 2023-10-24"
          ],
          [
            "Extra",
            "arXiv:2310.16035 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "What's Left?"
          ],
          [
            "Title",
            "What's Left? Concept Grounding with Logic-Enhanced Foundation Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2310.16035"
          ]
        ],
        "resource": "storage/i3367.pdf",
        "selectable": false
      }
    ],
    "item_title": "Neuro Symbolic",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "News",
    "item-id": "c7,i3663",
    "nodes": [
      {
        "text": "AI can fool voice recognition used to verify identity by Centrelink and Australian tax office",
        "item-id": "i2800",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "AI can fool voice recognition used to verify identity by Centrelink and Australian tax office",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Exclusive: Voiceprint program used by millions of Australians to access data held by government agencies shown to have a serious security flaw"
          ],
          [
            "Access Date",
            "2023-07-17 12:29:12"
          ],
          [
            "Creators",
            "Nick Evershed, Josh Taylor"
          ],
          [
            "Date",
            "2023-03-16 2023-03-16T14:00:15.000Z"
          ],
          [
            "ISSN",
            "0261-3077"
          ],
          [
            "Language",
            "en-GB"
          ],
          [
            "Library Catalog",
            "The Guardian"
          ],
          [
            "Publication Title",
            "The Guardian"
          ],
          [
            "Section",
            "Technology"
          ],
          [
            "Title",
            "AI can fool voice recognition used to verify identity by Centrelink and Australian tax office"
          ],
          [
            "URL",
            "https://www.theguardian.com/technology/2023/mar/16/voice-system-used-to-verify-identity-by-centrelink-can-be-fooled-by-ai"
          ]
        ]
      },
      {
        "text": "ChatGPT",
        "item-id": "i3236",
        "nodes": [
          {
            "text": "Emsley_2023_ChatGPT.pdf",
            "item-id": "i3350",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Emsley_2023_ChatGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Emsley_2023_ChatGPT.pdf"
              ]
            ],
            "resource": "storage/i3350.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ChatGPT",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-12-27 09:16:04"
          ],
          [
            "Creators",
            "Robin Emsley"
          ],
          [
            "DOI",
            "10.1038/s41537-023-00379-4"
          ],
          [
            "Date",
            "2023-08-19 2023-08-19"
          ],
          [
            "Extra",
            "Number: 1\nPublisher: Nature Publishing Group"
          ],
          [
            "ISSN",
            "2754-6993"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "Schizophr"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.nature.com"
          ],
          [
            "Pages",
            "1-2"
          ],
          [
            "Publication Title",
            "Schizophrenia"
          ],
          [
            "Rights",
            "2023 The Author(s)"
          ],
          [
            "Short Title",
            "ChatGPT"
          ],
          [
            "Title",
            "ChatGPT: these are not hallucinations \u2013 they\u2019re fabrications and falsifications"
          ],
          [
            "URL",
            "https://www.nature.com/articles/s41537-023-00379-4"
          ],
          [
            "Volume",
            "9"
          ]
        ],
        "resource": "storage/i3350.pdf",
        "selectable": false
      },
      {
        "text": "ChatGPT has entered the classroom",
        "item-id": "i3663",
        "nodes": [
          {
            "text": "Extance_2023_ChatGPT has entered the classroom.pdf",
            "item-id": "i3693",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Extance_2023_ChatGPT has entered the classroom.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Extance_2023_ChatGPT has entered the classroom.pdf"
              ]
            ],
            "resource": "storage/i3693.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ChatGPT has entered the classroom",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Researchers, educators and companies are experimenting with ways to turn flawed but famous large language models into trustworthy, accurate \u2018thought partners\u2019 for learning."
          ],
          [
            "Access Date",
            "2024-01-14 11:13:21"
          ],
          [
            "Creators",
            "Andy Extance"
          ],
          [
            "DOI",
            "10.1038/d41586-023-03507-3"
          ],
          [
            "Date",
            "2023-11-15 2023-11-15"
          ],
          [
            "Extra",
            "Bandiera_abtest: a\nCg_type: News Feature\nNumber: 7987\nPublisher: Nature Publishing Group\nSubject_term: Computer science, Education, Machine learning, Mathematics and computing"
          ],
          [
            "Issue",
            "7987"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.nature.com"
          ],
          [
            "Pages",
            "474-477"
          ],
          [
            "Publication Title",
            "Nature"
          ],
          [
            "Rights",
            "2023 Springer Nature Limited"
          ],
          [
            "Short Title",
            "ChatGPT has entered the classroom"
          ],
          [
            "Title",
            "ChatGPT has entered the classroom: how LLMs could transform education"
          ],
          [
            "URL",
            "https://www.nature.com/articles/d41586-023-03507-3"
          ],
          [
            "Volume",
            "623"
          ]
        ],
        "resource": "storage/i3693.pdf",
        "selectable": false
      },
      {
        "text": "ChatGPT is a data privacy nightmare. If you\u2019ve ever posted online, you ought to be concerned",
        "item-id": "i3239",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "ChatGPT is a data privacy nightmare. If you\u2019ve ever posted online, you ought to be concerned",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "ChatGPT is fuelled by our intimate online histories. It\u2019s trained on 300 billion words, yet users have no way of knowing which of their data it contains."
          ],
          [
            "Access Date",
            "2023-12-27 09:24:43"
          ],
          [
            "Creators",
            "Uri Gal"
          ],
          [
            "Date",
            "2023-02-08 2023-02-08"
          ],
          [
            "Language",
            "en-AU"
          ],
          [
            "Publication Title",
            "The Conversation"
          ],
          [
            "Title",
            "ChatGPT is a data privacy nightmare. If you\u2019ve ever posted online, you ought to be concerned"
          ],
          [
            "URL",
            "http://theconversation.com/chatgpt-is-a-data-privacy-nightmare-if-youve-ever-posted-online-you-ought-to-be-concerned-199283"
          ]
        ]
      },
      {
        "text": "Chatbots May \u2018Hallucinate\u2019 More Often Than Many Realize",
        "item-id": "i3241",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "Chatbots May \u2018Hallucinate\u2019 More Often Than Many Realize",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "When summarizing facts, ChatGPT technology makes things up about 3 percent of the time, according to research from a new start-up. A Google system\u2019s rate was 27 percent."
          ],
          [
            "Access Date",
            "2023-12-27 09:16:58"
          ],
          [
            "Creators",
            "Cade Metz"
          ],
          [
            "Date",
            "2023-11-06 2023-11-06"
          ],
          [
            "ISSN",
            "0362-4331"
          ],
          [
            "Language",
            "auto"
          ],
          [
            "Library Catalog",
            "NYTimes.com"
          ],
          [
            "Publication Title",
            "The New York Times"
          ],
          [
            "Section",
            "Technology"
          ],
          [
            "Title",
            "Chatbots May \u2018Hallucinate\u2019 More Often Than Many Realize"
          ],
          [
            "URL",
            "https://www.nytimes.com/2023/11/06/technology/chatbots-hallucination-rates.html"
          ]
        ]
      },
      {
        "text": "Deepfakes",
        "item-id": "i1053",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "Deepfakes",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfakes, or computer-generated images of people, can be dangerous for democracy, warn experts."
          ],
          [
            "Access Date",
            "2021-10-18 15:13:54"
          ],
          [
            "Creators",
            "Daniel Thomas"
          ],
          [
            "Date",
            "2020-01-23 2020-01-23"
          ],
          [
            "Language",
            "auto"
          ],
          [
            "Library Catalog",
            "www.bbc.com"
          ],
          [
            "Publication Title",
            "BBC News"
          ],
          [
            "Section",
            "Business"
          ],
          [
            "Short Title",
            "Deepfakes"
          ],
          [
            "Title",
            "Deepfakes: A threat to democracy or just a bit of fun?"
          ],
          [
            "URL",
            "https://www.bbc.com/news/business-51204954"
          ]
        ]
      },
      {
        "text": "Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content",
        "item-id": "i3238",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Getty Images claims Stability AI \u2018unlawfully\u2019 scraped millions of images from its site. It\u2019s a significant escalation in the developing legal battles between generative AI firms and content creators."
          ],
          [
            "Access Date",
            "2023-12-27 09:26:13"
          ],
          [
            "Creators",
            "James Vincent"
          ],
          [
            "Date",
            "2023-01-17 2023-01-17T10:30:16.195Z"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Publication Title",
            "The Verge"
          ],
          [
            "Title",
            "Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content"
          ],
          [
            "URL",
            "https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit"
          ]
        ]
      },
      {
        "text": "How Will Generative AI Change the Video Game Industry?",
        "item-id": "i3662",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "How Will Generative AI Change the Video Game Industry?",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Executives see AI improving quality and speeding time to market but not alleviating the talent shortage."
          ],
          [
            "Access Date",
            "2024-01-14 11:24:11"
          ],
          [
            "Creators",
            "Anders Christofferson, Andre James, Tom Rowland, Imogen Rey"
          ],
          [
            "Date",
            "2023-09-14 2023-09-14T13:00:00.0000000+00:00"
          ],
          [
            "Extra",
            "Section: Brief"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Publication Title",
            "Bain"
          ],
          [
            "Title",
            "How Will Generative AI Change the Video Game Industry?"
          ],
          [
            "URL",
            "https://www.bain.com/insights/how-will-generative-ai-change-the-video-game-industry/"
          ]
        ]
      },
      {
        "text": "Large language models generate functional protein sequences across diverse families",
        "item-id": "i3661",
        "nodes": [
          {
            "text": "Madani et al_2023_Large language models generate functional protein sequences across diverse.pdf",
            "item-id": "i3691",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Madani et al_2023_Large language models generate functional protein sequences across diverse.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_V7ENJR5C/2\">Results </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7ENJR5C/6\">Discussion </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7ENJR5C/7\">Online content </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7ENJR5C/2\">Fig. 1 Artificial protein generation with conditional language modeling.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7ENJR5C/3\">Fig. 2 Generated artificial antibacterial proteins are diverse and express well in our experimental system.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7ENJR5C/5\">Fig. 3 Artificial protein sequences are functional while reaching as low as 31% identity to any known protein, exhibit comparable catalytic efficiencies to a highly-evolved natural protein, and demonstrate similar structures to known natural folds.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_V7ENJR5C/6\">Fig. 4 Applicability of conditional language modeling to other protein systems.</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Madani et al_2023_Large language models generate functional protein sequences across diverse.pdf"
              ]
            ],
            "resource": "storage/i3691.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Large language models generate functional protein sequences across diverse families",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from >19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase."
          ],
          [
            "Access Date",
            "2024-01-14 11:35:48"
          ],
          [
            "Creators",
            "Ali Madani, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M. Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z. Sun, Richard Socher, James S. Fraser, Nikhil Naik"
          ],
          [
            "DOI",
            "10.1038/s41587-022-01618-2"
          ],
          [
            "Date",
            "2023-08-00 2023-08"
          ],
          [
            "Extra",
            "Number: 8\nPublisher: Nature Publishing Group"
          ],
          [
            "ISSN",
            "1546-1696"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Journal Abbreviation",
            "Nat Biotechnol"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.nature.com"
          ],
          [
            "Pages",
            "1099-1106"
          ],
          [
            "Publication Title",
            "Nature Biotechnology"
          ],
          [
            "Rights",
            "2023 The Author(s), under exclusive licence to Springer Nature America, Inc."
          ],
          [
            "Title",
            "Large language models generate functional protein sequences across diverse families"
          ],
          [
            "URL",
            "https://www.nature.com/articles/s41587-022-01618-2"
          ],
          [
            "Volume",
            "41"
          ]
        ],
        "resource": "storage/i3691.pdf",
        "selectable": false
      },
      {
        "text": "Revolutionizing Healthcare",
        "item-id": "i3655",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "Revolutionizing Healthcare",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "ChatGPT has the potential to improve patient outcomes, reduce healthcare costs, and enhance overall health and wellness experiences. Find out how this AI model can be used to transform healthcare worldwide."
          ],
          [
            "Access Date",
            "2024-01-14 11:09:28"
          ],
          [
            "Creators",
            "Bernard Marr"
          ],
          [
            "Date",
            "2023-03-02 2023-03-02"
          ],
          [
            "Extra",
            "Section: Enterprise Tech"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Publication Title",
            "Forbes"
          ],
          [
            "Short Title",
            "Revolutionizing Healthcare"
          ],
          [
            "Title",
            "Revolutionizing Healthcare: The Top 14 Uses Of ChatGPT In Medicine And Wellness"
          ],
          [
            "URL",
            "https://www.forbes.com/sites/bernardmarr/2023/03/02/revolutionizing-healthcare-the-top-14-uses-of-chatgpt-in-medicine-and-wellness/"
          ]
        ]
      },
      {
        "text": "There Are Now 15,000 Deepfake Videos on Social Media. Yes, You Should Worry.",
        "item-id": "i1052",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "There Are Now 15,000 Deepfake Videos on Social Media. Yes, You Should Worry.",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "After someone superimposes the face of a president or celebrity onto someone else\u2019s body (often in a pornographic movie), they can upload them to Facebook or any other social media platform."
          ],
          [
            "Access Date",
            "2021-10-18 15:18:14"
          ],
          [
            "Creators",
            "John Brandon"
          ],
          [
            "Date",
            "2019-10-08 2019-10-08"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.forbes.com"
          ],
          [
            "Publication Title",
            "Forbes"
          ],
          [
            "Section",
            "Social Media"
          ],
          [
            "Title",
            "There Are Now 15,000 Deepfake Videos on Social Media. Yes, You Should Worry."
          ],
          [
            "URL",
            "https://www.forbes.com/sites/johnbbrandon/2019/10/08/there-are-now-15000-deepfake-videos-on-social-media-yes-you-should-worry/"
          ]
        ]
      },
      {
        "text": "What are deepfakes \u2013\u00a0and how can you spot them?",
        "item-id": "i1058",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "What are deepfakes \u2013\u00a0and how can you spot them?",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "AI-generated fake videos are becoming more common (and convincing). Here\u2019s why we should be worried"
          ],
          [
            "Access Date",
            "2021-10-18 14:51:00"
          ],
          [
            "Creators",
            "Ian Sample"
          ],
          [
            "Date",
            "2020-01-13 2020-01-13"
          ],
          [
            "ISSN",
            "0261-3077"
          ],
          [
            "Language",
            "en-GB"
          ],
          [
            "Library Catalog",
            "The Guardian"
          ],
          [
            "Publication Title",
            "The Guardian"
          ],
          [
            "Section",
            "News"
          ],
          [
            "Title",
            "What are deepfakes \u2013\u00a0and how can you spot them?"
          ],
          [
            "URL",
            "https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them"
          ]
        ]
      },
      {
        "text": "What\u2019s in a text-to-image prompt?",
        "item-id": "i3654",
        "nodes": [
          {
            "text": "Dehouche_Dehouche_2023_What\u2019s in a text-to-image prompt.pdf",
            "item-id": "i3692",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dehouche_Dehouche_2023_What\u2019s in a text-to-image prompt.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dehouche_Dehouche_2023_What\u2019s in a text-to-image prompt.pdf"
              ]
            ],
            "resource": "storage/i3692.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "What\u2019s in a text-to-image prompt?",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text-to-Image artificial intelligence (AI) recently saw a major breakthrough with the release of Dall-E and its open-source counterpart, Stable Diffusion. These programs allow anyone to create original visual art pieces by simply providing descriptions in natural language (prompts). Using a sample of 72,980 Stable Diffusion prompts, we propose a formalization of this new medium of art creation and assess its potential for teaching the history of art, aesthetics, and technique. Our findings indicate that text-to-Image AI has the potential to revolutionize the way art is taught, offering new, cost-effective possibilities for experimentation and expression. However, it also raises important questions about the ownership of artistic works. As more and more art is created using these programs, it will be crucial to establish new legal and economic models to protect the rights of artists."
          ],
          [
            "Access Date",
            "2024-01-14 11:15:59"
          ],
          [
            "Creators",
            "Nassim Dehouche, Kullathida Dehouche"
          ],
          [
            "DOI",
            "10.1016/j.heliyon.2023.e16757"
          ],
          [
            "Date",
            "2023-06-01 2023-06-01"
          ],
          [
            "ISSN",
            "2405-8440"
          ],
          [
            "Issue",
            "6"
          ],
          [
            "Journal Abbreviation",
            "Heliyon"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "e16757"
          ],
          [
            "Publication Title",
            "Heliyon"
          ],
          [
            "Short Title",
            "What\u2019s in a text-to-image prompt?"
          ],
          [
            "Title",
            "What\u2019s in a text-to-image prompt? The potential of stable diffusion in visual arts education"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S2405844023039646"
          ],
          [
            "Volume",
            "9"
          ]
        ],
        "resource": "storage/i3692.pdf",
        "selectable": false
      },
      {
        "text": "You thought fake news was bad?",
        "item-id": "i1065",
        "icon": "glyphicon glyphicon-unchecked",
        "item_title": "You thought fake news was bad?",
        "item_type": "newspaperArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Technology can make it look as if anyone has said or done anything. Is it the next wave of (mis)information warfare?"
          ],
          [
            "Access Date",
            "2021-10-18 14:13:26"
          ],
          [
            "Creators",
            "Oscar Schwartz"
          ],
          [
            "Date",
            "2018-11-12 2018-11-12T10:00:35.000Z"
          ],
          [
            "ISSN",
            "0261-3077"
          ],
          [
            "Language",
            "en-GB"
          ],
          [
            "Library Catalog",
            "The Guardian"
          ],
          [
            "Publication Title",
            "The Guardian"
          ],
          [
            "Section",
            "Technology"
          ],
          [
            "Short Title",
            "You thought fake news was bad?"
          ],
          [
            "Title",
            "You thought fake news was bad? Deep fakes are where truth goes to die"
          ],
          [
            "URL",
            "https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth"
          ]
        ]
      }
    ],
    "item_title": "News",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Optimization",
    "item-id": "c29,i1822",
    "nodes": [
      {
        "text": "SGDR",
        "item-id": "i1822",
        "nodes": [
          {
            "text": "Loshchilov_Hutter_2022_SGDR.pdf",
            "item-id": "i2060",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Loshchilov_Hutter_2022_SGDR.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_62CTR5EK/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_62CTR5EK/3\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/3\">Restarts in gradient-free optimization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/3\">Restarts in gradient-based optimization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/4\">Stochastic Gradient Descent with warm restarts (SGDR)</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_62CTR5EK/4\">Experimental results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/4\">Experimental settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/6\">Single-Model Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/7\">Ensemble Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/8\">Experiments on a dataset of EEG recordings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/8\">Preliminary experiments on a downsampled ImageNet dataset</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/10\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/10\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/11\">Acknowledgments</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_62CTR5EK/14\">Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_62CTR5EK/14\">50k vs 100k examples per epoch</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Loshchilov_Hutter_2022_SGDR.pdf"
              ]
            ],
            "resource": "storage/i2060.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SGDR",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}"
          ],
          [
            "Access Date",
            "2022-10-22 08:37:45"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Ilya Loshchilov, Frank Hutter"
          ],
          [
            "Date",
            "2022-07-21 2022/07/21"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "SGDR"
          ],
          [
            "Title",
            "SGDR: Stochastic Gradient Descent with Warm Restarts"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=Skq89Scxx"
          ]
        ],
        "resource": "storage/i2060.pdf",
        "selectable": false
      }
    ],
    "item_title": "Optimization",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Package",
    "item-id": "c32,i3268",
    "nodes": [
      {
        "text": "Array programming with NumPy",
        "item-id": "i2269",
        "nodes": [
          {
            "text": "Harris et al_2020_Array programming with NumPy.pdf",
            "item-id": "i2271",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Harris et al_2020_Array programming with NumPy.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Harris et al_2020_Array programming with NumPy.pdf"
              ]
            ],
            "resource": "storage/i2271.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Array programming with NumPy",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis."
          ],
          [
            "Access Date",
            "2023-04-02 09:59:18"
          ],
          [
            "Creators",
            "Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, Travis E. Oliphant"
          ],
          [
            "DOI",
            "10.1038/s41586-020-2649-2"
          ],
          [
            "Date",
            "2020-09-00 2020-09"
          ],
          [
            "Extra",
            "Number: 7825\nPublisher: Nature Publishing Group"
          ],
          [
            "ISSN",
            "1476-4687"
          ],
          [
            "Issue",
            "7825"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.nature.com"
          ],
          [
            "Pages",
            "357-362"
          ],
          [
            "Publication Title",
            "Nature"
          ],
          [
            "Rights",
            "2020 The Author(s)"
          ],
          [
            "Title",
            "Array programming with NumPy"
          ],
          [
            "URL",
            "https://www.nature.com/articles/s41586-020-2649-2"
          ],
          [
            "Volume",
            "585"
          ]
        ],
        "resource": "storage/i2271.pdf",
        "selectable": false
      },
      {
        "text": "Converting video formats with FFmpeg",
        "item-id": "i1218",
        "icon": "glyphicon glyphicon-file",
        "item_title": "Converting video formats with FFmpeg",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "FFmpeg is a mini Swiss Army knife of format conversion tools."
          ],
          [
            "Creators",
            "Suramya Tomar"
          ],
          [
            "Date",
            "2006-06-01 June 1, 2006"
          ],
          [
            "ISSN",
            "1075-3583"
          ],
          [
            "Issue",
            "146"
          ],
          [
            "Journal Abbreviation",
            "Linux J."
          ],
          [
            "Library Catalog",
            "June 2006"
          ],
          [
            "Pages",
            "10"
          ],
          [
            "Publication Title",
            "Linux Journal"
          ],
          [
            "Title",
            "Converting video formats with FFmpeg"
          ],
          [
            "Volume",
            "2006"
          ]
        ]
      },
      {
        "text": "D\u00b3 Data-Driven Documents",
        "item-id": "i2333",
        "nodes": [
          {
            "text": "Bostock et al_2011_D\u00b3 Data-Driven Documents.pdf",
            "item-id": "i2345",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bostock et al_2011_D\u00b3 Data-Driven Documents.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bostock et al_2011_D\u00b3 Data-Driven Documents.pdf"
              ]
            ],
            "resource": "storage/i2345.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "D\u00b3 Data-Driven Documents",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-specific abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efficiency and retaining powerful declarative components. Immediate evaluation of operators further simplifies debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations."
          ],
          [
            "Creators",
            "Michael Bostock, Vadim Ogievetsky, Jeffrey Heer"
          ],
          [
            "DOI",
            "10.1109/TVCG.2011.185"
          ],
          [
            "Date",
            "2011-12-00 2011-12"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Visualization and Computer Graphics"
          ],
          [
            "ISSN",
            "1941-0506"
          ],
          [
            "Issue",
            "12"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "2301-2309"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Visualization and Computer Graphics"
          ],
          [
            "Title",
            "D\u00b3 Data-Driven Documents"
          ],
          [
            "Volume",
            "17"
          ]
        ],
        "resource": "storage/i2345.pdf",
        "selectable": false
      },
      {
        "text": "FaceX-Zoo",
        "item-id": "i1871",
        "nodes": [
          {
            "text": "Wang et al_2021_FaceX-Zoo.pdf",
            "item-id": "i2029",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2021_FaceX-Zoo.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XRRYIWLM/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/1\">2 Characteristics</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/2\">3 Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/2\">3.1 Training Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/3\">3.2 Evaluation Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/3\">3.3 Face SDK Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/3\">3.4 Additional Module</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/3\">4 Application Examples</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/3\">4.1 Regular Face Recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/4\">4.2 Shallow Face Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/4\">4.3 Masked Face Recognition</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/4\">5 Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XRRYIWLM/4\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2021_FaceX-Zoo.pdf"
              ]
            ],
            "resource": "storage/i2029.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FaceX-Zoo",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Due to the remarkable progress in recent years, deep face recognition is in great need of public support for practical model production and further exploration. The demands are in three folds, including 1) modular training scheme, 2) standard and automatic evaluation, and 3) groundwork of deployment. To meet these demands, we present a novel open-source project, named FaceX-Zoo, which is constructed with modular and scalable design, and oriented to the academic and industrial community of face-related analysis. FaceX-Zoo provides 1) the training module with various choices of backbone and supervisory head; 2) the evaluation module that enables standard and automatic test on most popular benchmarks; 3) the module of simple yet fully functional face SDK for the validation and primary application of end-to-end face recognition; 4) the additional module that integrates a group of useful tools. Based on these easy-to-use modules, FaceX-Zoo can help the community to easily build stateof-the-art solutions for deep face recognition and, such like the newly-emerged challenge of masked face recognition caused by the worldwide COVID-19 pandemic. Besides, FaceX-Zoo can be easily upgraded and scaled up along with further exploration in face related fields. The source codes and models have been released and received over 900 stars at https://github.com/JDAI-CV/FaceX-Zoo."
          ],
          [
            "Access Date",
            "2022-10-28"
          ],
          [
            "Creators",
            "Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, Tao Mei"
          ],
          [
            "DOI",
            "10.1145/3474085.3478324"
          ],
          [
            "Date",
            "2021-10-17 2021-10-17"
          ],
          [
            "ISBN",
            "978-1-4503-8651-7"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "3779\u20133782"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 29th ACM International Conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '21"
          ],
          [
            "Short Title",
            "FaceX-Zoo"
          ],
          [
            "Title",
            "FaceX-Zoo: A PyTorch Toolbox for Face Recognition"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3474085.3478324"
          ]
        ],
        "resource": "storage/i2029.pdf",
        "selectable": false
      },
      {
        "text": "Generalized End-to-End Loss for Speaker Verification",
        "item-id": "i1028",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n3289",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>Resemblyzer</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf",
            "item-id": "i1039",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf"
              ]
            ],
            "resource": "storage/i1039.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generalized End-to-End Loss for Speaker Verification",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., \u201cOK Google\u201d and \u201cHey Google\u201d) as well as multiple dialects."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8462665"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4879-4883"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Generalized End-to-End Loss for Speaker Verification"
          ]
        ],
        "resource": "storage/i1039.pdf",
        "selectable": false
      },
      {
        "text": "LangChain",
        "item-id": "i3212",
        "icon": "glyphicon glyphicon-floppy-disk",
        "item_title": "LangChain",
        "item_type": "computerProgram",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "\u26a1 Building applications with LLMs through composability \u26a1"
          ],
          [
            "Access Date",
            "2023-11-12 17:24:04"
          ],
          [
            "Creators",
            "Harrison Chase"
          ],
          [
            "Date",
            "2022-10-00 2022-10"
          ],
          [
            "Extra",
            "original-date: 2022-10-17T02:58:36Z"
          ],
          [
            "Library Catalog",
            "GitHub"
          ],
          [
            "Programming Language",
            "Python"
          ],
          [
            "Rights",
            "MIT"
          ],
          [
            "Title",
            "LangChain"
          ],
          [
            "URL",
            "https://github.com/langchain-ai/langchain"
          ]
        ]
      },
      {
        "text": "Natural Language Processing with Python",
        "item-id": "i1029",
        "icon": "glyphicon glyphicon-book",
        "item_title": "Natural Language Processing with Python",
        "item_type": "book",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.Packed with examples and exercises, Natural Language Processing with Python will help you:Extract information from unstructured text, either to guess the topic or identify \"named entities\"Analyze linguistic structure in text, including parsing and semantic analysisAccess popular linguistic databases, including WordNet and treebanksIntegrate techniques drawn from fields as diverse as linguistics and artificial intelligenceThis book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful."
          ],
          [
            "Creators",
            "Steven Bird, Ewan Klein, Edward Loper"
          ],
          [
            "Date",
            "2009-06-12 2009-06-12"
          ],
          [
            "Extra",
            "Google-Books-ID: KGIbfiiP1i4C"
          ],
          [
            "ISBN",
            "978-0-596-55571-9"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Google Books"
          ],
          [
            "Num Pages",
            "506"
          ],
          [
            "Publisher",
            "O'Reilly Media, Inc."
          ],
          [
            "Short Title",
            "Natural Language Processing with Python"
          ],
          [
            "Title",
            "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit"
          ]
        ]
      },
      {
        "text": "OpenFace",
        "item-id": "i2511",
        "nodes": [
          {
            "text": "Baltru\u0161aitis et al_2016_OpenFace.pdf",
            "item-id": "i2709",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Baltru\u0161aitis et al_2016_OpenFace.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Baltru\u0161aitis et al_2016_OpenFace.pdf"
              ]
            ],
            "resource": "storage/i2709.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "OpenFace",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system."
          ],
          [
            "Conference Name",
            "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
          ],
          [
            "Creators",
            "Tadas Baltru\u0161aitis, Peter Robinson, Louis-Philippe Morency"
          ],
          [
            "DOI",
            "10.1109/WACV.2016.7477553"
          ],
          [
            "Date",
            "2016-03-00 2016-03"
          ],
          [
            "ISBN",
            "978-1-5090-0641-0"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-10"
          ],
          [
            "Place",
            "Lake Placid, NY, USA"
          ],
          [
            "Proceedings Title",
            "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
          ],
          [
            "Publisher",
            "IEEE"
          ],
          [
            "Short Title",
            "OpenFace"
          ],
          [
            "Title",
            "OpenFace: An open source facial behavior analysis toolkit"
          ]
        ],
        "resource": "storage/i2709.pdf",
        "selectable": false
      },
      {
        "text": "PyTorch",
        "item-id": "i1197",
        "nodes": [
          {
            "text": "Paszke et al_2019_PyTorch.pdf",
            "item-id": "i1198",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Paszke et al_2019_PyTorch.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Paszke et al_2019_PyTorch.pdf"
              ]
            ],
            "resource": "storage/i1198.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PyTorch",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
          ],
          [
            "Access Date",
            "2021-11-08 13:06:48"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "PyTorch"
          ],
          [
            "Title",
            "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i1198.pdf",
        "selectable": false
      },
      {
        "text": "ReactJS",
        "item-id": "i3268",
        "nodes": [
          {
            "text": "Rawat_Mahajan_2020_ReactJS.pdf",
            "item-id": "i3272",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rawat_Mahajan_2020_ReactJS.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_TY7AFGPF/1\">I. INTRODUCTION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/1\">II. CREATING A  REACT APPLICATION</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/1\">A. Automatic installation of ReactJS:</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/1\">B. Manual installation of ReactJS:</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">III. FEATURES</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">C. Declarative</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">D. Short and Easy Learning Curve</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">E. One-way Data Binding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">F. JSX</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">G. Virtual DOM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">H. Performance</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">IV. NPM PACKAGES FOR REACT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">I. React Router</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">J. Create React App</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">It is a CLI (Command Line Interface) tool that doesn\u2019t require any building configuration [15]. So it gives ReactJS engineers an extraordinary head start when dealing with React ventures as it empowers the designers to produce their own standards. The...</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/2\">K. React Redux</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/3\">L. Material UI</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/3\">M. React Bootstrap</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/3\">N. Axios</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/3\">V. LITERATURE SURVEY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/4\">VI. CONCLUSION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TY7AFGPF/4\">REFERENCES</a></li></ul></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rawat_Mahajan_2020_ReactJS.pdf"
              ]
            ],
            "resource": "storage/i3272.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ReactJS",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-11-21 10:00:40"
          ],
          [
            "Creators",
            "Prateek Rawat, Archana N. Mahajan"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Library Catalog",
            "Google Scholar"
          ],
          [
            "Pages",
            "698\u2013702"
          ],
          [
            "Publication Title",
            "International Journal of Innovative Science and Research Technology"
          ],
          [
            "Short Title",
            "ReactJS"
          ],
          [
            "Title",
            "ReactJS: A modern web development framework"
          ],
          [
            "URL",
            "https://ijisrt.com/assets/upload/files/IJISRT20NOV485.pdf"
          ],
          [
            "Volume",
            "5"
          ]
        ],
        "resource": "storage/i3272.pdf",
        "selectable": false
      },
      {
        "text": "Real Time Speech Enhancement in the Waveform Domain",
        "item-id": "i3211",
        "nodes": [
          {
            "text": "D\u00e9fossez et al_2020_Real Time Speech Enhancement in the Waveform Domain.pdf",
            "item-id": "i3283",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "D\u00e9fossez et al_2020_Real Time Speech Enhancement in the Waveform Domain.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_TX9TB8KS/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/1\">2  Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/1\">2.1  Notations and problem settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/1\">2.2  Demucs architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/2\">2.3  Objective</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/2\">3  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/2\">3.1  Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/3\">3.2  Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/4\">3.3  Ablation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/4\">3.4  Real-Time Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/4\">3.5  The effect on ASR models</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/4\">4  Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/4\">5  Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_TX9TB8KS/5\">6  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "D\u00e9fossez et al_2020_Real Time Speech Enhancement in the Waveform Domain.pdf"
              ]
            ],
            "resource": "storage/i3283.pdf"
          },
          {
            "text": "[TLDR] Empirical evidence shows that the proposed causal speech enhancement model, based on an encoder-decoder architect",
            "item-id": "n3285",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "[TLDR] Empirical evidence shows that the proposed causal speech enhancement model, based on an encoder-decoder architect",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">[TLDR] Empirical evidence shows that the proposed causal speech enhancement model, based on an encoder-decoder architecture with skip-connections, is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb.</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Real Time Speech Enhancement in the Waveform Domain",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections. It is optimized on both time and frequency domains, using multiple loss functions. Empirical evidence shows that it is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb. Additionally, we suggest a set of data augmentation techniques applied directly on the raw waveform which further improve model performance and its generalization abilities. We perform evaluations on several standard benchmarks, both using objective metrics and human judgements. The proposed model matches state-of-the-art performance of both causal and non causal methods while working directly on the raw waveform."
          ],
          [
            "Access Date",
            "2023-11-12 18:28:21"
          ],
          [
            "Conference Name",
            "Interspeech 2020"
          ],
          [
            "Creators",
            "Alexandre D\u00e9fossez, Gabriel Synnaeve, Yossi Adi"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2020-2409"
          ],
          [
            "Date",
            "2020-10-25 2020-10-25"
          ],
          [
            "Extra",
            "Conference Name: Interspeech 2020\nPublisher: ISCA"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Semantic Scholar"
          ],
          [
            "Pages",
            "3291-3295"
          ],
          [
            "Place",
            "Shanghai, China"
          ],
          [
            "Proceedings Title",
            "Interspeech 2020"
          ],
          [
            "Title",
            "Real Time Speech Enhancement in the Waveform Domain"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2020/defossez20_interspeech.html"
          ]
        ],
        "resource": "storage/i3283.pdf",
        "selectable": false
      },
      {
        "text": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "item-id": "i3210",
        "nodes": [
          {
            "text": "Radford et al_2023_Robust Speech Recognition via Large-Scale Weak Supervision.pdf",
            "item-id": "i3288",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Radford et al_2023_Robust Speech Recognition via Large-Scale Weak Supervision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Radford et al_2023_Robust Speech Recognition via Large-Scale Weak Supervision.pdf"
              ]
            ],
            "resource": "storage/i3288.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing."
          ],
          [
            "Access Date",
            "2023-11-11 13:22:15"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever"
          ],
          [
            "Date",
            "2023-07-03 2023-07-03"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "28492-28518"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 40th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Robust Speech Recognition via Large-Scale Weak Supervision"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v202/radford23a.html"
          ]
        ],
        "resource": "storage/i3288.pdf",
        "selectable": false
      },
      {
        "text": "TensorFlow",
        "item-id": "i2366",
        "nodes": [
          {
            "text": "Abadi et al_2016_TensorFlow.pdf",
            "item-id": "i2477",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Abadi et al_2016_TensorFlow.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Abadi et al_2016_TensorFlow.pdf"
              ]
            ],
            "resource": "storage/i2477.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TensorFlow",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications."
          ],
          [
            "Access Date",
            "2023-05-06"
          ],
          [
            "Creators",
            "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
          ],
          [
            "Date",
            "2016-00-02 \u5341\u4e00\u6708 2, 2016"
          ],
          [
            "ISBN",
            "978-1-931971-33-1"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "265\u2013283"
          ],
          [
            "Place",
            "USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation"
          ],
          [
            "Publisher",
            "USENIX Association"
          ],
          [
            "Series",
            "OSDI'16"
          ],
          [
            "Short Title",
            "TensorFlow"
          ],
          [
            "Title",
            "TensorFlow: a system for large-scale machine learning"
          ]
        ],
        "resource": "storage/i2477.pdf",
        "selectable": false
      }
    ],
    "item_title": "Package",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Pose Estimation",
    "item-id": "c41,i2814",
    "nodes": [
      {
        "text": "Associative Embedding",
        "item-id": "i2809",
        "nodes": [
          {
            "text": "Newell et al_2017_Associative Embedding.pdf",
            "item-id": "i2870",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Newell et al_2017_Associative Embedding.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Newell et al_2017_Associative Embedding.pdf"
              ]
            ],
            "resource": "storage/i2870.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Associative Embedding",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that  produces pixel-wise predictions. We show how to apply this method to  multi-person pose estimation and report state-of-the-art performance  on the MPII and MS-COCO datasets."
          ],
          [
            "Access Date",
            "2023-07-17 05:46:22"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Alejandro Newell, Zhiao Huang, Jia Deng"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "Associative Embedding"
          ],
          [
            "Title",
            "Associative Embedding: End-to-End Learning for Joint Detection and Grouping"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2017/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i2870.pdf",
        "selectable": false
      },
      {
        "text": "Cascaded Pyramid Network for Multi-Person Pose Estimation",
        "item-id": "i2813",
        "nodes": [
          {
            "text": "Chen et al_2018_Cascaded Pyramid Network for Multi-Person Pose Estimation.pdf",
            "item-id": "i2874",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2018_Cascaded Pyramid Network for Multi-Person Pose Estimation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2018_Cascaded Pyramid Network for Multi-Person Pose Estimation.pdf"
              ]
            ],
            "resource": "storage/i2874.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Cascaded Pyramid Network for Multi-Person Pose Estimation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The topic of multi-person pose estimation has beenlargely improved recently, especially with the developmentof convolutional neural network. However, there still exista lot of challenging cases, such as occluded keypoints, in-visible keypoints and complex background, which cannot bewell addressed. In this paper, we present a novel networkstructure called Cascaded Pyramid Network (CPN) whichtargets to relieve the problem from these \u201chard\u201d keypoints.More specifically, our algorithm includes two stages: Glob-alNet and RefineNet. GlobalNet is a feature pyramid net-work which can successfully localize the \u201csimple\u201d key-points like eyes and hands but may fail to precisely rec-ognize the occluded or invisible keypoints. Our RefineNettries explicitly handling the \u201chard\u201d keypoints by integrat-ing all levels of feature representations from the Global-Net together with an online hard keypoint mining loss. Ingeneral, to address the multi-person pose estimation prob-lem, a top-down pipeline is adopted to first generate a setof human bounding boxes based on a detector, followed byour CPN for keypoint localization in each human boundingbox. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with averageprecision at 73.0 on the COCO test-dev dataset and 72.1 onthe COCO test-challenge dataset, which is a 19% relativeimprovement compared with 60.5 from the COCO 2016 key-point challenge. Code and the detection results for personused will be publicly available for further research."
          ],
          [
            "Access Date",
            "2023-07-17 05:44:11"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Jian Sun"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7103-7112"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Cascaded Pyramid Network for Multi-Person Pose Estimation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Cascaded_Pyramid_Network_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i2874.pdf",
        "selectable": false
      },
      {
        "text": "Convolutional Pose Machines",
        "item-id": "i2814",
        "nodes": [
          {
            "text": "Wei et al_2016_Convolutional Pose Machines.pdf",
            "item-id": "i2876",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wei et al_2016_Convolutional Pose Machines.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wei et al_2016_Convolutional Pose Machines.pdf"
              ]
            ],
            "resource": "storage/i2876.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Convolutional Pose Machines",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets."
          ],
          [
            "Access Date",
            "2023-07-17 05:43:06"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4724-4732"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Convolutional Pose Machines"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i2876.pdf",
        "selectable": false
      },
      {
        "text": "Deep High-Resolution Representation Learning for Human Pose Estimation",
        "item-id": "i2811",
        "nodes": [
          {
            "text": "Sun et al_2019_Deep High-Resolution Representation Learning for Human Pose Estimation.pdf",
            "item-id": "i2872",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sun et al_2019_Deep High-Resolution Representation Learning for Human Pose Estimation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sun et al_2019_Deep High-Resolution Representation Learning for Human Pose Estimation.pdf"
              ]
            ],
            "resource": "storage/i2872.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep High-Resolution Representation Learning for Human Pose Estimation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch."
          ],
          [
            "Access Date",
            "2023-07-17 05:45:00"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5693-5703"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Deep High-Resolution Representation Learning for Human Pose Estimation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Deep_High-Resolution_Representation_Learning_for_Human_Pose_Estimation_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i2872.pdf",
        "selectable": false
      },
      {
        "text": "HigherHRNet",
        "item-id": "i2799",
        "nodes": [
          {
            "text": "Cheng et al_2020_HigherHRNet.pdf",
            "item-id": "i2869",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cheng et al_2020_HigherHRNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cheng et al_2020_HigherHRNet.pdf"
              ]
            ],
            "resource": "storage/i2869.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HigherHRNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene."
          ],
          [
            "Access Date",
            "2023-07-17 05:47:50"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang, Lei Zhang"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5386-5395"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "HigherHRNet"
          ],
          [
            "Title",
            "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_HigherHRNet_Scale-Aware_Representation_Learning_for_Bottom-Up_Human_Pose_Estimation_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i2869.pdf",
        "selectable": false
      },
      {
        "text": "Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields",
        "item-id": "i2810",
        "nodes": [
          {
            "text": "Cao et al_2017_Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.pdf",
            "item-id": "i2871",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cao et al_2017_Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cao et al_2017_Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.pdf"
              ]
            ],
            "resource": "storage/i2871.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency."
          ],
          [
            "Access Date",
            "2023-07-17 05:46:02"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7291-7299"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i2871.pdf",
        "selectable": false
      },
      {
        "text": "Simple Baselines for Human Pose Estimation and Tracking",
        "item-id": "i2812",
        "nodes": [
          {
            "text": "Xiao et al_2018_Simple Baselines for Human Pose Estimation and Tracking.pdf",
            "item-id": "i2873",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xiao et al_2018_Simple Baselines for Human Pose Estimation and Tracking.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xiao et al_2018_Simple Baselines for Human Pose Estimation and Tracking.pdf"
              ]
            ],
            "resource": "storage/i2873.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Simple Baselines for Human Pose Estimation and Tracking",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch."
          ],
          [
            "Access Date",
            "2023-07-17 05:44:35"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Bin Xiao, Haiping Wu, Yichen Wei"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "466-481"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Title",
            "Simple Baselines for Human Pose Estimation and Tracking"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i2873.pdf",
        "selectable": false
      },
      {
        "text": "Stacked Hourglass Networks for Human Pose Estimation",
        "item-id": "i2801",
        "nodes": [
          {
            "text": "Newell et al_2016_Stacked Hourglass Networks for Human Pose Estimation.pdf",
            "item-id": "i2875",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Newell et al_2016_Stacked Hourglass Networks for Human Pose Estimation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LFTMI9GG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/4\">3 Network Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/4\">3.1 Hourglass Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/5\">3.2 Layer Implementation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/6\">3.3 Stacked Hourglass with Intermediate Supervision</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/7\">3.4 Training Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/9\">4 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/9\">4.1 Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/10\">4.2 Ablation Experiments</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/12\">5 Further Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/12\">5.1 Multiple People</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/13\">5.2 Occlusion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/14\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LFTMI9GG/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Newell et al_2016_Stacked Hourglass Networks for Human Pose Estimation.pdf"
              ]
            ],
            "resource": "storage/i2875.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Stacked Hourglass Networks for Human Pose Estimation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Alejandro Newell, Kaiyu Yang, Jia Deng, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46484-8_29"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46484-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "483-499"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Stacked Hourglass Networks for Human Pose Estimation"
          ]
        ],
        "resource": "storage/i2875.pdf",
        "selectable": false
      },
      {
        "text": "ViTPose+",
        "item-id": "i2798",
        "nodes": [
          {
            "text": "Comment: Extension of ViTPose paper",
            "item-id": "n2866",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Extension of ViTPose paper",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Extension of ViTPose paper</div>",
            "node_type": "note"
          },
          {
            "text": "Xu et al_2022_ViTPose+.pdf",
            "item-id": "i2865",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2022_ViTPose+.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GJHJERJK/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GJHJERJK/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GJHJERJK/2\">2.1 Representative pose estimation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/2\">2.1.1 Top-down methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/3\">2.1.2 Bottom-up methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/3\">2.2 Vision transformer pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/3\">2.3 Foundation models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/3\">2.4 Comparison to the conference version</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GJHJERJK/4\">3 ViTPose</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/4\">3.1 The simplicity of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/4\">3.2 The scalability of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/5\">3.3 The flexibility of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/5\">3.4 The transferability of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/6\">3.5 ViTPose+</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GJHJERJK/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/6\">4.1 Datasets and evaluation metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/6\">4.2 Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/7\">4.3 Ablation studies of ViTPose and analysis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GJHJERJK/8\">4.4 Ablation studies of ViTPose+ and analysis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/8\">4.4.1 Different settings of ViTPose+</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GJHJERJK/9\">4.5 Comparison with SOTA methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/9\">4.5.1 The performance on MS COCO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/10\">4.5.2 The performance on other datasets</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/12\">4.6 Subjective results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/12\">4.7 Data efficiency analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/14\">4.8 Visualization and analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/15\">5 Limitations and discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/15\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GJHJERJK/16\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2022_ViTPose+.pdf"
              ]
            ],
            "resource": "storage/i2865.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ViTPose+",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we show the surprisingly good properties of plain vision transformers for body pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model dubbed ViTPose. Specifically, ViTPose employs the plain and non-hierarchical vision transformer as an encoder to encode features and a lightweight decoder to decode body keypoints in either a top-down or a bottom-up manner. It can be scaled up from about 20M to 1B parameters by taking advantage of the scalable model capacity and high parallelism of the vision transformer, setting a new Pareto front for throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, and pre-training and fine-tuning strategy. Based on the flexibility, a novel ViTPose+ model is proposed to deal with heterogeneous body keypoint categories in different types of body pose estimation tasks via knowledge factorization, i.e., adopting task-agnostic and task-specific feed-forward networks in the transformer. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our ViTPose model outperforms representative methods on the challenging MS COCO Human Keypoint Detection benchmark at both top-down and bottom-up settings. Furthermore, our ViTPose+ model achieves state-of-the-art performance simultaneously on a series of body pose estimation tasks, including MS COCO, AI Challenger, OCHuman, MPII for human keypoint detection, COCO-Wholebody for whole-body keypoint detection, as well as AP-10K and APT-36K for animal keypoint detection, without sacrificing inference speed."
          ],
          [
            "Access Date",
            "2023-07-17 05:59:40"
          ],
          [
            "Archiveid",
            "arXiv:2212.04246"
          ],
          [
            "Creators",
            "Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2212.04246"
          ],
          [
            "Date",
            "2022-12-07 2022-12-07"
          ],
          [
            "Extra",
            "arXiv:2212.04246 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ViTPose+"
          ],
          [
            "Title",
            "ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2212.04246"
          ]
        ],
        "resource": "storage/i2865.pdf",
        "selectable": false
      },
      {
        "text": "ViTPose: Simple vision transformer baselines for human pose estimation",
        "item-id": "i2807",
        "nodes": [
          {
            "text": "Xu et al_2022_ViTPose.pdf",
            "item-id": "i2867",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2022_ViTPose.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_VYMJVJYN/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/3\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/3\">Vision transformer for pose estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/3\">Vision transformer pre-training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/3\">ViTPose</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/3\">The simplicity of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/4\">The scalability of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/5\">The flexibility of ViTPose</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/5\">The transferability of ViTPose</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/6\">Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/6\">Ablation study and analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/9\">Comparison with SOTA methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/10\">Subjective results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/10\">Limitation and Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VYMJVJYN/10\">Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2022_ViTPose.pdf"
              ]
            ],
            "resource": "storage/i2867.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ViTPose: Simple vision transformer baselines for human pose estimation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art. The code and models are available at https://github.com/ViTAE-Transformer/ViTPose."
          ],
          [
            "Conference Name",
            "Advances in neural information processing systems"
          ],
          [
            "Creators",
            "Yufei Xu, Jing Zhang, Qiming ZHANG, Dacheng Tao, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Pages",
            "38571\u201338584"
          ],
          [
            "Proceedings Title",
            "Advances in neural information processing systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "ViTPose: Simple vision transformer baselines for human pose estimation"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2022/file/fbb10d319d44f8c3b4720873e4177c65-Paper-Conference.pdf"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i2867.pdf",
        "selectable": false
      }
    ],
    "item_title": "Pose Estimation",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Reasoning",
    "item-id": "c45,iNone",
    "nodes": [
      {
        "text": "Image Reasoning",
        "item-id": "c46,i3463",
        "nodes": [
          {
            "text": "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection",
            "item-id": "i3247",
            "nodes": [
              {
                "text": "Yu et al_2022_A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual.pdf",
                "item-id": "i3377",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yu et al_2022_A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yu et al_2022_A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual.pdf"
                  ]
                ],
                "resource": "storage/i3377.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "This paper aims to leverage symbolic knowledge to improve the performance and interpretability of the Visual Relationship Detection (VRD) models. Existing VRD methods based on deep learning suffer from the problems of poor performance on insufficient labeled examples and lack of interpretability. To overcome the aforementioned weaknesses, we integrate symbolic knowledge into deep learning models and propose a bi-level probabilistic graphical reasoning framework called BPGR. Specifically, in the high-level structure, we take the objects and relationships detected by the VRD model as hidden variables (reasoning results); In the low-level structure of BPGR, we use Markov Logic Networks (MLNs) to project First-Order Logic (FOL) as observed variables (symbolic knowledge) to correct error reasoning results. We adopt a variational EM algorithm for optimization. Experiments results show that our BPGR improves the performance of the VRD models. In particular, BPGR can also provide easy-to-understand insights for reasoning results to show interpretability."
              ],
              [
                "Access Date",
                "2023-12-09 09:14:49"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Dongran Yu, Bo Yang, Qianhao Wei, Anchen Li, Shirui Pan"
              ],
              [
                "Date",
                "2022-00-00 2022"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "10609-10618"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Title",
                "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2022/html/Yu_A_Probabilistic_Graphical_Model_Based_on_Neural-Symbolic_Reasoning_for_Visual_CVPR_2022_paper.html"
              ]
            ],
            "resource": "storage/i3377.pdf",
            "selectable": false
          },
          {
            "text": "Compositional Attention Networks for Machine Reasoning",
            "item-id": "i3262",
            "nodes": [
              {
                "text": "Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf",
                "item-id": "i3394",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JAHGBETI/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/3\">Compositional Attention Networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/3\">The Input Unit</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/3\">The MAC cell</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/4\">The Control Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/5\">The Read Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/6\">The Write Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/8\">Discussion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/8\">The Output Unit</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/9\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/10\">Data Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/11\">CLEVR Humans - Natural Language Questions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/11\">Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/13\">Interpretability</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/14\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/17\">Details of Input Unit</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/17\">The Query Unit</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/17\">The Image Unit</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Implementation and Training details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Further discussion of related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Module Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/18\">Augmented Convolutional Neural Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/19\">Memory and Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JAHGBETI/20\">Attention vs. Convolution</a></li></ul></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf"
                  ]
                ],
                "resource": "storage/i3394.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Compositional Attention Networks for Machine Reasoning",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results."
              ],
              [
                "Access Date",
                "2023-12-09 01:02:47"
              ],
              [
                "Conference Name",
                "International Conference on Learning Representations"
              ],
              [
                "Creators",
                "Drew A. Hudson, Christopher D. Manning"
              ],
              [
                "Date",
                "2018-02-15 2018/02/15"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Title",
                "Compositional Attention Networks for Machine Reasoning"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=S1Euwz-Rb"
              ]
            ],
            "resource": "storage/i3394.pdf",
            "selectable": false
          },
          {
            "text": "Explainable Neural Computation via Stack Neural Module Networks",
            "item-id": "i3265",
            "nodes": [
              {
                "text": "Hu et al_2018_Explainable Neural Computation via Stack Neural Module Networks.pdf",
                "item-id": "i3397",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Hu et al_2018_Explainable Neural Computation via Stack Neural Module Networks.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Hu et al_2018_Explainable Neural Computation via Stack Neural Module Networks.pdf"
                  ]
                ],
                "resource": "storage/i3397.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Explainable Neural Computation via Stack Neural Module Networks",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs."
              ],
              [
                "Access Date",
                "2023-12-09 00:35:04"
              ],
              [
                "Conference Name",
                "Proceedings of the European Conference on Computer Vision (ECCV)"
              ],
              [
                "Creators",
                "Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko"
              ],
              [
                "Date",
                "2018-00-00 2018"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "53-69"
              ],
              [
                "Proceedings Title",
                "Proceedings of the European Conference on Computer Vision (ECCV)"
              ],
              [
                "Title",
                "Explainable Neural Computation via Stack Neural Module Networks"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_ECCV_2018/html/Ronghang_Hu_Explainable_Neural_Computation_ECCV_2018_paper.html"
              ]
            ],
            "resource": "storage/i3397.pdf",
            "selectable": false
          },
          {
            "text": "HuggingGPT",
            "item-id": "i3249",
            "nodes": [
              {
                "text": "Shen et al_2023_HuggingGPT.pdf",
                "item-id": "i3376",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Shen et al_2023_HuggingGPT.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Shen et al_2023_HuggingGPT.pdf"
                  ]
                ],
                "resource": "storage/i3376.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "HuggingGPT",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence."
              ],
              [
                "Access Date",
                "2023-12-11 04:47:18"
              ],
              [
                "Archiveid",
                "arXiv:2303.17580"
              ],
              [
                "Creators",
                "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang"
              ],
              [
                "DOI",
                "10.48550/arXiv.2303.17580"
              ],
              [
                "Date",
                "2023-03-30 2023-03-30"
              ],
              [
                "Extra",
                "arXiv:2303.17580 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "HuggingGPT"
              ],
              [
                "Title",
                "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2303.17580"
              ]
            ],
            "resource": "storage/i3376.pdf",
            "selectable": false
          },
          {
            "text": "IdealGPT",
            "item-id": "i3248",
            "nodes": [
              {
                "text": "You et al_2023_IdealGPT.pdf",
                "item-id": "i3391",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "You et al_2023_IdealGPT.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "You et al_2023_IdealGPT.pdf"
                  ]
                ],
                "resource": "storage/i3391.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "IdealGPT",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT."
              ],
              [
                "Access Date",
                "2023-12-09 03:20:25"
              ],
              [
                "Conference Name",
                "The 2023 Conference on Empirical Methods in Natural Language Processing"
              ],
              [
                "Creators",
                "Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang, Shih-Fu Chang"
              ],
              [
                "Date",
                "2023-12-01 2023/12/01"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Proceedings Title",
                "The 2023 Conference on Empirical Methods in Natural Language Processing"
              ],
              [
                "Short Title",
                "IdealGPT"
              ],
              [
                "Title",
                "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=IvwcvJHLpc"
              ]
            ],
            "resource": "storage/i3391.pdf",
            "selectable": false
          },
          {
            "text": "Inferring and Executing Programs for Visual Reasoning",
            "item-id": "i3263",
            "nodes": [
              {
                "text": "Johnson et al_2017_Inferring and Executing Programs for Visual Reasoning.pdf",
                "item-id": "i3395",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Johnson et al_2017_Inferring and Executing Programs for Visual Reasoning.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Johnson et al_2017_Inferring and Executing Programs for Visual Reasoning.pdf"
                  ]
                ],
                "resource": "storage/i3395.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Inferring and Executing Programs for Visual Reasoning",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings."
              ],
              [
                "Access Date",
                "2023-12-09 00:38:13"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE International Conference on Computer Vision"
              ],
              [
                "Creators",
                "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick"
              ],
              [
                "Date",
                "2017-00-00 2017"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "2989-2998"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE International Conference on Computer Vision"
              ],
              [
                "Title",
                "Inferring and Executing Programs for Visual Reasoning"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_iccv_2017/html/Johnson_Inferring_and_Executing_ICCV_2017_paper.html"
              ]
            ],
            "resource": "storage/i3395.pdf",
            "selectable": false
          },
          {
            "text": "Learning to Reason",
            "item-id": "i3264",
            "nodes": [
              {
                "text": "Hu et al_2017_Learning to Reason.pdf",
                "item-id": "i3396",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Hu et al_2017_Learning to Reason.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Hu et al_2017_Learning to Reason.pdf"
                  ]
                ],
                "resource": "storage/i3396.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Learning to Reason",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer \"is there an equal number of balls and boxes?\" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question."
              ],
              [
                "Access Date",
                "2023-12-09 00:36:12"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE International Conference on Computer Vision"
              ],
              [
                "Creators",
                "Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko"
              ],
              [
                "Date",
                "2017-00-00 2017"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "804-813"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE International Conference on Computer Vision"
              ],
              [
                "Short Title",
                "Learning to Reason"
              ],
              [
                "Title",
                "Learning to Reason: End-To-End Module Networks for Visual Question Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_iccv_2017/html/Hu_Learning_to_Reason_ICCV_2017_paper.html"
              ]
            ],
            "resource": "storage/i3396.pdf",
            "selectable": false
          },
          {
            "text": "Neural Module Networks",
            "item-id": "i3251",
            "nodes": [
              {
                "text": "Andreas et al_2016_Neural Module Networks.pdf",
                "item-id": "i3401",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Andreas et al_2016_Neural Module Networks.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Andreas et al_2016_Neural Module Networks.pdf"
                  ]
                ],
                "resource": "storage/i3401.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Neural Module Networks",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning _neural module networks_, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes."
              ],
              [
                "Access Date",
                "2023-12-09 00:17:22"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein"
              ],
              [
                "Date",
                "2016-00-00 2016"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "39-48"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Title",
                "Neural Module Networks"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html"
              ]
            ],
            "resource": "storage/i3401.pdf",
            "selectable": false
          },
          {
            "text": "Neural-Symbolic VQA",
            "item-id": "i3463",
            "nodes": [
              {
                "text": "Yi et al_2018_Neural-Symbolic VQA.pdf",
                "item-id": "i3484",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yi et al_2018_Neural-Symbolic VQA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yi et al_2018_Neural-Symbolic VQA.pdf"
                  ]
                ],
                "resource": "storage/i3484.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Neural-Symbolic VQA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step."
              ],
              [
                "Access Date",
                "2024-01-08 01:31:59"
              ],
              [
                "Conference Name",
                "Advances in Neural Information Processing Systems"
              ],
              [
                "Creators",
                "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum"
              ],
              [
                "Date",
                "2018-00-00 2018"
              ],
              [
                "Library Catalog",
                "Neural Information Processing Systems"
              ],
              [
                "Proceedings Title",
                "Advances in Neural Information Processing Systems"
              ],
              [
                "Publisher",
                "Curran Associates, Inc."
              ],
              [
                "Short Title",
                "Neural-Symbolic VQA"
              ],
              [
                "Title",
                "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"
              ],
              [
                "URL",
                "https://proceedings.neurips.cc/paper_files/paper/2018/hash/5e388103a391daabe3de1d76a6739ccd-Abstract.html"
              ],
              [
                "Volume",
                "31"
              ]
            ],
            "resource": "storage/i3484.pdf",
            "selectable": false
          },
          {
            "text": "The Neuro-Symbolic Concept Learner",
            "item-id": "i3260",
            "nodes": [
              {
                "text": "Mao et al_2018_The Neuro-Symbolic Concept Learner.pdf",
                "item-id": "i3392",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Mao et al_2018_The Neuro-Symbolic Concept Learner.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Mao et al_2018_The Neuro-Symbolic Concept Learner.pdf"
                  ]
                ],
                "resource": "storage/i3392.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "The Neuro-Symbolic Concept Learner",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval."
              ],
              [
                "Access Date",
                "2023-12-09 01:05:45"
              ],
              [
                "Conference Name",
                "International Conference on Learning Representations"
              ],
              [
                "Creators",
                "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu"
              ],
              [
                "Date",
                "2018-09-27 2018/09/27"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Proceedings Title",
                "International Conference on Learning Representations"
              ],
              [
                "Short Title",
                "The Neuro-Symbolic Concept Learner"
              ],
              [
                "Title",
                "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=rJgMlhRctm"
              ]
            ],
            "resource": "storage/i3392.pdf",
            "selectable": false
          },
          {
            "text": "ViperGPT",
            "item-id": "i3035",
            "nodes": [
              {
                "text": "Comment: Website: https://viper.cs.columbia.edu/",
                "item-id": "n3068",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: Website: https://viper.cs.columbia.edu/",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">Comment: Website: https://viper.cs.columbia.edu/</div>",
                "node_type": "note"
              },
              {
                "text": "Sur\u00eds et al_2023_ViperGPT.pdf",
                "item-id": "i3067",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Sur\u00eds et al_2023_ViperGPT.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Sur\u00eds et al_2023_ViperGPT.pdf"
                  ]
                ],
                "resource": "storage/i3067.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "ViperGPT",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks."
              ],
              [
                "Access Date",
                "2023-10-05 07:40:57"
              ],
              [
                "Archiveid",
                "arXiv:2303.08128"
              ],
              [
                "Creators",
                "D\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick"
              ],
              [
                "DOI",
                "10.48550/arXiv.2303.08128"
              ],
              [
                "Date",
                "2023-03-14 2023-03-14"
              ],
              [
                "Extra",
                "arXiv:2303.08128 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "ViperGPT"
              ],
              [
                "Title",
                "ViperGPT: Visual Inference via Python Execution for Reasoning"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2303.08128"
              ]
            ],
            "resource": "storage/i3067.pdf",
            "selectable": false
          },
          {
            "text": "Visual ChatGPT",
            "item-id": "i2941",
            "nodes": [
              {
                "text": "Wu et al_2023_Visual ChatGPT.pdf",
                "item-id": "i2968",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Wu et al_2023_Visual ChatGPT.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Wu et al_2023_Visual ChatGPT.pdf"
                  ]
                ],
                "resource": "storage/i2968.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "Visual ChatGPT",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}."
              ],
              [
                "Access Date",
                "2023-07-27 01:21:19"
              ],
              [
                "Archiveid",
                "arXiv:2303.04671"
              ],
              [
                "Creators",
                "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan"
              ],
              [
                "DOI",
                "10.48550/arXiv.2303.04671"
              ],
              [
                "Date",
                "2023-03-08 2023-03-08"
              ],
              [
                "Extra",
                "arXiv:2303.04671 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "Visual ChatGPT"
              ],
              [
                "Title",
                "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2303.04671"
              ]
            ],
            "resource": "storage/i2968.pdf",
            "selectable": false
          },
          {
            "text": "Visual Programming",
            "item-id": "i3036",
            "nodes": [
              {
                "text": "Gupta_Kembhavi_2023_Visual Programming.pdf",
                "item-id": "i3070",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Gupta_Kembhavi_2023_Visual Programming.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Gupta_Kembhavi_2023_Visual Programming.pdf"
                  ]
                ],
                "resource": "storage/i3070.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Visual Programming",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform."
              ],
              [
                "Access Date",
                "2023-10-05 07:33:31"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Tanmay Gupta, Aniruddha Kembhavi"
              ],
              [
                "Date",
                "2023-00-00 2023"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "14953-14962"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Short Title",
                "Visual Programming"
              ],
              [
                "Title",
                "Visual Programming: Compositional Visual Reasoning Without Training"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2023/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.html"
              ]
            ],
            "resource": "storage/i3070.pdf",
            "selectable": false
          },
          {
            "text": "What's Left?",
            "item-id": "i3245",
            "nodes": [
              {
                "text": "Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/lef",
                "item-id": "n3368",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/lef",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">Comment: NeurIPS 2023. First two authors contributed equally. Project page: https://web.stanford.edu/~joycj/projects/left_neurips_2023</div>",
                "node_type": "note"
              },
              {
                "text": "Hsu et al_2023_What's Left.pdf",
                "item-id": "i3367",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Hsu et al_2023_What's Left.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_J5VBVVPX/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/3\">Logic-Enhanced Foundation Model (LEFT)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/3\">Domain-independent LLM interpreter</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/5\">Domain-independent first-order logic executor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/6\">Domain-specific grounding modules</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/7\">Concept learning across domains</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/7\">Accuracy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/8\">Data efficiency</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/9\">Reasoning generalization across tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/9\">Accuracy</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/10\">Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/14\">LEFT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/14\">Function definitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/15\">Broader impact</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Error bars</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/16\">Transfer task construction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Compute</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J5VBVVPX/17\">Code</a></li></ul></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Hsu et al_2023_What's Left.pdf"
                  ]
                ],
                "resource": "storage/i3367.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "What's Left?",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like \"left\" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains."
              ],
              [
                "Access Date",
                "2023-12-20 06:23:46"
              ],
              [
                "Archiveid",
                "arXiv:2310.16035"
              ],
              [
                "Creators",
                "Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu"
              ],
              [
                "DOI",
                "10.48550/arXiv.2310.16035"
              ],
              [
                "Date",
                "2023-10-24 2023-10-24"
              ],
              [
                "Extra",
                "arXiv:2310.16035 [cs, stat]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "What's Left?"
              ],
              [
                "Title",
                "What's Left? Concept Grounding with Logic-Enhanced Foundation Models"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2310.16035"
              ]
            ],
            "resource": "storage/i3367.pdf",
            "selectable": false
          }
        ],
        "item_title": "Image Reasoning",
        "item_type": null,
        "item_note": null,
        "node_type": "collection",
        "selectable": false
      },
      {
        "text": "Video Reasoning",
        "item-id": "c47,i3588",
        "nodes": [
          {
            "text": "ActivityNet-QA",
            "item-id": "i2793",
            "nodes": [
              {
                "text": "Yu et al_2019_ActivityNet-QA.pdf",
                "item-id": "i2830",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yu et al_2019_ActivityNet-QA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yu et al_2019_ActivityNet-QA.pdf"
                  ]
                ],
                "resource": "storage/i2830.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "ActivityNet-QA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos."
              ],
              [
                "Access Date",
                "2023-07-18 07:07:32"
              ],
              [
                "Creators",
                "Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, Dacheng Tao"
              ],
              [
                "DOI",
                "10.1609/aaai.v33i01.33019127"
              ],
              [
                "Date",
                "2019-07-17 2019-07-17"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "DOI.org (Crossref)"
              ],
              [
                "Pages",
                "9127-9134"
              ],
              [
                "Proceedings Title",
                "Proceedings of the AAAI Conference on Artificial Intelligence"
              ],
              [
                "Short Title",
                "ActivityNet-QA"
              ],
              [
                "Title",
                "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"
              ],
              [
                "URL",
                "https://ojs.aaai.org/index.php/AAAI/article/view/4946"
              ],
              [
                "Volume",
                "33"
              ]
            ],
            "resource": "storage/i2830.pdf",
            "selectable": false
          },
          {
            "text": "BLIP-2",
            "item-id": "i3446",
            "nodes": [
              {
                "text": "Li et al_2023_BLIP-2.pdf",
                "item-id": "i3519",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Li et al_2023_BLIP-2.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Li et al_2023_BLIP-2.pdf"
                  ]
                ],
                "resource": "storage/i3519.pdf"
              },
              {
                "text": "[TLDR] BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer",
                "item-id": "n3521",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "[TLDR] BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">[TLDR] BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</div>",
                "node_type": "note"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "BLIP-2",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
              ],
              [
                "Access Date",
                "2024-01-08 09:03:15"
              ],
              [
                "Conference Name",
                "International Conference on Machine Learning"
              ],
              [
                "Creators",
                "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
              ],
              [
                "DOI",
                "10.48550/ARXIV.2301.12597"
              ],
              [
                "Date",
                "2023-00-00 2023"
              ],
              [
                "Extra",
                "Version Number: 3"
              ],
              [
                "Library Catalog",
                "Semantic Scholar"
              ],
              [
                "Proceedings Title",
                "International Conference on Machine Learning"
              ],
              [
                "Rights",
                "Creative Commons Attribution 4.0 International"
              ],
              [
                "Short Title",
                "BLIP-2"
              ],
              [
                "Title",
                "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
              ],
              [
                "URL",
                "https://arxiv.org/abs/2301.12597"
              ]
            ],
            "resource": "storage/i3519.pdf",
            "selectable": false
          },
          {
            "text": "Bridge To Answer",
            "item-id": "i3451",
            "nodes": [
              {
                "text": "Park et al_2021_Bridge To Answer.pdf",
                "item-id": "i3523",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Park et al_2021_Bridge To Answer.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Park et al_2021_Bridge To Answer.pdf"
                  ]
                ],
                "resource": "storage/i3523.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Bridge To Answer",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "This paper presents a novel method, termed Bridge to Answer, to infer correct answers for questions about a given video by leveraging adequate graph interactions of heterogeneous crossmodal graphs. To realize this, we learn question conditioned visual graphs by exploiting the relation between video and question to enable each visual node using question-to-visual interactions to encompass both visual and linguistic cues. In addition, we propose bridged visual-to-visual interactions to incorporate two complementary visual information on appearance and motion by placing the question graph as an intermediate bridge. This bridged architecture allows reliable message passing through compositional semantics of the question to generate an appropriate answer. As a result, our method can learn the question conditioned visual representations attributed to appearance and motion that show powerful capability for video question answering. Extensive experiments prove that the proposed method provides effective and superior performance than state-of-the-art methods on several benchmarks."
              ],
              [
                "Access Date",
                "2024-01-08 08:50:10"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Jungin Park, Jiyoung Lee, Kwanghoon Sohn"
              ],
              [
                "Date",
                "2021-00-00 2021"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "15526-15535"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Short Title",
                "Bridge To Answer"
              ],
              [
                "Title",
                "Bridge To Answer: Structure-Aware Graph Interaction Network for Video Question Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2021/html/Park_Bridge_To_Answer_Structure-Aware_Graph_Interaction_Network_for_Video_Question_CVPR_2021_paper.html"
              ]
            ],
            "resource": "storage/i3523.pdf",
            "selectable": false
          },
          {
            "text": "CATER",
            "item-id": "i3462",
            "nodes": [
              {
                "text": "Girdhar_Ramanan_2019_CATER.pdf",
                "item-id": "i3568",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Girdhar_Ramanan_2019_CATER.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Girdhar_Ramanan_2019_CATER.pdf"
                  ]
                ],
                "resource": "storage/i3568.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "CATER",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controllable. Using CATER, we provide insights into some of the most recent state of the art deep video architectures."
              ],
              [
                "Access Date",
                "2024-01-08 02:12:47"
              ],
              [
                "Conference Name",
                "International Conference on Learning Representations"
              ],
              [
                "Creators",
                "Rohit Girdhar, Deva Ramanan"
              ],
              [
                "Date",
                "2019-09-23 2019/09/23"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Proceedings Title",
                "International Conference on Learning Representations"
              ],
              [
                "Short Title",
                "CATER"
              ],
              [
                "Title",
                "CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=HJgzt2VKPB"
              ]
            ],
            "resource": "storage/i3568.pdf",
            "selectable": false
          },
          {
            "text": "CLEVRER",
            "item-id": "i3466",
            "nodes": [
              {
                "text": "Yi et al_2019_CLEVRER.pdf",
                "item-id": "i3551",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yi et al_2019_CLEVRER.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DRVMZGMK/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/3\">The CLEVRER Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/3\">Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/4\">Questions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/5\">Baseline Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/5\">Model Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/6\">Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/7\">Neuro-Symbolic Dynamic Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/9\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/14\">Descriptive Question Sub-types and Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/14\">NS-DR Model Details and Training Paradigm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DRVMZGMK/15\">Extra Examples from CLEVRER</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yi et al_2019_CLEVRER.pdf"
                  ]
                ],
                "resource": "storage/i3551.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "CLEVRER",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations."
              ],
              [
                "Access Date",
                "2024-01-08 03:17:28"
              ],
              [
                "Conference Name",
                "International Conference on Learning Representations"
              ],
              [
                "Creators",
                "Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum"
              ],
              [
                "Date",
                "2019-09-25 2019/09/25"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Proceedings Title",
                "International Conference on Learning Representations"
              ],
              [
                "Short Title",
                "CLEVRER"
              ],
              [
                "Title",
                "CLEVRER: Collision Events for Video Representation and Reasoning"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=HkxYzANYDB"
              ]
            ],
            "resource": "storage/i3551.pdf",
            "selectable": false
          },
          {
            "text": "Deep Modular Co-Attention Networks for Visual Question Answering",
            "item-id": "i3453",
            "nodes": [
              {
                "text": "Yu et al_2019_Deep Modular Co-Attention Networks for Visual Question Answering.pdf",
                "item-id": "i3522",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yu et al_2019_Deep Modular Co-Attention Networks for Visual Question Answering.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yu et al_2019_Deep Modular Co-Attention Networks for Visual Question Answering.pdf"
                  ]
                ],
                "resource": "storage/i3522.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Deep Modular Co-Attention Networks for Visual Question Answering",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Visual Question Answering (VQA) requires a fine-grained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective `co-attention' model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN's effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63% overall accuracy on the test-dev set."
              ],
              [
                "Access Date",
                "2024-01-08 08:53:44"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian"
              ],
              [
                "Date",
                "2019-00-00 2019"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "6281-6290"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Title",
                "Deep Modular Co-Attention Networks for Visual Question Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.html"
              ]
            ],
            "resource": "storage/i3522.pdf",
            "selectable": false
          },
          {
            "text": "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA",
            "item-id": "i3450",
            "nodes": [
              {
                "text": "Kim et al_2020_Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in.pdf",
                "item-id": "i3525",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Kim et al_2020_Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Kim et al_2020_Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in.pdf"
                  ]
                ],
                "resource": "storage/i3525.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies."
              ],
              [
                "Access Date",
                "2024-01-08 08:45:45"
              ],
              [
                "Conference Name",
                "ACL 2020"
              ],
              [
                "Creators",
                "Hyounghun Kim, Zineng Tang, Mohit Bansal, Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault"
              ],
              [
                "DOI",
                "10.18653/v1/2020.acl-main.435"
              ],
              [
                "Date",
                "2020-07-00 2020-07"
              ],
              [
                "Library Catalog",
                "ACLWeb"
              ],
              [
                "Pages",
                "4812\u20134822"
              ],
              [
                "Place",
                "Online"
              ],
              [
                "Proceedings Title",
                "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
              ],
              [
                "Publisher",
                "Association for Computational Linguistics"
              ],
              [
                "Title",
                "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA"
              ],
              [
                "URL",
                "https://aclanthology.org/2020.acl-main.435"
              ]
            ],
            "resource": "storage/i3525.pdf",
            "selectable": false
          },
          {
            "text": "Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language",
            "item-id": "i3459",
            "nodes": [
              {
                "text": "Ding et al_2021_Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video.pdf",
                "item-id": "i3548",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Ding et al_2021_Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Ding et al_2021_Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video.pdf"
                  ]
                ],
                "resource": "storage/i3548.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "In this work, we propose a unified framework, called Visual Reasoning with Differ-entiable Physics (VRDP), that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, performs differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing benefits.  More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efficient: physical parameters can be optimized from very few videos, and even a single video can be sufficient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from a few examples."
              ],
              [
                "Access Date",
                "2024-01-08 03:24:31"
              ],
              [
                "Conference Name",
                "Advances in Neural Information Processing Systems"
              ],
              [
                "Creators",
                "Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenenbaum, Chuang Gan"
              ],
              [
                "Date",
                "2021-00-00 2021"
              ],
              [
                "Library Catalog",
                "Neural Information Processing Systems"
              ],
              [
                "Pages",
                "887\u2013899"
              ],
              [
                "Proceedings Title",
                "Advances in Neural Information Processing Systems"
              ],
              [
                "Publisher",
                "Curran Associates, Inc."
              ],
              [
                "Title",
                "Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language"
              ],
              [
                "URL",
                "https://proceedings.neurips.cc/paper/2021/hash/07845cd9aefa6cde3f8926d25138a3a2-Abstract.html"
              ],
              [
                "Volume",
                "34"
              ]
            ],
            "resource": "storage/i3548.pdf",
            "selectable": false
          },
          {
            "text": "EclipSE",
            "item-id": "i3442",
            "nodes": [
              {
                "text": "Lin et al_2022_EclipSE.pdf",
                "item-id": "i3500",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Lin et al_2022_EclipSE.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_QJIJVUDJ/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/4\">3 EclipSE: Efficient CLIP with Sound Encoding</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/4\">3.1 Obtaining Multimodal Input Embeddings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/5\">3.2 Audiovisual Attention Block</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/7\">3.3 Loss Function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/7\">3.4 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/8\">4 Experimental Setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/8\">4.1 Downstream Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/9\">4.2 Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/9\">5 Results and Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/9\">5.1 ActivityNet Captions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/10\">5.2 Results on Other Long-Range Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/11\">5.3 Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/13\">5.4 Qualitative Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/14\">6 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QJIJVUDJ/15\">References</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Lin et al_2022_EclipSE.pdf"
                  ]
                ],
                "resource": "storage/i3500.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "EclipSE",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We introduce an audiovisual method for long-range text-to-video retrieval. Unlike previous approaches designed for short video retrieval (e.g., 5\u201315 s in duration), our approach aims to retrieve minute-long videos that capture complex human actions. One challenge of standard video-only approaches is the large computational cost associated with processing hundreds of densely extracted frames from such long videos. To address this issue, we propose to replace parts of the video with compact audio cues that succinctly summarize dynamic audio events and are cheap to process. Our method, named EclipSE (Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an audiovisual video setting, by adding a unified audiovisual transformer block that captures complementary cues from the video and audio streams. In addition to being $$2.92\\times $$2.92\u00d7faster and $$2.34\\times $$2.34\u00d7memory-efficient than long-range video-only approaches, our method also achieves better text-to-video retrieval accuracy on several diverse long-range video datasets such as ActivityNet, QVHighlights, YouCook2, DiDeMo, and Charades. Our code is available at https://github.com/GenjiB/ECLIPSE."
              ],
              [
                "Conference Name",
                "Computer Vision \u2013 ECCV 2022"
              ],
              [
                "Creators",
                "Yan-Bo Lin, Jie Lei, Mohit Bansal, Gedas Bertasius, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
              ],
              [
                "DOI",
                "10.1007/978-3-031-19830-4_24"
              ],
              [
                "Date",
                "2022-00-00 2022"
              ],
              [
                "ISBN",
                "978-3-031-19830-4"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "Springer Link"
              ],
              [
                "Pages",
                "413-430"
              ],
              [
                "Place",
                "Cham"
              ],
              [
                "Proceedings Title",
                "Computer Vision \u2013 ECCV 2022"
              ],
              [
                "Publisher",
                "Springer Nature Switzerland"
              ],
              [
                "Series",
                "Lecture Notes in Computer Science"
              ],
              [
                "Short Title",
                "EclipSE"
              ],
              [
                "Title",
                "EclipSE: Efficient Long-Range Video Retrieval Using Sight and\u00a0Sound"
              ]
            ],
            "resource": "storage/i3500.pdf",
            "selectable": false
          },
          {
            "text": "EgoSchema",
            "item-id": "i3457",
            "nodes": [
              {
                "text": "Comment: https://egoschema.github.io/",
                "item-id": "n3540",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: https://egoschema.github.io/",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">Comment: https://egoschema.github.io/</div>",
                "node_type": "note"
              },
              {
                "text": "Mangalam et al_2023_EgoSchema.pdf",
                "item-id": "i3539",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Mangalam et al_2023_EgoSchema.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_7Z3G4UR9/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/4\">Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">Collecting 0.96plus0.96minus0.96100.96 EgoSchema</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">0.96plus0.96minus0.96100.96 EgoSchema Pipeline</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">Stage I: Raw Data Filtering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/5\">Stage II: Question Answer Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/6\">Stage III: Generated Question Answer Filtering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Stage IV: Manual QAW Curation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Temporal Certificates</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Benchmarking0.96plus0.96minus0.96100.96 EgoSchema</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/7\">Evaluating Certificate Lengths</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/8\">Evaluating Multiple-choice Question Answering on0.96plus0.96minus0.96100.96 EgoSchema</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/9\">Conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/21\">Set A</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/21\">Question prompt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/22\">Answer prompt</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/24\">Set B</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/24\">Question and answer prompt</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/26\">Wrong answer prompt</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/29\">Our clip length and narration density choice</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/29\">Human curation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/29\">Curation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/30\">Benchmarking details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/30\">Violet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/31\">mPLUG-Owl</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/31\">InternVideo</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7Z3G4UR9/31\">Human</a></li></ul></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Mangalam et al_2023_EgoSchema.pdf"
                  ]
                ],
                "resource": "storage/i3539.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "EgoSchema",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over 5.7x longer than the second closest dataset and 10x to 100x longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that \\name{}{}, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code are open-sourced for both public and commercial use under the Ego4D license at http://egoschema.github.io"
              ],
              [
                "Access Date",
                "2024-01-08 03:38:58"
              ],
              [
                "Archiveid",
                "arXiv:2308.09126"
              ],
              [
                "Creators",
                "Karttikeya Mangalam, Raiymbek Akshulakov, Jitendra Malik"
              ],
              [
                "Date",
                "2023-08-17 2023-08-17"
              ],
              [
                "Extra",
                "arXiv:2308.09126 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "EgoSchema"
              ],
              [
                "Title",
                "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2308.09126"
              ]
            ],
            "resource": "storage/i3539.pdf",
            "selectable": false
          },
          {
            "text": "Explore Multi-Step Reasoning in Video Question Answering",
            "item-id": "i3473",
            "nodes": [
              {
                "text": "Annotations",
                "item-id": "n3563",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Annotations",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>SVQA</p>\n</div></div>",
                "node_type": "note"
              },
              {
                "text": "Song et al_2018_Explore Multi-Step Reasoning in Video Question Answering.pdf",
                "item-id": "i3564",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Song et al_2018_Explore Multi-Step Reasoning in Video Question Answering.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YR62CMTZ/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/3\">3 The Proposed Benchmark SVQA</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/3\">3.1 Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">3.2 QA Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">3.3 Quality Control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">3.4 Balance Answer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/4\">4 The Proposed Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/5\">4.1 Question Embedding Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/5\">4.2 Video Embedding Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/5\">4.3 Attention Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/6\">4.4 Answer Decoder Module</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/6\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/6\">5.1 Dataset Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/7\">5.2 Performance Comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/7\">5.3 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/7\">5.4 Generalization Ability</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YR62CMTZ/9\">References</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Song et al_2018_Explore Multi-Step Reasoning in Video Question Answering.pdf"
                  ]
                ],
                "resource": "storage/i3564.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Explore Multi-Step Reasoning in Video Question Answering",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Video question answering (VideoQA) always involves visual reasoning. When answering questions composing of multiple logic correlations, models need to perform multi-step reasoning. In this paper, we formulate multi-step reasoning in VideoQA as a new task to answer compositional and logical structured questions based on video content. Existing VideoQA datasets are inadequate as benchmarks for the multi-step reasoning due to limitations such as lacking logical structure and having language biases. Thus we design a system to automatically generate a large-scale dataset, namely SVQA (Synthetic Video Question Answering). Compared with other VideoQA datasets, SVQA contains exclusively long and structured questions with various spatial and temporal relations between objects. More importantly, questions in SVQA can be decomposed into human readable logical tree or chain layouts, each node of which represents a sub-task requiring a reasoning operation such as comparison or arithmetic. Towards automatic question answering in SVQA, we develop a new VideoQA model. Particularly, we construct a new attention module, which contains spatial attention mechanism to address crucial and multiple logical sub-tasks embedded in questions, as well as a refined GRU called ta-GRU (temporal-attention GRU) to capture the long-term temporal dependency and gather complete visual cues. Experimental results show the capability of multi-step reasoning of SVQA and the effectiveness of our model when compared with other existing models."
              ],
              [
                "Access Date",
                "2024-01-07"
              ],
              [
                "Creators",
                "Xiaomeng Song, Yucheng Shi, Xin Chen, Yahong Han"
              ],
              [
                "DOI",
                "10.1145/3240508.3240563"
              ],
              [
                "Date",
                "2018-00-15 \u5341\u6708 15, 2018"
              ],
              [
                "ISBN",
                "978-1-4503-5665-7"
              ],
              [
                "Library Catalog",
                "ACM Digital Library"
              ],
              [
                "Pages",
                "239\u2013247"
              ],
              [
                "Place",
                "New York, NY, USA"
              ],
              [
                "Proceedings Title",
                "Proceedings of the 26th ACM international conference on Multimedia"
              ],
              [
                "Publisher",
                "Association for Computing Machinery"
              ],
              [
                "Series",
                "MM '18"
              ],
              [
                "Title",
                "Explore Multi-Step Reasoning in Video Question Answering"
              ],
              [
                "URL",
                "https://dl.acm.org/doi/10.1145/3240508.3240563"
              ]
            ],
            "resource": "storage/i3564.pdf",
            "selectable": false
          },
          {
            "text": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
            "item-id": "i3444",
            "nodes": [
              {
                "text": "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf",
                "item-id": "i3549",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_H2CF698R/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2CF698R/3\">Dynamic Concept Learner</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/4\">Model Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/5\">Training and inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/6\">Comparisons on Temporal and Causal Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/7\">Evaluation of Object and Event Concept Grounding in Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/7\">Generalization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/9\">Extension to real videos and the new concept</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/9\">Discussion and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/13\">Dynamic Predictor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/14\">Program Parser</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/14\">CLEVRER Operations and Program Execution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/15\">Trajectory Performance Evaluation.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/16\">Statistics for CLEVRER-Grounding and CLEVRER-Retrieval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2CF698R/16\">Training Objectives</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Chen et al_2020_Grounding Physical Concepts of Objects and Events Through Dynamic Visual.pdf"
                  ]
                ],
                "resource": "storage/i3549.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from video and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity."
              ],
              [
                "Access Date",
                "2024-01-08 03:21:42"
              ],
              [
                "Conference Name",
                "International Conference on Learning Representations"
              ],
              [
                "Creators",
                "Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, Chuang Gan"
              ],
              [
                "Date",
                "2020-10-02 2020/10/02"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Proceedings Title",
                "International Conference on Learning Representations"
              ],
              [
                "Title",
                "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=bhCDO_cEGCz"
              ]
            ],
            "resource": "storage/i3549.pdf",
            "selectable": false
          },
          {
            "text": "Hierarchical Conditional Relation Networks for Video Question Answering",
            "item-id": "i3447",
            "nodes": [
              {
                "text": "Le et al_2020_Hierarchical Conditional Relation Networks for Video Question Answering.pdf",
                "item-id": "i3524",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Le et al_2020_Hierarchical Conditional Relation Networks for Video Question Answering.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Le et al_2020_Hierarchical Conditional Relation Networks for Video Question Answering.pdf"
                  ]
                ],
                "resource": "storage/i3524.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Hierarchical Conditional Relation Networks for Video Question Answering",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Video question answering (VideoQA) is challenging as it requires modeling capacity to distill dynamic visual artifacts and distant relations and to associate them with linguistic concepts. We introduce a general-purpose reusable neural unit called Conditional Relation Network (CRN) that serves as a building block to construct more sophisticated structures for representation and reasoning over video. CRN takes as input an array of tensorial objects and a conditioning feature, and computes an array of encoded output objects. Model building becomes a simple exercise of replication, rearrangement and stacking of these reusable units for diverse modalities and contextual information. This design thus supports high-order relational and multi-step reasoning. The resulting architecture for VideoQA is a CRN hierarchy whose branches represent sub-videos or clips, all sharing the same question as the contextual condition. Our evaluations on well-known datasets achieved new SoTA results, demonstrating the impact of building a general-purpose reasoning unit on complex domains such as VideoQA."
              ],
              [
                "Access Date",
                "2024-01-08 08:47:31"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran"
              ],
              [
                "Date",
                "2020-00-00 2020"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "9972-9981"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Title",
                "Hierarchical Conditional Relation Networks for Video Question Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_CVPR_2020/html/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.html"
              ]
            ],
            "resource": "storage/i3524.pdf",
            "selectable": false
          },
          {
            "text": "Just Ask",
            "item-id": "i3445",
            "nodes": [
              {
                "text": "Yang et al_2021_Just Ask.pdf",
                "item-id": "i3518",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yang et al_2021_Just Ask.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yang et al_2021_Just Ask.pdf"
                  ]
                ],
                "resource": "storage/i3518.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Just Ask",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Recent methods for visual question answering rely on large-scale annotated datasets. Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability. In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision. We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations. Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets. To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers. Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations."
              ],
              [
                "Access Date",
                "2024-01-08 09:12:09"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF International Conference on Computer Vision"
              ],
              [
                "Creators",
                "Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid"
              ],
              [
                "Date",
                "2021-00-00 2021"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "1686-1697"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF International Conference on Computer Vision"
              ],
              [
                "Short Title",
                "Just Ask"
              ],
              [
                "Title",
                "Just Ask: Learning To Answer Questions From Millions of Narrated Videos"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Just_Ask_Learning_To_Answer_Questions_From_Millions_of_Narrated_ICCV_2021_paper.html"
              ]
            ],
            "resource": "storage/i3518.pdf",
            "selectable": false
          },
          {
            "text": "KnowIT VQA",
            "item-id": "i3471",
            "nodes": [
              {
                "text": "Garcia et al_2020_KnowIT VQA.pdf",
                "item-id": "i3558",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Garcia et al_2020_KnowIT VQA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Garcia et al_2020_KnowIT VQA.pdf"
                  ]
                ],
                "resource": "storage/i3558.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "KnowIT VQA",
            "item_type": "journalArticle",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations."
              ],
              [
                "Access Date",
                "2024-01-08 03:09:31"
              ],
              [
                "Creators",
                "Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima"
              ],
              [
                "DOI",
                "10.1609/aaai.v34i07.6713"
              ],
              [
                "Date",
                "2020-04-03 2020-04-03"
              ],
              [
                "Extra",
                "Number: 07"
              ],
              [
                "ISSN",
                "2374-3468"
              ],
              [
                "Issue",
                "07"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "ojs.aaai.org"
              ],
              [
                "Pages",
                "10826-10834"
              ],
              [
                "Publication Title",
                "Proceedings of the AAAI Conference on Artificial Intelligence"
              ],
              [
                "Rights",
                "Copyright (c) 2020 Association for the Advancement of Artificial Intelligence"
              ],
              [
                "Short Title",
                "KnowIT VQA"
              ],
              [
                "Title",
                "KnowIT VQA: Answering Knowledge-Based Questions about Videos"
              ],
              [
                "URL",
                "https://ojs.aaai.org/index.php/AAAI/article/view/6713"
              ],
              [
                "Volume",
                "34"
              ]
            ],
            "resource": "storage/i3558.pdf",
            "selectable": false
          },
          {
            "text": "LifeQA",
            "item-id": "i3456",
            "nodes": [
              {
                "text": "Castro et al_2020_LifeQA.pdf",
                "item-id": "i3529",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Castro et al_2020_LifeQA.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8XPS6BD3/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/1\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/1\">Text-based Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/2\">Multimodal Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/2\">In-the-Wild Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/2\">LifeQA Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/2\">Dataset Collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/2\">Dataset Analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/3\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/4\">Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/5\">Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8XPS6BD3/5\">Conclusion</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Castro et al_2020_LifeQA.pdf"
                  ]
                ],
                "resource": "storage/i3529.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "LifeQA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We introduce LifeQA, a benchmark dataset for video question answering that focuses on day-to-day real-life situations. Current video question answering datasets consist of movies and TV shows. However, it is well-known that these visual domains are not representative of our day-to-day lives. Movies and TV shows, for example, benefit from professional camera movements, clean editing, crisp audio recordings, and scripted dialog between professional actors. While these domains provide a large amount of data for training models, their properties make them unsuitable for testing real-life question answering systems. Our dataset, by contrast, consists of video clips that represent only real-life scenarios. We collect 275 such video clips and over 2.3k multiple-choice questions. In this paper, we analyze the challenging but realistic aspects of LifeQA, and we apply several state-of-the-art video question answering models to provide benchmarks for future research. The full dataset is publicly available at https://lit.eecs.umich.edu/lifeqa/."
              ],
              [
                "Access Date",
                "2024-01-08 07:00:07"
              ],
              [
                "Conference Name",
                "LREC 2020"
              ],
              [
                "Creators",
                "Santiago Castro, Mahmoud Azab, Jonathan Stroud, Cristina Noujaim, Ruoyao Wang, Jia Deng, Rada Mihalcea, Nicoletta Calzolari, Fr\u00e9d\u00e9ric B\u00e9chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H\u00e9l\u00e8ne Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis"
              ],
              [
                "Date",
                "2020-05-00 2020-05"
              ],
              [
                "ISBN",
                "979-10-95546-34-4"
              ],
              [
                "Language",
                "English"
              ],
              [
                "Library Catalog",
                "ACLWeb"
              ],
              [
                "Pages",
                "4352\u20134358"
              ],
              [
                "Place",
                "Marseille, France"
              ],
              [
                "Proceedings Title",
                "Proceedings of the Twelfth Language Resources and Evaluation Conference"
              ],
              [
                "Publisher",
                "European Language Resources Association"
              ],
              [
                "Short Title",
                "LifeQA"
              ],
              [
                "Title",
                "LifeQA: A Real-life Dataset for Video Question Answering"
              ],
              [
                "URL",
                "https://aclanthology.org/2020.lrec-1.536"
              ]
            ],
            "resource": "storage/i3529.pdf",
            "selectable": false
          },
          {
            "text": "Long-Term Feature Banks for Detailed Video Understanding",
            "item-id": "i3437",
            "nodes": [
              {
                "text": "Wu et al_2019_Long-Term Feature Banks for Detailed Video Understanding.pdf",
                "item-id": "i3501",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Wu et al_2019_Long-Term Feature Banks for Detailed Video Understanding.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Wu et al_2019_Long-Term Feature Banks for Detailed Video Understanding.pdf"
                  ]
                ],
                "resource": "storage/i3501.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Long-Term Feature Banks for Detailed Video Understanding",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank--supportive information extracted over the entire span of a video--to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online."
              ],
              [
                "Access Date",
                "2024-01-08 15:00:00"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, Ross Girshick"
              ],
              [
                "Date",
                "2019-00-00 2019"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "284-293"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Title",
                "Long-Term Feature Banks for Detailed Video Understanding"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Long-Term_Feature_Banks_for_Detailed_Video_Understanding_CVPR_2019_paper.html"
              ]
            ],
            "resource": "storage/i3501.pdf",
            "selectable": false
          },
          {
            "text": "MIST",
            "item-id": "i3436",
            "nodes": [
              {
                "text": "Gao et al_2023_MIST.pdf",
                "item-id": "i3502",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Gao et al_2023_MIST.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Gao et al_2023_MIST.pdf"
                  ]
                ],
                "resource": "storage/i3502.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "MIST",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "To build Video Question Answering (VideoQA) systems capable of assisting humans in daily activities, seeking answers from long-form videos with diverse and complex events is a must. Existing multi-modal VQA models achieve promising performance on images or short video clips, especially with the recent success of large-scale multi-modal pre-training. However, when extending these methods to long-form videos, new challenges arise. On the one hand, using a dense video sampling strategy is computationally prohibitive. On the other hand, methods relying on sparse sampling struggle in scenarios where multi-event and multi-granularity visual reasoning are required. In this work, we introduce a new model named Multi-modal Iterative Spatial-temporal Transformer (MIST) to better adapt pre-trained models for long-form VideoQA. Specifically, MIST decomposes traditional dense spatial-temporal self-attention into cascaded segment and region selection modules that adaptively select frames and image regions that are closely relevant to the question itself. Visual concepts at different granularities are then processed efficiently through an attention module. In addition, MIST iteratively conducts selection and attention over multiple layers to support reasoning over multiple events. The experimental results on four VideoQA datasets, including AGQA, NExT-QA, STAR, and Env-QA, show that MIST achieves state-of-the-art performance and is superior at computation efficiency and interpretability."
              ],
              [
                "Access Date",
                "2024-01-08 12:46:07"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang, Mike Zheng Shou"
              ],
              [
                "Date",
                "2023-00-00 2023"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "14773-14783"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Short Title",
                "MIST"
              ],
              [
                "Title",
                "MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2023/html/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.html"
              ]
            ],
            "resource": "storage/i3502.pdf",
            "selectable": false
          },
          {
            "text": "MVBench",
            "item-id": "i3449",
            "nodes": [
              {
                "text": "Comment: 18 pages, 7 figures, 19 tables",
                "item-id": "n3541",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: 18 pages, 7 figures, 19 tables",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">Comment: 18 pages, 7 figures, 19 tables</div>",
                "node_type": "note"
              },
              {
                "text": "Li et al_2023_MVBench.pdf",
                "item-id": "i3538",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Li et al_2023_MVBench.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_I3MT7G7N/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/2\">. Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/3\">. MVBench</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/3\">. Temporal Task Definition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/4\">. Automatic QA Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/5\">. Prompt Design for Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/5\">. VideoChat2</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/5\">. Instruction-Tuning Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/6\">. Progressive Multi-Modal Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/6\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/7\">. Results on MVBench</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/7\">. More Comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/8\">. Ablations of VideoChat2</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/8\">. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/9\">. Training Hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/9\">. More Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/10\">. Details of QA Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/10\">. Results on Challenging Video QA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/11\">. Comparisons with GPT-4V</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/11\">. Leaderboards and Analyses</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I3MT7G7N/13\">. Qualitative Results</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Li et al_2023_MVBench.pdf"
                  ]
                ],
                "resource": "storage/i3538.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "MVBench",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiple-choice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https://github.com/OpenGVLab/Ask-Anything."
              ],
              [
                "Access Date",
                "2024-01-08 03:38:59"
              ],
              [
                "Archiveid",
                "arXiv:2311.17005"
              ],
              [
                "Creators",
                "Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao"
              ],
              [
                "Date",
                "2023-12-03 2023-12-03"
              ],
              [
                "Extra",
                "arXiv:2311.17005 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "MVBench"
              ],
              [
                "Title",
                "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2311.17005"
              ]
            ],
            "resource": "storage/i3538.pdf",
            "selectable": false
          },
          {
            "text": "MoVQA",
            "item-id": "i3439",
            "nodes": [
              {
                "text": "Zhang et al_2023_MoVQA.pdf",
                "item-id": "i3534",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Zhang et al_2023_MoVQA.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BXZY4RUU/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/3\">. MoVQA Benchmark</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/3\">. Question-Answering Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/4\">. Distractor Options Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/5\">. Data Statistics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/5\">. Proposed Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Keyframe Branch</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Context Branch</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/6\">. VideoQA Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/7\">. Multimodal LLMs Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/7\">. Qualitative Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/8\">. Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/12\">. More data statistics of our MoVQA.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/12\">. More examples</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BXZY4RUU/12\">. The 100 movies used in our MoVQA.</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Zhang et al_2023_MoVQA.pdf"
                  ]
                ],
                "resource": "storage/i3534.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "MoVQA",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "While several long-form VideoQA datasets have been introduced, the length of both videos used to curate questions and sub-clips of clues leveraged to answer those questions have not yet reached the criteria for genuine long-form video understanding. Moreover, their QAs are unduly narrow and modality-biased, lacking a wider view of understanding long-term video content with rich dynamics and complex narratives. To remedy this, we introduce MoVQA, a long-form movie question-answering dataset, and benchmark to assess the diverse cognitive capabilities of multimodal systems rely on multi-level temporal lengths, with considering both video length and clue length. Additionally, to take a step towards human-level understanding in long-form video, versatile and multimodal question-answering is designed from the moviegoer-perspective to assess the model capabilities on various perceptual and cognitive axes.Through analysis involving various baselines reveals a consistent trend: the performance of all methods significantly deteriorate with increasing video and clue length. Meanwhile, our established baseline method has shown some improvements, but there is still ample scope for enhancement on our challenging MoVQA dataset. We expect our MoVQA to provide a new perspective and encourage inspiring works on long-form video understanding research."
              ],
              [
                "Access Date",
                "2024-01-08 03:38:58"
              ],
              [
                "Archiveid",
                "arXiv:2312.04817"
              ],
              [
                "Creators",
                "Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, Yu Qiao"
              ],
              [
                "Date",
                "2023-12-07 2023-12-07"
              ],
              [
                "Extra",
                "arXiv:2312.04817 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "MoVQA"
              ],
              [
                "Title",
                "MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2312.04817"
              ]
            ],
            "resource": "storage/i3534.pdf",
            "selectable": false
          },
          {
            "text": "MovieQA",
            "item-id": "i3458",
            "nodes": [
              {
                "text": "Tapaswi et al_2016_MovieQA.pdf",
                "item-id": "i3545",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Tapaswi et al_2016_MovieQA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Tapaswi et al_2016_MovieQA.pdf"
                  ]
                ],
                "resource": "storage/i3545.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "MovieQA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-08 03:28:01"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler"
              ],
              [
                "Date",
                "2016-00-00 2016"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "4631-4640"
              ],
              [
                "Short Title",
                "MovieQA"
              ],
              [
                "Title",
                "MovieQA: Understanding Stories in Movies Through Question-Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_cvpr_2016/html/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.html"
              ]
            ],
            "resource": "storage/i3545.pdf",
            "selectable": false
          },
          {
            "text": "NExT-QA",
            "item-id": "i3455",
            "nodes": [
              {
                "text": "Xiao et al_2021_NExT-QA.pdf",
                "item-id": "i3532",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Xiao et al_2021_NExT-QA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Xiao et al_2021_NExT-QA.pdf"
                  ]
                ],
                "resource": "storage/i3532.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "NExT-QA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting at causal action reasoning, temporal action reasoning and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial description towards a deeper understanding of videos."
              ],
              [
                "Access Date",
                "2024-01-08 06:24:52"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua"
              ],
              [
                "Date",
                "2021-00-00 2021"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "9777-9786"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Short Title",
                "NExT-QA"
              ],
              [
                "Title",
                "NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2021/html/Xiao_NExT-QA_Next_Phase_of_Question-Answering_to_Explaining_Temporal_Actions_CVPR_2021_paper.html"
              ]
            ],
            "resource": "storage/i3532.pdf",
            "selectable": false
          },
          {
            "text": "SCSampler",
            "item-id": "i3441",
            "nodes": [
              {
                "text": "Korbar et al_2019_SCSampler.pdf",
                "item-id": "i3494",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Korbar et al_2019_SCSampler.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Korbar et al_2019_SCSampler.pdf"
                  ]
                ],
                "resource": "storage/i3494.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "SCSampler",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive. Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight \"clip-sampling\" model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost."
              ],
              [
                "Access Date",
                "2024-01-08 15:41:56"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF International Conference on Computer Vision"
              ],
              [
                "Creators",
                "Bruno Korbar, Du Tran, Lorenzo Torresani"
              ],
              [
                "Date",
                "2019-00-00 2019"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "6232-6242"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF International Conference on Computer Vision"
              ],
              [
                "Short Title",
                "SCSampler"
              ],
              [
                "Title",
                "SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_ICCV_2019/html/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.html"
              ]
            ],
            "resource": "storage/i3494.pdf",
            "selectable": false
          },
          {
            "text": "STAR",
            "item-id": "i3454",
            "nodes": [
              {
                "text": "Wu et al_2021_STAR.pdf",
                "item-id": "i3546",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Wu et al_2021_STAR.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Wu et al_2021_STAR.pdf"
                  ]
                ],
                "resource": "storage/i3546.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "STAR",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Reasoning in the real world is not divorced from situations. How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence. This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR). This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical. The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships). Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning. Questions and answers are procedurally generated. The answering logic of each question is represented by a functional program based on a situation hyper-graph. We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task. We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark."
              ],
              [
                "Access Date",
                "2024-01-08 03:25:45"
              ],
              [
                "Conference Name",
                "Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)"
              ],
              [
                "Creators",
                "Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, Chuang Gan"
              ],
              [
                "Date",
                "2021-08-29 2021/08/29"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openreview.net"
              ],
              [
                "Proceedings Title",
                "Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)"
              ],
              [
                "Short Title",
                "STAR"
              ],
              [
                "Title",
                "STAR: A Benchmark for Situated Reasoning in Real-World Videos"
              ],
              [
                "URL",
                "https://openreview.net/forum?id=EfgNF5-ZAjM"
              ]
            ],
            "resource": "storage/i3546.pdf",
            "selectable": false
          },
          {
            "text": "Self-Chained Image-Language Model for Video Localization and Question Answering",
            "item-id": "i3438",
            "nodes": [
              {
                "text": "Comment: NeurIPS 2023; Our code and checkpoints are available at: https://github.com/Yui010206/SeViLA",
                "item-id": "n3498",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: NeurIPS 2023; Our code and checkpoints are available at: https://github.com/Yui010206/SeViLA",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">Comment: NeurIPS 2023; Our code and checkpoints are available at: https://github.com/Yui010206/SeViLA</div>",
                "node_type": "note"
              },
              {
                "text": "Yu et al_2023_Self-Chained Image-Language Model for Video Localization and Question Answering.pdf",
                "item-id": "i3497",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yu et al_2023_Self-Chained Image-Language Model for Video Localization and Question Answering.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yu et al_2023_Self-Chained Image-Language Model for Video Localization and Question Answering.pdf"
                  ]
                ],
                "resource": "storage/i3497.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "Self-Chained Image-Language Model for Video Localization and Question Answering",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Recent studies have shown promising results on utilizing large pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and Answerer, where both are parameter-efficiently fine-tuned from BLIP-2. We propose two ways of chaining these modules for cascaded inference and self-refinement. First, in the forward chain, the Localizer finds multiple language-aware keyframes in a video, which the Answerer uses to predict the answer. Second, in the reverse chain, the Answerer generates keyframe pseudo-labels to refine the Localizer, alleviating the need for expensive video moment localization annotations. Our SeViLA framework outperforms several strong baselines on 5 challenging video QA and event prediction benchmarks, and achieves the state-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We also analyze the impact of Localizer, comparisons of Localizer with other temporal localization models, pre-training/self-refinement of Localizer, and varying the number of keyframes."
              ],
              [
                "Access Date",
                "2024-01-08 15:36:03"
              ],
              [
                "Archiveid",
                "arXiv:2305.06988"
              ],
              [
                "Creators",
                "Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal"
              ],
              [
                "DOI",
                "10.48550/arXiv.2305.06988"
              ],
              [
                "Date",
                "2023-11-29 2023-11-29"
              ],
              [
                "Extra",
                "arXiv:2305.06988 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Title",
                "Self-Chained Image-Language Model for Video Localization and Question Answering"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2305.06988"
              ]
            ],
            "resource": "storage/i3497.pdf",
            "selectable": false
          },
          {
            "text": "TGIF-QA",
            "item-id": "i2795",
            "nodes": [
              {
                "text": "Jang et al_2017_TGIF-QA.pdf",
                "item-id": "i2834",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Jang et al_2017_TGIF-QA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Jang et al_2017_TGIF-QA.pdf"
                  ]
                ],
                "resource": "storage/i2834.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "TGIF-QA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations."
              ],
              [
                "Access Date",
                "2023-07-18 06:59:26"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim"
              ],
              [
                "Date",
                "2017-00-00 2017"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "2758-2766"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Short Title",
                "TGIF-QA"
              ],
              [
                "Title",
                "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html"
              ]
            ],
            "resource": "storage/i2834.pdf",
            "selectable": false
          },
          {
            "text": "TVQA",
            "item-id": "i3472",
            "nodes": [
              {
                "text": "Lei et al_2018_TVQA.pdf",
                "item-id": "i3562",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Lei et al_2018_TVQA.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Lei et al_2018_TVQA.pdf"
                  ]
                ],
                "resource": "storage/i3562.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "TVQA",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http://tvqa.cs.unc.edu."
              ],
              [
                "Access Date",
                "2024-01-08 03:02:07"
              ],
              [
                "Conference Name",
                "EMNLP 2018"
              ],
              [
                "Creators",
                "Jie Lei, Licheng Yu, Mohit Bansal, Tamara Berg, Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii"
              ],
              [
                "DOI",
                "10.18653/v1/D18-1167"
              ],
              [
                "Date",
                "2018-10-00 2018-10"
              ],
              [
                "Library Catalog",
                "ACLWeb"
              ],
              [
                "Pages",
                "1369\u20131379"
              ],
              [
                "Place",
                "Brussels, Belgium"
              ],
              [
                "Proceedings Title",
                "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
              ],
              [
                "Publisher",
                "Association for Computational Linguistics"
              ],
              [
                "Short Title",
                "TVQA"
              ],
              [
                "Title",
                "TVQA: Localized, Compositional Video Question Answering"
              ],
              [
                "URL",
                "https://aclanthology.org/D18-1167"
              ]
            ],
            "resource": "storage/i3562.pdf",
            "selectable": false
          },
          {
            "text": "TVQA+",
            "item-id": "i3460",
            "nodes": [
              {
                "text": "Lei et al_2020_TVQA+.pdf",
                "item-id": "i3561",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Lei et al_2020_TVQA+.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Lei et al_2020_TVQA+.pdf"
                  ]
                ],
                "resource": "storage/i3561.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "TVQA+",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations."
              ],
              [
                "Access Date",
                "2024-01-08 03:02:27"
              ],
              [
                "Conference Name",
                "ACL 2020"
              ],
              [
                "Creators",
                "Jie Lei, Licheng Yu, Tamara Berg, Mohit Bansal, Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault"
              ],
              [
                "DOI",
                "10.18653/v1/2020.acl-main.730"
              ],
              [
                "Date",
                "2020-07-00 2020-07"
              ],
              [
                "Library Catalog",
                "ACLWeb"
              ],
              [
                "Pages",
                "8211\u20138225"
              ],
              [
                "Place",
                "Online"
              ],
              [
                "Proceedings Title",
                "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
              ],
              [
                "Publisher",
                "Association for Computational Linguistics"
              ],
              [
                "Short Title",
                "TVQA+"
              ],
              [
                "Title",
                "TVQA+: Spatio-Temporal Grounding for Video Question Answering"
              ],
              [
                "URL",
                "https://aclanthology.org/2020.acl-main.730"
              ]
            ],
            "resource": "storage/i3561.pdf",
            "selectable": false
          },
          {
            "text": "Towards Long-Form Video Understanding",
            "item-id": "i3443",
            "nodes": [
              {
                "text": "Wu_Krahenbuhl_2021_Towards Long-Form Video Understanding.pdf",
                "item-id": "i3504",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Wu_Krahenbuhl_2021_Towards Long-Form Video Understanding.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Wu_Krahenbuhl_2021_Towards Long-Form Video Understanding.pdf"
                  ]
                ],
                "resource": "storage/i3504.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Towards Long-Form Video Understanding",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-08 12:15:31"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Chao-Yuan Wu, Philipp Krahenbuhl"
              ],
              [
                "Date",
                "2021-00-00 2021"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "1884-1894"
              ],
              [
                "Proceedings Title",
                "Towards long-form video understanding."
              ],
              [
                "Title",
                "Towards Long-Form Video Understanding"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Towards_Long-Form_Video_Understanding_CVPR_2021_paper.html"
              ]
            ],
            "resource": "storage/i3504.pdf",
            "selectable": false
          },
          {
            "text": "Transferring Domain-Agnostic Knowledge in Video Question Answering",
            "item-id": "i3469",
            "nodes": [
              {
                "text": "Wu et al_2021_Transferring Domain-Agnostic Knowledge in Video Question Answering.pdf",
                "item-id": "i3556",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Wu et al_2021_Transferring Domain-Agnostic Knowledge in Video Question Answering.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Wu et al_2021_Transferring Domain-Agnostic Knowledge in Video Question Answering.pdf"
                  ]
                ],
                "resource": "storage/i3556.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Transferring Domain-Agnostic Knowledge in Video Question Answering",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2024-01-08 03:12:44"
              ],
              [
                "Conference Name",
                "Proceedings of the British Machine Vision Conference (BMVC)"
              ],
              [
                "Creators",
                "Tianran Wu, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima, Haruo Takemura"
              ],
              [
                "Date",
                "2021-00-00 2021"
              ],
              [
                "Library Catalog",
                "Google Scholar"
              ],
              [
                "Proceedings Title",
                "Proceedings of the British Machine Vision Conference (BMVC)"
              ],
              [
                "Title",
                "Transferring Domain-Agnostic Knowledge in Video Question Answering"
              ],
              [
                "URL",
                "https://www.bmvc2021-virtualconference.com/assets/papers/1187.pdf"
              ]
            ],
            "resource": "storage/i3556.pdf",
            "selectable": false
          },
          {
            "text": "Video Question Answering",
            "item-id": "i3467",
            "nodes": [
              {
                "text": "Comment: Accepted by EMNLP 2022",
                "item-id": "n3554",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: Accepted by EMNLP 2022",
                "item_type": "note",
                "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted by EMNLP 2022</div>",
                "node_type": "note"
              },
              {
                "text": "Zhong et al_2022_Video Question Answering.pdf",
                "item-id": "i3553",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Zhong et al_2022_Video Question Answering.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Zhong et al_2022_Video Question Answering.pdf"
                  ]
                ],
                "resource": "storage/i3553.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "Video Question Answering",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Video Question Answering (VideoQA) aims to answer natural language questions according to the given videos. It has earned increasing attention with recent research trends in joint vision and language understanding. Yet, compared with ImageQA, VideoQA is largely underexplored and progresses slowly. Although different algorithms have continually been proposed and shown success on different VideoQA datasets, we find that there lacks a meaningful survey to categorize them, which seriously impedes its advancements. This paper thus provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on the datasets, algorithms, and unique challenges. We then point out the research trend of studying beyond factoid QA to inference QA towards the cognition of video contents, Finally, we conclude some promising directions for future exploration."
              ],
              [
                "Access Date",
                "2024-01-08 03:16:41"
              ],
              [
                "Archiveid",
                "arXiv:2203.01225"
              ],
              [
                "Creators",
                "Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Weihong Deng, Tat-Seng Chua"
              ],
              [
                "Date",
                "2022-11-02 2022-11-02"
              ],
              [
                "Extra",
                "arXiv:2203.01225 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "Video Question Answering"
              ],
              [
                "Title",
                "Video Question Answering: Datasets, Algorithms and Challenges"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2203.01225"
              ]
            ],
            "resource": "storage/i3553.pdf",
            "selectable": false
          },
          {
            "text": "Video Question Answering via Gradually Refined Attention over Appearance and Motion",
            "item-id": "i2794",
            "nodes": [
              {
                "text": "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf",
                "item-id": "i2833",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2QNGQXBI/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2.1 Video Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/2\">2.2 Image Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/3\">2.3 Video Question Answering</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/3\">3 Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/4\">3.1 Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/4\">3.2 Attention Memory Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/5\">3.3 Answer Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.1 Data Preparation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.2 Model details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/6\">4.3 Baseline methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/7\">4.4 Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/7\">4.5 Results and Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/8\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2QNGQXBI/9\">References</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Xu et al_2017_Video Question Answering via Gradually Refined Attention over Appearance and.pdf"
                  ]
                ],
                "resource": "storage/i2833.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Recently image question answering (ImageQA) has gained lots of attention in the research community. However, as its natural extension, video question answering (VideoQA) is less explored. Although both tasks look similar, VideoQA is more challenging mainly because of the complexity and diversity of videos. As such, simply extending the ImageQA methods to videos is insufficient and suboptimal. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this paper, we consider exploiting the appearance and motion information resided in the video with a novel attention mechanism. More specifically, we propose an end-to-end model which gradually refines its attention over the appearance and motion features of the video using the question as guidance. The question is processed word by word until the model generates the final optimized attention. The weighted representation of the video, as well as other contextual information, are used to generate the answer. Extensive experiments show the advantages of our model compared to other baseline models. We also demonstrate the effectiveness of our model by analyzing the refined attention weights during the question answering procedure."
              ],
              [
                "Access Date",
                "2023-07-17"
              ],
              [
                "Creators",
                "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, Yueting Zhuang"
              ],
              [
                "DOI",
                "10.1145/3123266.3123427"
              ],
              [
                "Date",
                "2017-00-23 \u5341\u6708 23, 2017"
              ],
              [
                "ISBN",
                "978-1-4503-4906-2"
              ],
              [
                "Library Catalog",
                "ACM Digital Library"
              ],
              [
                "Pages",
                "1645\u20131653"
              ],
              [
                "Place",
                "New York, NY, USA"
              ],
              [
                "Proceedings Title",
                "Proceedings of the 25th ACM international conference on Multimedia"
              ],
              [
                "Publisher",
                "Association for Computing Machinery"
              ],
              [
                "Series",
                "MM '17"
              ],
              [
                "Title",
                "Video Question Answering via Gradually Refined Attention over Appearance and Motion"
              ],
              [
                "URL",
                "https://dl.acm.org/doi/10.1145/3123266.3123427"
              ]
            ],
            "resource": "storage/i2833.pdf",
            "selectable": false
          },
          {
            "text": "Video-GroundingDINO",
            "item-id": "i3588",
            "nodes": [
              {
                "text": "Wasim et al_2023_Video-GroundingDINO.pdf",
                "item-id": "i3590",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Wasim et al_2023_Video-GroundingDINO.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Wasim et al_2023_Video-GroundingDINO.pdf"
                  ]
                ],
                "resource": "storage/i3590.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-log-in",
            "item_title": "Video-GroundingDINO",
            "item_type": "preprint",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Video grounding aims to localize a spatio-temporal section in a video corresponding to an input text query. This paper addresses a critical limitation in current video grounding methodologies by introducing an Open-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent closed-set approaches that struggle with open-vocabulary scenarios due to limited training data and predefined vocabularies, our model leverages pre-trained representations from foundational spatial grounding models. This empowers it to effectively bridge the semantic gap between natural language and diverse visual content, achieving strong performance in closed-set and open-vocabulary settings. Our contributions include a novel spatio-temporal video grounding model, surpassing state-of-the-art results in closed-set evaluations on multiple datasets and demonstrating superior performance in open-vocabulary scenarios. Notably, the proposed model outperforms state-of-the-art methods in closed-set settings on VidSTG (Declarative and Interrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in open-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model surpasses the recent best-performing models by $4.26$ m_vIoU and $1.83\\%$ accuracy, demonstrating its efficacy in handling diverse linguistic and visual concepts for improved video understanding. Our codes will be released at https://github.com/TalalWasim/Video-GroundingDINO."
              ],
              [
                "Access Date",
                "2024-01-09 04:41:09"
              ],
              [
                "Archiveid",
                "arXiv:2401.00901"
              ],
              [
                "Creators",
                "Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan"
              ],
              [
                "Date",
                "2023-12-31 2023-12-31"
              ],
              [
                "Extra",
                "arXiv:2401.00901 [cs]"
              ],
              [
                "Library Catalog",
                "arXiv.org"
              ],
              [
                "Repository",
                "arXiv"
              ],
              [
                "Short Title",
                "Video-GroundingDINO"
              ],
              [
                "Title",
                "Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding"
              ],
              [
                "URL",
                "http://arxiv.org/abs/2401.00901"
              ]
            ],
            "resource": "storage/i3590.pdf",
            "selectable": false
          },
          {
            "text": "Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training",
            "item-id": "i3440",
            "nodes": [
              {
                "text": "Huang et al_2023_Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided.pdf",
                "item-id": "i3495",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Huang et al_2023_Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided.pdf",
                "item_type": "attachment",
                "item_note": null,
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Huang et al_2023_Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided.pdf"
                  ]
                ],
                "resource": "storage/i3495.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training",
            "item_type": "conferencePaper",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "The task of weakly supervised temporal sentence grounding aims at finding the corresponding temporal moments of a language description in the video, given video-language correspondence only at video-level. Most existing works select mismatched video-language pairs as negative samples and train the model to generate better positive proposals that are distinct from the negative ones. However, due to the complex temporal structure of videos, proposals distinct from the negative ones may correspond to several video segments but not necessarily the correct ground truth. To alleviate this problem, we propose an uncertainty-guided self-training technique to provide extra self-supervision signals to guide the weakly-supervised learning. The self-training process is based on teacher-student mutual learning with weak-strong augmentation, which enables the teacher network to generate relatively more reliable outputs compared to the student network, so that the student network can learn from the teacher's output. Since directly applying existing self-training methods in this task easily causes error accumulation, we specifically design two techniques in our self-training method: (1) we construct a Bayesian teacher network, leveraging its uncertainty as a weight to suppress the noisy teacher supervisory signals; (2) we leverage the cycle consistency brought by temporal data augmentation to perform mutual learning between the two networks. Experiments demonstrate our method's superiority on Charades-STA and ActivityNet Captions datasets. We also show in the experiment that our self-training method can be applied to improve the performance of multiple backbone methods."
              ],
              [
                "Access Date",
                "2024-01-08 15:40:49"
              ],
              [
                "Conference Name",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Creators",
                "Yifei Huang, Lijin Yang, Yoichi Sato"
              ],
              [
                "Date",
                "2023-00-00 2023"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "openaccess.thecvf.com"
              ],
              [
                "Pages",
                "18908-18918"
              ],
              [
                "Proceedings Title",
                "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
              ],
              [
                "Title",
                "Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training"
              ],
              [
                "URL",
                "https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.html"
              ]
            ],
            "resource": "storage/i3495.pdf",
            "selectable": false
          },
          {
            "text": "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "item-id": "i3448",
            "nodes": [
              {
                "text": "Yang et al_2022_Zero-Shot Video Question Answering via Frozen Bidirectional Language Models.pdf",
                "item-id": "i3517",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Yang et al_2022_Zero-Shot Video Question Answering via Frozen Bidirectional Language Models.pdf",
                "item_type": "attachment",
                "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_M8QZ4A6T/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/3\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/3\">Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/5\">Cross-modal training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/5\">Adapting to downstream tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/6\">Experimental setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/6\">Ablation studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/8\">Comparison with frozen autoregressive models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/8\">Comparison to the state of the art for zero-shot VideoQA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/9\">Freezing the BiLM is also beneficial in supervised settings</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QZ4A6T/10\">Conclusion</a></li></ul></div>",
                "node_type": "item",
                "metadata": [
                  [
                    "Title",
                    "Yang et al_2022_Zero-Shot Video Question Answering via Frozen Bidirectional Language Models.pdf"
                  ]
                ],
                "resource": "storage/i3517.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "item_type": "journalArticle",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Abstract Note",
                "Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training. Manual annotation of question and answers for videos, however, is tedious and prohibits scalability. To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs. In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules, (ii) we train such modules using Web-scraped multi-modal data, and finally (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question. Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in the few-shot and fully-supervised setting. Our code and models are publicly available at https://github.com/antoyang/FrozenBiLM."
              ],
              [
                "Access Date",
                "2024-01-08 09:13:31"
              ],
              [
                "Creators",
                "Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid"
              ],
              [
                "Date",
                "2022-12-06 2022-12-06"
              ],
              [
                "Language",
                "en"
              ],
              [
                "Library Catalog",
                "proceedings.neurips.cc"
              ],
              [
                "Pages",
                "124-141"
              ],
              [
                "Publication Title",
                "Advances in Neural Information Processing Systems"
              ],
              [
                "Title",
                "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models"
              ],
              [
                "URL",
                "https://proceedings.neurips.cc/paper_files/paper/2022/hash/00d1f03b87a401b1c7957e0cc785d0bc-Abstract-Conference.html"
              ],
              [
                "Volume",
                "35"
              ]
            ],
            "resource": "storage/i3517.pdf",
            "selectable": false
          }
        ],
        "item_title": "Video Reasoning",
        "item_type": null,
        "item_note": null,
        "node_type": "collection",
        "selectable": false
      }
    ],
    "item_title": "Reasoning",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Representation Learning",
    "item-id": "c23,i3219",
    "nodes": [
      {
        "text": "Active Multi-Task Representation Learning",
        "item-id": "i2223",
        "nodes": [
          {
            "text": "Chen et al_2022_Active Multi-Task Representation Learning.pdf",
            "item-id": "i2266",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2022_Active Multi-Task Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2022_Active Multi-Task Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i2266.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Active Multi-Task Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "To leverage the power of big data from source domains and overcome the scarcity of target domain samples, representation learning based on multi-task pretraining has become a standard approach in many applications. However, large-scale pretraining is often computationally expensive and not affordable for small organizations. When there is only one target task, most source tasks can be irrelevant, and we can actively sample a subset of source data from the most To leverage the power of big data from source tasks and overcome the scarcity of the target task samples, representation learning based on multi-task pretraining has become a standard approach in many applications. However, up until now, choosing which source tasks to include in the multi-task learning has been more art than science. In this paper, we give the first formal study on resource task sampling by leveraging the techniques from active learning. We propose an algorithm that iteratively estimates the relevance of each source task to the target task and samples from each source task based on the estimated relevance. Theoretically, we show that for the linear representation class, to achieve the same error rate, our algorithm can save up to a textit{number of source tasks} factor in the source task sample complexity, compared with the naive uniform sampling from all source tasks. We also provide experiments on real-world computer vision datasets to illustrate the effectiveness of our proposed method on both linear and convolutional neural network representation classes. We believe our paper serves as an important initial step to bring techniques from active learning to representation learning."
          ],
          [
            "Access Date",
            "2023-02-02 15:18:31"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Yifang Chen, Kevin Jamieson, Simon Du"
          ],
          [
            "Date",
            "2022-06-28 2022-06-28"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "3271-3298"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 39th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Active Multi-Task Representation Learning"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v162/chen22j.html"
          ]
        ],
        "resource": "storage/i2266.pdf",
        "selectable": false
      },
      {
        "text": "AdaMAE",
        "item-id": "i2756",
        "nodes": [
          {
            "text": "Bandara et al_2023_AdaMAE.pdf",
            "item-id": "i2758",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bandara et al_2023_AdaMAE.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bandara et al_2023_AdaMAE.pdf"
              ]
            ],
            "resource": "storage/i2758.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AdaMAE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Masked Autoencoders (MAEs) learn generalizable representations for image, text, audio, video, etc., by reconstructing masked input data from tokens of the visible data. Current MAE approaches for videos rely on random patch, tube, or frame based masking strategies to select these tokens. This paper proposes AdaMAE, an adaptive masking strategy for MAEs that is end-to-end trainable. Our adaptive masking strategy samples visible tokens based on the semantic context using an auxiliary sampling network. This network estimates a categorical distribution over spacetime-patch tokens. The tokens that increase the expected reconstruction error are rewarded and selected as visible tokens, motivated by the policy gradient algorithm in reinforcement learning. We show that AdaMAE samples more tokens from the high spatiotemporal information regions, thereby allowing us to mask 95% of tokens, resulting in lower memory requirements and faster pre-training. We conduct ablation studies on the Something-Something v2 (SSv2) dataset to demonstrate the efficacy of our adaptive sampling approach and report state-of-the-art results of 70.0% and 81.7% in top-1 accuracy on SSv2 and Kinetics-400 action classification datasets with a ViT-Base backbone and 800 pre-training epochs. Code and pre-trained models are available at: https://github.com/wgcban/adamae.git"
          ],
          [
            "Access Date",
            "2023-06-22 21:12:43"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Wele Gedara Chaminda Bandara, Naman Patel, Ali Gholami, Mehdi Nikkhah, Motilal Agrawal, Vishal M. Patel"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14507-14517"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "AdaMAE"
          ],
          [
            "Title",
            "AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning With Masked Autoencoders"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Bandara_AdaMAE_Adaptive_Masking_for_Efficient_Spatiotemporal_Learning_With_Masked_Autoencoders_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2758.pdf",
        "selectable": false
      },
      {
        "text": "Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation",
        "item-id": "i1780",
        "nodes": [
          {
            "text": "Luo et al_2020_Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation.pdf",
            "item-id": "i1782",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Luo et al_2020_Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_HLPUELU2/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HLPUELU2/2\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/2\">Domain Adaptation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/3\">Style Transfer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HLPUELU2/3\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/3\">Problem Settings and Overall Idea</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/3\">Random AdaIN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/4\">Adversarial Style Mining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/5\">Cost Function</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HLPUELU2/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/6\">Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/6\">Image Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/6\">Semantic Segmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/8\">Analysis of the proposed method</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HLPUELU2/9\">Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Luo et al_2020_Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation.pdf"
              ]
            ],
            "resource": "storage/i1782.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We aim at the problem named One-Shot Unsupervised Domain Adaptation. Unlike traditional Unsupervised Domain Adaptation, it assumes that only one unlabeled target sample can be available when learning to adapt. This setting is realistic but more challenging, in which conventional adaptation approaches are prone to failure due to the scarce of unlabeled target data. To this end, we propose a novel Adversarial Style Mining approach, which combines the style transfer module and task-specific module into an adversarial manner. Specifically, the style transfer module iteratively searches for harder stylized images around the one-shot target sample according to the current learning state, leading the task model to explore the potential styles that are difficult to solve in the almost unseen target domain, thus boosting the adaptation performance in a data-scarce scenario. The adversarial learning framework makes the style transfer module and task-specific module benefit each other during the competition. Extensive experiments on both cross-domain classification and segmentation benchmarks verify that ASM achieves state-of-the-art adaptation performance under the challenging one-shot setting."
          ],
          [
            "Access Date",
            "2022-09-14 09:06:04"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "20612\u201320623"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1782.pdf",
        "selectable": false
      },
      {
        "text": "Are Large-scale Datasets Necessary for Self-Supervised Pre-training?",
        "item-id": "i1883",
        "nodes": [
          {
            "text": "El-Nouby et al_2021_Are Large-scale Datasets Necessary for Self-Supervised Pre-training.pdf",
            "item-id": "i1993",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "El-Nouby et al_2021_Are Large-scale Datasets Necessary for Self-Supervised Pre-training.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_QS2J6TYL/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/2\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/3\">3 . Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/3\">3.1 . Sample Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/4\">3.2 . Learning using non object-centric images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/4\">3.3 . Tokenizers</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/5\">4 . Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/5\">4.1 . SplitMask</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/5\">4.2 . Encoder-Decoder Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/5\">4.3 . Global Contrastive Loss</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/6\">5 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/6\">5.1 . Datasets</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/6\">5.2 . Dense Prediction</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/6\">5.2.1 Object detection and Instance Segmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/6\">5.2.2 Semantic Segmentation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/7\">5.3 . Image Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/8\">5.4 . Pre-training using ImageNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/8\">5.5 . Implementation Details</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/9\">6 . Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/13\">A . SplitMask vs BEiT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/13\">B . Encoder-Decoder vs BEiT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/13\">C . Overfitting during pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_QS2J6TYL/14\">D . Image Classification Finetuning</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "El-Nouby et al_2021_Are Large-scale Datasets Necessary for Self-Supervised Pre-training.pdf"
              ]
            ],
            "resource": "storage/i1993.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Are Large-scale Datasets Necessary for Self-Supervised Pre-training?",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Pre-training models on large scale datasets, like ImageNet, is a standard practice in computer vision. This paradigm is especially effective for tasks with small training sets, for which high-capacity models tend to overfit. In this work, we consider a self-supervised pre-training scenario that only leverages the target task data. We consider datasets, like Stanford Cars, Sketch or COCO, which are order(s) of magnitude smaller than Imagenet. Our study shows that denoising autoencoders, such as BEiT or a variant that we introduce in this paper, are more robust to the type and size of the pre-training data than popular self-supervised methods trained by comparing image embeddings.We obtain competitive performance compared to ImageNet pre-training on a variety of classification datasets, from different domains. On COCO, when pre-training solely using COCO images, the detection and instance segmentation performance surpasses the supervised ImageNet pre-training in a comparable setting."
          ],
          [
            "Access Date",
            "2022-10-30 16:46:30"
          ],
          [
            "Archiveid",
            "arXiv:2112.10740"
          ],
          [
            "Creators",
            "Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv\u00e9 Jegou, Edouard Grave"
          ],
          [
            "DOI",
            "10.48550/arXiv.2112.10740"
          ],
          [
            "Date",
            "2021-12-20 2021-12-20"
          ],
          [
            "Extra",
            "arXiv:2112.10740 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Are Large-scale Datasets Necessary for Self-Supervised Pre-training?"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2112.10740"
          ]
        ],
        "resource": "storage/i1993.pdf",
        "selectable": false
      },
      {
        "text": "BART",
        "item-id": "i1860",
        "nodes": [
          {
            "text": "Lewis et al_2019_BART.pdf",
            "item-id": "i1957",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lewis et al_2019_BART.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lewis et al_2019_BART.pdf"
              ]
            ],
            "resource": "storage/i1957.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "BART",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance."
          ],
          [
            "Access Date",
            "2022-11-01 15:57:37"
          ],
          [
            "Archiveid",
            "arXiv:1910.13461"
          ],
          [
            "Creators",
            "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer"
          ],
          [
            "DOI",
            "10.48550/arXiv.1910.13461"
          ],
          [
            "Date",
            "2019-10-29 2019-10-29"
          ],
          [
            "Extra",
            "arXiv:1910.13461 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "BART"
          ],
          [
            "Title",
            "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1910.13461"
          ]
        ],
        "resource": "storage/i1957.pdf",
        "selectable": false
      },
      {
        "text": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
        "item-id": "i1840",
        "nodes": [
          {
            "text": "Chen et al_2020_Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf",
            "item-id": "i1925",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf"
              ]
            ],
            "resource": "storage/i1925.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels (\u226413 labeled images per class) using ResNet-50, a 10X improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels."
          ],
          [
            "Access Date",
            "2022-11-03 06:11:36"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey E Hinton"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "22243\u201322255"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Big Self-Supervised Models are Strong Semi-Supervised Learners"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1925.pdf",
        "selectable": false
      },
      {
        "text": "Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning",
        "item-id": "i1858",
        "nodes": [
          {
            "text": "Grill et al_2020_Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning.pdf",
            "item-id": "i1953",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Grill et al_2020_Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Grill et al_2020_Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning.pdf"
              ]
            ],
            "resource": "storage/i1953.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks."
          ],
          [
            "Access Date",
            "2022-11-01 16:00:16"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, Michal Valko"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "21271\u201321284"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1953.pdf",
        "selectable": false
      },
      {
        "text": "Context Encoders",
        "item-id": "i1856",
        "nodes": [
          {
            "text": "Pathak et al_2016_Context Encoders.pdf",
            "item-id": "i1991",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Pathak et al_2016_Context Encoders.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Pathak et al_2016_Context Encoders.pdf"
              ]
            ],
            "resource": "storage/i1991.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Context Encoders",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
          ],
          [
            "Access Date",
            "2022-10-30 16:48:06"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2536-2544"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Context Encoders"
          ],
          [
            "Title",
            "Context Encoders: Feature Learning by Inpainting"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i1991.pdf",
        "selectable": false
      },
      {
        "text": "EVA",
        "item-id": "i2206",
        "nodes": [
          {
            "text": "Comment: v2: (i) fix / update EVA IN-1K variants results. (ii) add / update EVA-CLIP results. (iii) add Appendix. (iv) r",
            "item-id": "n2239",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: v2: (i) fix / update EVA IN-1K variants results. (ii) add / update EVA-CLIP results. (iii) add Appendix. (iv) r",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: v2: (i) fix / update EVA IN-1K variants results. (ii) add / update EVA-CLIP results. (iii) add Appendix. (iv) release all the code and models at https://github.com/baaivision/EVA</div>",
            "node_type": "note"
          },
          {
            "text": "Fang et al_2022_EVA.pdf",
            "item-id": "i2238",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Fang et al_2022_EVA.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Fang et al_2022_EVA.pdf"
              ]
            ],
            "resource": "storage/i2238.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "EVA",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and models at https://github.com/baaivision/EVA."
          ],
          [
            "Access Date",
            "2023-02-28 13:33:55"
          ],
          [
            "Archiveid",
            "arXiv:2211.07636"
          ],
          [
            "Creators",
            "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2211.07636"
          ],
          [
            "Date",
            "2022-12-05 2022-12-05"
          ],
          [
            "Extra",
            "arXiv:2211.07636 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "EVA"
          ],
          [
            "Title",
            "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2211.07636"
          ]
        ],
        "resource": "storage/i2238.pdf",
        "selectable": false
      },
      {
        "text": "Emerging Properties in Self-Supervised Vision Transformers",
        "item-id": "i1863",
        "nodes": [
          {
            "text": "Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf",
            "item-id": "i1963",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf"
              ]
            ],
            "resource": "storage/i1963.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Emerging Properties in Self-Supervised Vision Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base."
          ],
          [
            "Access Date",
            "2022-11-01 15:53:53"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9650-9660"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Emerging Properties in Self-Supervised Vision Transformers"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1963.pdf",
        "selectable": false
      },
      {
        "text": "General Facial Representation Learning in a Visual-Linguistic Manner",
        "item-id": "i1909",
        "nodes": [
          {
            "text": "Zheng et al_2022_General Facial Representation Learning in a Visual-Linguistic Manner.pdf",
            "item-id": "i2081",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zheng et al_2022_General Facial Representation Learning in a Visual-Linguistic Manner.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zheng et al_2022_General Facial Representation Learning in a Visual-Linguistic Manner.pdf"
              ]
            ],
            "resource": "storage/i2081.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "General Facial Representation Learning in a Visual-Linguistic Manner",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "How to learn a universal facial representation that boosts all face analysis tasks This paper takes one step toward this goal. In this paper, we study the transfer performance of pre-trained models on face analysis tasks and introduce a framework, called FaRL, for general facial representation learning. On one hand, the framework involves a contrastive loss to learn high-level semantic meaning from image-text pairs. On the other hand, we propose exploring low-level information simultaneously to further enhance the face representation by adding a masked image modeling. We perform pre-training on LAION-FACE, a dataset containing a large amount of face image-text pairs, and evaluate the representation capability on multiple downstream tasks. We show that FaRL achieves better transfer performance compared with previous pre-trained models. We also verify its superiority in the low-data regime. More importantly, our model surpasses the state-of-the-art methods on face analysis tasks including face parsing and face alignment."
          ],
          [
            "Access Date",
            "2022-10-12 15:53:26"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, Fang Wen"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "18697-18709"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "General Facial Representation Learning in a Visual-Linguistic Manner"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_General_Facial_Representation_Learning_in_a_Visual-Linguistic_Manner_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2081.pdf",
        "selectable": false
      },
      {
        "text": "Graph Masked Autoencoders with Transformers",
        "item-id": "i1684",
        "nodes": [
          {
            "text": "Zhang et al_2022_Graph Masked Autoencoders with Transformers.pdf",
            "item-id": "i1716",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2022_Graph Masked Autoencoders with Transformers.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YSVXHQJJ/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/2\">2.1 Self-supervised Graph-level Representation Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/2\">2.2 Self-supervised Node-level Representation Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/3\">2.3 Transformers</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/3\">3 Preliminaries</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/3\">3.1 Notations and Definitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/3\">3.2 Graph Transformers</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/4\">4 Graph Masked Autoencoder</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/4\">4.1 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/4\">4.2 Positional Embeddings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/4\">4.3 Evaluation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/4\">4.4 Properties</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/4\">4.4.1 Masking mechanism</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">4.4.2 Asymmetric encoder-decoder design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">4.4.3 Scalability</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">5.1 GMAE Self-supervised Training for Graph Classification</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">5.1.1 Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">5.1.2 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/5\">5.1.3 Experiment Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/6\">5.1.4 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/6\">5.2 GMAE Self-supervised Training for Node Classification</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/6\">5.2.1 Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/6\">5.2.2 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/6\">5.2.3 Experiment Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/7\">5.2.4 Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/7\">5.3 GMAE Pre-training &amp;amp; Fine-tuning</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/8\">5.4 Analysis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/8\">5.4.1 Memory Consumption</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/9\">5.4.2 Mask Ratio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/9\">5.4.3 Number of Decoder Layers</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/9\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/9\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/11\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/11\">Sixiao Zhang</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/11\">Hongxu Chen</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/11\">Haoran Yang</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/11\">Xiangguo Sun</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/11\">Philip S. Yu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YSVXHQJJ/12\">Guandong Xu</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2022_Graph Masked Autoencoders with Transformers.pdf"
              ]
            ],
            "resource": "storage/i1716.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Graph Masked Autoencoders with Transformers",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, transformers have shown promising performance in learning graph representations. However, there are still some challenges when applying transformers to real-world scenarios due to the fact that deep transformers are hard to train from scratch and the quadratic memory consumption w.r.t. the number of nodes. In this paper, we propose Graph Masked Autoencoders (GMAEs), a self-supervised transformer-based model for learning graph representations. To address the above two challenges, we adopt the masking mechanism and the asymmetric encoder-decoder design. Specifically, GMAE takes partially masked graphs as input, and reconstructs the features of the masked nodes. The encoder and decoder are asymmetric, where the encoder is a deep transformer and the decoder is a shallow transformer. The masking mechanism and the asymmetric design make GMAE a memory-efficient model compared with conventional transformers. We show that, when serving as a conventional self-supervised graph representation model, GMAE achieves state-of-the-art performance on both the graph classification task and the node classification task under common downstream evaluation protocols. We also show that, compared with training in an end-to-end manner from scratch, we can achieve comparable performance after pre-training and fine-tuning using GMAE while simplifying the training process."
          ],
          [
            "Access Date",
            "2022-07-26 02:58:42"
          ],
          [
            "Archiveid",
            "arXiv:2202.08391"
          ],
          [
            "Creators",
            "Sixiao Zhang, Hongxu Chen, Haoran Yang, Xiangguo Sun, Philip S. Yu, Guandong Xu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2202.08391"
          ],
          [
            "Date",
            "2022-05-12 2022-05-12"
          ],
          [
            "Extra",
            "arXiv:2202.08391 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Graph Masked Autoencoders with Transformers"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2202.08391"
          ]
        ],
        "resource": "storage/i1716.pdf",
        "selectable": false
      },
      {
        "text": "Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation",
        "item-id": "i2561",
        "nodes": [
          {
            "text": "Adiban et al_2022_Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder.pdf",
            "item-id": "i2638",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Adiban et al_2022_Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Adiban et al_2022_Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder.pdf"
              ]
            ],
            "resource": "storage/i2638.pdf"
          },
          {
            "text": "Other",
            "item-id": "n2640",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Other",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><h2>Other</h2>\n12 pages plus supplementary material. Submitted to BMVC 2022</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a multi-layer variational autoencoder method, we call HR-VQVAE, that learns hierarchical discrete representations of the data. By utilizing a novel objective function, each layer in HR-VQVAE learns a discrete representation of the residual from previous layers through a vector quantized encoder. Furthermore, the representations at each layer are hierarchically linked to those at previous layers. We evaluate our method on the tasks of image reconstruction and generation. Experimental results demonstrate that the discrete representations learned by HR-VQVAE enable the decoder to reconstruct high-quality images with less distortion than the baseline methods, namely VQVAE and VQVAE-2. HR-VQVAE can also generate high-quality and diverse images that outperform state-of-the-art generative models, providing further verification of the efficiency of the learned representations. The hierarchical nature of HR-VQVAE i) reduces the decoding search time, making the method particularly suitable for high-load tasks and ii) allows to increase the codebook size without incurring the codebook collapse problem."
          ],
          [
            "Access Date",
            "2023-06-02 07:17:42"
          ],
          [
            "Conference Name",
            "British Machine Vision Conference"
          ],
          [
            "Creators",
            "Mohammad Adiban, Kalin Stefanov, Sabato Marco Siniscalchi, Giampiero Salvi"
          ],
          [
            "DOI",
            "10.48550/ARXIV.2208.04554"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Version Number: 1"
          ],
          [
            "Library Catalog",
            "Semantic Scholar"
          ],
          [
            "Place",
            "London, UK"
          ],
          [
            "Proceedings Title",
            "British Machine Vision Conference"
          ],
          [
            "Publisher",
            "BMVA Press"
          ],
          [
            "Rights",
            "Creative Commons Attribution Non Commercial No Derivatives 4.0 International"
          ],
          [
            "Title",
            "Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation"
          ],
          [
            "URL",
            "https://bmvc2022.mpi-inf.mpg.de/0636.pdf"
          ]
        ],
        "resource": "storage/i2638.pdf",
        "selectable": false
      },
      {
        "text": "ImageBind",
        "item-id": "i2384",
        "nodes": [
          {
            "text": "Comment: CVPR 2023 (Highlighted Paper). Website: https://imagebind.metademolab.com/ Code/Models: https://github.com/face",
            "item-id": "n2455",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: CVPR 2023 (Highlighted Paper). Website: https://imagebind.metademolab.com/ Code/Models: https://github.com/face",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: CVPR 2023 (Highlighted Paper). Website: https://imagebind.metademolab.com/ Code/Models: https://github.com/facebookresearch/ImageBind</div>",
            "node_type": "note"
          },
          {
            "text": "Girdhar et al_2023_ImageBind.pdf",
            "item-id": "i2454",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Girdhar et al_2023_ImageBind.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Girdhar et al_2023_ImageBind.pdf"
              ]
            ],
            "resource": "storage/i2454.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "ImageBind",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
          ],
          [
            "Access Date",
            "2023-05-12 05:24:13"
          ],
          [
            "Archiveid",
            "arXiv:2305.05665"
          ],
          [
            "Creators",
            "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra"
          ],
          [
            "DOI",
            "10.48550/arXiv.2305.05665"
          ],
          [
            "Date",
            "2023-05-09 2023-05-09"
          ],
          [
            "Extra",
            "arXiv:2305.05665 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "ImageBind"
          ],
          [
            "Title",
            "ImageBind: One Embedding Space To Bind Them All"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.05665"
          ]
        ],
        "resource": "storage/i2454.pdf",
        "selectable": false
      },
      {
        "text": "Improving Visual Representation Learning Through Perceptual Understanding",
        "item-id": "i2753",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n2919",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotation</p>\n<p>Also uses GAN adversarial training for masked auto encoder (MAE).</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Tukra et al_2023_Improving Visual Representation Learning Through Perceptual Understanding.pdf",
            "item-id": "i2755",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tukra et al_2023_Improving Visual Representation Learning Through Perceptual Understanding.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tukra et al_2023_Improving Visual Representation Learning Through Perceptual Understanding.pdf"
              ]
            ],
            "resource": "storage/i2755.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Improving Visual Representation Learning Through Perceptual Understanding",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present an extension to masked autoencoders (MAE) which improves on the representations learnt by the model by explicitly encouraging the learning of higher scene-level features. We do this by: (i) the introduction of a perceptual similarity term between generated and real images (ii) incorporating several techniques from the adversarial training literature including multi-scale training and adaptive discriminator augmentation. The combination of these results in not only better pixel reconstruction but also representations which appear to capture better higher-level details within images. More consequentially, we show how our method, Perceptual MAE, leads to better performance when used for downstream tasks outperforming previous methods. We achieve 78.1% top-1 accuracy linear probing on ImageNet-1K and up to 88.1% when fine-tuning, with similar results for other downstream tasks, all without use of additional pre-trained models or data."
          ],
          [
            "Access Date",
            "2023-06-22 21:11:42"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Samyakh Tukra, Frederick Hoffman, Ken Chatfield"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14486-14495"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Improving Visual Representation Learning Through Perceptual Understanding"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Tukra_Improving_Visual_Representation_Learning_Through_Perceptual_Understanding_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2755.pdf",
        "selectable": false
      },
      {
        "text": "InfoGAN",
        "item-id": "i1834",
        "nodes": [
          {
            "text": "Chen et al_2016_InfoGAN.pdf",
            "item-id": "i1916",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2016_InfoGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2016_InfoGAN.pdf"
              ]
            ],
            "resource": "storage/i1916.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "InfoGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods."
          ],
          [
            "Access Date",
            "2022-11-03 10:58:01"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "InfoGAN"
          ],
          [
            "Title",
            "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html"
          ],
          [
            "Volume",
            "29"
          ]
        ],
        "resource": "storage/i1916.pdf",
        "selectable": false
      },
      {
        "text": "InternVideo",
        "item-id": "i3218",
        "nodes": [
          {
            "text": "Comment: technical report",
            "item-id": "n3301",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: technical report",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: technical report</div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2022_InternVideo.pdf",
            "item-id": "i3300",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_InternVideo.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_InternVideo.pdf"
              ]
            ],
            "resource": "storage/i3300.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "InternVideo",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The foundation models have recently shown excellent performance on a variety of downstream tasks in computer vision. However, most existing vision foundation models simply focus on image-level pretraining and adpation, which are limited for dynamic and complex video-level understanding tasks. To fill the gap, we present general video foundation models, InternVideo, by taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Especially, our methods can obtain 91.1% and 77.2% top-1 accuracy on the challenging Kinetics-400 and Something-Something V2 benchmarks, respectively. All of these results effectively show the generality of our InternVideo for video understanding. The code will be released at https://github.com/OpenGVLab/InternVideo ."
          ],
          [
            "Access Date",
            "2023-11-08 14:46:33"
          ],
          [
            "Archiveid",
            "arXiv:2212.03191"
          ],
          [
            "Creators",
            "Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao"
          ],
          [
            "DOI",
            "10.48550/arXiv.2212.03191"
          ],
          [
            "Date",
            "2022-12-07 2022-12-07"
          ],
          [
            "Extra",
            "arXiv:2212.03191 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "InternVideo"
          ],
          [
            "Title",
            "InternVideo: General Video Foundation Models via Generative and Discriminative Learning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2212.03191"
          ]
        ],
        "resource": "storage/i3300.pdf",
        "selectable": false
      },
      {
        "text": "Large Scale Adversarial Representation Learning",
        "item-id": "i1833",
        "nodes": [
          {
            "text": "Donahue_Simonyan_2019_Large Scale Adversarial Representation Learning.pdf",
            "item-id": "i1915",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Donahue_Simonyan_2019_Large Scale Adversarial Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Donahue_Simonyan_2019_Large Scale Adversarial Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i1915.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Large Scale Adversarial Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as compelling results in unconditional image generation."
          ],
          [
            "Access Date",
            "2022-11-03 11:06:36"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Jeff Donahue, Karen Simonyan"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Large Scale Adversarial Representation Learning"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/18cdf49ea54eec029238fcc95f76ce41-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i1915.pdf",
        "selectable": false
      },
      {
        "text": "Learning Emotion Representations From Verbal and Nonverbal Communication",
        "item-id": "i2759",
        "nodes": [
          {
            "text": "Zhang et al_2023_Learning Emotion Representations From Verbal and Nonverbal Communication.pdf",
            "item-id": "i2761",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_Learning Emotion Representations From Verbal and Nonverbal Communication.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_Learning Emotion Representations From Verbal and Nonverbal Communication.pdf"
              ]
            ],
            "resource": "storage/i2761.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Emotion Representations From Verbal and Nonverbal Communication",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Emotion understanding is an essential but highly challenging component of artificial general intelligence. The absence of extensive annotated datasets has significantly impeded advancements in this field. We present EmotionCLIP, the first pre-training paradigm to extract visual emotion representations from verbal and nonverbal communication using only uncurated data. Compared to numerical labels or descriptions used in previous methods, communication naturally contains emotion information. Furthermore, acquiring emotion representations from communication is more congruent with the human learning process. We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emotion cues using sentiment-guided contrastive learning. Extensive experiments validate the effectiveness and transferability of EmotionCLIP. Using merely linear-probe evaluation protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and rivals many multimodal approaches across various benchmarks. We anticipate that the advent of EmotionCLIP will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related domains. The code and pre-trained models are available at https://github.com/Xeaver/EmotionCLIP."
          ],
          [
            "Access Date",
            "2023-06-22 21:14:25"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Sitao Zhang, Yimu Pan, James Z. Wang"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "18993-19004"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Learning Emotion Representations From Verbal and Nonverbal Communication"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2761.pdf",
        "selectable": false
      },
      {
        "text": "Learning Transferable Visual Models From Natural Language Supervision",
        "item-id": "i1829",
        "nodes": [
          {
            "text": "Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf",
            "item-id": "i2076",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf"
              ]
            ],
            "resource": "storage/i2076.pdf"
          },
          {
            "text": "Supplementary PDF",
            "item-id": "i2077",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2022-10-12 16:26:23"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v139/radford21a/radford21a-supp.pdf"
              ]
            ],
            "resource": "storage/i2077.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Transferable Visual Models From Natural Language Supervision",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on."
          ],
          [
            "Access Date",
            "2022-10-12 16:26:21"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "8748-8763"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 38th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Learning Transferable Visual Models From Natural Language Supervision"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v139/radford21a.html"
          ]
        ],
        "selectable": false
      },
      {
        "text": "M$^3$Video",
        "item-id": "i1901",
        "nodes": [
          {
            "text": "Sun et al_2022_M$^3$Video.pdf",
            "item-id": "i2032",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sun et al_2022_M$^3$Video.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sun et al_2022_M$^3$Video.pdf"
              ]
            ],
            "resource": "storage/i2032.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "M$^3$Video",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We study self-supervised video representation learning that seeks to learn video features from unlabeled videos, which is widely used for video analysis as labeling videos is labor-intensive. Current methods often mask some video regions and then train a model to reconstruct spatial information in these regions (e.g., original pixels). However, the model is easy to reconstruct this information by considering content in a single frame. As a result, it may neglect to learn the interactions between frames, which are critical for video analysis. In this paper, we present a new self-supervised learning task, called Masked Motion Modeling (M$^3$Video), for learning representation by enforcing the model to predict the motion of moving objects in the masked regions. To generate motion targets for this task, we track the objects using optical flow. The motion targets consist of position transitions and shape changes of the tracked objects, thus the model has to consider multiple frames comprehensively. Besides, to help the model capture fine-grained motion details, we enforce the model to predict trajectory motion targets in high temporal resolution based on a video in low temporal resolution. After pre-training using our M$^3$Video task, the model is able to anticipate fine-grained motion details even taking a sparsely sampled video as input. We conduct extensive experiments on four benchmark datasets. Remarkably, when doing pre-training with 400 epochs, we improve the accuracy from 67.6\\% to 69.2\\% and from 78.8\\% to 79.7\\% on Something-Something V2 and Kinetics-400 datasets, respectively."
          ],
          [
            "Access Date",
            "2022-10-28 13:11:03"
          ],
          [
            "Archiveid",
            "arXiv:2210.06096"
          ],
          [
            "Creators",
            "Xinyu Sun, Peihao Chen, Liangwei Chen, Thomas H. Li, Mingkui Tan, Chuang Gan"
          ],
          [
            "DOI",
            "10.48550/arXiv.2210.06096"
          ],
          [
            "Date",
            "2022-10-12 2022-10-12"
          ],
          [
            "Extra",
            "arXiv:2210.06096 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "M$^3$Video"
          ],
          [
            "Title",
            "M$^3$Video: Masked Motion Modeling for Self-Supervised Video Representation Learning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2210.06096"
          ]
        ],
        "resource": "storage/i2032.pdf",
        "selectable": false
      },
      {
        "text": "MARLIN",
        "item-id": "i2371",
        "nodes": [
          {
            "text": "Cai et al_2023_MARLIN.pdf",
            "item-id": "i2405",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cai et al_2023_MARLIN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cai et al_2023_MARLIN.pdf"
              ]
            ],
            "resource": "storage/i2405.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MARLIN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper proposes a self-supervised approach to learn universal facial representations from videos, that can transfer across a variety of facial analysis tasks such as Facial Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our proposed framework, named MARLIN, is a facial video masked autoencoder, that learns highly robust and generic facial embeddings from abundantly available non-annotated web crawled facial videos. As a challenging auxiliary task, MARLIN reconstructs the spatio-temporal details of the face from the densely masked facial regions which mainly include eyes, nose, mouth, lips, and skin to capture local and global aspects that in turn help in encoding generic and transferable features. Through a variety of experiments on diverse downstream tasks, we demonstrate MARLIN to be an excellent facial video encoder as well as feature extractor, that performs consistently well across a variety of downstream tasks including FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for Frechet Inception Distance), and even in low data regime. Our code and models are available at https://github.com/ControlNet/MARLIN."
          ],
          [
            "Access Date",
            "2023-05-25 02:56:02"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza Haffari, Munawar Hayat"
          ],
          [
            "DOI",
            "10.1109/CVPR52729.2023.00150"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "ISBN",
            "979-8-3503-0129-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1493-1504"
          ],
          [
            "Place",
            "Vancouver, BC, Canada"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Publisher",
            "IEEE"
          ],
          [
            "Rights",
            "All rights reserved"
          ],
          [
            "Short Title",
            "MARLIN"
          ],
          [
            "Title",
            "MARLIN: Masked Autoencoder for Facial Video Representation LearnINg"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper"
          ]
        ],
        "resource": "storage/i2405.pdf",
        "selectable": false
      },
      {
        "text": "MERLOT Reserve",
        "item-id": "i1682",
        "nodes": [
          {
            "text": "Zellers et al_2022_MERLOT Reserve.pdf",
            "item-id": "i1712",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zellers et al_2022_MERLOT Reserve.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zellers et al_2022_MERLOT Reserve.pdf"
              ]
            ],
            "resource": "storage/i1712.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MERLOT Reserve",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce MERLOT Reserve, a model that represents videos jointly over time -- through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos. Empirical results show that MERLOT Reserve learns strong multimodal representations. When finetuned, it sets state-of-the-art on Visual Commonsense Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%, and 1.5% respectively. Ablations show that these tasks benefit from audio pretraining -- even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark. We analyze why audio enables better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining."
          ],
          [
            "Access Date",
            "2022-07-29 07:39:40"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, Yejin Choi"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "16375-16387"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MERLOT Reserve"
          ],
          [
            "Title",
            "MERLOT Reserve: Neural Script Knowledge Through Vision and Language and Sound"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1712.pdf",
        "selectable": false
      },
      {
        "text": "MaskViT",
        "item-id": "i1686",
        "nodes": [
          {
            "text": "Comment: Project page: https://maskedvit.github.io/",
            "item-id": "n1721",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page: https://maskedvit.github.io/",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page: https://maskedvit.github.io/</div>",
            "node_type": "note"
          },
          {
            "text": "Gupta et al_2022_MaskViT.pdf",
            "item-id": "i1719",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gupta et al_2022_MaskViT.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_3HC7FEX3/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/3\">3 MaskViT: Masked Video Transformer</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/3\">3.1 Learning Visual Tokens</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/3\">3.2 Masked Visual Modeling (MVM)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/4\">3.3 Bidirectional Window Transformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/4\">3.4 Iterative Decoding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/4\">4 Experimental Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/5\">4.1 Experimental Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/5\">4.2 Comparison with Prior Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/6\">4.3 Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/7\">4.4 Visual Model Predictive Control with MaskViT on a Real Robot</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/8\">5 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/13\">A Implementation Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/13\">A.1 Training MaskViT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/14\">A.2 Real Robot Experiments</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/15\">B Additional Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/15\">B.1 Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3HC7FEX3/15\">B.2 Quantitative Results</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gupta et al_2022_MaskViT.pdf"
              ]
            ],
            "resource": "storage/i1719.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MaskViT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The ability to predict future visual observations conditioned on past observations and motor commands can enable embodied agents to plan solutions to a variety of tasks in complex environments. This work shows that we can create good video prediction models by pre-training transformers via masked visual modeling. Our approach, named MaskViT, is based on two simple design decisions. First, for memory and training efficiency, we use two types of window attention: spatial and spatiotemporal. Second, during training, we mask a variable percentage of tokens instead of a fixed mask ratio. For inference, MaskViT generates all tokens via iterative refinement where we incrementally decrease the masking ratio following a mask scheduling function. On several datasets we demonstrate that MaskViT outperforms prior works in video prediction, is parameter efficient, and can generate high-resolution videos (256x256). Further, we demonstrate the benefits of inference speedup (up to 512x) due to iterative decoding by using MaskViT for planning on a real robot. Our work suggests that we can endow embodied agents with powerful predictive models by leveraging the general framework of masked visual modeling with minimal domain knowledge."
          ],
          [
            "Access Date",
            "2022-07-26 02:57:35"
          ],
          [
            "Archiveid",
            "arXiv:2206.11894"
          ],
          [
            "Creators",
            "Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00edn-Mart\u00edn, Li Fei-Fei"
          ],
          [
            "DOI",
            "10.48550/arXiv.2206.11894"
          ],
          [
            "Date",
            "2022-06-23 2022-06-23"
          ],
          [
            "Extra",
            "arXiv:2206.11894 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MaskViT"
          ],
          [
            "Title",
            "MaskViT: Masked Visual Pre-Training for Video Prediction"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2206.11894"
          ]
        ],
        "resource": "storage/i1719.pdf",
        "selectable": false
      },
      {
        "text": "Masked Autoencoders Are Scalable Vision Learners",
        "item-id": "i1687",
        "nodes": [
          {
            "text": "He et al_2022_Masked Autoencoders Are Scalable Vision Learners.pdf",
            "item-id": "i1723",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "He et al_2022_Masked Autoencoders Are Scalable Vision Learners.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "He et al_2022_Masked Autoencoders Are Scalable Vision Learners.pdf"
              ]
            ],
            "resource": "storage/i1723.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Masked Autoencoders Are Scalable Vision Learners",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior."
          ],
          [
            "Access Date",
            "2022-07-26 02:56:11"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "16000-16009"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Masked Autoencoders Are Scalable Vision Learners"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1723.pdf",
        "selectable": false
      },
      {
        "text": "Masked Feature Prediction for Self-Supervised Visual Pre-Training",
        "item-id": "i1689",
        "nodes": [
          {
            "text": "Wei et al_2022_Masked Feature Prediction for Self-Supervised Visual Pre-Training.pdf",
            "item-id": "i1728",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wei et al_2022_Masked Feature Prediction for Self-Supervised Visual Pre-Training.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wei et al_2022_Masked Feature Prediction for Self-Supervised Visual Pre-Training.pdf"
              ]
            ],
            "resource": "storage/i1728.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7% with MViTv2-L on Kinetics-400, 88.3% on Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet."
          ],
          [
            "Access Date",
            "2022-07-26 02:52:43"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, Christoph Feichtenhofer"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14668-14678"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Masked Feature Prediction for Self-Supervised Visual Pre-Training"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1728.pdf",
        "selectable": false
      },
      {
        "text": "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation",
        "item-id": "i1705",
        "nodes": [
          {
            "text": "Comment: 22 pages, 8 figures. Under the review process",
            "item-id": "n1757",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 22 pages, 8 figures. Under the review process",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 22 pages, 8 figures. Under the review process</div>",
            "node_type": "note"
          },
          {
            "text": "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf",
            "item-id": "i1756",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G7GYACED/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/4\">2.1 Masked Autoencoders (MAE)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3 Masked Spectrogram Modeling using Masked Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3.1 Input Audio Duration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3.2 Patch Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">3.3 Feature Calculation for Downstream Tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4.1 Experimental Details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4.1.1 Pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/7\">4.1.2 Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/7\">4.2 Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/8\">4.3 Experimental Results: Comparison with the HEAR 2021 Results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4 Experimental Results: Impact of Design Choices on Performance</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4.1 Impact of Input Audio Duration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4.2 Impact of Patch Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/11\">4.4.3 Impact of Input Splitting: Patches vs. Strips</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/12\">4.5 Qualitative Analysis with Visualizations</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/12\">4.5.1 Reconstructions with Random Masks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/13\">4.5.2 Reconstructions with Patterned Masks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/13\">4.5.3 Reconstructions with Various Mask Ratios</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/15\">4.5.4 Self-Attention Map Visualizations</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/17\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/21\">A Reconstruction Examples of Various MSM-MAE Models</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf"
              ]
            ],
            "resource": "storage/i1756.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent general-purpose audio representations show state-of-the-art performance on various audio tasks. These representations are pre-trained by self-supervised learning methods that create training signals from the input. For example, typical audio contrastive learning uses temporal relationships among input sounds to create training signals, whereas some methods use a difference among input views created by data augmentations. However, these training signals do not provide information derived from the intact input sound, which we think is suboptimal for learning representation that describes the input as it is. In this paper, we seek to learn audio representations from the input itself as supervision using a pretext task of auto-encoding of masked spectrogram patches, Masked Spectrogram Modeling (MSM, a variant of Masked Image Modeling applied to audio spectrogram). To implement MSM, we use Masked Autoencoders (MAE), an image self-supervised learning method. MAE learns to efficiently encode the small number of visible patches into latent representations to carry essential information for reconstructing a large number of masked patches. While training, MAE minimizes the reconstruction error, which uses the input as training signal, consequently achieving our goal. We conducted experiments on our MSM using MAE (MSM-MAE) models under the evaluation benchmark of the HEAR 2021 NeurIPS Challenge. Our MSM-MAE models outperformed the HEAR 2021 Challenge results on seven out of 15 tasks (e.g., accuracies of 73.4% on CREMA-D and 85.8% on LibriCount), while showing top performance on other tasks where specialized models perform better. We also investigate how the design choices of MSM-MAE impact the performance and conduct qualitative analysis of visualization outcomes to gain an understanding of learned representations. We make our code available online."
          ],
          [
            "Access Date",
            "2022-07-17 14:53:45"
          ],
          [
            "Archiveid",
            "arXiv:2204.12260"
          ],
          [
            "Creators",
            "Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.12260"
          ],
          [
            "Date",
            "2022-04-26 2022-04-26"
          ],
          [
            "Extra",
            "arXiv:2204.12260 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.12260"
          ]
        ],
        "resource": "storage/i1756.pdf",
        "selectable": false
      },
      {
        "text": "Masked Visual Pre-training for Motor Control",
        "item-id": "i1685",
        "nodes": [
          {
            "text": "Comment: Code and videos at: https://tetexiao.com/projects/mvp",
            "item-id": "n1720",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Code and videos at: https://tetexiao.com/projects/mvp",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Code and videos at: https://tetexiao.com/projects/mvp</div>",
            "node_type": "note"
          },
          {
            "text": "Xiao et al_2022_Masked Visual Pre-training for Motor Control.pdf",
            "item-id": "i1717",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xiao et al_2022_Masked Visual Pre-training for Motor Control.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xiao et al_2022_Masked Visual Pre-training for Motor Control.pdf"
              ]
            ],
            "resource": "storage/i1717.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Masked Visual Pre-training for Motor Control",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images."
          ],
          [
            "Access Date",
            "2022-07-26 02:58:21"
          ],
          [
            "Archiveid",
            "arXiv:2203.06173"
          ],
          [
            "Creators",
            "Tete Xiao, Ilija Radosavovic, Trevor Darrell, Jitendra Malik"
          ],
          [
            "DOI",
            "10.48550/arXiv.2203.06173"
          ],
          [
            "Date",
            "2022-03-11 2022-03-11"
          ],
          [
            "Extra",
            "arXiv:2203.06173 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Masked Visual Pre-training for Motor Control"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2203.06173"
          ]
        ],
        "resource": "storage/i1717.pdf",
        "selectable": false
      },
      {
        "text": "Masked autoencoders as spatiotemporal learners",
        "item-id": "i2502",
        "nodes": [
          {
            "text": "Feichtenhofer et al_2022_Masked Autoencoders As Spatiotemporal Learners.pdf",
            "item-id": "i2698",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Feichtenhofer et al_2022_Masked Autoencoders As Spatiotemporal Learners.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Feichtenhofer et al_2022_Masked Autoencoders As Spatiotemporal Learners.pdf"
              ]
            ],
            "resource": "storage/i2698.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Masked autoencoders as spatiotemporal learners",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge."
          ],
          [
            "Conference Name",
            "Advances in neural information processing systems"
          ],
          [
            "Creators",
            "Christoph Feichtenhofer, haoqi fan, Yanghao Li, Kaiming He, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Citation Key: NEURIPS2022_e97d1081"
          ],
          [
            "Pages",
            "35946\u201335958"
          ],
          [
            "Proceedings Title",
            "Advances in neural information processing systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Masked autoencoders as spatiotemporal learners"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2022/file/e97d1081481a4017df96b51be31001d3-Paper-Conference.pdf"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i2698.pdf",
        "selectable": false
      },
      {
        "text": "Mixed Autoencoder for Self-Supervised Visual Representation Learning",
        "item-id": "i2786",
        "nodes": [
          {
            "text": "Chen et al_2023_Mixed Autoencoder for Self-Supervised Visual Representation Learning.pdf",
            "item-id": "i2915",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2023_Mixed Autoencoder for Self-Supervised Visual Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2023_Mixed Autoencoder for Self-Supervised Visual Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i2915.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Mixed Autoencoder for Self-Supervised Visual Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base. Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2x. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available."
          ],
          [
            "Access Date",
            "2023-07-01 07:11:44"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "22742-22751"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Mixed Autoencoder for Self-Supervised Visual Representation Learning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Mixed_Autoencoder_for_Self-Supervised_Visual_Representation_Learning_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2915.pdf",
        "selectable": false
      },
      {
        "text": "MultiMAE",
        "item-id": "i1683",
        "nodes": [
          {
            "text": "Bachmann et al_2022_MultiMAE.pdf",
            "item-id": "i1710",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bachmann et al_2022_MultiMAE.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2LUPTYTA/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/2\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/3\">3 . Method Description</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/3\">3.1 . Multi-modal encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/4\">3.2 . Decoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/4\">3.3 . Multi-modal masking strategies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/4\">3.4 . Pseudo labeled multi-task training dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/5\">3.5 . Pre-training details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4.1 . Transfer tasks and datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4.2 . Transfers with RGB-only</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4.3 . Transfers with multiple modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/7\">4.4 . Influence of pre-training task choices and masking on transfer performance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/8\">4.5 . Cross-modal exchange of information</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/9\">5 . Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/10\"></a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\"> </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">A . Additional pre-training implementation details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">B . Transfer implementation details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">B.1 . ImageNet classification fine-tuning setting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">B.2 . Semantic segmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/17\">B.3 . NYUv2 depth estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/17\">B.4 . Taskonomy dense regression tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/17\">C . Mask sampling strategies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/18\">D . Detailed Taskonomy transfer results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/18\">E . Robustness evaluation on ImageNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/18\">F . Comparison of MAE variants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/19\">G . Comparison of pre-training time </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/19\">H . Additional visualizations</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bachmann et al_2022_MultiMAE.pdf"
              ]
            ],
            "resource": "storage/i1710.pdf"
          },
          {
            "text": "Comment: Project page at https://multimae.epfl.ch",
            "item-id": "n1713",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page at https://multimae.epfl.ch",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page at https://multimae.epfl.ch</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MultiMAE",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two key aspects: I) it can optionally accept additional modalities of information in the input besides the RGB image (hence \"multi-modal\"), and II) its training objective accordingly includes predicting multiple outputs besides the RGB image (hence \"multi-task\"). We make use of masking (across image patches and input modalities) to make training MultiMAE tractable as well as to ensure cross-modality predictive coding is indeed learned by the network. We show this pre-training strategy leads to a flexible, simple, and efficient framework with improved transfer results to downstream tasks. In particular, the same exact pre-trained network can be flexibly used when additional information besides RGB images is available or when no information other than RGB is available - in all configurations yielding competitive to or significantly better results than the baselines. To avoid needing training datasets with multiple modalities and tasks, we train MultiMAE entirely using pseudo labeling, which makes the framework widely applicable to any RGB dataset. The experiments are performed on multiple transfer tasks (image classification, semantic segmentation, depth estimation) and datasets (ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an intriguingly impressive capability by the model in cross-modal/task predictive coding and transfer."
          ],
          [
            "Access Date",
            "2022-07-29 07:39:07"
          ],
          [
            "Archiveid",
            "arXiv:2204.01678"
          ],
          [
            "Creators",
            "Roman Bachmann, David Mizrahi, Andrei Atanov, Amir Zamir"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.01678"
          ],
          [
            "Date",
            "2022-04-04 2022-04-04"
          ],
          [
            "Extra",
            "arXiv:2204.01678 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MultiMAE"
          ],
          [
            "Title",
            "MultiMAE: Multi-modal Multi-task Masked Autoencoders"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.01678"
          ]
        ],
        "resource": "storage/i1710.pdf",
        "selectable": false
      },
      {
        "text": "Point Adversarial Self-Mining",
        "item-id": "i1781",
        "nodes": [
          {
            "text": "Liu et al_2021_Point Adversarial Self-Mining.pdf",
            "item-id": "i1783",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2021_Point Adversarial Self-Mining.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2021_Point Adversarial Self-Mining.pdf"
              ]
            ],
            "resource": "storage/i1783.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Point Adversarial Self-Mining",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this article, we propose a simple yet effective approach, called point adversarial self mining (PASM), to improve the recognition accuracy in facial expression recognition (FER). Unlike previous works focusing on designing specific architectures or loss functions to solve this problem, PASM boosts the network capability by simulating human learning processes: providing updated learning materials and guidance from more capable teachers. Specifically, to generate new learning materials, PASM leverages a point adversarial attack method and a trained teacher network to locate the most informative position related to the target task, generating harder learning samples to refine the network. The searched position is highly adaptive since it considers both the statistical information of each sample and the teacher network capability. Other than being provided new learning materials, the student network also receives guidance from the teacher network. After the student network finishes training, the student network changes its role and acts as a teacher, generating new learning materials and providing stronger guidance to train a better student network. The adaptive learning materials generation and teacher/student update can be conducted more than one time, improving the network capability iteratively. Extensive experimental results validate the efficacy of our method over the existing state of the arts for FER."
          ],
          [
            "Creators",
            "Ping Liu, Yuewei Lin, Zibo Meng, Lu Lu, Weihong Deng, Joey Tianyi Zhou, Yi Yang"
          ],
          [
            "DOI",
            "10.1109/TCYB.2021.3085744"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Cybernetics"
          ],
          [
            "ISSN",
            "2168-2275"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-12"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Cybernetics"
          ],
          [
            "Short Title",
            "Point Adversarial Self-Mining"
          ],
          [
            "Title",
            "Point Adversarial Self-Mining: A Simple Method for Facial Expression Recognition"
          ]
        ],
        "resource": "storage/i1783.pdf",
        "selectable": false
      },
      {
        "text": "Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation",
        "item-id": "i2750",
        "nodes": [
          {
            "text": "Liu et al_2023_Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation.pdf",
            "item-id": "i2752",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2023_Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2023_Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation.pdf"
              ]
            ],
            "resource": "storage/i2752.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-supervised facial representation has recently attracted increasing attention due to its ability to perform face understanding without relying on large-scale annotated datasets heavily. However, analytically, current contrastive-based self-supervised learning (SSL) still performs unsatisfactorily for learning facial representation. More specifically, existing contrastive learning (CL) tends to learn pose-invariant features that cannot depict the pose details of faces, compromising the learning performance. To conquer the above limitation of CL, we propose a novel Pose-disentangled Contrastive Learning (PCL) method for general self-supervised facial representation. Our PCL first devises a pose-disentangled decoder (PDD) with a delicately designed orthogonalizing regulation, which disentangles the pose-related features from the face-aware features; therefore, pose-related and other pose-unrelated facial information could be performed in individual subnetworks and do not affect each other's training. Furthermore, we introduce a pose-related contrastive learning scheme that learns pose-related information based on data augmentation of the same image, which would deliver more effective face-aware representation for various downstream tasks. We conducted linear evaluation on four challenging downstream facial understanding tasks, i.e., facial expression recognition, face recognition, AU detection and head pose estimation.Experimental results demonstrate that PCL significantly outperforms cutting-edge SSL methods. Our Code is available at https://github.com/DreamMr/PCL."
          ],
          [
            "Access Date",
            "2023-06-22 21:09:50"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yuanyuan Liu, Wenbin Wang, Yibing Zhan, Shaoze Feng, Kejun Liu, Zhe Chen"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9717-9728"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Pose-Disentangled_Contrastive_Learning_for_Self-Supervised_Facial_Representation_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2752.pdf",
        "selectable": false
      },
      {
        "text": "Pre-training Strategies and\u00a0Datasets for\u00a0Facial Representation Learning",
        "item-id": "i2479",
        "nodes": [
          {
            "text": "Bulat et al_2022_Pre-training Strategies and Datasets for Facial Representation Learning.pdf",
            "item-id": "i2480",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bulat et al_2022_Pre-training Strategies and Datasets for Facial Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bulat et al_2022_Pre-training Strategies and Datasets for Facial Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i2480.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pre-training Strategies and\u00a0Datasets for\u00a0Facial Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "What is the best way to learn a universal face representation? Recent work on Deep Learning in the area of face analysis has focused on supervised learning for specific tasks of interest (e.g. face recognition, facial landmark localization etc.) but has overlooked the overarching question of how to find a facial representation that can be readily adapted to several facial analysis tasks and datasets. To this end, we make the following 4 contributions: (a) we introduce, for the first time, a comprehensive evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks. (b) We systematically investigate two ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of few-shot facial learning. (c) We investigate important properties of the training datasets including their size and quality (labelled, unlabelled or even uncurated). (d) To draw our conclusions, we conducted a very large number of experiments. Our main two findings are: (1) Unsupervised pre-training on completely in-the-wild, uncurated data provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (2) Many existing facial video datasets seem to have a large amount of redundancy. We will release code, and pre-trained models to facilitate future research."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Adrian Bulat, Shiyang Cheng, Jing Yang, Andrew Garbett, Enrique Sanchez, Georgios Tzimiropoulos, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19778-9_7"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19778-9"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "107-125"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Pre-training Strategies and\u00a0Datasets for\u00a0Facial Representation Learning"
          ]
        ],
        "resource": "storage/i2480.pdf",
        "selectable": false
      },
      {
        "text": "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis",
        "item-id": "i2766",
        "nodes": [
          {
            "text": "Wang et al_2023_Progressive Disentangled Representation Learning for Fine-Grained Controllable.pdf",
            "item-id": "i2768",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_Progressive Disentangled Representation Learning for Fine-Grained Controllable.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_Progressive Disentangled Representation Learning for Fine-Grained Controllable.pdf"
              ]
            ],
            "resource": "storage/i2768.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them. To effectively disentangle each motion factor, we propose a progressive disentangled representation learning strategy by separating the factors in a coarse-to-fine manner, where we first extract unified motion feature from the driving signal, and then isolate each fine-grained motion from the unified feature. We introduce motion-specific contrastive learning and regressing for non-emotional motions, and feature-level decorrelation and self-reconstruction for emotional expression, to fully utilize the inherent properties of each motion factor in unstructured video data to achieve disentanglement. Experiments show that our method provides high quality speech&lip-motion synchronization along with precise and disentangled control over multiple extra facial motions, which can hardly be achieved by previous methods."
          ],
          [
            "Access Date",
            "2023-06-22 21:20:37"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "17979-17989"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2768.pdf",
        "selectable": false
      },
      {
        "text": "Representation Learning",
        "item-id": "i2293",
        "nodes": [
          {
            "text": "Bengio et al_2013_Representation Learning.pdf",
            "item-id": "i2295",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bengio et al_2013_Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bengio et al_2013_Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i2295.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Representation Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."
          ],
          [
            "Creators",
            "Yoshua Bengio, Aaron Courville, Pascal Vincent"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2013.50"
          ],
          [
            "Date",
            "2013-08-00 2013-08"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1798-1828"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "Representation Learning"
          ],
          [
            "Title",
            "Representation Learning: A Review and New Perspectives"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i2295.pdf",
        "selectable": false
      },
      {
        "text": "Revisiting Self-Supervised Visual Representation Learning",
        "item-id": "i1839",
        "nodes": [
          {
            "text": "Kolesnikov et al_2019_Revisiting Self-Supervised Visual Representation Learning.pdf",
            "item-id": "i1924",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kolesnikov et al_2019_Revisiting Self-Supervised Visual Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kolesnikov et al_2019_Revisiting Self-Supervised Visual Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i1924.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Revisiting Self-Supervised Visual Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in self-supervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin. We will release the code for reproducing our experiments when the anonymity requirements are lifted."
          ],
          [
            "Access Date",
            "2022-11-03 06:13:35"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1920-1929"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Revisiting Self-Supervised Visual Representation Learning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html"
          ]
        ],
        "resource": "storage/i1924.pdf",
        "selectable": false
      },
      {
        "text": "Self-Supervised MultiModal Versatile Networks",
        "item-id": "i2815",
        "nodes": [
          {
            "text": "Alayrac et al_2020_Self-Supervised MultiModal Versatile Networks.pdf",
            "item-id": "i2922",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Alayrac et al_2020_Self-Supervised MultiModal Versatile Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Alayrac et al_2020_Self-Supervised MultiModal Versatile Networks.pdf"
              ]
            ],
            "resource": "storage/i2922.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Self-Supervised MultiModal Versatile Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network -- a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to previous self-supervised work. Our models are publicly available."
          ],
          [
            "Access Date",
            "2023-06-30 08:13:57"
          ],
          [
            "Creators",
            "Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovi\u0107, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "25\u201337"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Self-Supervised MultiModal Versatile Networks"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i2922.pdf",
        "selectable": false
      },
      {
        "text": "SupMAE",
        "item-id": "i1688",
        "nodes": [
          {
            "text": "Comment: Technical report. Codes are available",
            "item-id": "n1726",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Technical report. Codes are available",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Technical report. Codes are available</div>",
            "node_type": "note"
          },
          {
            "text": "Liang et al_2022_SupMAE.pdf",
            "item-id": "i1725",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liang et al_2022_SupMAE.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liang et al_2022_SupMAE.pdf"
              ]
            ],
            "resource": "storage/i1725.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "SupMAE",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-supervised Masked Autoencoders (MAE) are emerging as a new pre-training paradigm in computer vision. MAE learns semantics implicitly via reconstructing local patches, requiring thousands of pre-training epochs to achieve favorable performance. This paper incorporates explicit supervision, i.e., golden labels, into the MAE framework. The proposed Supervised MAE (SupMAE) only exploits a visible subset of image patches for classification, unlike the standard supervised pre-training where all image patches are used. SupMAE is efficient and can achieve comparable performance with MAE using only 30% compute when evaluated on ImageNet with the ViT-B/16 model. Detailed ablation studies are conducted to verify the proposed components."
          ],
          [
            "Access Date",
            "2022-07-26 02:54:08"
          ],
          [
            "Archiveid",
            "arXiv:2205.14540"
          ],
          [
            "Creators",
            "Feng Liang, Yangguang Li, Diana Marculescu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2205.14540"
          ],
          [
            "Date",
            "2022-05-28 2022-05-28"
          ],
          [
            "Extra",
            "arXiv:2205.14540 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "SupMAE"
          ],
          [
            "Title",
            "SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2205.14540"
          ]
        ],
        "resource": "storage/i1725.pdf",
        "selectable": false
      },
      {
        "text": "TokenLearner",
        "item-id": "i1764",
        "nodes": [
          {
            "text": "Ryoo et al_2021_TokenLearner.pdf",
            "item-id": "i1769",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ryoo et al_2021_TokenLearner.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ryoo et al_2021_TokenLearner.pdf"
              ]
            ],
            "resource": "storage/i1769.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TokenLearner",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in image frames. Our experiments demonstrate strong performance on several challenging benchmarks for video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced computational cost. We establish new state-of-the-arts on multiple video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD."
          ],
          [
            "Access Date",
            "2022-09-04 23:59:20"
          ],
          [
            "Creators",
            "Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, Anelia Angelova"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "12786\u201312797"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "TokenLearner"
          ],
          [
            "Title",
            "TokenLearner: Adaptive Space-Time Tokenization for Videos"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2021/hash/6a30e32e56fce5cf381895dfe6ca7b6f-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i1769.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
        "item-id": "i1859",
        "nodes": [
          {
            "text": "Wu et al_2018_Unsupervised Feature Learning via Non-Parametric Instance Discrimination.pdf",
            "item-id": "i1955",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2018_Unsupervised Feature Learning via Non-Parametric Instance Discrimination.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2018_Unsupervised Feature Learning via Non-Parametric Instance Discrimination.pdf"
              ]
            ],
            "resource": "storage/i1955.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsu- pervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time."
          ],
          [
            "Access Date",
            "2022-11-01 15:59:27"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zhirong Wu, Yuanjun Xiong, Stella X. Yu, Dahua Lin"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3733-3742"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Unsupervised Feature Learning via Non-Parametric Instance Discrimination"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i1955.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
        "item-id": "i1843",
        "nodes": [
          {
            "text": "Noroozi_Favaro_2016_Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf",
            "item-id": "i1930",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Noroozi_Favaro_2016_Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CHRNQIZY/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/5\">3 Solving Jigsaw Puzzles</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/5\">3.1 The Jigsaw Puzzle Task</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/6\">3.2 The Context-Free Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/7\">3.3 Training the CFN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/9\">3.4 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/9\">3.5 CFN Filter Activations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/10\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/11\">4.1 ImageNet Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/11\">4.2 Pascal VOC 2007 Classification and Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/12\">4.3 Image Retrieval</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/14\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CHRNQIZY/14\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Noroozi_Favaro_2016_Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf"
              ]
            ],
            "resource": "storage/i1930.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with $$51.8\\,\\%$$for detection and $$68.6\\,\\%$$for classification, and reduce the gap with supervised learning ($$56.5\\,\\%$$and $$78.2\\,\\%$$respectively)."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Mehdi Noroozi, Paolo Favaro, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46466-4_5"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46466-4"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "69-84"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles"
          ]
        ],
        "resource": "storage/i1930.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Multimodal Language Representations using Convolutional Autoencoders",
        "item-id": "i1882",
        "nodes": [
          {
            "text": "Comment: 5 pages",
            "item-id": "n1989",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 5 pages",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 5 pages</div>",
            "node_type": "note"
          },
          {
            "text": "Koromilas_Giannakopoulos_2022_Unsupervised Multimodal Language Representations using Convolutional.pdf",
            "item-id": "i1988",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Koromilas_Giannakopoulos_2022_Unsupervised Multimodal Language Representations using Convolutional.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_B89YR8PN/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B89YR8PN/2\">2  Convolutional Autoencoder for multimodal sequences</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/2\">2.1  2-D multimodal sequence representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/2\">2.2  Representation Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/2\">2.3  Downstream Classification</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B89YR8PN/2\">3  Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/2\">3.1  Experimental Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/3\">3.2  Results on Downstream Classification</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B89YR8PN/3\">3.3  Ablation Study</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/4\">3.3.1  The role of different modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/4\">3.3.2  Cross-domain generalization ability</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/4\">3.4  Model Complexity</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/4\">4  Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B89YR8PN/5\">5  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Koromilas_Giannakopoulos_2022_Unsupervised Multimodal Language Representations using Convolutional.pdf"
              ]
            ],
            "resource": "storage/i1988.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Unsupervised Multimodal Language Representations using Convolutional Autoencoders",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multimodal Language Analysis is a demanding area of research, since it is associated with two requirements: combining different modalities and capturing temporal information. During the last years, several works have been proposed in the area, mostly centered around supervised learning in downstream tasks. In this paper we propose extracting unsupervised Multimodal Language representations that are universal and can be applied to different tasks. Towards this end, we map the word-level aligned multimodal sequences to 2-D matrices and then use Convolutional Autoencoders to learn embeddings by combining multiple datasets. Extensive experimentation on Sentiment Analysis (MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned representations can achieve near-state-of-the-art performance with just the use of a Logistic Regression algorithm for downstream classification. It is also shown that our method is extremely lightweight and can be easily generalized to other tasks and unseen data with small performance drop and almost the same number of parameters. The proposed multimodal representation models are open-sourced and will help grow the applicability of Multimodal Language."
          ],
          [
            "Access Date",
            "2022-10-30 17:39:04"
          ],
          [
            "Archiveid",
            "arXiv:2110.03007"
          ],
          [
            "Creators",
            "Panagiotis Koromilas, Theodoros Giannakopoulos"
          ],
          [
            "DOI",
            "10.48550/arXiv.2110.03007"
          ],
          [
            "Date",
            "2022-01-07 2022-01-07"
          ],
          [
            "Extra",
            "arXiv:2110.03007 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Unsupervised Multimodal Language Representations using Convolutional Autoencoders"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2110.03007"
          ]
        ],
        "resource": "storage/i1988.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "item-id": "i90",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n292",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>DCGAN</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Comment: Under review as a conference paper at ICLR 2016",
            "item-id": "n293",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Under review as a conference paper at ICLR 2016",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Under review as a conference paper at ICLR 2016</div>",
            "node_type": "note"
          },
          {
            "text": "Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf",
            "item-id": "i291",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XULW8HJK/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2.1 Representation Learning from unlabeled data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2.2 Generating natural images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">2.3 Visualizing the internals of CNNs</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/2\">3 Approach and Model Architecture</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/3\">4 Details of adversarial training</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/4\">4.1 LSUN</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/4\">4.1.1 Deduplication</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/4\">4.2 Faces</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/5\">4.3 Imagenet-1k</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">5 Empirical Validation of DCGANs capabilities</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">5.1 Classifying CIFAR-10 using GANs as a feature extractor</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">5.2 Classifying SVHN digits using GANs as a feature extractor</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/6\">6 Investigating and visualizing the internals of the networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.1 Walking in the latent space</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.2 Visualizing the Discriminator Features</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.3 Manipulating the Generator Representation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/7\">6.3.1 Forgetting to draw certain objects</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/8\">6.3.2 Vector arithmetic on face samples</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/9\">7 Conclusion and Future Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XULW8HJK/14\">8 Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XULW8HJK/14\">8.1 Evaluating DCGANs capability to capture data distributions</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf"
              ]
            ],
            "resource": "storage/i291.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
          ],
          [
            "Access Date",
            "2021-03-16 14:49:34"
          ],
          [
            "Conference Name",
            "4th International Conference on Learning Representations, ICLR 2016"
          ],
          [
            "Creators",
            "Alec Radford, Luke Metz, Soumith Chintala"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Proceedings Title",
            "4th International Conference on Learning Representations, ICLR 2016"
          ],
          [
            "Title",
            "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
          ],
          [
            "URL",
            "https://arxiv.org/abs/1511.06434"
          ]
        ],
        "resource": "storage/i291.pdf",
        "selectable": false
      },
      {
        "text": "VideoMAE",
        "item-id": "i2397",
        "nodes": [
          {
            "text": "Tong et al_2022_VideoMAE.pdf",
            "item-id": "i2449",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tong et al_2022_VideoMAE.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tong et al_2022_VideoMAE.pdf"
              ]
            ],
            "resource": "storage/i2449.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VideoMAE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging and meaningful self-supervision task, thus encouraging extracting more effective video representations during the pre-training process. We obtain three important findings with VideoMAE: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance for VideoMAE. The temporally redundant video content enables higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. This is partially ascribed to the challenging task of video reconstruction to enforce high-level structure learning. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important factor. Notably, our VideoMAE with the vanilla ViT backbone can achieve 87.4% on Kinects-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE."
          ],
          [
            "Access Date",
            "2023-05-22 04:32:05"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Zhan Tong, Yibing Song, Jue Wang, Limin Wang"
          ],
          [
            "Date",
            "2022-10-31 2022/10/31"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Short Title",
            "VideoMAE"
          ],
          [
            "Title",
            "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=AhccnBXSne"
          ]
        ],
        "resource": "storage/i2449.pdf",
        "selectable": false
      },
      {
        "text": "VideoMAE V2",
        "item-id": "i3219",
        "nodes": [
          {
            "text": "Wang et al_2023_VideoMAE V2.pdf",
            "item-id": "i3302",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2023_VideoMAE V2.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_VQHNZHW7/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/2\">. Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/3\">. VideoMAE V2</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/3\">. VideoMAE Revisited</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/3\">. Dual Masking for VideoMAE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/4\">. Scaling VideoMAE</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/5\">. Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/5\">. Implementation and Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/6\">. Main Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/7\">. Performance on Downstream Tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VQHNZHW7/8\">. Conclusion and Discussion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2023_VideoMAE V2.pdf"
              ]
            ],
            "resource": "storage/i3302.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VideoMAE V2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also introduce a progressive training paradigm that involves initial pre-training on the diverse multi-sourced unlabeled dataset, followed by fine-tuning on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner."
          ],
          [
            "Access Date",
            "2023-11-08 14:45:22"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "14549-14560"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "VideoMAE V2"
          ],
          [
            "Title",
            "VideoMAE V2: Scaling Video Masked Autoencoders With Dual Masking"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i3302.pdf",
        "selectable": false
      },
      {
        "text": "data2vec",
        "item-id": "i1857",
        "nodes": [
          {
            "text": "Baevski et al_2022_data2vec.pdf",
            "item-id": "i1997",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Baevski et al_2022_data2vec.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Baevski et al_2022_data2vec.pdf"
              ]
            ],
            "resource": "storage/i1997.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "data2vec",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches."
          ],
          [
            "Access Date",
            "2022-10-30 16:43:40"
          ],
          [
            "Archiveid",
            "arXiv:2202.03555"
          ],
          [
            "Creators",
            "Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli"
          ],
          [
            "DOI",
            "10.48550/arXiv.2202.03555"
          ],
          [
            "Date",
            "2022-10-25 2022-10-25"
          ],
          [
            "Extra",
            "arXiv:2202.03555 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "data2vec"
          ],
          [
            "Title",
            "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2202.03555"
          ]
        ],
        "resource": "storage/i1997.pdf",
        "selectable": false
      }
    ],
    "item_title": "Representation Learning",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Review",
    "item-id": "c39,i3583",
    "nodes": [
      {
        "text": "INFFUS-D-23-02332.pdf",
        "item-id": "i3583",
        "icon": "glyphicon glyphicon-paperclip",
        "item_title": "INFFUS-D-23-02332.pdf",
        "item_type": "attachment",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Title",
            "INFFUS-D-23-02332.pdf"
          ]
        ],
        "resource": "storage/i3583.pdf"
      }
    ],
    "item_title": "Review",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Semi-Supervised Learning",
    "item-id": "c9,i2822",
    "nodes": [
      {
        "text": "A Survey on Deep Semi-supervised Learning",
        "item-id": "i807",
        "nodes": [
          {
            "text": "Comment: 24 pages, 6 figures",
            "item-id": "n875",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 24 pages, 6 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 24 pages, 6 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf",
            "item-id": "i874",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SQATXA2A/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/2\">2 Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/2\">2.1 Assumptions for semi-supervised learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/3\">2.2 Traditional Semi-supervised learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">2.3 Related Concepts</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">3 Generative methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">3.1 Datasets and applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">3.2 Semi-supervised GANs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/8\">3.3 Semi-supervised VAE</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/10\">4 Consistency Regularization</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/12\">5 Graph-based methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/13\">5.1 AutoEncoder-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/13\">5.2 GNN-based methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/15\">6 Pseudo-labeling methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/15\">6.1 Disagreement-based models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/16\">6.2 Self-training models </a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/17\">7 Hybrid methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/18\">8 Challenges and future directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/19\">9 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/19\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf"
              ]
            ],
            "resource": "storage/i874.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Deep Semi-supervised Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from model design perspectives and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we offer a detailed comparison of these methods in terms of the type of losses, contributions, and architecture differences. In addition to the past few years' progress, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems."
          ],
          [
            "Access Date",
            "2021-08-03 17:02:21"
          ],
          [
            "Creators",
            "Xiangli Yang, Zixing Song, Irwin King, Zenglin Xu"
          ],
          [
            "Date",
            "2021-02-28 2021-02-28"
          ],
          [
            "Extra",
            "arXiv: 2103.00550"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2103.00550 [cs]"
          ],
          [
            "Title",
            "A Survey on Deep Semi-supervised Learning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2103.00550"
          ]
        ],
        "resource": "storage/i874.pdf",
        "selectable": false
      },
      {
        "text": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
        "item-id": "i1840",
        "nodes": [
          {
            "text": "Chen et al_2020_Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf",
            "item-id": "i1925",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf"
              ]
            ],
            "resource": "storage/i1925.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels (\u226413 labeled images per class) using ResNet-50, a 10X improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels."
          ],
          [
            "Access Date",
            "2022-11-03 06:11:36"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey E Hinton"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "22243\u201322255"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Big Self-Supervised Models are Strong Semi-Supervised Learners"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i1925.pdf",
        "selectable": false
      },
      {
        "text": "FixMatch",
        "item-id": "i801",
        "nodes": [
          {
            "text": "Comment: Published at NeurIPS 2020 as a conference paper",
            "item-id": "n862",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Published at NeurIPS 2020 as a conference paper",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Published at NeurIPS 2020 as a conference paper</div>",
            "node_type": "note"
          },
          {
            "text": "Sohn et al_2020_FixMatch.pdf",
            "item-id": "i861",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sohn et al_2020_FixMatch.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sohn et al_2020_FixMatch.pdf"
              ]
            ],
            "resource": "storage/i861.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FixMatch",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch."
          ],
          [
            "Access Date",
            "2021-08-04 12:30:34"
          ],
          [
            "Creators",
            "Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel"
          ],
          [
            "Date",
            "2020-11-25 2020-11-25"
          ],
          [
            "Extra",
            "arXiv: 2001.07685"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2001.07685 [cs, stat]"
          ],
          [
            "Short Title",
            "FixMatch"
          ],
          [
            "Title",
            "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2001.07685"
          ]
        ],
        "resource": "storage/i861.pdf",
        "selectable": false
      },
      {
        "text": "Improved techniques for training GANs",
        "item-id": "i800",
        "nodes": [
          {
            "text": "Salimans et al_2016_Improved techniques for training GANs.pdf",
            "item-id": "i859",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Salimans et al_2016_Improved techniques for training GANs.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Salimans et al_2016_Improved techniques for training GANs.pdf"
              ]
            ],
            "resource": "storage/i859.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Improved techniques for training GANs",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."
          ],
          [
            "Access Date",
            "2021-08-04"
          ],
          [
            "Creators",
            "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
          ],
          [
            "Date",
            "2016-12-05 December 5, 2016"
          ],
          [
            "ISBN",
            "978-1-5108-3881-9"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "2234\u20132242"
          ],
          [
            "Place",
            "Red Hook, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 30th International Conference on Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates Inc."
          ],
          [
            "Series",
            "NIPS'16"
          ],
          [
            "Title",
            "Improved techniques for training GANs"
          ]
        ],
        "resource": "storage/i859.pdf",
        "selectable": false
      },
      {
        "text": "KFC",
        "item-id": "i808",
        "nodes": [
          {
            "text": "Ding et al_2021_KFC.pdf",
            "item-id": "i876",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ding et al_2021_KFC.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ding et al_2021_KFC.pdf"
              ]
            ],
            "resource": "storage/i876.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "KFC",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In temporal action localization (TAL), semi-supervised learning is a promising technique to mitigate the cost of precise boundary annotations. Semi-supervised approaches employing consistency regularization (CR), encouraging models to be robust to the perturbed inputs, have achieved great success in image classification problems. The success of CR is largely depended on the perturbations, where instances are perturbed to train a robust model without altering their semantic information. However, the perturbations for image or video classification tasks are not fit to apply to TAL. Since videos in TAL are too long to train the model with raw videos in an end-to-end manner. In this paper, we devise a method named K-farthest crossover to construct perturbations based on video features and apply it to TAL. Motivated by the observation that features in the same action instance become more and more similar during the training process while those in different action instances or backgrounds become more and more divergent, we add perturbations to each feature along temporal axis and adopt CR to encourage the model to retain this observation. Specifically, for a feature, we first find the top-k dissimilar features and average them to form a perturbation. Then, similar to chromosomal crossover, we select a large part of the feature and a small part of the perturbation to recombine a perturbed feature, which preserves the feature semantics yet enough discrepancy."
          ],
          [
            "Creators",
            "Xinpeng Ding, Nannan Wang, Xinbo Gao, Jie Li, Xiaoyu Wang, Tongliang Liu"
          ],
          [
            "DOI",
            "10.1109/TIP.2021.3099407"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Short Title",
            "KFC"
          ],
          [
            "Title",
            "KFC: An Efficient Framework for Semi-supervised Temporal Action Localization"
          ]
        ],
        "resource": "storage/i876.pdf",
        "selectable": false
      },
      {
        "text": "Mean teachers are better role models",
        "item-id": "i803",
        "nodes": [
          {
            "text": "Comment: In this version: Corrected hyperparameters of the 4000-label CIFAR-10 ResNet experiment. Changed Antti's contac",
            "item-id": "n868",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: In this version: Corrected hyperparameters of the 4000-label CIFAR-10 ResNet experiment. Changed Antti's contac",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: In this version: Corrected hyperparameters of the 4000-label CIFAR-10 ResNet experiment. Changed Antti's contact info, Advances in Neural Information Processing Systems 30 (NIPS 2017) pre-proceedings</div>",
            "node_type": "note"
          },
          {
            "text": "Tarvainen_Valpola_2018_Mean teachers are better role models.pdf",
            "item-id": "i867",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tarvainen_Valpola_2018_Mean teachers are better role models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KMT2CIAJ/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/2\">2 Mean Teacher</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/4\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/4\">3.1 Comparison to other methods on SVHN and CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/5\">3.2 SVHN with extra unlabeled data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/5\">3.3 Analysis of the training curves</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/6\">3.4 Ablation experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/7\">3.5 Mean Teacher with residual networks on CIFAR-10 and ImageNet</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/7\">4 Related work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/8\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/11\">A Results without input augmentation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/11\">B Experimental setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/11\">B.1 Convolutional network models</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/12\">B.1.1 ConvNet on CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/13\">B.1.2 ConvNet on SVHN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/13\">B.1.3 The baseline ConvNet models</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/13\">B.2 Residual network models</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/13\">B.2.1 ResNet on CIFAR-10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/14\">B.2.2 ResNet on ImageNet</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/15\">B.3 Use of training, validation and test data</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KMT2CIAJ/15\">C Varying between mean squared error and KL-divergence</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tarvainen_Valpola_2018_Mean teachers are better role models.pdf"
              ]
            ],
            "resource": "storage/i867.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Mean teachers are better role models",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%."
          ],
          [
            "Access Date",
            "2021-08-04 08:03:32"
          ],
          [
            "Creators",
            "Antti Tarvainen, Harri Valpola"
          ],
          [
            "Date",
            "2018-04-16 2018-04-16"
          ],
          [
            "Extra",
            "arXiv: 1703.01780"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1703.01780 [cs, stat]"
          ],
          [
            "Short Title",
            "Mean teachers are better role models"
          ],
          [
            "Title",
            "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1703.01780"
          ]
        ],
        "resource": "storage/i867.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Ensembling for Semi-Supervised Learning",
        "item-id": "i2822",
        "nodes": [
          {
            "text": "Laine_Aila_2017_Temporal Ensembling for Semi-Supervised Learning.pdf",
            "item-id": "i870",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Laine_Aila_2017_Temporal Ensembling for Semi-Supervised Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Laine_Aila_2017_Temporal Ensembling for Semi-Supervised Learning.pdf"
              ]
            ],
            "resource": "storage/i870.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Ensembling for Semi-Supervised Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels."
          ],
          [
            "Access Date",
            "2023-07-12 15:03:45"
          ],
          [
            "Conference Name",
            "5th International Conference on Learning Representations, ICLR 2017"
          ],
          [
            "Creators",
            "Samuli Laine, Timo Aila"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "DBLP Computer Science Bibliography"
          ],
          [
            "Place",
            "Toulon, France"
          ],
          [
            "Proceedings Title",
            "5th International Conference on Learning Representations, ICLR 2017"
          ],
          [
            "Publisher",
            "OpenReview.net"
          ],
          [
            "Title",
            "Temporal Ensembling for Semi-Supervised Learning"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=BJ6oOfqge"
          ]
        ],
        "resource": "storage/i870.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Data Augmentation for Consistency Training",
        "item-id": "i802",
        "nodes": [
          {
            "text": "Comment: NeurIPS 2020",
            "item-id": "n865",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NeurIPS 2020",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NeurIPS 2020</div>",
            "node_type": "note"
          },
          {
            "text": "Xie et al_2020_Unsupervised Data Augmentation for Consistency Training.pdf",
            "item-id": "i864",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xie et al_2020_Unsupervised Data Augmentation for Consistency Training.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CC8BX56B/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CC8BX56B/2\">2 Unsupervised Data Augmentation (UDA)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/2\">2.1 Background: Supervised Data Augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/2\">2.2 Unsupervised Data Augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/4\">2.3 Augmentation Strategies for Different Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/4\">2.4 Additional Training Techniques</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/5\">3 Theoretical Analysis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CC8BX56B/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/6\">4.1 Correlation between Supervised and Semi-supervised Performances</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/7\">4.2 Algorithm Comparison on Vision Semi-supervised Learning Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/7\">4.3 Evaluation on Text Classification Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/9\">4.4 Scalability Test on the ImageNet Dataset</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/9\">5 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/9\">6 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CC8BX56B/14\">A Extended Method Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/14\">A.1 Training Signal Annealing for Low-data Regime</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/14\">A.2 Extended Augmentation Strategies for Different Tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CC8BX56B/15\">B Extended Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/15\">B.1 Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/16\">B.2 More Results on CIFAR-10, SVHN and Text Classification Datasets</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/17\">C Proof for Theoretical Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/18\">D Extended Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CC8BX56B/19\">E Experiment Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/19\">E.1 Text Classifications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/20\">E.2 Semi-supervised learning benchmarks CIFAR-10 and SVHN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CC8BX56B/20\">E.3 ImageNet</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xie et al_2020_Unsupervised Data Augmentation for Consistency Training.pdf"
              ]
            ],
            "resource": "storage/i864.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Data Augmentation for Consistency Training",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda."
          ],
          [
            "Access Date",
            "2021-08-04 08:14:05"
          ],
          [
            "Creators",
            "Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le"
          ],
          [
            "Date",
            "2020-11-05 2020-11-05"
          ],
          [
            "Extra",
            "arXiv: 1904.12848"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1904.12848 [cs, stat]"
          ],
          [
            "Title",
            "Unsupervised Data Augmentation for Consistency Training"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1904.12848"
          ]
        ],
        "resource": "storage/i864.pdf",
        "selectable": false
      }
    ],
    "item_title": "Semi-Supervised Learning",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Seq2Seq",
    "item-id": "c1,i2821",
    "nodes": [
      {
        "text": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "item-id": "i2821",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n143",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>GRU</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf",
            "item-id": "i2888",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_P3MNUFJI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/2\">2 RNN Encoder\u2013Decoder</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/2\">2.1 Preliminary: Recurrent Neural Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/2\">2.2 RNN Encoder\u2013Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/3\">2.3 Hidden Unit that Adaptively Remembers and Forgets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/3\">3 Statistical Machine Translation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/4\">3.1 Scoring Phrase Pairs with RNN Encoder\u2013Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/4\">3.2 Related Approaches: Neural Networks in Machine Translation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/5\">4.1 Data and Baseline System</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/5\">4.1.1 RNN Encoder\u2013Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/6\">4.1.2 Neural Language Model</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/6\">4.2 Quantitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/7\">4.3 Qualitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/9\">4.4 Word and Phrase Representations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/9\">5 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/12\">A RNN Encoder\u2013Decoder</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/12\">A.1 Encoder</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/12\">A.1.1 Decoder</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_P3MNUFJI/13\">B Word and Phrase Representations</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf"
              ]
            ],
            "resource": "storage/i2888.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
          ],
          [
            "Conference Name",
            "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL"
          ],
          [
            "Creators",
            "Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Alessandro Moschitti, Bo Pang, Walter Daelemans"
          ],
          [
            "DOI",
            "10.3115/v1/d14-1179"
          ],
          [
            "Date",
            "2014-00-00 2014"
          ],
          [
            "Library Catalog",
            "DBLP Computer Science Bibliography"
          ],
          [
            "Pages",
            "1724\u20131734"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL"
          ],
          [
            "Publisher",
            "ACL"
          ],
          [
            "Title",
            "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
          ]
        ],
        "resource": "storage/i2888.pdf",
        "selectable": false
      },
      {
        "text": "Long Short-Term Memory",
        "item-id": "i26",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n147",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>LSTM</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf",
            "item-id": "i149",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf"
              ]
            ],
            "resource": "storage/i149.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Long Short-Term Memory",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
          ],
          [
            "Access Date",
            "2021-05-01 07:07:59"
          ],
          [
            "Creators",
            "Sepp Hochreiter, J\u00fcrgen Schmidhuber"
          ],
          [
            "DOI",
            "10.1162/neco.1997.9.8.1735"
          ],
          [
            "Date",
            "1997-11-15 November 15, 1997"
          ],
          [
            "ISSN",
            "0899-7667"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Journal Abbreviation",
            "Neural Computation"
          ],
          [
            "Library Catalog",
            "Silverchair"
          ],
          [
            "Pages",
            "1735-1780"
          ],
          [
            "Publication Title",
            "Neural Computation"
          ],
          [
            "Title",
            "Long Short-Term Memory"
          ],
          [
            "URL",
            "https://doi.org/10.1162/neco.1997.9.8.1735"
          ],
          [
            "Volume",
            "9"
          ]
        ],
        "resource": "storage/i149.pdf",
        "selectable": false
      }
    ],
    "item_title": "Seq2Seq",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Speech Diarization",
    "item-id": "c35,i2382",
    "nodes": [
      {
        "text": "AVA-AVD",
        "item-id": "i2382",
        "nodes": [
          {
            "text": "Comment: ACMMM 2022",
            "item-id": "n2423",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ACMMM 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ACMMM 2022</div>",
            "node_type": "note"
          },
          {
            "text": "Xu et al_2022_AVA-AVD.pdf",
            "item-id": "i2422",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2022_AVA-AVD.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KI5EPALE/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KI5EPALE/3\">3 AVA-AVD</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/3\">3.1 Video Source</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/3\">3.2 Annotation Process</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/4\">3.3 Statistics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KI5EPALE/4\">4 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/4\">4.1 Problem Definition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/4\">4.2 Audio-Visual Relation Network</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/5\">5 Implementation Details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KI5EPALE/5\">6 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/5\">6.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/5\">6.2 Evaluation Metric</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/5\">6.3 Evaluate the Challenges of AVA-AVD</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/6\">6.4 Necessity of AVA-AVD Train Set</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/6\">6.5 Comparisons with State-of-the-art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/7\">6.6 Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/8\">6.7 Analysis of modality masks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/8\">7 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/8\">8 Acknowledgement</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KI5EPALE/9\">A Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/9\">A.1 Train/val/test split.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/9\">A.2 Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/9\">A.3 Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/9\">A.4 Annotation Tool</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KI5EPALE/10\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2022_AVA-AVD.pdf"
              ]
            ],
            "resource": "storage/i2422.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AVA-AVD",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio-visual speaker diarization aims at detecting \"who spoke when\" using both auditory and visual signals. Existing audio-visual diarization datasets are mainly focused on indoor environments like meeting rooms or news studios, which are quite different from in-the-wild videos in many scenarios such as movies, documentaries, and audience sitcoms. To develop diarization methods for these challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD) dataset. Our experiments demonstrate that adding AVA-AVD into training set can produce significantly better diarization models for in-the-wild videos despite that the data is relatively small. Moreover, this benchmark is challenging due to the diverse scenes, complicated acoustic conditions, and completely off-screen speakers. As a first step towards addressing the challenges, we design the Audio-Visual Relation Network (AVR-Net) which introduces a simple yet effective modality mask to capture discriminative information based on face visibility. Experiments show that our method not only can outperform state-of-the-art methods but is more robust as varying the ratio of off-screen speakers. Our data and code has been made publicly available at https://github.com/showlab/AVA-AVD."
          ],
          [
            "Access Date",
            "2023-05-23 08:00:46"
          ],
          [
            "Creators",
            "Eric Zhongcong Xu, Zeyang Song, Satoshi Tsutsui, Chao Feng, Mang Ye, Mike Zheng Shou"
          ],
          [
            "DOI",
            "10.1145/3503161.3548027"
          ],
          [
            "Date",
            "2022-10-10 2022-10-10"
          ],
          [
            "Extra",
            "arXiv:2111.14448 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "3838-3847"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 30th ACM International Conference on Multimedia"
          ],
          [
            "Short Title",
            "AVA-AVD"
          ],
          [
            "Title",
            "AVA-AVD: Audio-Visual Speaker Diarization in the Wild"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2111.14448"
          ]
        ],
        "resource": "storage/i2422.pdf",
        "selectable": false
      },
      {
        "text": "Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion",
        "item-id": "i2379",
        "nodes": [
          {
            "text": "Gebru et al_2018_Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion.pdf",
            "item-id": "i2415",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gebru et al_2018_Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_HNVTNW7Y/1\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/2\">II Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/3\">III Proposed Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/4\">III-A Speaker Diarization Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/5\">III-B State Transition Model</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/5\">IV Visual Observations</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/6\">V Audio Observations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/6\">V-A Single Audio Source</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/6\">V-B Multiple Speech Sources</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/7\">VI Audio-Visual Fusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/7\">VII Audio-Visual Datasets</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/10\">VIII Experimental Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/10\">VIII-A Diarization Performance Measure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/10\">VIII-B Diarization Algorithms and Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/10\">VIII-C Results and Discussion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/13\">IX Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HNVTNW7Y/13\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gebru et al_2018_Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion.pdf"
              ]
            ],
            "resource": "storage/i2415.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Speaker diarization consists of assigning speech signals to people engaged in a dialogue. An audio-visual spatiotemporal diarization model is proposed. The model is well suited for challenging scenarios that consist of several participants engaged in multi-party interaction while they move around and turn their heads towards the other participants rather than facing the cameras and the microphones. Multiple-person visual tracking is combined with multiple speech-source localization in order to tackle the speech-to-person association problem. The latter is solved within a novel audio-visual fusion method on the following grounds: binaural spectral features are first extracted from a microphone pair, then a supervised audio-visual alignment technique maps these features onto an image, and finally a semi-supervised clustering method assigns binaural spectral features to visible persons. The main advantage of this method over previous work is that it processes in a principled way speech signals uttered simultaneously by multiple persons. The diarization itself is cast into a latent-variable temporal graphical model that infers speaker identities and speech turns, based on the output of an audio-visual association process, executed at each time slice, and on the dynamics of the diarization variable itself. The proposed formulation yields an efficient exact inference procedure. A novel dataset, that contains audio-visual training data as well as a number of scenarios involving several participants engaged in formal and informal dialogue, is introduced. The proposed method is thoroughly tested and benchmarked with respect to several state-of-the art diarization algorithms."
          ],
          [
            "Creators",
            "Israel D. Gebru, Sil\u00e8ye Ba, Xiaofei Li, Radu Horaud"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2017.2648793"
          ],
          [
            "Date",
            "2018-05-00 2018-05"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1086-1099"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Title",
            "Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion"
          ],
          [
            "Volume",
            "40"
          ]
        ],
        "resource": "storage/i2415.pdf",
        "selectable": false
      },
      {
        "text": "UniCon+",
        "item-id": "i2368",
        "nodes": [
          {
            "text": "Comment: 5 pages, 3 figures; technical report for AVA Challenge (see https://research.google.com/ava/challenge.html) at ",
            "item-id": "n2401",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 5 pages, 3 figures; technical report for AVA Challenge (see https://research.google.com/ava/challenge.html) at ",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 5 pages, 3 figures; technical report for AVA Challenge (see https://research.google.com/ava/challenge.html) at the International Challenge on Activity Recognition (ActivityNet), CVPR 2022</div>",
            "node_type": "note"
          },
          {
            "text": "Zhang et al_2022_UniCon+.pdf",
            "item-id": "i2400",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2022_UniCon+.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2022_UniCon+.pdf"
              ]
            ],
            "resource": "storage/i2400.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "UniCon+",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This report presents a brief description of our winning solution to the AVA Active Speaker Detection (ASD) task at ActivityNet Challenge 2022. Our underlying model UniCon+ continues to build on our previous work, the Unified Context Network (UniCon) and Extended UniCon which are designed for robust scene-level ASD. We augment the architecture with a simple GRU-based module that allows information of recurring identities to flow across scenes through read and update operations. We report a best result of 94.47% mAP on the AVA-ActiveSpeaker test set, which continues to rank first on this year's challenge leaderboard and significantly pushes the state-of-the-art."
          ],
          [
            "Access Date",
            "2023-05-26 05:01:29"
          ],
          [
            "Archiveid",
            "arXiv:2206.10861"
          ],
          [
            "Creators",
            "Yuanhang Zhang, Susan Liang, Shuang Yang, Shiguang Shan"
          ],
          [
            "DOI",
            "10.48550/arXiv.2206.10861"
          ],
          [
            "Date",
            "2022-06-22 2022-06-22"
          ],
          [
            "Extra",
            "arXiv:2206.10861 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "UniCon+"
          ],
          [
            "Title",
            "UniCon+: ICTCAS-UCAS Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2022"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2206.10861"
          ]
        ],
        "resource": "storage/i2400.pdf",
        "selectable": false
      }
    ],
    "item_title": "Speech Diarization",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Speech Embedding",
    "item-id": "c38,i3658",
    "nodes": [
      {
        "text": "Audioclip",
        "item-id": "i2790",
        "nodes": [
          {
            "text": "Guzhov et al_2022_Audioclip.pdf",
            "item-id": "i2828",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Guzhov et al_2022_Audioclip.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_VV9IWYLS/1\">AudioCLIP: Extending CLIP to Image, Text and Audio</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Guzhov et al_2022_Audioclip.pdf"
              ]
            ],
            "resource": "storage/i2828.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audioclip",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The rapidly evolving field of sound classification has greatly benefited from the methods of other domains. Today, the trend is to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models.We present AudioCLIP \u2013 an extension of the CLIP model that handles audio in addition to text and images. Utilizing the AudioSet dataset, our proposed model incorporates the ESResNeXt audio-model into the CLIP framework, thus enabling it to perform multimodal classification and keeping CLIP\u2019s zero-shot capabilities.AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task and out-performs others by reaching accuracies of 97.15 % on ESC-50 and 90.07 % on UrbanSound8K. Further, it sets new baselines in the zero-shot ESC-task on the same datasets (69.40 % and 68.78 %, respectively).We also asses the influence of different training setups on the final performance of the proposed model. For the sake of reproducibility, our code is published."
          ],
          [
            "Conference Name",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Andrey Guzhov, Federico Raue, J\u00f6rn Hees, Andreas Dengel"
          ],
          [
            "DOI",
            "10.1109/ICASSP43922.2022.9747631"
          ],
          [
            "Date",
            "2022-05-00 2022-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "976-980"
          ],
          [
            "Proceedings Title",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Audioclip"
          ],
          [
            "Title",
            "Audioclip: Extending Clip to Image, Text and Audio"
          ]
        ],
        "resource": "storage/i2828.pdf",
        "selectable": false
      },
      {
        "text": "BYOL for Audio",
        "item-id": "i3216",
        "nodes": [
          {
            "text": "Niizumi et al_2021_BYOL for Audio.pdf",
            "item-id": "i3295",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Niizumi et al_2021_BYOL for Audio.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Niizumi et al_2021_BYOL for Audio.pdf"
              ]
            ],
            "resource": "storage/i3295.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BYOL for Audio",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Inspired by the recent progress in self-supervised learning for computer vision that generates supervision using data augmentations, we explore a new general-purpose audio representation learning approach. We propose learning general-purpose audio representation from a single audio segment without expecting relationships between different time segments of audio samples. To implement this principle, we introduce Bootstrap Your Own Latent (BYOL) for Audio (BYOL-A, pronounced \u201cviola\u201d), an audio self-supervised learning method based on BYOL for learning general-purpose audio representation. Unlike most previous audio self-supervised learning methods that rely on agreement of vicinity audio segments or disagreement of remote ones, BYOL-A creates contrasts in an augmented audio segment pair derived from a single audio segment. With a combination of normalization and augmentation techniques, BYOL-A achieves state-of-the-art results in various downstream tasks. Extensive ablation studies also clarified the contribution of each component and their combinations."
          ],
          [
            "Access Date",
            "2023-11-10 15:01:21"
          ],
          [
            "Conference Name",
            "2021 International Joint Conference on Neural Networks (IJCNN)"
          ],
          [
            "Creators",
            "Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino"
          ],
          [
            "DOI",
            "10.1109/IJCNN52387.2021.9534474"
          ],
          [
            "Date",
            "2021-07-00 2021-07"
          ],
          [
            "Extra",
            "ISSN: 2161-4407"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-8"
          ],
          [
            "Proceedings Title",
            "2021 International Joint Conference on Neural Networks (IJCNN)"
          ],
          [
            "Short Title",
            "BYOL for Audio"
          ],
          [
            "Title",
            "BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/9534474"
          ]
        ],
        "resource": "storage/i3295.pdf",
        "selectable": false
      },
      {
        "text": "BigSSL",
        "item-id": "i2495",
        "nodes": [
          {
            "text": "Zhang et al_2022_BigSSL.pdf",
            "item-id": "i2678",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2022_BigSSL.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2022_BigSSL.pdf"
              ]
            ],
            "resource": "storage/i2678.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BigSSL",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled datasets containing approximately a million hours of audio. We find that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data. In particular, on an ASR task with 34 k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set. We also report on the universal benefits gained from using big pre-trained and self-trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes, including obtaining SoTA performance on many public benchmarks. In addition, we utilize the learned representation of pre-trained networks to achieve SoTA results on non-ASR tasks."
          ],
          [
            "Creators",
            "Yu Zhang, Daniel S. Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Fran\u00e7oise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang, Yonghui Wu"
          ],
          [
            "DOI",
            "10.1109/JSTSP.2022.3182537"
          ],
          [
            "Date",
            "2022-10-00 2022-10"
          ],
          [
            "Extra",
            "Conference Name: IEEE Journal of Selected Topics in Signal Processing"
          ],
          [
            "ISSN",
            "1941-0484"
          ],
          [
            "Issue",
            "6"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1519-1532"
          ],
          [
            "Publication Title",
            "IEEE Journal of Selected Topics in Signal Processing"
          ],
          [
            "Short Title",
            "BigSSL"
          ],
          [
            "Title",
            "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition"
          ],
          [
            "Volume",
            "16"
          ]
        ],
        "resource": "storage/i2678.pdf",
        "selectable": false
      },
      {
        "text": "CNN architectures for large-scale audio classification",
        "item-id": "i2521",
        "nodes": [
          {
            "text": "Hershey et al_2017_CNN architectures for large-scale audio classification.pdf",
            "item-id": "i2723",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hershey et al_2017_CNN architectures for large-scale audio classification.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hershey et al_2017_CNN architectures for large-scale audio classification.pdf"
              ]
            ],
            "resource": "storage/i2723.pdf"
          },
          {
            "text": "VGGish",
            "item-id": "n2722",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "VGGish",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>VGGish</p>\n</div></div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CNN architectures for large-scale audio classification",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task."
          ],
          [
            "Conference Name",
            "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron J. Weiss, Kevin Wilson"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2017.7952132"
          ],
          [
            "Date",
            "2017-03-00 2017-03"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "131-135"
          ],
          [
            "Proceedings Title",
            "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "CNN architectures for large-scale audio classification"
          ]
        ],
        "resource": "storage/i2723.pdf",
        "selectable": false
      },
      {
        "text": "ContentVec",
        "item-id": "i3658",
        "nodes": [
          {
            "text": "Qian et al_2022_ContentVec.pdf",
            "item-id": "i3686",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Qian et al_2022_ContentVec.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Qian et al_2022_ContentVec.pdf"
              ]
            ],
            "resource": "storage/i3686.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ContentVec",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-supervised learning in speech involves training a speech representation network on a large-scale unannotated speech corpus, and then applying the learned representations to downstream tasks. Since the majority of the downstream tasks of SSL learning in speech largely focus on the content information in speech, the most desirable speech representations should be able to disentangle unwanted variations, such as speaker variations, from the content. However, disentangling speakers is very challenging, because removing the speaker information could easily result in a loss of content as well, and the damage of the latter usually far outweighs the benefit of the former. In this paper, we propose a new SSL method that can achieve speaker disentanglement without severe loss of content. Our approach is adapted from the HuBERT framework, and incorporates disentangling mechanisms to regularize both the teacher labels and the learned representations. We evaluate the benefit of speaker disentanglement on a set of content-related downstream tasks, and observe a consistent and notable performance advantage of our speaker-disentangled representations."
          ],
          [
            "Access Date",
            "2024-01-14 15:17:13"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Kaizhi Qian, Yang Zhang, Heting Gao, Junrui Ni, Cheng-I. Lai, David Cox, Mark Hasegawa-Johnson, Shiyu Chang"
          ],
          [
            "Date",
            "2022-06-28 2022-06-28"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "18003-18017"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 39th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Short Title",
            "ContentVec"
          ],
          [
            "Title",
            "ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v162/qian22b.html"
          ]
        ],
        "resource": "storage/i3686.pdf",
        "selectable": false
      },
      {
        "text": "Deep Speech",
        "item-id": "i2519",
        "nodes": [
          {
            "text": "Hannun et al_2014_Deep Speech.pdf",
            "item-id": "i2719",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hannun et al_2014_Deep Speech.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hannun et al_2014_Deep Speech.pdf"
              ]
            ],
            "resource": "storage/i2719.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Deep Speech",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems."
          ],
          [
            "Access Date",
            "2023-06-08 05:46:21"
          ],
          [
            "Archiveid",
            "arXiv:1412.5567"
          ],
          [
            "Creators",
            "Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng"
          ],
          [
            "DOI",
            "10.48550/arXiv.1412.5567"
          ],
          [
            "Date",
            "2014-12-19 2014-12-19"
          ],
          [
            "Extra",
            "arXiv:1412.5567 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Deep Speech"
          ],
          [
            "Title",
            "Deep Speech: Scaling up end-to-end speech recognition"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1412.5567"
          ]
        ],
        "resource": "storage/i2719.pdf",
        "selectable": false
      },
      {
        "text": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
        "item-id": "i3028",
        "nodes": [
          {
            "text": "Comment: Preprint. Submitted to ICASSP 2024",
            "item-id": "n3064",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Preprint. Submitted to ICASSP 2024",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Preprint. Submitted to ICASSP 2024</div>",
            "node_type": "note"
          },
          {
            "text": "Puvvada et al_2023_Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker.pdf",
            "item-id": "i3063",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Puvvada et al_2023_Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_X7PRD5JR/1\"> Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/1\"> Audio Representation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Datasets</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Training Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Evaluation Data - Speaker Verification &amp;amp; Diarization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Evaluation Data - Automatic Speech Recognition</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Experiment Setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Speaker Embeddings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Automatic Speech Recognition</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> RESULTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/2\"> Speaker Verification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/3\"> Speaker Diarization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/3\"> Automatic Speech Recognition</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/3\"> Ablations</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/3\"> Effect of bit-rate</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/4\"> Audio Tokenizer</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/4\"> CONCLUSION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X7PRD5JR/5\"> References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Puvvada et al_2023_Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker.pdf"
              ]
            ],
            "resource": "storage/i3063.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs."
          ],
          [
            "Access Date",
            "2023-10-06 05:33:08"
          ],
          [
            "Archiveid",
            "arXiv:2309.10922"
          ],
          [
            "Creators",
            "Krishna C. Puvvada, Nithin Rao Koluguri, Kunal Dhawan, Jagadeesh Balam, Boris Ginsburg"
          ],
          [
            "DOI",
            "10.48550/arXiv.2309.10922"
          ],
          [
            "Date",
            "2023-09-19 2023-09-19"
          ],
          [
            "Extra",
            "arXiv:2309.10922 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2309.10922"
          ]
        ],
        "resource": "storage/i3063.pdf",
        "selectable": false
      },
      {
        "text": "FRILL",
        "item-id": "i2520",
        "nodes": [
          {
            "text": "Peplinski et al_2021_FRILL.pdf",
            "item-id": "i2674",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Peplinski et al_2021_FRILL.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Peplinski et al_2021_FRILL.pdf"
              ]
            ],
            "resource": "storage/i2674.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FRILL",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Learned speech representations can drastically improve performance on tasks with limited labeled data. However, due to their size and complexity, learned representations have limited utility in mobile settings where run-time performance can be a signi\ufb01cant bottleneck. In this work, we propose a class of lightweight non-semantic speech embedding models that run ef\ufb01ciently on mobile devices based on the recently proposed TRILL speech embedding. We combine novel architectural modi\ufb01cations with existing speed-up techniques to create embedding models that are fast enough to run in real-time on a mobile device and exhibit minimal performance degradation on a benchmark of nonsemantic speech tasks. One such model (FRILL) is 32x faster on a Pixel 1 smartphone and 40% the size of TRILL, with an average decrease in accuracy of only 2%. To our knowledge, FRILL is the highest-quality non-semantic embedding designed for use on mobile devices. Furthermore, we demonstrate that these representations are useful for mobile health tasks such as non-speech human sounds detection and face-masked speech detection. Our models1 and code2 are publicly available."
          ],
          [
            "Access Date",
            "2023-06-08 05:43:31"
          ],
          [
            "Conference Name",
            "Interspeech 2021"
          ],
          [
            "Creators",
            "Jacob Peplinski, Joel Shor, Sachin Joglekar, Jake Garrison, Shwetak Patel"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-2070"
          ],
          [
            "Date",
            "2021-08-30 2021-8-30"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "1204-1208"
          ],
          [
            "Proceedings Title",
            "Interspeech 2021"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "FRILL"
          ],
          [
            "Title",
            "FRILL: A Non-Semantic Speech Embedding for Mobile Devices"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2021/peplinski21_interspeech.html"
          ]
        ],
        "resource": "storage/i2674.pdf",
        "selectable": false
      },
      {
        "text": "HuBERT",
        "item-id": "i2497",
        "nodes": [
          {
            "text": "Hsu et al_2021_HuBERT.pdf",
            "item-id": "i2687",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Hsu et al_2021_HuBERT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Hsu et al_2021_HuBERT.pdf"
              ]
            ],
            "resource": "storage/i2687.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HuBERT",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.12"
          ],
          [
            "Creators",
            "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed"
          ],
          [
            "DOI",
            "10.1109/TASLP.2021.3122291"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "ISSN",
            "2329-9304"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "3451-3460"
          ],
          [
            "Publication Title",
            "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "Short Title",
            "HuBERT"
          ],
          [
            "Title",
            "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
          ],
          [
            "Volume",
            "29"
          ]
        ],
        "resource": "storage/i2687.pdf",
        "selectable": false
      },
      {
        "text": "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation",
        "item-id": "i3221",
        "nodes": [
          {
            "text": "Wu et al_2023_Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and.pdf",
            "item-id": "i3305",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2023_Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2023_Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and.pdf"
              ]
            ],
            "resource": "storage/i3305.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models\u2019 results in the non-zero-shot setting. LAION-Audio-630K1 and the proposed model 2 are both available to the public."
          ],
          [
            "Access Date",
            "2023-11-02 14:23:51"
          ],
          [
            "Conference Name",
            "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov"
          ],
          [
            "DOI",
            "10.1109/ICASSP49357.2023.10095969"
          ],
          [
            "Date",
            "2023-06-00 2023-06"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-5"
          ],
          [
            "Proceedings Title",
            "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"
          ],
          [
            "URL",
            "https://ieeexplore.ieee.org/abstract/document/10095969"
          ]
        ],
        "resource": "storage/i3305.pdf",
        "selectable": false
      },
      {
        "text": "Masked Autoencoders that Listen",
        "item-id": "i2925",
        "nodes": [
          {
            "text": "Huang et al_2022_Masked Autoencoders that Listen.pdf",
            "item-id": "i2931",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2022_Masked Autoencoders that Listen.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2022_Masked Autoencoders that Listen.pdf"
              ]
            ],
            "resource": "storage/i2931.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Masked Autoencoders that Listen",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. Our code and models is available at https://github.com/facebookresearch/AudioMAE."
          ],
          [
            "Creators",
            "Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, Christoph Feichtenhofer, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Pages",
            "28708\u201328720"
          ],
          [
            "Proceedings Title",
            "Advances in neural information processing systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Masked Autoencoders that Listen"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2022/file/b89d5e209990b19e33b418e14f323998-Paper-Conference.pdf"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i2931.pdf",
        "selectable": false
      },
      {
        "text": "Representation Learning with Contrastive Predictive Coding",
        "item-id": "i2499",
        "nodes": [
          {
            "text": "Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf",
            "item-id": "i2693",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WYALZVHI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WYALZVHI/2\">2 Contrastive Predicting Coding</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/2\">2.1 Motivation and Intuitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/3\">2.2 Contrastive Predictive Coding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/3\">2.3 InfoNCE Loss and Mutual Information Estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/4\">2.4 Related Work</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WYALZVHI/4\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/4\">3.1 Audio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/6\">3.2 Vision</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/7\">3.3 Natural Language</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/9\">3.4 Reinforcement Learning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/9\">4 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/9\">5 Acknowledgements</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WYALZVHI/13\">A Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYALZVHI/13\">A.1 Estimating the Mutual Information with InfoNCE</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf"
              ]
            ],
            "resource": "storage/i2693.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Representation Learning with Contrastive Predictive Coding",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments."
          ],
          [
            "Access Date",
            "2023-06-08 15:49:13"
          ],
          [
            "Archiveid",
            "arXiv:1807.03748"
          ],
          [
            "Creators",
            "Aaron van den Oord, Yazhe Li, Oriol Vinyals"
          ],
          [
            "DOI",
            "10.48550/arXiv.1807.03748"
          ],
          [
            "Date",
            "2019-01-22 2019-01-22"
          ],
          [
            "Extra",
            "arXiv:1807.03748 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Representation Learning with Contrastive Predictive Coding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1807.03748"
          ]
        ],
        "resource": "storage/i2693.pdf",
        "selectable": false
      },
      {
        "text": "SpeechTripleNet",
        "item-id": "i3025",
        "nodes": [
          {
            "text": "Lu et al_2023_SpeechTripleNet.pdf",
            "item-id": "i3061",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lu et al_2023_SpeechTripleNet.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GP44DNSA/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GP44DNSA/2\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/2\">2.1 VAE-based disentanglement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/3\">2.2 Prosody modeling</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GP44DNSA/3\">3 Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/3\">3.1 Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/3\">3.2 Latent structures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/4\">3.3 Learning objective</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GP44DNSA/5\">4 Implementation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/5\">4.1 Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/5\">4.2 Model structure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/6\">4.3 Training specifics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/6\">4.4 Speech editing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GP44DNSA/6\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/6\">5.1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/6\">5.2 Speech disentanglement evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/7\">5.3 Speech editing evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/8\">5.4 Ablation study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/8\">7 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GP44DNSA/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lu et al_2023_SpeechTripleNet.pdf"
              ]
            ],
            "resource": "storage/i3061.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SpeechTripleNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-10-06 05:33:41"
          ],
          [
            "Creators",
            "Hui Lu, Xixin Wu, Zhiyong Wu, Helen Meng"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Library Catalog",
            "Google Scholar"
          ],
          [
            "Short Title",
            "SpeechTripleNet"
          ],
          [
            "Title",
            "SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody"
          ],
          [
            "URL",
            "https://www1.se.cuhk.edu.hk/~hccl/publications/pub/mmfp3442-lu-CC-BY.pdf"
          ]
        ],
        "resource": "storage/i3061.pdf",
        "selectable": false
      },
      {
        "text": "TRILLsson",
        "item-id": "i2301",
        "nodes": [
          {
            "text": "Comment: Submitted to Interspeech 2022",
            "item-id": "n2302",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Submitted to Interspeech 2022",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Submitted to Interspeech 2022</div>",
            "node_type": "note"
          },
          {
            "text": "Shor_Venugopalan_2022_TRILLsson.pdf",
            "item-id": "i2304",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shor_Venugopalan_2022_TRILLsson.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NTPIULGX/1\">1  Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/1\">2  Background and related works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NTPIULGX/2\">3  Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/2\">3.1  Student architectures.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/2\">3.2  Distillation targets: global vs local matching</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/3\">3.3  Training datasets</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NTPIULGX/3\">3.4  Representation Evaluation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/3\">3.4.1  Aggregation metric: Equivalent D-Prime (d')</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/3\">3.5  Models for comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/4\">4  Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/4\">5  Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/4\">6  Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NTPIULGX/5\">7  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shor_Venugopalan_2022_TRILLsson.pdf"
              ]
            ],
            "resource": "storage/i2304.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TRILLsson",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent advances in self-supervision have dramatically improved the quality of speech representations. However, deployment of state-of-the-art embedding models on devices has been restricted due to their limited public availability and large resource footprint. Our work addresses these issues by publicly releasing a collection of paralinguistic speech models that are small and near state-of-the-art performance. Our approach is based on knowledge distillation, and our models are distilled on public data only. We explore different architectures and thoroughly evaluate our models on the Non-Semantic Speech (NOSS) benchmark. Our largest distilled model is less than 15% the size of the original model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7 tasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB) and achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model outperforms the open source Wav2Vec 2.0 on both emotion recognition tasks despite being 7% the size."
          ],
          [
            "Access Date",
            "2023-04-14 10:27:52"
          ],
          [
            "Conference Name",
            "Interspeech 2022"
          ],
          [
            "Creators",
            "Joel Shor, Subhashini Venugopalan"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2022-118"
          ],
          [
            "Date",
            "2022-09-18 2022-9-18"
          ],
          [
            "Extra",
            "arXiv:2203.00236 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "356-360"
          ],
          [
            "Proceedings Title",
            "Interspeech 2022"
          ],
          [
            "Short Title",
            "TRILLsson"
          ],
          [
            "Title",
            "TRILLsson: Distilled Universal Paralinguistic Speech Representations"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2203.00236"
          ]
        ],
        "resource": "storage/i2304.pdf",
        "selectable": false
      },
      {
        "text": "Towards Learning a Universal Non-Semantic Representation of Speech",
        "item-id": "i681",
        "nodes": [
          {
            "text": "Shor et al_2020_Towards Learning a Universal Non-Semantic Representation of Speech.pdf",
            "item-id": "i685",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shor et al_2020_Towards Learning a Universal Non-Semantic Representation of Speech.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_9MQ3XVUC/1\">1  Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/1\">2  Background</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/2\">3  Non-Semantic Speech Benchmark (NOSS)</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/2\">4  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/2\">4.1  TRILL representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/3\">4.2  Other representations</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/3\">4.3  Experimental Method</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/3\">4.3.1  Evaluation methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/3\">4.3.2  Network layer and model distillation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/3\">4.3.3  Fine-tuning</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/4\">5  Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/4\">6  Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/4\">7  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9MQ3XVUC/5\">8  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shor et al_2020_Towards Learning a Universal Non-Semantic Representation of Speech.pdf"
              ]
            ],
            "resource": "storage/i685.pdf"
          },
          {
            "text": "TRILL",
            "item-id": "n2721",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "TRILL",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>TRILL</p>\n</div></div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Towards Learning a Universal Non-Semantic Representation of Speech",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark, models, and evaluation code are publicly released."
          ],
          [
            "Access Date",
            "2021-06-05 18:23:32"
          ],
          [
            "Creators",
            "Joel Shor, Aren Jansen, Ronnie Maor, Oran Lang, Omry Tuval, Felix de Chaumont Quitry, Marco Tagliasacchi, Ira Shavitt, Dotan Emanuel, Yinnon Haviv"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2020-1242"
          ],
          [
            "Date",
            "2020-10-25 2020-10-25"
          ],
          [
            "Extra",
            "arXiv: 2002.12764"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "140-144"
          ],
          [
            "Publication Title",
            "Interspeech 2020"
          ],
          [
            "Title",
            "Towards Learning a Universal Non-Semantic Representation of Speech"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2002.12764"
          ]
        ],
        "resource": "storage/i685.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Learning of Semantic Audio Representations",
        "item-id": "i2496",
        "nodes": [
          {
            "text": "Jansen et al_2018_Unsupervised Learning of Semantic Audio Representations.pdf",
            "item-id": "i2682",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jansen et al_2018_Unsupervised Learning of Semantic Audio Representations.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jansen et al_2018_Unsupervised Learning of Semantic Audio Representations.pdf"
              ]
            ],
            "resource": "storage/i2682.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Learning of Semantic Audio Representations",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel P. W. Ellis, Shawn Hershey, Jiayang Liu, R. Channing Moore, Rif A. Saurous"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8461684"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "126-130"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Unsupervised Learning of Semantic Audio Representations"
          ]
        ],
        "resource": "storage/i2682.pdf",
        "selectable": false
      },
      {
        "text": "VATT",
        "item-id": "i2789",
        "nodes": [
          {
            "text": "Akbari et al_2021_VATT.pdf",
            "item-id": "i2923",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Akbari et al_2021_VATT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Akbari et al_2021_VATT.pdf"
              ]
            ],
            "resource": "storage/i2923.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "VATT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training."
          ],
          [
            "Access Date",
            "2023-06-30 08:13:38"
          ],
          [
            "Creators",
            "Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, Boqing Gong"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "24206\u201324221"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "VATT"
          ],
          [
            "Title",
            "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2021/hash/cb3213ada48302953cb0f166464ab356-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i2923.pdf",
        "selectable": false
      },
      {
        "text": "Wav2vec-Switch",
        "item-id": "i2514",
        "nodes": [
          {
            "text": "Wang et al_2022_Wav2vec-Switch.pdf",
            "item-id": "i2714",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_Wav2vec-Switch.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_Wav2vec-Switch.pdf"
              ]
            ],
            "resource": "storage/i2714.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Wav2vec-Switch",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The goal of self-supervised learning (SSL) for automatic speech recognition (ASR) is to learn good speech representations from a large amount of unlabeled speech for the downstream ASR task. However, most SSL frameworks do not consider noise robustness which is crucial for real-world applications. In this paper we propose wav2vec-Switch, a method to encode noise robustness into contextualized representations of speech via contrastive learning. Specifically, we feed original-noisy speech pairs simultaneously into the wav2vec 2.0 network. In addition to the existing contrastive learning task, we switch the quantized representations of the original and noisy speech as additional prediction targets of each other. By doing this, it enforces the network to have consistent predictions for the original and noisy speech, thus allows to learn contextualized representation with noise robustness. Our experiments on synthe-sized and real noisy data show the effectiveness of our method: it achieves 2.9\u20134.9% relative word error rate (WER) reduction on the synthesized noisy LibriSpeech data without deterioration on the original data, and 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation baseline even with a strong language model for decoding. Our results on CHiME-4 can match or even surpass those with well-designed speech enhancement components."
          ],
          [
            "Conference Name",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Yiming Wang, Jinyu Li, Heming Wang, Yao Qian, Chengyi Wang, Yu Wu"
          ],
          [
            "DOI",
            "10.1109/ICASSP43922.2022.9746929"
          ],
          [
            "Date",
            "2022-05-00 2022-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "ISBN",
            "978-1-66540-540-9"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "7097-7101"
          ],
          [
            "Place",
            "Singapore, Singapore"
          ],
          [
            "Proceedings Title",
            "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Wav2vec-Switch"
          ],
          [
            "Title",
            "Wav2vec-Switch: Contrastive Learning from Original-Noisy Speech Pairs for Robust Speech Recognition"
          ]
        ],
        "resource": "storage/i2714.pdf",
        "selectable": false
      },
      {
        "text": "WavLM",
        "item-id": "i2498",
        "nodes": [
          {
            "text": "Chen et al_2022_WavLM.pdf",
            "item-id": "i2689",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2022_WavLM.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2022_WavLM.pdf"
              ]
            ],
            "resource": "storage/i2689.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "WavLM",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60 k hours to 94 k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks."
          ],
          [
            "Creators",
            "Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei"
          ],
          [
            "DOI",
            "10.1109/JSTSP.2022.3188113"
          ],
          [
            "Date",
            "2022-10-00 2022-10"
          ],
          [
            "Extra",
            "Conference Name: IEEE Journal of Selected Topics in Signal Processing"
          ],
          [
            "ISSN",
            "1941-0484"
          ],
          [
            "Issue",
            "6"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1505-1518"
          ],
          [
            "Publication Title",
            "IEEE Journal of Selected Topics in Signal Processing"
          ],
          [
            "Short Title",
            "WavLM"
          ],
          [
            "Title",
            "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing"
          ],
          [
            "Volume",
            "16"
          ]
        ],
        "resource": "storage/i2689.pdf",
        "selectable": false
      },
      {
        "text": "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings",
        "item-id": "i2542",
        "nodes": [
          {
            "text": "Cooper et al_2020_Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker.pdf",
            "item-id": "i2598",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Cooper et al_2020_Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Cooper et al_2020_Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker.pdf"
              ]
            ],
            "resource": "storage/i2598.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While speaker adaptation for end-to-end speech synthesis using speaker embeddings can produce good speaker similarity for speakers seen during training, there remains a gap for zero-shot adaptation to unseen speakers. We investigate multi-speaker modeling for end-to-end text-to-speech synthesis and study the effects of different types of state-of-the-art neural speaker embeddings on speaker similarity for unseen speakers. Learnable dictionary encoding-based speaker embeddings with angular softmax loss can improve equal error rates over x-vectors in a speaker verification task; these embeddings also improve speaker similarity and naturalness for unseen speakers when used for zero-shot adaptation to new speakers in end-to-end speech synthesis."
          ],
          [
            "Conference Name",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen, Junichi Yamagishi"
          ],
          [
            "DOI",
            "10.1109/ICASSP40776.2020.9054535"
          ],
          [
            "Date",
            "2020-05-00 2020-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6184-6188"
          ],
          [
            "Proceedings Title",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings"
          ]
        ],
        "resource": "storage/i2598.pdf",
        "selectable": false
      },
      {
        "text": "vq-wav2vec",
        "item-id": "i2518",
        "nodes": [
          {
            "text": "Baevski et al_2019_vq-wav2vec.pdf",
            "item-id": "i2717",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Baevski et al_2019_vq-wav2vec.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Baevski et al_2019_vq-wav2vec.pdf"
              ]
            ],
            "resource": "storage/i2717.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "vq-wav2vec",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition."
          ],
          [
            "Access Date",
            "2023-06-08 05:48:01"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Alexei Baevski, Steffen Schneider, Michael Auli"
          ],
          [
            "Date",
            "2019-12-20 2019/12/20"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "vq-wav2vec"
          ],
          [
            "Title",
            "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=rylwJxrYDS"
          ]
        ],
        "resource": "storage/i2717.pdf",
        "selectable": false
      },
      {
        "text": "wav2vec",
        "item-id": "i2513",
        "nodes": [
          {
            "text": "Schneider et al_2019_wav2vec.pdf",
            "item-id": "i2641",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Schneider et al_2019_wav2vec.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_23HGAQBV/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_23HGAQBV/2\">2 Pre-training Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/2\">2.1 Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/3\">2.2 Objective</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_23HGAQBV/3\">3 Experimental Setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/3\">3.1 Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/3\">3.2 Acoustic Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/4\">3.3 Decoding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/4\">3.4 Pre-training Models</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_23HGAQBV/5\">4 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/5\">4.1 Pre-training for the WSJ benchmark</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/6\">4.2 Pre-training for TIMIT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/6\">4.3 Ablations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_23HGAQBV/7\">5 Conclusions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Schneider et al_2019_wav2vec.pdf"
              ]
            ],
            "resource": "storage/i2641.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "wav2vec",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36 % when only a few hours of transcribed data is available. Our approach achieves 2.43 % WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using three orders of magnitude less labeled training data."
          ],
          [
            "Conference Name",
            "Interspeech 2019, 20th Annual Conference of the International Speech Communication Association"
          ],
          [
            "Creators",
            "Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli, Gernot Kubin, Zdravko Kacic"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2019-1873"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "DBLP Computer Science Bibliography"
          ],
          [
            "Pages",
            "3465\u20133469"
          ],
          [
            "Place",
            "Graz, Austria"
          ],
          [
            "Proceedings Title",
            "Interspeech 2019, 20th Annual Conference of the International Speech Communication Association"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "wav2vec"
          ],
          [
            "Title",
            "wav2vec: Unsupervised Pre-Training for Speech Recognition"
          ]
        ],
        "resource": "storage/i2641.pdf",
        "selectable": false
      },
      {
        "text": "wav2vec 2.0",
        "item-id": "i2201",
        "nodes": [
          {
            "text": "Baevski et al_2020_wav2vec 2.pdf",
            "item-id": "i2229",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Baevski et al_2020_wav2vec 2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Baevski et al_2020_wav2vec 2.pdf"
              ]
            ],
            "resource": "storage/i2229.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "wav2vec 2.0",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data."
          ],
          [
            "Access Date",
            "2023-03-12 14:22:58"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, Michael Auli"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "12449\u201312460"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "wav2vec 2.0"
          ],
          [
            "Title",
            "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html"
          ],
          [
            "Volume",
            "33"
          ]
        ],
        "resource": "storage/i2229.pdf",
        "selectable": false
      },
      {
        "text": "wav2vec-C",
        "item-id": "i2517",
        "nodes": [
          {
            "text": "Sadhu et al. - 2021 - wav2vec-C A Self-Supervised Model for Speech Repr.pdf",
            "item-id": "i2715",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sadhu et al. - 2021 - wav2vec-C A Self-Supervised Model for Speech Repr.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-06-08 06:00:49"
              ],
              [
                "Title",
                "Sadhu et al. - 2021 - wav2vec-C A Self-Supervised Model for Speech Repr.pdf"
              ],
              [
                "URL",
                "https://www.isca-speech.org/archive/pdfs/interspeech_2021/sadhu21_interspeech.pdf"
              ]
            ],
            "resource": "storage/i2715.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "wav2vec-C",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Wav2vec-C introduces a novel representation learning technique combining elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized representations from partially masked speech encoding using a contrastive loss in a way similar to wav2vec 2.0. However, the quantization process is regularized by an additional consistency network that learns to reconstruct the input features to the wav2vec 2.0 network from the quantized representations in a way similar to a VQ-VAE model. The proposed self-supervised model is trained on 10k hours of unlabeled data and subsequently used as the speech encoder in a RNN-T ASR model and \ufb01ne-tuned with 1k hours of labeled data. This work is one of the very few studies of selfsupervised learning on speech tasks with a large volume of real far-\ufb01eld labeled data. The wav2vec-C encoded representations achieve, on average, twice the error reduction over baseline and a higher codebook utilization in comparison to wav2vec 2.0."
          ],
          [
            "Access Date",
            "2023-06-08 06:00:53"
          ],
          [
            "Conference Name",
            "Interspeech 2021"
          ],
          [
            "Creators",
            "Samik Sadhu, Di He, Che-Wei Huang, Sri Harish Mallidi, Minhua Wu, Ariya Rastrow, Andreas Stolcke, Jasha Droppo, Roland Maas"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-717"
          ],
          [
            "Date",
            "2021-08-30 2021-8-30"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "711-715"
          ],
          [
            "Proceedings Title",
            "Interspeech 2021"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "wav2vec-C"
          ],
          [
            "Title",
            "wav2vec-C: A Self-Supervised Model for Speech Representation Learning"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2021/sadhu21_interspeech.html"
          ]
        ],
        "resource": "storage/i2715.pdf",
        "selectable": false
      }
    ],
    "item_title": "Speech Embedding",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Speech Separation",
    "item-id": "c34,i2396",
    "nodes": [
      {
        "text": "Attention Is All You Need In Speech Separation",
        "item-id": "i2362",
        "nodes": [
          {
            "text": "Subakan et al_2021_Attention Is All You Need In Speech Separation.pdf",
            "item-id": "i2471",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Subakan et al_2021_Attention Is All You Need In Speech Separation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Subakan et al_2021_Attention Is All You Need In Speech Separation.pdf"
              ]
            ],
            "resource": "storage/i2471.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attention Is All You Need In Speech Separation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism.In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The Sep-Former learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance."
          ],
          [
            "Conference Name",
            "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, Jianyuan Zhong"
          ],
          [
            "DOI",
            "10.1109/ICASSP39728.2021.9413901"
          ],
          [
            "Date",
            "2021-06-00 2021-06"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "21-25"
          ],
          [
            "Proceedings Title",
            "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Attention Is All You Need In Speech Separation"
          ]
        ],
        "resource": "storage/i2471.pdf",
        "selectable": false
      },
      {
        "text": "Compute and Memory Efficient Universal Sound Source Separation",
        "item-id": "i2396",
        "nodes": [
          {
            "text": "Tzinis et al_2022_Compute and Memory Efficient Universal Sound Source Separation.pdf",
            "item-id": "i2448",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tzinis et al_2022_Compute and Memory Efficient Universal Sound Source Separation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MMXIHEBR/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/2\">Sudo RM -RF Network Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/3\">Encoder</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/3\">Separator</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/4\">U-Convolutional Block (U-ConvBlock)</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/5\">Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/5\">Improved Version with no Mask Estimation SuDoRM-RF++</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/6\">Group Communication Variation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/6\">Causal Version C-SuDoRM-RF++</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/6\">Experimental Setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/6\">Audio Source Separation Tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/6\">Speech Separation (2 Active Speakers)</a><ul style=\"list-style-type: none; padding-left:36px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Non-Speech Sound Separation (2 Active Sources)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Universal Sound Separation (Variable Number Of Sources 1-4)</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Data Pre-Processing and Generation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Fixed Number of Sources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Variable Number of Sources</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Training Details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Fixed Number of Sources</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/7\">Variable Number of Sources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/8\">Evaluation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/8\">SuDoRM-RF Configurations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/8\">Literature Models Configurations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/9\">Measuring Computational Resources</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/9\">Results &amp;amp; Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/10\">Floating Point Operations (FLOPs)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/11\">Cost-Efficient Training</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/11\">Trainable Parameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/11\">Memory Requirements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/12\">Ablation Study on WSJ0-2mix</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/12\">Variable Number of Sources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/13\">Causal Setup</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/13\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MMXIHEBR/14\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tzinis et al_2022_Compute and Memory Efficient Universal Sound Source Separation.pdf"
              ]
            ],
            "resource": "storage/i2448.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Compute and Memory Efficient Universal Sound Source Separation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent progress in audio source separation led by deep learning has enabled many neural network models to provide robust solutions to this fundamental estimation problem. In this study, we provide a family of efficient neural network architectures for general purpose audio source separation while focusing on multiple computational aspects that hinder the application of neural networks in real-world scenarios. The backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. This mechanism enables our models to obtain high fidelity signal separation in a wide variety of settings where a variable number of sources are present and with limited computational resources (e.g. floating point operations, memory footprint, number of parameters and latency). Our experiments show that SuDoRM-RF models perform comparably and even surpass several state-of-the-art benchmarks with significantly higher computational resource requirements. The causal variation of SuDoRM-RF is able to obtain competitive performance in real-time speech separation of around 10dB scale-invariant signal-to-distortion ratio improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a laptop device."
          ],
          [
            "Access Date",
            "2023-05-22 17:11:47"
          ],
          [
            "Creators",
            "Efthymios Tzinis, Zhepei Wang, Xilin Jiang, Paris Smaragdis"
          ],
          [
            "DOI",
            "10.1007/s11265-021-01683-x"
          ],
          [
            "Date",
            "2022-02-01 2022-02-01"
          ],
          [
            "ISSN",
            "1939-8115"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Journal Abbreviation",
            "J Sign Process Syst"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "245-259"
          ],
          [
            "Publication Title",
            "Journal of Signal Processing Systems"
          ],
          [
            "Title",
            "Compute and Memory Efficient Universal Sound Source Separation"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11265-021-01683-x"
          ],
          [
            "Volume",
            "94"
          ]
        ],
        "resource": "storage/i2448.pdf",
        "selectable": false
      },
      {
        "text": "Conv-TasNet",
        "item-id": "i2361",
        "nodes": [
          {
            "text": "Luo_Mesgarani_2019_Conv-TasNet.pdf",
            "item-id": "i2468",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Luo_Mesgarani_2019_Conv-TasNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Luo_Mesgarani_2019_Conv-TasNet.pdf"
              ]
            ],
            "resource": "storage/i2468.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Conv-TasNet",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time\u2013frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time\u2013frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network consisting of stacked one-dimensional dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time\u2013frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time\u2013frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study, therefore, represents a major step toward the realization of speech separation systems for real-world speech processing technologies."
          ],
          [
            "Creators",
            "Yi Luo, Nima Mesgarani"
          ],
          [
            "DOI",
            "10.1109/TASLP.2019.2915167"
          ],
          [
            "Date",
            "2019-08-00 2019-08"
          ],
          [
            "Extra",
            "Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "ISSN",
            "2329-9304"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1256-1266"
          ],
          [
            "Publication Title",
            "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
          ],
          [
            "Short Title",
            "Conv-TasNet"
          ],
          [
            "Title",
            "Conv-TasNet: Surpassing Ideal Time\u2013Frequency Magnitude Masking for Speech Separation"
          ],
          [
            "Volume",
            "27"
          ]
        ],
        "resource": "storage/i2468.pdf",
        "selectable": false
      },
      {
        "text": "Dual-Path RNN",
        "item-id": "i2363",
        "nodes": [
          {
            "text": "Luo et al_2020_Dual-Path RNN.pdf",
            "item-id": "i2473",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Luo et al_2020_Dual-Path RNN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Luo et al_2020_Dual-Path RNN.pdf"
              ]
            ],
            "resource": "storage/i2473.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Dual-Path RNN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional time-frequency-based methods. Unlike the time-frequency domain approaches, the time-domain separation systems often receive input sequences consisting of a huge number of time steps, which introduces challenges for modeling extremely long sequences. Conventional recurrent neural networks (RNNs) are not effective for modeling such long sequences due to optimization difficulties, while one-dimensional convolutional neural networks (1-D CNNs) cannot perform utterance-level sequence modeling when its receptive field is smaller than the sequence length. In this paper, we propose dual-path recurrent neural network (DPRNN), a simple yet effective method for organizing RNN layers in a deep structure to model extremely long sequences. DPRNN splits the long sequential input into smaller chunks and applies intra- and inter-chunk operations iteratively, where the input length can be made proportional to the square root of the original sequence length in each operation. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system."
          ],
          [
            "Conference Name",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Yi Luo, Zhuo Chen, Takuya Yoshioka"
          ],
          [
            "DOI",
            "10.1109/ICASSP40776.2020.9054266"
          ],
          [
            "Date",
            "2020-05-00 2020-05"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "46-50"
          ],
          [
            "Proceedings Title",
            "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "Dual-Path RNN"
          ],
          [
            "Title",
            "Dual-Path RNN: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation"
          ]
        ],
        "resource": "storage/i2473.pdf",
        "selectable": false
      },
      {
        "text": "Dual-Path Transformer Network",
        "item-id": "i2360",
        "nodes": [
          {
            "text": "Chen et al_2020_Dual-Path Transformer Network.pdf",
            "item-id": "i2466",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Dual-Path Transformer Network.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5NTNWC6D/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/2\">2  Speech separation with dual-path transformer network</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/2\">2.1  Encoder</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/2\">2.2  Separation layer: dual-path transformer network</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/2\">2.2.1  Segmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/2\">2.2.2  Dual-path transformer processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/3\">2.2.3  Overlap-Add</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/3\">2.3  Decoder</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/3\">3  Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/3\">3.1  Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/4\">3.2  Experiment setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/4\">3.3  Training objective</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/4\">4  Performance evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/4\">5  Conclusion and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5NTNWC6D/4\">6  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Dual-Path Transformer Network.pdf"
              ]
            ],
            "resource": "storage/i2466.pdf"
          },
          {
            "text": "ResearchGate Link",
            "item-id": "i2467",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "ResearchGate Link",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-05-09 01:18:15"
              ],
              [
                "Title",
                "ResearchGate Link"
              ],
              [
                "URL",
                "https://www.researchgate.net/publication/354141072_Dual-Path_Transformer_Network_Direct_Context-Aware_Modeling_for_End-to-End_Monaural_Speech_Separation"
              ]
            ]
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Dual-Path Transformer Network",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Conference Name",
            "INTERSPEECH 2020"
          ],
          [
            "Creators",
            "Jingjing Chen, Qirong Mao, Dong Liu"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2020-2205"
          ],
          [
            "Date",
            "2020-10-25 2020-10-25"
          ],
          [
            "Library Catalog",
            "ResearchGate"
          ],
          [
            "Pages",
            "2642-2646"
          ],
          [
            "Proceedings Title",
            "INTERSPEECH 2020"
          ],
          [
            "Short Title",
            "Dual-Path Transformer Network"
          ],
          [
            "Title",
            "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation"
          ]
        ],
        "resource": "storage/i2466.pdf",
        "selectable": false
      },
      {
        "text": "MossFormer",
        "item-id": "i2376",
        "nodes": [
          {
            "text": "Comment: 5 pages, 3 figures, accepted by ICASSP 2023",
            "item-id": "n2465",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 5 pages, 3 figures, accepted by ICASSP 2023",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 5 pages, 3 figures, accepted by ICASSP 2023</div>",
            "node_type": "note"
          },
          {
            "text": "Zhao_Ma_2023_MossFormer.pdf",
            "item-id": "i2464",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao_Ma_2023_MossFormer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_4JDRS2ME/1\">1  Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/2\">2  The MossFormer Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/2\">2.1  Encoder and Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/2\">2.2  Masking Net</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/2\">2.3  MossFormer Block</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/2\">2.3.1  Convolution Module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/3\">2.3.2  Attentive Gating Mechanism</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/3\">2.3.3  Joint Local and Global Single-Head Self-Attention</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/3\">3  Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/3\">3.1  Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/3\">3.2  Training Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/3\">3.3  Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/4\">3.4  Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/4\">4  Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4JDRS2ME/5\">5  References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao_Ma_2023_MossFormer.pdf"
              ]
            ],
            "resource": "storage/i2464.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MossFormer",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Transformer based models have provided significant performance improvements in monaural speech separation. However, there is still a performance gap compared to a recent proposed upper bound. The major limitation of the current dual-path Transformer models is the inefficient modelling of long-range elemental interactions and local feature patterns. In this work, we achieve the upper bound by proposing a gated single-head transformer architecture with convolution-augmented joint self-attentions, named \\textit{MossFormer} (\\textit{Mo}naural \\textit{s}peech \\textit{s}eparation Trans\\textit{Former}). To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence. The joint attention enables MossFormer model full-sequence elemental interaction directly. In addition, we employ a powerful attentive gating mechanism with simplified single-head self-attentions. Besides the attentive long-range modelling, we also augment MossFormer with convolutions for the position-wise local pattern modelling. As a consequence, MossFormer significantly outperforms the previous models and achieves the state-of-the-art results on WSJ0-2/3mix and WHAM!/WHAMR! benchmarks. Our model achieves the SI-SDRi upper bound of 21.2 dB on WSJ0-3mix and only 0.3 dB below the upper bound of 23.1 dB on WSJ0-2mix."
          ],
          [
            "Access Date",
            "2023-05-09 04:25:12"
          ],
          [
            "Archiveid",
            "arXiv:2302.11824"
          ],
          [
            "Creators",
            "Shengkui Zhao, Bin Ma"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.11824"
          ],
          [
            "Date",
            "2023-02-23 2023-02-23"
          ],
          [
            "Extra",
            "arXiv:2302.11824 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MossFormer"
          ],
          [
            "Title",
            "MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.11824"
          ]
        ],
        "resource": "storage/i2464.pdf",
        "selectable": false
      },
      {
        "text": "Separate And Diffuse",
        "item-id": "i2359",
        "nodes": [
          {
            "text": "Lutati et al_2023_Separate And Diffuse.pdf",
            "item-id": "i2462",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lutati et al_2023_Separate And Diffuse.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lutati et al_2023_Separate And Diffuse.pdf"
              ]
            ],
            "resource": "storage/i2462.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Separate And Diffuse",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals. Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is derived for deterministic models. Recent advancements in generative models challenge this bound. We show how the upper bound can be generalized to the case of random generative models. Applying a diffusion model Vocoder that was pretrained to model single-speaker voices on the output of a deterministic separation model leads to state-of-the-art separation results. It is shown that this requires one to combine the output of the separation model with that of the diffusion model. In our method, a linear combination is performed, in the frequency domain, using weights that are inferred by a learned model. We show state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple benchmarks. In particular, for two speakers, our method is able to surpass what was previously considered the upper performance bound."
          ],
          [
            "Access Date",
            "2023-05-09 04:25:29"
          ],
          [
            "Archiveid",
            "arXiv:2301.10752"
          ],
          [
            "Creators",
            "Shahar Lutati, Eliya Nachmani, Lior Wolf"
          ],
          [
            "DOI",
            "10.48550/arXiv.2301.10752"
          ],
          [
            "Date",
            "2023-01-25 2023-01-25"
          ],
          [
            "Extra",
            "arXiv:2301.10752 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Separate And Diffuse"
          ],
          [
            "Title",
            "Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2301.10752"
          ]
        ],
        "resource": "storage/i2462.pdf",
        "selectable": false
      },
      {
        "text": "Sudo RM -RF",
        "item-id": "i2395",
        "nodes": [
          {
            "text": "Tzinis et al_2020_Sudo RM -RF.pdf",
            "item-id": "i2446",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tzinis et al_2020_Sudo RM -RF.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tzinis et al_2020_Sudo RM -RF.pdf"
              ]
            ],
            "resource": "storage/i2446.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Sudo RM -RF",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwn-sampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that SuDoRM - RF performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements."
          ],
          [
            "Conference Name",
            "2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)"
          ],
          [
            "Creators",
            "Efthymios Tzinis, Zhepei Wang, Paris Smaragdis"
          ],
          [
            "DOI",
            "10.1109/MLSP49062.2020.9231900"
          ],
          [
            "Date",
            "2020-09-00 2020-09"
          ],
          [
            "Extra",
            "ISSN: 1551-2541"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-6"
          ],
          [
            "Proceedings Title",
            "2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)"
          ],
          [
            "Short Title",
            "Sudo RM -RF"
          ],
          [
            "Title",
            "Sudo RM -RF: Efficient Networks for Universal Audio Source Separation"
          ]
        ],
        "resource": "storage/i2446.pdf",
        "selectable": false
      },
      {
        "text": "TaSNet",
        "item-id": "i2364",
        "nodes": [
          {
            "text": "IEEE Xplore Full Text PDF",
            "item-id": "i2475",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "IEEE Xplore Full Text PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-05-09 01:13:39"
              ],
              [
                "Title",
                "IEEE Xplore Full Text PDF"
              ],
              [
                "URL",
                "https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=8462116&ref="
              ]
            ],
            "resource": "storage/i2475.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TaSNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Robust speech processing in multi-talker environments requires effective speech separation. Recent deep learning systems have made significant progress toward solving this problem, yet it remains challenging particularly in real-time, short latency applications. Most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation. In addition, time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution. We propose Time-domain Audio Separation Network (TasNet) to overcome these limitations. We directly model the signal in the time-domain using an encoder-decoder framework and perform the source separation on nonnegative encoder outputs. This method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder. Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output. This makes TasNet suitable for applications where low-power, real-time implementation is desirable such as in hearable and telecommunication devices."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Yi Luo, Nima Mesgarani"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8462116"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "696-700"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Short Title",
            "TaSNet"
          ],
          [
            "Title",
            "TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation"
          ]
        ],
        "resource": "storage/i2475.pdf",
        "selectable": false
      },
      {
        "text": "Visual Scene Graphs for Audio Source Separation",
        "item-id": "i2385",
        "nodes": [
          {
            "text": "Chatterjee et al_2021_Visual Scene Graphs for Audio Source Separation.pdf",
            "item-id": "i2425",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chatterjee et al_2021_Visual Scene Graphs for Audio Source Separation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chatterjee et al_2021_Visual Scene Graphs for Audio Source Separation.pdf"
              ]
            ],
            "resource": "storage/i2425.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Visual Scene Graphs for Audio Source Separation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "State-of-the-art approaches for visually-guided audio source separation typically assume sources that have characteristic sounds, such as musical instruments. These approaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from distinct interactions. To address this challenging problem, we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning model that embeds the visual structure of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artificially mixed sounds. In this paper, we also introduce an \"\"in the wild\" video dataset for sound source separation that contains multiple non-musical sources, which we call Audio Separation in the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed ASIW and the standard MUSIC datasets demonstrate state-of-the-art sound separation performance of our method against recent prior approaches."
          ],
          [
            "Access Date",
            "2023-05-23 07:12:14"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Moitreya Chatterjee, Jonathan Le Roux, Narendra Ahuja, Anoop Cherian"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1204-1213"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Visual Scene Graphs for Audio Source Separation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Chatterjee_Visual_Scene_Graphs_for_Audio_Source_Separation_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i2425.pdf",
        "selectable": false
      }
    ],
    "item_title": "Speech Separation",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Super Resolution",
    "item-id": "c2,i2820",
    "nodes": [
      {
        "text": "A comprehensive review of deep learning-based single image super-resolution",
        "item-id": "i1234",
        "nodes": [
          {
            "text": "Bashir et al_2021_A comprehensive review of deep learning-based single image super-resolution.pdf",
            "item-id": "i1235",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bashir et al_2021_A comprehensive review of deep learning-based single image super-resolution.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KGXUKPXC/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/4\">Super-resolution: definitions and terminologies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/12\">Survey methodology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/13\">Conventional methods of super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/15\">Supervised super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/32\">Unsupervised super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/35\">Domain-specific applications of super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/38\">Discussion and future directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/40\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/40\">flink10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/41\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bashir et al_2021_A comprehensive review of deep learning-based single image super-resolution.pdf"
              ]
            ],
            "resource": "storage/i1235.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A comprehensive review of deep learning-based single image super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Image super-resolution (SR) is one of the vital image processing methods that improve the resolution of an image in the field of computer vision. In the last two decades, significant progress has been made in the field of super-resolution, especially by utilizing deep learning methods. This survey is an effort to provide a detailed survey of recent progress in single-image super-resolution in the perspective of deep learning while also informing about the initial classical methods used for image super-resolution. The survey classifies the image SR methods into four categories, i.e., classical methods, supervised learning-based methods, unsupervised learning-based methods, and domain-specific SR methods. We also introduce the problem of SR to provide intuition about image quality metrics, available reference datasets, and SR challenges. Deep learning-based approaches of SR are evaluated using a reference dataset. Some of the reviewed state-of-the-art image SR methods include the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN), multiscale residual network (MSRN), meta residual dense network (Meta-RDN), recurrent back-projection network (RBPN), second-order attention network (SAN), SR feedback network (SRFBN) and the wavelet-based residual attention network (WRAN). Finally, this survey is concluded with future directions and trends in SR and open problems in SR to be addressed by the researchers."
          ],
          [
            "Access Date",
            "2021-12-06 13:22:55"
          ],
          [
            "Creators",
            "Syed Muhammad Arsalan Bashir, Yi Wang, Mahrukh Khan, Yilong Niu"
          ],
          [
            "DOI",
            "10.7717/peerj-cs.621"
          ],
          [
            "Date",
            "2021-07-13 2021-07-13"
          ],
          [
            "Extra",
            "Publisher: PeerJ Inc."
          ],
          [
            "ISSN",
            "2376-5992"
          ],
          [
            "Journal Abbreviation",
            "PeerJ Comput. Sci."
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "peerj.com"
          ],
          [
            "Pages",
            "e621"
          ],
          [
            "Publication Title",
            "PeerJ Computer Science"
          ],
          [
            "Title",
            "A comprehensive review of deep learning-based single image super-resolution"
          ],
          [
            "URL",
            "https://peerj.com/articles/cs-621"
          ],
          [
            "Volume",
            "7"
          ]
        ],
        "resource": "storage/i1235.pdf",
        "selectable": false
      },
      {
        "text": "BasicVSR",
        "item-id": "i1121",
        "nodes": [
          {
            "text": "Chan et al_2021_BasicVSR.pdf",
            "item-id": "i1148",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chan et al_2021_BasicVSR.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chan et al_2021_BasicVSR.pdf"
              ]
            ],
            "resource": "storage/i1148.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BasicVSR",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches."
          ],
          [
            "Access Date",
            "2021-10-20 12:05:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Kelvin C. K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4947-4956"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "BasicVSR"
          ],
          [
            "Title",
            "BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Chan_BasicVSR_The_Search_for_Essential_Components_in_Video_Super-Resolution_and_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1148.pdf",
        "selectable": false
      },
      {
        "text": "Blind Image Super-Resolution",
        "item-id": "i1231",
        "nodes": [
          {
            "text": "Liu et al_2021_Blind Image Super-Resolution.pdf",
            "item-id": "i1233",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2021_Blind Image Super-Resolution.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EDJE2HQM/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/2\">2 Problem Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/3\">3 Challenges from Real-World Images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/4\">4 Taxonomy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/5\">5 Overview of Non-Blind Single-Image Super-Resolution</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/5\">6 Explicit Degradation Modelling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/5\">6.1 Classical Degradation Model with External Dataset</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/6\">6.1.1 Image-Specific Adaptation without Kernel Estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/8\">6.1.2 Image-Specific Adaptation with Kernel Estimation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/9\">6.2 Single Image Modelling with Internal Statistics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/11\">7 Implicit Degradation Modelling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/11\">7.1 Learning Data Distribution within External Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/12\">7.2 Implicit Modelling with a Single Image: a Future Direction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8 Datasets and Competitions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.1 Datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.1.1 Synthetic Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.1.2 Real-World Dataset</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.2 Competitions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">9 Quantitative Comparison</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/14\">9.1 Problem: Difficulty of Fair Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/14\">9.2 Comparison and Analysis Based on Pre-Trained Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/17\">9.3 Suggestions on Fair Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/17\">10 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/17\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Anran Liu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Yihao Liu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Jinjin Gu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Yu Qiao</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/20\">Chao Dong</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2021_Blind Image Super-Resolution.pdf"
              ]
            ],
            "resource": "storage/i1233.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Blind Image Super-Resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Blind image super-resolution (SR), aiming to super-resolve low-resolution images with unknown degradation, has attracted increasing attention due to its significance in promoting real-world applications. Many novel and effective solutions have been proposed recently, especially with the powerful deep learning techniques. Despite years of efforts, it still remains as a challenging research problem. This paper serves as a systematic review on recent progress in blind image SR, and proposes a taxonomy to categorize existing methods into three different classes according to their ways of degradation modelling and the data used for solving the SR model. This taxonomy helps summarize and distinguish among existing methods. We hope to provide insights into current research states, as well as to reveal novel research directions worth exploring. In addition, we make a summary on commonly used datasets and previous competitions related to blind image SR. Last but not least, a comparison among different methods is provided with detailed analysis on their merits and demerits using both synthetic and real testing images."
          ],
          [
            "Access Date",
            "2021-12-06 13:05:13"
          ],
          [
            "Creators",
            "Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, Chao Dong"
          ],
          [
            "Date",
            "2021-07-07 2021-07-07"
          ],
          [
            "Extra",
            "arXiv: 2107.03055"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2107.03055 [cs]"
          ],
          [
            "Short Title",
            "Blind Image Super-Resolution"
          ],
          [
            "Title",
            "Blind Image Super-Resolution: A Survey and Beyond"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2107.03055"
          ]
        ],
        "resource": "storage/i1233.pdf",
        "selectable": false
      },
      {
        "text": "Constructing multilayer locality-constrained matrix regression framework for noise robust face super-resolution",
        "item-id": "i1229",
        "nodes": [
          {
            "text": "Gao et al_2021_Constructing multilayer locality-constrained matrix regression framework for.pdf",
            "item-id": "i1230",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gao et al_2021_Constructing multilayer locality-constrained matrix regression framework for.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MM63LEWG/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MM63LEWG/2\">2 Related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/2\">2.1 Least square estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/2\">2.2 Sparse representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/2\">2.3 Locality-constrained representation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MM63LEWG/3\">3 Multilayer locality-constrained matrix regression framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/3\">3.1 Main motivation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/3\">3.2 Locality-constrained matrix regression</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/4\">3.3 Optimization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/5\">3.4 Face super-resolution via LCMR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/5\">3.5 Constructing MLCMR for efficient face super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/6\">3.6 Complexity and convergence analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MM63LEWG/6\">4 Experimental results and discussions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/6\">4.1 Dataset description</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/6\">4.2 Ablation study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/6\">4.3 Result comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/9\">4.4 Compared results on real-world images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/10\">4.5 Parameter analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/11\">4.6 Computational time</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/11\">4.7 Recognition tests</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/13\">5 Conclusions and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/14\">Declaration of Competing Interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/14\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MM63LEWG/14\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gao et al_2021_Constructing multilayer locality-constrained matrix regression framework for.pdf"
              ]
            ],
            "resource": "storage/i1230.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Constructing multilayer locality-constrained matrix regression framework for noise robust face super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Representation learning methods have attracted considerable attention for learning-based face super-resolution in recent years. Conventional methods perform local models learning on low-resolution (LR) manifold and face reconstruction on high-resolution (HR) manifold respectively, leading to unsatisfactory reconstruction performance when the acquired LR face images are severely degraded (e.g., noisy, blurred). To tackle this issue, this paper proposes an efficient multilayer locality-constrained matrix regression (MLCMR) framework to learn the representation of the input LR patch and meanwhile preserve the manifold of the original HR space. Particularly, MLCMR uses nuclear norm regularization to capture the structural characteristic of the representation residual and applies an adaptive neighborhood selection scheme to find the HR patches that are compatible with its neighbors. Also, MLCMR iteratively applies the manifold structure of the desired HR space to induce the representation weights learning in the LR space, aims at reducing the inconsistency gap between different manifolds. Experimental results on widely used FEI database and real-world faces have demonstrated that compared with several state-of-the-art face super-resolution approaches, our proposed approach has the capability of obtaining better results both in objective metrics and visual quality."
          ],
          [
            "Access Date",
            "2021-12-06 12:52:12"
          ],
          [
            "Creators",
            "Guangwei Gao, Yi Yu, Jin Xie, Jian Yang, Meng Yang, Jian Zhang"
          ],
          [
            "DOI",
            "10.1016/j.patcog.2020.107539"
          ],
          [
            "Date",
            "2021-02-01 2021-02-01"
          ],
          [
            "ISSN",
            "0031-3203"
          ],
          [
            "Journal Abbreviation",
            "Pattern Recognition"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "107539"
          ],
          [
            "Publication Title",
            "Pattern Recognition"
          ],
          [
            "Title",
            "Constructing multilayer locality-constrained matrix regression framework for noise robust face super-resolution"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0031320320303423"
          ],
          [
            "Volume",
            "110"
          ]
        ],
        "resource": "storage/i1230.pdf",
        "selectable": false
      },
      {
        "text": "Deep Learning for Image Super-resolution",
        "item-id": "i16",
        "nodes": [
          {
            "text": "Related blog",
            "item-id": "n117",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Related blog",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Related blog</p>\n<p>https://zhuanlan.zhihu.com/p/143380729</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2020_Deep Learning for Image Super-resolution.pdf",
            "item-id": "i136",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2020_Deep Learning for Image Super-resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2020_Deep Learning for Image Super-resolution.pdf"
              ]
            ],
            "resource": "storage/i136.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Learning for Image Super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future."
          ],
          [
            "Creators",
            "Zhihao Wang, Jian Chen, Steven C.H. Hoi"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2020.2982166"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "Deep Learning for Image Super-resolution"
          ],
          [
            "Title",
            "Deep Learning for Image Super-resolution: A Survey"
          ]
        ],
        "resource": "storage/i136.pdf",
        "selectable": false
      },
      {
        "text": "Deep Learning-based Face Super-resolution",
        "item-id": "i2820",
        "nodes": [
          {
            "text": "Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf",
            "item-id": "i2887",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf"
              ]
            ],
            "resource": "storage/i2887.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Learning-based Face Super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face super-resolution (FSR), also known as face hallucination, which is aimed at enhancing the resolution of low-resolution (LR) face images to generate high-resolution face images, is a domain-specific image super-resolution problem. Recently, FSR has received considerable attention and witnessed dazzling advances with the development of deep learning techniques. To date, few summaries of the studies on the deep learning-based FSR are available. In this survey, we present a comprehensive review of deep learning-based FSR methods in a systematic manner. First, we summarize the problem formulation of FSR and introduce popular assessment metrics and loss functions. Second, we elaborate on the facial characteristics and popular datasets used in FSR. Third, we roughly categorize existing methods according to the utilization of facial characteristics. In each category, we start with a general description of design principles, present an overview of representative approaches, and then discuss the pros and cons among them. Fourth, we evaluate the performance of some state-of-the-art methods. Fifth, joint FSR and other tasks, and FSR-related applications are roughly introduced. Finally, we envision the prospects of further technological advancement in this field."
          ],
          [
            "Access Date",
            "2023-07-12 15:07:01"
          ],
          [
            "Creators",
            "Junjun Jiang, Chenyang Wang, Xianming Liu, Jiayi Ma"
          ],
          [
            "DOI",
            "10.1145/3485132"
          ],
          [
            "Date",
            "2021-00-23 \u5341\u4e00\u6708 23, 2021"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "13:1\u201313:36"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Deep Learning-based Face Super-resolution"
          ],
          [
            "Title",
            "Deep Learning-based Face Super-resolution: A Survey"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3485132"
          ],
          [
            "Volume",
            "55"
          ]
        ],
        "resource": "storage/i2887.pdf",
        "selectable": false
      },
      {
        "text": "Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild",
        "item-id": "i2389",
        "nodes": [
          {
            "text": "Sahak et al_2023_Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in.pdf",
            "item-id": "i2436",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sahak et al_2023_Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sahak et al_2023_Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in.pdf"
              ]
            ],
            "resource": "storage/i2436.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Diffusion models have shown promising results on single-image super-resolution and other image- to-image translation tasks. Despite this success, they have not outperformed state-of-the-art GAN models on the more challenging blind super-resolution task, where the input images are out of distribution, with unknown degradations. This paper introduces SR3+, a diffusion-based model for blind super-resolution, establishing a new state-of-the-art. To this end, we advocate self-supervised training with a combination of composite, parameterized degradations for self-supervised training, and noise-conditioing augmentation during training and testing. With these innovations, a large-scale convolutional architecture, and large-scale datasets, SR3+ greatly outperforms SR3. It outperforms Real-ESRGAN when trained on the same data, with a DRealSR FID score of 36.82 vs. 37.22, which further improves to FID of 32.37 with larger models, and further still with larger training sets."
          ],
          [
            "Access Date",
            "2023-05-23 06:58:31"
          ],
          [
            "Archiveid",
            "arXiv:2302.07864"
          ],
          [
            "Creators",
            "Hshmat Sahak, Daniel Watson, Chitwan Saharia, David Fleet"
          ],
          [
            "DOI",
            "10.48550/arXiv.2302.07864"
          ],
          [
            "Date",
            "2023-02-15 2023-02-15"
          ],
          [
            "Extra",
            "arXiv:2302.07864 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2302.07864"
          ]
        ],
        "resource": "storage/i2436.pdf",
        "selectable": false
      },
      {
        "text": "ESRGAN",
        "item-id": "i13",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n128",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>ESRGAN</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2018_ESRGAN.pdf",
            "item-id": "i130",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2018_ESRGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2018_ESRGAN.pdf"
              ]
            ],
            "resource": "storage/i130.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ESRGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN \u2013 network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN."
          ],
          [
            "Access Date",
            "2021-05-03 15:08:43"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
          ],
          [
            "Creators",
            "Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "0-0"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
          ],
          [
            "Short Title",
            "ESRGAN"
          ],
          [
            "Title",
            "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html"
          ]
        ],
        "resource": "storage/i130.pdf",
        "selectable": false
      },
      {
        "text": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
        "item-id": "i14",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n131",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Batch Normalization is bad in SR.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf",
            "item-id": "i132",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf"
              ]
            ],
            "resource": "storage/i132.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge."
          ],
          [
            "Access Date",
            "2021-05-03 15:01:34"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Creators",
            "Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "136-144"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Title",
            "Enhanced Deep Residual Networks for Single Image Super-Resolution"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i132.pdf",
        "selectable": false
      },
      {
        "text": "FSRNet",
        "item-id": "i10",
        "nodes": [
          {
            "text": "Chen et al_2018_FSRNet.pdf",
            "item-id": "i114",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2018_FSRNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2018_FSRNet.pdf"
              ]
            ],
            "resource": "storage/i114.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FSRNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to superresolve very low-resolution (LR) face images without wellaligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively."
          ],
          [
            "Access Date",
            "2021-05-04 08:15:10"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2492-2501"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "FSRNet"
          ],
          [
            "Title",
            "FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i114.pdf",
        "selectable": false
      },
      {
        "text": "Image Super-Resolution Using Knowledge Distillation",
        "item-id": "i83",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n268",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Perform super resolution with knowledge distillation.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf",
            "item-id": "i269",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_C3G4QF6R/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/3\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/3\">2.1 Image Super Resolution via Deep Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/4\">2.2 Knowledge Distillation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/5\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/5\">3.1 Structure of Teacher Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/7\">3.2 Structure of Student Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/8\">3.3 Knowledge Distillation and Propagation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.1 Datasets and Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.2 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.3 Important of Different Level of Feature Maps</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/10\">4.4 Comparison of Different Ways for Knowledge Distillation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/12\">4.5 Improvement of Student SR Network</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/14\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C3G4QF6R/14\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf"
              ]
            ],
            "resource": "storage/i269.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Image Super-Resolution Using Knowledge Distillation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The significant improvements in image super-resolution (SR) in recent years is majorly resulted from the use of deeper and deeper convolutional neural networks (CNN). However, both computational time and memory consumption simultaneously increase with the utilization of very deep CNN models, posing challenges to deploy SR models in realtime on computationally limited devices. In this work, we propose a novel strategy that uses a teacher-student network to improve the image SR performance. The training of a small but efficient student network is guided by a deep and powerful teacher network. We have evaluated the performance using different ways of knowledge distillation. Through the validations on four datasets, the proposed method significantly improves the SR performance of a student network without changing its structure. This means that the computational time and the memory consumption do not increase during the testing stage while the SR performance is significantly improved."
          ],
          [
            "Conference Name",
            "Computer Vision - ACCV 2018"
          ],
          [
            "Creators",
            "Qinquan Gao, Yan Zhao, Gen Li, Tong Tong, C. V. Jawahar, Hongdong Li, Greg Mori, Konrad Schindler"
          ],
          [
            "DOI",
            "10.1007/978-3-030-20890-5_34"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "ISBN",
            "978-3-030-20890-5"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "527-541"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Computer Vision - ACCV 2018"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Image Super-Resolution Using Knowledge Distillation"
          ]
        ],
        "resource": "storage/i269.pdf",
        "selectable": false
      },
      {
        "text": "Learning a Deep Convolutional Network for Image Super-Resolution",
        "item-id": "i15",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n134",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>SRCNN</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf",
            "item-id": "i135",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf"
              ]
            ],
            "resource": "storage/i135.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning a Deep Convolutional Network for Image Super-Resolution",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, David Fleet, Tomas Pajdla, Bernt Schiele, Tinne Tuytelaars"
          ],
          [
            "DOI",
            "10.1007/978-3-319-10593-2_13"
          ],
          [
            "Date",
            "2014-00-00 2014"
          ],
          [
            "ISBN",
            "978-3-319-10593-2"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "184-199"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Learning a Deep Convolutional Network for Image Super-Resolution"
          ]
        ],
        "resource": "storage/i135.pdf",
        "selectable": false
      },
      {
        "text": "Learning to Have an Ear for Face Super-Resolution",
        "item-id": "i8",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n109",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Add audio for image super resolution</p>\n<p>The important semantic information lossed whtn the resolution is extremely low. Do SR in this LR images might cause wrong gender and ages.</p>\n<p>The audio information might be helpful to extract gender and age features. Model can learn the correlation between the voice to the personal identity.</p>\n<p>The method they proposed has several steps to train. Step 1: Train a HR encoder to reverse the pretrained StyleGAN (generator) as auto-encoder with L1 and VGG-percepture loss. Step 2: Train both the encoder and StyleGAN to fine tune in the dataset with loss of step 1 and generator-weights regularization. Step 3: Train the LR encoder to regress the same latent representations from HR encoder with L1 loss for latent representations and the pairs of LR and downsampling of SR. Then, use the HR encoder and filpped face to get latent representations of neutral frontal facing poses. Train the A (audio) encoder to predict the neutral front latent representations with L1 loss. Step 4: Train the fusing layer to regress the latents of HR encoder with L1 loss of latents and the pairs of LR and downsampling of SR.</p>\n<p>The dataset is VoxCeleba2. </p>\n<p>The metrics is PSNR, SSIM, the classification error of identity classifier, gender classifier and age classifier. The proposed method is better than others for recovery of identity, gender and age.</p>\n<p>+ve: It can recover the age and gender attributes. Utilize the information from audio, which is easily accessed in video.</p>\n<p>-ve: The model is not robust to some cases when the gender can be easily guessed from the LR image. Naive end-to-end training is not applied.</p>\n<p>This paper introduced the first method for image face super resolution combined the LR image and audio. The information of audio contains some kind of informations which is not easily gussed from LR.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf",
            "item-id": "i121",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf"
              ]
            ],
            "resource": "storage/i121.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Learning to Have an Ear for Face Super-Resolution",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a novel method to use both audio and a low-resolution image to perform extreme face super-resolution (a 16x increase of the input size). When the resolution of the input image is very low (e.g., 8x8 pixels), the loss of information is so dire that important details of the original identity have been lost and audio can aid the recovery of a plausible high-resolution image. In fact, audio carries information about facial attributes, such as gender and age. To combine the aural and visual modalities, we propose a method to first build the latent representations of a face from the lone audio track and then from the lone low-resolution image. We then train a network to fuse these two representations. We show experimentally that audio can assist in recovering attributes such as the gender, the age and the identity, and thus improve the correctness of the high-resolution image reconstruction process. Our procedure does not make use of human annotation and thus can be easily trained with existing video datasets. Moreover, we show that our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations."
          ],
          [
            "Access Date",
            "2021-05-04 07:28:39"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Givi Meishvili, Simon Jenni, Paolo Favaro"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1364-1374"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Learning to Have an Ear for Face Super-Resolution"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i121.pdf",
        "selectable": false
      },
      {
        "text": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
        "item-id": "i92",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n263",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Perceptual loss is the difference (MSE) between features of hidden layers in loss network from y_true and y_pred.</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf",
            "item-id": "i264",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_3FEWUKQT/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/5\">3.1 Image Transformation Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/6\">3.2 Perceptual Loss Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/8\">3.3 Simple Loss Functions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/9\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/9\">4.1 Style Transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/12\">4.2 Single-Image Super-Resolution</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/15\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3FEWUKQT/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf"
              ]
            ],
            "resource": "storage/i264.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Justin Johnson, Alexandre Alahi, Li Fei-Fei, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46475-6_43"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46475-6"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "694-711"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"
          ]
        ],
        "resource": "storage/i264.pdf",
        "selectable": false
      },
      {
        "text": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "item-id": "i12",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n125",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>SRGAN</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf",
            "item-id": "i126",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf"
              ]
            ],
            "resource": "storage/i126.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method."
          ],
          [
            "Access Date",
            "2021-05-03 15:09:33"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4681-4690"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i126.pdf",
        "selectable": false
      },
      {
        "text": "Real-ESRGAN",
        "item-id": "i1119",
        "nodes": [
          {
            "text": "Wang et al_2021_Real-ESRGAN.pdf",
            "item-id": "i1144",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2021_Real-ESRGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2021_Real-ESRGAN.pdf"
              ]
            ],
            "resource": "storage/i1144.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Real-ESRGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly. Code: https://github.com/xinntao/Real-ESRGAN"
          ],
          [
            "Access Date",
            "2021-10-20 12:06:24"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Xintao Wang, Liangbin Xie, Chao Dong, Ying Shan"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1905-1914"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "Real-ESRGAN"
          ],
          [
            "Title",
            "Real-ESRGAN: Training Real-World Blind Super-Resolution With Pure Synthetic Data"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021_paper.html"
          ]
        ],
        "resource": "storage/i1144.pdf",
        "selectable": false
      },
      {
        "text": "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge",
        "item-id": "i1237",
        "nodes": [
          {
            "text": "Ignatov et al_2021_Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI.pdf",
            "item-id": "i1239",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ignatov et al_2021_Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ignatov et al_2021_Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI.pdf"
              ]
            ],
            "resource": "storage/i1239.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper."
          ],
          [
            "Access Date",
            "2021-12-06 13:24:41"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Andrey Ignatov, Andres Romero, Heewon Kim, Radu Timofte"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2535-2544"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge"
          ],
          [
            "Title",
            "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge: Report"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Real-Time_Video_Super-Resolution_on_Smartphones_With_Deep_Learning_Mobile_AI_CVPRW_2021_paper.html"
          ]
        ],
        "resource": "storage/i1239.pdf",
        "selectable": false
      },
      {
        "text": "Video Super Resolution Based on Deep Learning",
        "item-id": "i7",
        "nodes": [
          {
            "text": "Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf",
            "item-id": "i119",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KJPJ4DX4/1\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/2\">II Background</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/3\">III Video Super-resolution Methods</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/3\">IV Methods with Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/3\">IV-A Motion Estimation and Compensation Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/4\">IV-A1 Deep-DE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/4\">IV-A2 VSRnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/5\">IV-A3 VESPCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/5\">IV-A4 DRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/6\">IV-A5 RVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/6\">IV-A6 FRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/6\">IV-A7 STTN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/7\">IV-A8 SOFVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/7\">IV-A9 TecoGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A10 TOFlow</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A11 MMCNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A12 RBPN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A13 MEMC-Net</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A14 RRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A15 RTVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A16 MultiBoot VSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A17 MAFN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A18 STARnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/10\">IV-B Deformable Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/10\">IV-B1 EDVR </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/10\">IV-B2 DNLN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">IV-B3 TDAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">IV-B4 D3Dnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">IV-B5 VESR-Net</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">V Methods without Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-A 2D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-A1 VSRResFeatGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-A2 FFCVSR</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-B 3D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-B1 DUF</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-B2 FSTRN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-B3 3DSRnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C Recurrent Convolutional Neural Networks (RCNNs)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C1 BRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C2 STCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C3 RISTN</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/14\">V-D Non-Local Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/14\">V-D1 PFNL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/14\">V-D2 MuCAN</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/15\">VI Performance Comparisons</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/15\">VI-A Datasets and Competitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VI-B Performance of Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII Trends and Challenges</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII-A Lightweight Super-Resolution Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII-B Interpretability of Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII-C Super-Resolution with Larger Scaling Factors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/17\">VII-D Super-Resolution with Random Scaling Factors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/17\">VII-E More Reasonable &amp;amp; Proper Degradation Process of Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-F Unsupervised Super-Resolution Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-G More Effective Scene Change Algorithms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-H More Reasonable Evaluation Criteria for Video Quality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-I More Effective Methods for Leveraging Information</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VIII Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">IX Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X Methods with Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A Motion Estimation and Compensation Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A1 Deep-DE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A2 VSRnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A3 VESPCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A4 DRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A5 RVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A6 FRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A7 STTN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A8 SOFVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A9 TecoGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A10 TOFlow</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A11 MMCNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A12 RBPN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A13 MEMC-Net</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A14 RRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A15 RTVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A16 MultiBoot VSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A17 MAFN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A18 STARnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B Deformable Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B1 EDVR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B2 DNLN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B3 TDAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B4 D3Dnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B5 VESR-Net</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B6 STVSR</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI Methods without Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-A 2D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-A1 VSRResFeatGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-A2 FFCVSR</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-B 3D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-B1 DUF</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-B2 FSTRN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-B3 3DSRnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C Recurrent Convolutional Neural Networks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C1 BRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C2 STCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C3 RISTN</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-D Non-Local Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-D1 PFNL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-D2 MuCAN</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/29\">XII Performance Comparisons</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/29\">XII-A Performance of Methods</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i119.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Video Super Resolution Based on Deep Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning."
          ],
          [
            "Access Date",
            "2021-05-04 07:47:31"
          ],
          [
            "Creators",
            "Hongying Liu, Zhubo Ruan, Peng Zhao, Chao Dong, Fanhua Shang, Yuanyuan Liu, Linlin Yang"
          ],
          [
            "Date",
            "2020-12-20 2020-12-20"
          ],
          [
            "Extra",
            "arXiv: 2007.12928"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2007.12928 [cs, eess]"
          ],
          [
            "Short Title",
            "Video Super Resolution Based on Deep Learning"
          ],
          [
            "Title",
            "Video Super Resolution Based on Deep Learning: A Comprehensive Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2007.12928"
          ]
        ],
        "resource": "storage/i119.pdf",
        "selectable": false
      }
    ],
    "item_title": "Super Resolution",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Survey",
    "item-id": "c3,i3435",
    "nodes": [
      {
        "text": "A Review of Text Style Transfer using Deep Learning",
        "item-id": "i1256",
        "nodes": [
          {
            "text": "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf",
            "item-id": "i1259",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i1259.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Review of Text Style Transfer using Deep Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves, however, they adjust their speaking and writing style to a social context, an audience, an interlocutor or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence. A systematic review of text style transfer methodologies using deep learning is presented in this paper. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field."
          ],
          [
            "Creators",
            "Martina Toshevska, Sonja Gievska"
          ],
          [
            "DOI",
            "10.1109/TAI.2021.3115992"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Artificial Intelligence"
          ],
          [
            "ISSN",
            "2691-4581"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Artificial Intelligence"
          ],
          [
            "Title",
            "A Review of Text Style Transfer using Deep Learning"
          ]
        ],
        "resource": "storage/i1259.pdf",
        "selectable": false
      },
      {
        "text": "A Review on Speech Synthesis Based on Machine Learning",
        "item-id": "i2557",
        "nodes": [
          {
            "text": "Kumari et al_2022_A Review on Speech Synthesis Based on Machine Learning.pdf",
            "item-id": "i2632",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kumari et al_2022_A Review on Speech Synthesis Based on Machine Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kumari et al_2022_A Review on Speech Synthesis Based on Machine Learning.pdf"
              ]
            ],
            "resource": "storage/i2632.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Review on Speech Synthesis Based on Machine Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, Speech synthesis is one of the growing techniques in the research domain that takes input as text and provides output as acoustical form. The speech synthesis system is more advantageous to physically impaired people. In execution process, there arise some complications by surrounding noises and communication style. To neglect such unnecessary noises various machine learning techniques are employed. In this paper, we described various techniques adopted to improve the naturalness and quality of synthesized speech. The main contribution of this paper is to elaborate and compare the characteristics of techniques utilized in speech synthesis for different languages. The techniques such as support vector machine, Artificial Neural Network, Gaussian mixture modeling, Generative adversarial network, Deep Neural Network and Hidden Markov Model are employed in this work to enhance the speech naturalness and quality of synthesized speech signals."
          ],
          [
            "Creators",
            "Ruchika Kumari, Amita Dev, Ashwni Kumar, Amita Dev, S. S. Agrawal, Arun Sharma"
          ],
          [
            "DOI",
            "10.1007/978-3-030-95711-7_3"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-030-95711-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "23-35"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Artificial Intelligence and Speech Technology"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Communications in Computer and Information Science"
          ],
          [
            "Title",
            "A Review on Speech Synthesis Based on Machine Learning"
          ]
        ],
        "resource": "storage/i2632.pdf",
        "selectable": false
      },
      {
        "text": "A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation",
        "item-id": "i2092",
        "nodes": [
          {
            "text": "Kadam et al_2021_A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation.pdf",
            "item-id": "i2121",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kadam et al_2021_A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kadam et al_2021_A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation.pdf"
              ]
            ],
            "resource": "storage/i2121.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The fields like Media, Education and Corporations etc have started focusing on content creation. This has led to the huge demand for synthetic media generation using less data. To synthesize a high-grade artificial video, the lip must be synchronized with the audio. Here we have compared the various methods for voice-cloning and lip synchronization. Voice cloning procedure include state of the art methods like wavenet and other text-to-speech approaches. Lip synchronization methods describe constrained and unconstrained methods. Various recent research like LipGan, Wav2Lip are discussed. The methods are compared and the best method is suggested. Apart from studying and comparing the various methods, their drawbacks, future scopes, and application are also there. Different social and ethical issues are also discussed."
          ],
          [
            "Access Date",
            "2022-11-11 06:26:50"
          ],
          [
            "Creators",
            "Anup Kadam, Sagar Rane, Arpit Kumar Mishra, Shailesh Kumar Sahu, Shubham Singh, Shivam Kumar Pathak"
          ],
          [
            "DOI",
            "10.4108/eai.14-4-2021.169187"
          ],
          [
            "Date",
            "2021-04-14 2021-04-14"
          ],
          [
            "ISSN",
            "2409-9708"
          ],
          [
            "Issue",
            "28"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "publications.eai.eu"
          ],
          [
            "Pages",
            "e2-e2"
          ],
          [
            "Publication Title",
            "EAI Endorsed Transactions on Creative Technologies"
          ],
          [
            "Title",
            "A Survey of Audio Synthesis and Lip-syncing for Synthetic Video Generation"
          ],
          [
            "URL",
            "https://publications.eai.eu/index.php/ct/article/view/1417"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i2121.pdf",
        "selectable": false
      },
      {
        "text": "A Survey of Deep Facial Attribute Analysis",
        "item-id": "i70",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n294",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Introduction to all facial analysis</p></div></div>",
            "node_type": "note"
          },
          {
            "text": "Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf",
            "item-id": "i295",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf"
              ]
            ],
            "resource": "storage/i295.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey of Deep Facial Attribute Analysis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions."
          ],
          [
            "Access Date",
            "2021-03-16 14:46:55"
          ],
          [
            "Creators",
            "Xin Zheng, Yanqing Guo, Huaibo Huang, Yi Li, Ran He"
          ],
          [
            "DOI",
            "10.1007/s11263-020-01308-z"
          ],
          [
            "Date",
            "2020-09-01 2020-09-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "2002-2034"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Title",
            "A Survey of Deep Facial Attribute Analysis"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-020-01308-z"
          ],
          [
            "Volume",
            "128"
          ]
        ],
        "resource": "storage/i295.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Anomaly Detection for Technical Systems using LSTM Networks",
        "item-id": "i699",
        "nodes": [
          {
            "text": "Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry",
            "item-id": "n700",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 14 pages, 6 figures, 4 tables. Accepted for publication by Computers in Industry</div>",
            "node_type": "note"
          },
          {
            "text": "Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf",
            "item-id": "i702",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf"
              ]
            ],
            "resource": "storage/i702.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Anomaly Detection for Technical Systems using LSTM Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Anomalies represent deviations from the intended system operation and can lead to decreased efficiency as well as partial or complete system failure. As the causes of anomalies are often unknown due to complex system dynamics, efficient anomaly detection is necessary. Conventional detection approaches rely on statistical and time-invariant methods that fail to address the complex and dynamic nature of anomalies. With advances in artificial intelligence and increasing importance for anomaly detection and prevention in various domains, artificial neural network approaches enable the detection of more complex anomaly types while considering temporal and contextual characteristics. In this article, a survey on state-of-the-art anomaly detection using deep neural and especially long short-term memory networks is conducted. The investigated approaches are evaluated based on the application scenario, data and anomaly types as well as further metrics. To highlight the potential of upcoming anomaly detection techniques, graph-based and transfer learning approaches are also included in the survey, enabling the analysis of heterogeneous data as well as compensating for its shortage and improving the handling of dynamic processes."
          ],
          [
            "Access Date",
            "2021-07-12 05:29:42"
          ],
          [
            "Creators",
            "Benjamin Lindemann, Benjamin Maschler, Nada Sahlab, Michael Weyrich"
          ],
          [
            "Date",
            "2021-05-28 2021-05-28"
          ],
          [
            "Extra",
            "arXiv: 2105.13810"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2105.13810 [cs, stat]"
          ],
          [
            "Title",
            "A Survey on Anomaly Detection for Technical Systems using LSTM Networks"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2105.13810"
          ]
        ],
        "resource": "storage/i702.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
        "item-id": "i31",
        "nodes": [
          {
            "text": "Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf",
            "item-id": "i153",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_DWEV7245/1\">3.1 Introduction to Emotion Recognition</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DWEV7245/2\">3.2 Emotion Representation Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/3\">3.2.1 Categorical Emotion Representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/3\">3.2.2 Facial Action Coding System</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/4\">3.2.3 Dimensional (Continous) Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/4\">3.2.4 Micro-Expressions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/4\">3.3 Emotion Recognition Based Databases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/5\">3.4 Challenges</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_DWEV7245/8\">3.5 Visual Emotion Recognition Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/8\">3.5.1 Data Pre-processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/9\">3.5.2 Feature Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/12\">3.5.3 Pooling Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/13\">3.5.4 Deep Learning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/15\">3.6 Speech Based Emotion Recognition Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/17\">3.7 Text Based Emotion Recognition Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/18\">3.8 Physiological Signals Based Emotion Recognition Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/20\">3.9 Fusion Methods Across Modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/21\">3.10 Applications of Automatic Emotion Recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/22\">3.11 Privacy in Affective Computing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/22\">3.12 Ethics and Fairness in Automatic Emotion Recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/23\">3.13 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_DWEV7245/23\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf"
              ]
            ],
            "resource": "storage/i153.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-book",
        "item_title": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
        "item_type": "bookSection",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Affective computing has been an active area of research for the past two decades. One of the major component of affective computing is automatic emotion recognition. This chapter gives a detailed overview of different emotion recognition techniques and the predominantly used signal modalities. The discussion starts with the different emotion representations and their limitations. Given that affective computing is a data-driven research area, a thorough comparison of standard emotion labelled databases is presented. Based on the source of the data, feature extraction and analysis techniques are presented for emotion recognition. Further, applications of automatic emotion recognition are discussed along with current and important issues such as privacy and fairness."
          ],
          [
            "Access Date",
            "2021-04-23 07:21:27"
          ],
          [
            "Book Title",
            "Advances in Data Science: Methodologies and Applications"
          ],
          [
            "Creators",
            "Garima Sharma, Abhinav Dhall, Gloria Phillips-Wren, Anna Esposito, Lakhmi C. Jain"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "DOI: 10.1007/978-3-030-51870-7_3"
          ],
          [
            "ISBN",
            "978-3-030-51870-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "35-64"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Intelligent Systems Reference Library"
          ],
          [
            "Title",
            "A Survey on Automatic Multimodal Emotion Recognition in the Wild"
          ],
          [
            "URL",
            "https://doi.org/10.1007/978-3-030-51870-7_3"
          ]
        ],
        "resource": "storage/i153.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Deep Semi-supervised Learning",
        "item-id": "i807",
        "nodes": [
          {
            "text": "Comment: 24 pages, 6 figures",
            "item-id": "n875",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 24 pages, 6 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 24 pages, 6 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf",
            "item-id": "i874",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SQATXA2A/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/2\">2 Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/2\">2.1 Assumptions for semi-supervised learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/3\">2.2 Traditional Semi-supervised learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">2.3 Related Concepts</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">3 Generative methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">3.1 Datasets and applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/4\">3.2 Semi-supervised GANs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/8\">3.3 Semi-supervised VAE</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/10\">4 Consistency Regularization</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/12\">5 Graph-based methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/13\">5.1 AutoEncoder-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/13\">5.2 GNN-based methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SQATXA2A/15\">6 Pseudo-labeling methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/15\">6.1 Disagreement-based models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/16\">6.2 Self-training models </a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/17\">7 Hybrid methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/18\">8 Challenges and future directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/19\">9 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SQATXA2A/19\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf"
              ]
            ],
            "resource": "storage/i874.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Deep Semi-supervised Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from model design perspectives and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we offer a detailed comparison of these methods in terms of the type of losses, contributions, and architecture differences. In addition to the past few years' progress, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems."
          ],
          [
            "Access Date",
            "2021-08-03 17:02:21"
          ],
          [
            "Creators",
            "Xiangli Yang, Zixing Song, Irwin King, Zenglin Xu"
          ],
          [
            "Date",
            "2021-02-28 2021-02-28"
          ],
          [
            "Extra",
            "arXiv: 2103.00550"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2103.00550 [cs]"
          ],
          [
            "Title",
            "A Survey on Deep Semi-supervised Learning"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2103.00550"
          ]
        ],
        "resource": "storage/i874.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Multimodal Large Language Models",
        "item-id": "i2791",
        "nodes": [
          {
            "text": "Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
            "item-id": "n2837",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</div>",
            "node_type": "note"
          },
          {
            "text": "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf",
            "item-id": "i2836",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BKHSHM5R/1\">. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Overview</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">. Multimodal Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/2\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/3\">Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/4\">Modality Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/4\">Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/5\">Modality Bridging</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/5\">Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/6\">. Multimodal In-Context Learning</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">. Multimodal Chain of Thought</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">Modality bridging</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/7\">Learning Paradigms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/8\">Chain Configuration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/8\">Generation Patterns</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">. LLM-Aided Visual Reasoning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/9\">Training Paradigms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">Evaluation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/10\">. Challenges and Future Directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BKHSHM5R/11\">. Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yin et al_2023_A Survey on Multimodal Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i2836.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "A Survey on Multimodal Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models."
          ],
          [
            "Access Date",
            "2023-07-18 06:46:19"
          ],
          [
            "Archiveid",
            "arXiv:2306.13549"
          ],
          [
            "Creators",
            "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen"
          ],
          [
            "DOI",
            "10.48550/arXiv.2306.13549"
          ],
          [
            "Date",
            "2023-06-23 2023-06-23"
          ],
          [
            "Extra",
            "arXiv:2306.13549 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "A Survey on Multimodal Large Language Models"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2306.13549"
          ]
        ],
        "resource": "storage/i2836.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Neural Speech Synthesis",
        "item-id": "i890",
        "nodes": [
          {
            "text": "Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references",
            "item-id": "n891",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references</div>",
            "node_type": "note"
          },
          {
            "text": "Tan et al_2021_A Survey on Neural Speech Synthesis.pdf",
            "item-id": "i893",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tan et al_2021_A Survey on Neural Speech Synthesis.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XPKM6J8W/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/2\">1.1 History of TTS Technology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/3\">1.2 Organization of This Survey</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/4\">2 Key Components in TTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/6\">2.1 Main Taxonomy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/6\">2.2 Text Analysis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/8\">2.3 Acoustic Models</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/8\">2.3.1 Acoustic Models in SPSS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/10\">2.3.2 Acoustic Models in End-to-End TTS</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/12\">2.4 Vocoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/16\">2.5 Towards Fully End-to-End TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/17\">2.6 Other Taxonomies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/18\">3 Advanced Topics in TTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/18\">3.1 Background and Taxonomy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/19\">3.2 Fast TTS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/22\">3.3 Low-Resource TTS</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/23\">3.4 Robust TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/24\">3.4.1 Enhancing Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/25\">3.4.2 Replacing Attention with Duration Prediction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/26\">3.4.3 Enhancing AR Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/27\">3.4.4 Replacing AR Generation with NAR Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/27\">3.5 Expressive TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/27\">3.5.1 Categorization of Variation Information</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/28\">3.5.2 Modeling Variation Information</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/29\">3.5.3 Disentangling, Controlling and Transferring</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/30\">3.6 Adaptive TTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/31\">3.6.1 General Adaptation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/31\">3.6.2 Efficient Adaptation</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/32\">4 Resources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XPKM6J8W/33\">5 Future Directions</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tan et al_2021_A Survey on Neural Speech Synthesis.pdf"
              ]
            ],
            "resource": "storage/i893.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Neural Speech Synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS."
          ],
          [
            "Access Date",
            "2021-08-11 08:05:27"
          ],
          [
            "Creators",
            "Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu"
          ],
          [
            "Date",
            "2021-07-23 2021-07-23"
          ],
          [
            "Extra",
            "arXiv: 2106.15561"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2106.15561 [cs, eess]"
          ],
          [
            "Title",
            "A Survey on Neural Speech Synthesis"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2106.15561"
          ]
        ],
        "resource": "storage/i893.pdf",
        "selectable": false
      },
      {
        "text": "A Survey on Temporal Action Localization",
        "item-id": "i1012",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1172",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>SS-TAD</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf",
            "item-id": "i1015",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_R3R6LIQ6/1\">INTRODUCTION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/2\">RELATED TECHNIQUES</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/2\">TRADITIONAL METHODS</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/3\">DEEP LEARNING METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/3\">TWO-STAGE LOCALIZATION METHODS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/4\">ONE-STAGE LOCALIZATION METHODS</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">BENCHMARK DATASETS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">THUMOS'14 6</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">ActivityNet 7</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">MEXaction2 47</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">MUTITHUMOS 48</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">CHARADES 8</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">AVA 9</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">EVALUATION METRICS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">BASIC CONCEPTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">ACCURACY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">RECALL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">PRECISION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">INTERSECTION-OVER-UNION (IoU)</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">EVALUATION METRICS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">AVERAGE RECALL (AR)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">MEAN AVERAGE PRECISION (mAP)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">RECENT METHODS AND DEVELOPMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">FULLY-SUPETVISED TEMPORAL ACTION LOCALIZATION (F-TAL)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">FULLY-SUPETVISED LEARNING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/7\">CURRENT REPRESENTATIVE METHODS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/7\">WEAKLY-SUPETVISED TEMPORAL ACTION LOCALIZATION (W-TAL)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/7\">WEAKLY-SUPETVISED LEARNING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/8\">CURRENT REPRESENTATIVE METHODS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/8\">INSIGHTS ON THE PROBLEM OF W-TAL</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/9\">FUTURE DIRECTIONS AND TRENDS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/9\">CONCLUSION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/9\">REFERENCES</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/11\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/11\">HUIFEN XIA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/11\">YONGZHAO ZHAN</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf"
              ]
            ],
            "resource": "storage/i1015.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Temporal Action Localization",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action localization is one of the most crucial and challenging problems for video understanding in computer vision. It has received a lot of attention in recent years because of the extensive application of daily life. Temporal action localization has made some significant progress, especially with the development of deep learning recently. And more demand is for temporal action localization in untrimmed videos. In this paper, our target is to survey the state-of-the-art techniques and models for video temporal action localization. It mainly includes the related techniques, some benchmark datasets and the evaluation metrics of temporal action localization. In addition, we summarize temporal action localization from two aspects: fully-supervised learning and weakly-supervised learning. And we list several representative works and compare their performances respectively. Finally, we make some deep analysis and propose potential research directions, and conclude the survey."
          ],
          [
            "Creators",
            "Huifen Xia, Yongzhao Zhan"
          ],
          [
            "DOI",
            "10.1109/ACCESS.2020.2986861"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Access"
          ],
          [
            "ISSN",
            "2169-3536"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "70477-70487"
          ],
          [
            "Publication Title",
            "IEEE Access"
          ],
          [
            "Title",
            "A Survey on Temporal Action Localization"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i1015.pdf",
        "selectable": false
      },
      {
        "text": "A comprehensive review of deep learning-based single image super-resolution",
        "item-id": "i1234",
        "nodes": [
          {
            "text": "Bashir et al_2021_A comprehensive review of deep learning-based single image super-resolution.pdf",
            "item-id": "i1235",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bashir et al_2021_A comprehensive review of deep learning-based single image super-resolution.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KGXUKPXC/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/4\">Super-resolution: definitions and terminologies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/12\">Survey methodology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/13\">Conventional methods of super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/15\">Supervised super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/32\">Unsupervised super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/35\">Domain-specific applications of super-resolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/38\">Discussion and future directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/40\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/40\">flink10</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KGXUKPXC/41\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bashir et al_2021_A comprehensive review of deep learning-based single image super-resolution.pdf"
              ]
            ],
            "resource": "storage/i1235.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A comprehensive review of deep learning-based single image super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Image super-resolution (SR) is one of the vital image processing methods that improve the resolution of an image in the field of computer vision. In the last two decades, significant progress has been made in the field of super-resolution, especially by utilizing deep learning methods. This survey is an effort to provide a detailed survey of recent progress in single-image super-resolution in the perspective of deep learning while also informing about the initial classical methods used for image super-resolution. The survey classifies the image SR methods into four categories, i.e., classical methods, supervised learning-based methods, unsupervised learning-based methods, and domain-specific SR methods. We also introduce the problem of SR to provide intuition about image quality metrics, available reference datasets, and SR challenges. Deep learning-based approaches of SR are evaluated using a reference dataset. Some of the reviewed state-of-the-art image SR methods include the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN), multiscale residual network (MSRN), meta residual dense network (Meta-RDN), recurrent back-projection network (RBPN), second-order attention network (SAN), SR feedback network (SRFBN) and the wavelet-based residual attention network (WRAN). Finally, this survey is concluded with future directions and trends in SR and open problems in SR to be addressed by the researchers."
          ],
          [
            "Access Date",
            "2021-12-06 13:22:55"
          ],
          [
            "Creators",
            "Syed Muhammad Arsalan Bashir, Yi Wang, Mahrukh Khan, Yilong Niu"
          ],
          [
            "DOI",
            "10.7717/peerj-cs.621"
          ],
          [
            "Date",
            "2021-07-13 2021-07-13"
          ],
          [
            "Extra",
            "Publisher: PeerJ Inc."
          ],
          [
            "ISSN",
            "2376-5992"
          ],
          [
            "Journal Abbreviation",
            "PeerJ Comput. Sci."
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "peerj.com"
          ],
          [
            "Pages",
            "e621"
          ],
          [
            "Publication Title",
            "PeerJ Computer Science"
          ],
          [
            "Title",
            "A comprehensive review of deep learning-based single image super-resolution"
          ],
          [
            "URL",
            "https://peerj.com/articles/cs-621"
          ],
          [
            "Volume",
            "7"
          ]
        ],
        "resource": "storage/i1235.pdf",
        "selectable": false
      },
      {
        "text": "A detailed analysis of image and video forgery detection techniques",
        "item-id": "i2216",
        "nodes": [
          {
            "text": "Tyagi_Yadav_2022_A detailed analysis of image and video forgery detection techniques.pdf",
            "item-id": "i2255",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tyagi_Yadav_2022_A detailed analysis of image and video forgery detection techniques.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G58JDD4K/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/2\">2 Prerequisites for visual imagery manipulation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/3\">2.1 Image forgery (IF)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/3\">2.2 Image tampering (IT)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/4\">2.3 Image generation (IG)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/4\">2.4 Image warping and morphing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/4\">3 Image and video manipulation datasets</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1 Image tampering datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1.1 The Columbia gray dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1.2 The Columbia color dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/5\">3.1.3 The CASIA datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.4 The MICC datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.5 The DRESDEN image dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.6 The IMD dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/7\">3.1.7 The CoFoMo dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/8\">3.1.8 The IEEE IFS-TC dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/8\">3.1.9 The wild web dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/8\">3.1.10 Retouching forgery datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.1.11 Other dataset resources</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.2 Video tampering datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.2.1 First-generation datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/9\">3.2.2 Second-generation datasets</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/10\">4 Image manipulation and detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/10\">4.1 Image manipulation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/10\">4.1.1 Graphics-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/11\">4.1.2 Learning-based methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/11\">4.2 Image forgery detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/11\">4.2.1 Traditional methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">4.2.2 Deep learning-based approaches</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">5 Video manipulation and detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">5.1 Types of video manipulation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/12\">5.1.1  Missing context</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.1.2 Deceptive editing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.1.3 Malicious transformation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.2 Video forgery detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.2.1 Identity swap</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/15\">5.2.2 Attribute manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/17\">5.2.3 Expression swap</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G58JDD4K/17\">6 Evaluation and findings</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/17\">6.1 Research challenges and future scopes</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/18\">7 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G58JDD4K/18\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tyagi_Yadav_2022_A detailed analysis of image and video forgery detection techniques.pdf"
              ]
            ],
            "resource": "storage/i2255.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A detailed analysis of image and video forgery detection techniques",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site\u2019s or can be as malicious as to defame or hurt one\u2019s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one\u2019s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery."
          ],
          [
            "Access Date",
            "2023-02-16 23:08:34"
          ],
          [
            "Creators",
            "Shobhit Tyagi, Divakar Yadav"
          ],
          [
            "DOI",
            "10.1007/s00371-021-02347-4"
          ],
          [
            "Date",
            "2022-01-13 2022-01-13"
          ],
          [
            "ISSN",
            "1432-2315"
          ],
          [
            "Journal Abbreviation",
            "Vis Comput"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Publication Title",
            "The Visual Computer"
          ],
          [
            "Title",
            "A detailed analysis of image and video forgery detection techniques"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s00371-021-02347-4"
          ]
        ],
        "resource": "storage/i2255.pdf",
        "selectable": false
      },
      {
        "text": "A review of deep learning techniques for speech processing",
        "item-id": "i2551",
        "nodes": [
          {
            "text": "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf",
            "item-id": "i2620",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mehrish et al_2023_A review of deep learning techniques for speech processing.pdf"
              ]
            ],
            "resource": "storage/i2620.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A review of deep learning techniques for speech processing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field\u2019s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field."
          ],
          [
            "Access Date",
            "2023-06-07 15:17:12"
          ],
          [
            "Creators",
            "Ambuj Mehrish, Navonil Majumder, Rishabh Bharadwaj, Rada Mihalcea, Soujanya Poria"
          ],
          [
            "DOI",
            "10.1016/j.inffus.2023.101869"
          ],
          [
            "Date",
            "2023-06-03 2023-06-03"
          ],
          [
            "ISSN",
            "1566-2535"
          ],
          [
            "Journal Abbreviation",
            "Information Fusion"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "101869"
          ],
          [
            "Publication Title",
            "Information Fusion"
          ],
          [
            "Title",
            "A review of deep learning techniques for speech processing"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1566253523001859"
          ]
        ],
        "resource": "storage/i2620.pdf",
        "selectable": false
      },
      {
        "text": "A survey of network anomaly detection techniques",
        "item-id": "i704",
        "nodes": [
          {
            "text": "Ahmed et al_2016_A survey of network anomaly detection techniques.pdf",
            "item-id": "i706",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ahmed et al_2016_A survey of network anomaly detection techniques.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JGRGT7VZ/2\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/3\">Roadmap of the paper</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/3\">Preliminary discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/3\">Types of anomalies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Output of anomaly detection techniques</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Types of network attacks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Mapping of network attacks with anomalies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/4\">Classification based network anomaly detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/5\">Support vector machine</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/5\">Bayesian network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Neural network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Rule-based</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Statistical anomaly detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/6\">Mixture model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/7\">Signal processing technique</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/7\">Principal component analysis (PCA)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/7\">Information theory</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/8\">Correlation analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/8\">Clustering-based</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/8\">Regular clustering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/9\">Co-clustering</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/9\">Intrusion detection datasets and issues</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/9\">Limitations of DARPA/KDD datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/10\">Contemporary network attacks evaluation dataset: ADFA-LD12</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/10\">Current network data repositories</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/10\">Evaluation of network anomaly detection techniques</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/11\">Conclusions and future research directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JGRGT7VZ/11\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ahmed et al_2016_A survey of network anomaly detection techniques.pdf"
              ]
            ],
            "resource": "storage/i706.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A survey of network anomaly detection techniques",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Information and Communication Technology (ICT) has a great impact on social wellbeing, economic growth and national security in todays world. Generally, ICT includes computers, mobile communication devices and networks. ICT is also embraced by a group of people with malicious intent, also known as network intruders, cyber criminals, etc. Confronting these detrimental cyber activities is one of the international priorities and important research area. Anomaly detection is an important data analysis task which is useful for identifying the network intrusions. This paper presents an in-depth analysis of four major categories of anomaly detection techniques which include classification, statistical, information theory and clustering. The paper also discusses research challenges with the datasets used for network intrusion detection."
          ],
          [
            "Access Date",
            "2021-07-12 05:30:56"
          ],
          [
            "Creators",
            "Mohiuddin Ahmed, Abdun Naser Mahmood, Jiankun Hu"
          ],
          [
            "DOI",
            "10.1016/j.jnca.2015.11.016"
          ],
          [
            "Date",
            "2016-01-01 January 1, 2016"
          ],
          [
            "ISSN",
            "1084-8045"
          ],
          [
            "Journal Abbreviation",
            "Journal of Network and Computer Applications"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "19-31"
          ],
          [
            "Publication Title",
            "Journal of Network and Computer Applications"
          ],
          [
            "Title",
            "A survey of network anomaly detection techniques"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1084804515002891"
          ],
          [
            "Volume",
            "60"
          ]
        ],
        "resource": "storage/i706.pdf",
        "selectable": false
      },
      {
        "text": "A survey on neural-symbolic learning systems",
        "item-id": "i3254",
        "nodes": [
          {
            "text": "Yu et al_2023_A survey on neural-symbolic learning systems.pdf",
            "item-id": "i3405",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yu et al_2023_A survey on neural-symbolic learning systems.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yu et al_2023_A survey on neural-symbolic learning systems.pdf"
              ]
            ],
            "resource": "storage/i3405.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A survey on neural-symbolic learning systems",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, neural systems have demonstrated highly effective learning ability and superior perception intelligence. However, they have been found to lack effective reasoning and cognitive ability. On the other hand, symbolic systems exhibit exceptional cognitive intelligence but suffer from poor learning capabilities when compared to neural systems. Recognizing the advantages and disadvantages of both methodologies, an ideal solution emerges: combining neural systems and symbolic systems to create neural-symbolic learning systems that possess powerful perception and cognition. The purpose of this paper is to survey the advancements in neural-symbolic learning systems from four distinct perspectives: challenges, methods, applications, and future directions. By doing so, this research aims to propel this emerging field forward, offering researchers a comprehensive and holistic overview. This overview will not only highlight the current state-of-the-art but also identify promising avenues for future research."
          ],
          [
            "Access Date",
            "2023-12-08 21:53:57"
          ],
          [
            "Creators",
            "Dongran Yu, Bo Yang, Dayou Liu, Hui Wang, Shirui Pan"
          ],
          [
            "DOI",
            "10.1016/j.neunet.2023.06.028"
          ],
          [
            "Date",
            "2023-09-01 2023-09-01"
          ],
          [
            "ISSN",
            "0893-6080"
          ],
          [
            "Journal Abbreviation",
            "Neural Networks"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "105-126"
          ],
          [
            "Publication Title",
            "Neural Networks"
          ],
          [
            "Title",
            "A survey on neural-symbolic learning systems"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S0893608023003398"
          ],
          [
            "Volume",
            "166"
          ]
        ],
        "resource": "storage/i3405.pdf",
        "selectable": false
      },
      {
        "text": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
        "item-id": "i2369",
        "nodes": [
          {
            "text": "Triantafyllopoulos et al_2023_An Overview of Affective Speech Synthesis and Conversion in the Deep Learning.pdf",
            "item-id": "i2402",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Triantafyllopoulos et al_2023_An Overview of Affective Speech Synthesis and Conversion in the Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Triantafyllopoulos et al_2023_An Overview of Affective Speech Synthesis and Conversion in the Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i2402.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Speech is the fundamental mode of human communication, and its synthesis has long been a core priority in human\u2013computer interaction research. In recent years, machines have managed to master the art of generating speech that is understandable by humans. However, the linguistic content of an utterance encompasses only a part of its meaning. Affect, or expressivity, has the capacity to turn speech into a medium capable of conveying intimate thoughts, feelings, and emotions\u2014aspects that are essential for engaging and naturalistic interpersonal communication. While the goal of imparting expressivity to synthesized utterances has so far remained elusive, following recent advances in text-to-speech synthesis, a paradigm shift is well under way in the fields of affective speech synthesis and conversion as well. Deep learning, as the technology that underlies most of the recent advances in artificial intelligence, is spearheading these efforts. In this overview, we outline ongoing trends and summarize state-of-the-art approaches in an attempt to provide a broad overview of this exciting field."
          ],
          [
            "Creators",
            "Andreas Triantafyllopoulos, Bj\u00f6rn W. Schuller, G\u00f6k\u00e7e \u0130ymen, Metin Sezgin, Xiangheng He, Zijiang Yang, Panagiotis Tzirakis, Shuo Liu, Silvan Mertes, Elisabeth Andr\u00e9, Ruibo Fu, Jianhua Tao"
          ],
          [
            "DOI",
            "10.1109/JPROC.2023.3250266"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Extra",
            "Conference Name: Proceedings of the IEEE"
          ],
          [
            "ISSN",
            "1558-2256"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-27"
          ],
          [
            "Publication Title",
            "Proceedings of the IEEE"
          ],
          [
            "Title",
            "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era"
          ]
        ],
        "resource": "storage/i2402.pdf",
        "selectable": false
      },
      {
        "text": "Blind Image Super-Resolution",
        "item-id": "i1231",
        "nodes": [
          {
            "text": "Liu et al_2021_Blind Image Super-Resolution.pdf",
            "item-id": "i1233",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2021_Blind Image Super-Resolution.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EDJE2HQM/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/2\">2 Problem Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/3\">3 Challenges from Real-World Images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/4\">4 Taxonomy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/5\">5 Overview of Non-Blind Single-Image Super-Resolution</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/5\">6 Explicit Degradation Modelling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/5\">6.1 Classical Degradation Model with External Dataset</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/6\">6.1.1 Image-Specific Adaptation without Kernel Estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/8\">6.1.2 Image-Specific Adaptation with Kernel Estimation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/9\">6.2 Single Image Modelling with Internal Statistics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/11\">7 Implicit Degradation Modelling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/11\">7.1 Learning Data Distribution within External Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/12\">7.2 Implicit Modelling with a Single Image: a Future Direction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8 Datasets and Competitions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.1 Datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.1.1 Synthetic Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.1.2 Real-World Dataset</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">8.2 Competitions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/13\">9 Quantitative Comparison</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/14\">9.1 Problem: Difficulty of Fair Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/14\">9.2 Comparison and Analysis Based on Pre-Trained Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/17\">9.3 Suggestions on Fair Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/17\">10 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/17\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Anran Liu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Yihao Liu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Jinjin Gu</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/19\">Yu Qiao</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EDJE2HQM/20\">Chao Dong</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2021_Blind Image Super-Resolution.pdf"
              ]
            ],
            "resource": "storage/i1233.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Blind Image Super-Resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Blind image super-resolution (SR), aiming to super-resolve low-resolution images with unknown degradation, has attracted increasing attention due to its significance in promoting real-world applications. Many novel and effective solutions have been proposed recently, especially with the powerful deep learning techniques. Despite years of efforts, it still remains as a challenging research problem. This paper serves as a systematic review on recent progress in blind image SR, and proposes a taxonomy to categorize existing methods into three different classes according to their ways of degradation modelling and the data used for solving the SR model. This taxonomy helps summarize and distinguish among existing methods. We hope to provide insights into current research states, as well as to reveal novel research directions worth exploring. In addition, we make a summary on commonly used datasets and previous competitions related to blind image SR. Last but not least, a comparison among different methods is provided with detailed analysis on their merits and demerits using both synthetic and real testing images."
          ],
          [
            "Access Date",
            "2021-12-06 13:05:13"
          ],
          [
            "Creators",
            "Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, Chao Dong"
          ],
          [
            "Date",
            "2021-07-07 2021-07-07"
          ],
          [
            "Extra",
            "arXiv: 2107.03055"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2107.03055 [cs]"
          ],
          [
            "Short Title",
            "Blind Image Super-Resolution"
          ],
          [
            "Title",
            "Blind Image Super-Resolution: A Survey and Beyond"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2107.03055"
          ]
        ],
        "resource": "storage/i1233.pdf",
        "selectable": false
      },
      {
        "text": "Deep Audio-visual Learning",
        "item-id": "i622",
        "nodes": [
          {
            "text": "Zhu et al_2021_Deep Audio-visual Learning.pdf",
            "item-id": "i624",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu et al_2021_Deep Audio-visual Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WCTESQ9M/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/2\">2 Audio-visual separation and localization</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/3\">2.1 Speaker separation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/3\">2.2 Separating and localizing objects\u2032 sounds</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/3\">2.2.1 Separation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/5\">2.2.2 Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/5\">2.2.3 Simultaneous separation and localization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/6\">2.3 Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/6\">3 Audio-visual correspondence learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/6\">3.1 Audio-visual matching</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/6\">3.1.1 voice-face matching</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/7\">3.1.2 Audio-image retrieval</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/8\">3.2 Audio-visual speech recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/9\">3.3 Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/9\">4 Audio and visual generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/9\">4.1 Vision-to-audio generation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/10\">4.1.1 Lip sequence to speech</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/10\">4.1.2 General video to audio</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/12\">4.2 Audio to vision</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/12\">4.2.1 Audio to image</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/12\">4.2.2 Speech to image generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/12\">4.2.3 Body motion generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/13\">4.2.4 Talking face generation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/13\">4.3 Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/14\">5 Audio-visual representation learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/14\">5.1 Single-Modality representation learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/14\">5.2 Learning an audio-visual representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/15\">5.3 Discussions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6 Recent public audio-visual datasets</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6.1 Audio-visual speech datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6.1.1 Lab-controlled environment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6.1.2 In-the-wild environment</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6.2 Audio-visual event datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6.2.1 Music-related datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/16\">6.2.2 Real events-related datasets</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/18\">7 Discussions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/18\">7.1 Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/18\">7.2 Directions for future research</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/19\">8 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/19\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WCTESQ9M/19\">Open Access</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu et al_2021_Deep Audio-visual Learning.pdf"
              ]
            ],
            "resource": "storage/i624.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Audio-visual Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Audio-visual learning, aimed at exploiting the relationship between audio and visual modalities, has drawn considerable attention since deep learning started to be used successfully. Researchers tend to leverage these two modalities to improve the performance of previously considered single-modality tasks or address new challenging problems. In this paper, we provide a comprehensive survey of recent audio-visual learning development. We divide the current audio-visual learning tasks into four different subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. State-of-the-art methods, as well as the remaining challenges of each subfield, are further discussed. Finally, we summarize the commonly used datasets and challenges."
          ],
          [
            "Access Date",
            "2021-05-14 01:26:24"
          ],
          [
            "Creators",
            "Hao Zhu, Man-Di Luo, Rui Wang, Ai-Hua Zheng, Ran He"
          ],
          [
            "DOI",
            "10.1007/s11633-021-1293-0"
          ],
          [
            "Date",
            "2021-06-01 2021-06-01"
          ],
          [
            "ISSN",
            "1751-8520"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Journal Abbreviation",
            "Int. J. Autom. Comput."
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "351-376"
          ],
          [
            "Publication Title",
            "International Journal of Automation and Computing"
          ],
          [
            "Short Title",
            "Deep Audio-visual Learning"
          ],
          [
            "Title",
            "Deep Audio-visual Learning: A Survey"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11633-021-1293-0"
          ],
          [
            "Volume",
            "18"
          ]
        ],
        "resource": "storage/i624.pdf",
        "selectable": false
      },
      {
        "text": "Deep Facial Expression Recognition",
        "item-id": "i1907",
        "nodes": [
          {
            "text": "Li_Deng_2022_Deep Facial Expression Recognition.pdf",
            "item-id": "i2042",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li_Deng_2022_Deep Facial Expression Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li_Deng_2022_Deep Facial Expression Recognition.pdf"
              ]
            ],
            "resource": "storage/i2042.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Facial Expression Recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose, and identity bias. In this survey, we provide a comprehensive review of deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions for applicable implementations for each stage. For the state-of-the-art in deep FER, we introduce existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems."
          ],
          [
            "Creators",
            "Shan Li, Weihong Deng"
          ],
          [
            "DOI",
            "10.1109/TAFFC.2020.2981446"
          ],
          [
            "Date",
            "2022-07-00 2022-07"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Affective Computing"
          ],
          [
            "ISSN",
            "1949-3045"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1195-1215"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Affective Computing"
          ],
          [
            "Short Title",
            "Deep Facial Expression Recognition"
          ],
          [
            "Title",
            "Deep Facial Expression Recognition: A Survey"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "resource": "storage/i2042.pdf",
        "selectable": false
      },
      {
        "text": "Deep Learning for Image Super-resolution",
        "item-id": "i16",
        "nodes": [
          {
            "text": "Related blog",
            "item-id": "n117",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Related blog",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Related blog</p>\n<p>https://zhuanlan.zhihu.com/p/143380729</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Wang et al_2020_Deep Learning for Image Super-resolution.pdf",
            "item-id": "i136",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2020_Deep Learning for Image Super-resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2020_Deep Learning for Image Super-resolution.pdf"
              ]
            ],
            "resource": "storage/i136.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Learning for Image Super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future."
          ],
          [
            "Creators",
            "Zhihao Wang, Jian Chen, Steven C.H. Hoi"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2020.2982166"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "Deep Learning for Image Super-resolution"
          ],
          [
            "Title",
            "Deep Learning for Image Super-resolution: A Survey"
          ]
        ],
        "resource": "storage/i136.pdf",
        "selectable": false
      },
      {
        "text": "Deep Learning-based Face Super-resolution",
        "item-id": "i2820",
        "nodes": [
          {
            "text": "Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf",
            "item-id": "i2887",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf"
              ]
            ],
            "resource": "storage/i2887.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Learning-based Face Super-resolution",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face super-resolution (FSR), also known as face hallucination, which is aimed at enhancing the resolution of low-resolution (LR) face images to generate high-resolution face images, is a domain-specific image super-resolution problem. Recently, FSR has received considerable attention and witnessed dazzling advances with the development of deep learning techniques. To date, few summaries of the studies on the deep learning-based FSR are available. In this survey, we present a comprehensive review of deep learning-based FSR methods in a systematic manner. First, we summarize the problem formulation of FSR and introduce popular assessment metrics and loss functions. Second, we elaborate on the facial characteristics and popular datasets used in FSR. Third, we roughly categorize existing methods according to the utilization of facial characteristics. In each category, we start with a general description of design principles, present an overview of representative approaches, and then discuss the pros and cons among them. Fourth, we evaluate the performance of some state-of-the-art methods. Fifth, joint FSR and other tasks, and FSR-related applications are roughly introduced. Finally, we envision the prospects of further technological advancement in this field."
          ],
          [
            "Access Date",
            "2023-07-12 15:07:01"
          ],
          [
            "Creators",
            "Junjun Jiang, Chenyang Wang, Xianming Liu, Jiayi Ma"
          ],
          [
            "DOI",
            "10.1145/3485132"
          ],
          [
            "Date",
            "2021-00-23 \u5341\u4e00\u6708 23, 2021"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "13:1\u201313:36"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Deep Learning-based Face Super-resolution"
          ],
          [
            "Title",
            "Deep Learning-based Face Super-resolution: A Survey"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3485132"
          ],
          [
            "Volume",
            "55"
          ]
        ],
        "resource": "storage/i2887.pdf",
        "selectable": false
      },
      {
        "text": "Deep Person Generation",
        "item-id": "i2147",
        "nodes": [
          {
            "text": "Sha et al_2022_Deep Person Generation.pdf",
            "item-id": "i2162",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sha et al_2022_Deep Person Generation.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NYKSXY4R/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/4\">2 Talking-head Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/5\">2.1 Motion-driven Talking-Head Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/7\">2.2 Audio-driven Talking-Head Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/9\">2.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/10\">3 Pose-guided Person Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/10\">3.1 Pose-guided Person Image Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/14\">3.2 Pose-guided Person Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/14\">3.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/15\">4 Garment-Oriented Person Generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/15\">4.1 Virtual try-on</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">4.2 Garment Manipulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">4.3 Discussion</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">5 Benchmarks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/19\">5.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/20\">5.2 Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/21\">5.3 Performance Comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/22\">6 Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7 Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7.1 Generative Data Augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/25\">7.2 Virtual Fitting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/26\">7.3 Digital Human</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/27\">8 Future Directions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/28\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NYKSXY4R/28\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sha et al_2022_Deep Person Generation.pdf"
              ]
            ],
            "resource": "storage/i2162.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deep Person Generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep person generation has attracted extensive research attention due to its wide applications in virtual agents, video conferencing, online shopping and art/movie production. With the advancement of deep learning, visual appearances (face, pose, cloth) of a person image can be easily generated on demand. In this survey, we first summarize the scope of person generation, and then systematically review recent progress and technical trends in identity-preserving deep person generation, covering three major tasks: talking-head generation (face), pose-guided person generation (pose) and garment-oriented person generation (cloth). More than two hundred papers are covered for a thorough overview, and the milestone works are highlighted to witness the major technical breakthrough. Based on these fundamental tasks, many applications are investigated, e.g., virtual fitting, digital human, generative data augmentation. We hope this survey could shed some light on the future prospects of identity-preserving deep person generation, and provide a helpful foundation for full applications towards the digital human."
          ],
          [
            "Access Date",
            "2022-12-12 11:08:18"
          ],
          [
            "Creators",
            "Tong Sha, Wei Zhang, Tong Shen, Zhoujun Li, Tao Mei"
          ],
          [
            "DOI",
            "10.1145/3575656"
          ],
          [
            "Date",
            "2022-00-07 \u5341\u4e8c\u6708 7, 2022"
          ],
          [
            "Extra",
            "Just Accepted"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Deep Person Generation"
          ],
          [
            "Title",
            "Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis"
          ],
          [
            "URL",
            "https://doi.org/10.1145/3575656"
          ]
        ],
        "resource": "storage/i2162.pdf",
        "selectable": false
      },
      {
        "text": "Deepfake generation and detection, a survey",
        "item-id": "i2777",
        "nodes": [
          {
            "text": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item-id": "i2921",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang_2022_Deepfake generation and detection, a survey.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/2\">2 Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3 Deepfake generation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.1 Types of\u00a0Deepfake</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/3\">3.2 Face-based generation methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.1 Face swapping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/5\">3.2.2 Facial reenactment</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4 Deepfake detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1 Detection methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/6\">4.1.1 Features based detection methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/10\">4.1.2 Machine learning-based detection methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/11\">4.2 Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5 Discussions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/13\">5.1 Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">5.2 Future directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/14\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WA8LLUNG/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang_2022_Deepfake generation and detection, a survey.pdf"
              ]
            ],
            "resource": "storage/i2921.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Deepfake generation and detection, a survey",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deepfake\u00a0refers to realistic, but\u00a0fake images, sounds, and videos generated by articial intelligence methods. Recent advances in deepfake generation make\u00a0deepfake more realistic and easier to make. Deepfake has been a signicant threat to national security, democracy, society, and\u00a0our privacy, which calls for deepfake detection methods to combat potential threats. In the paper, we make a survey on state-ofthe-art deepfake generation methods, detection methods, and existing datasets. Current deepfake generation methods can be\u00a0classified into face swapping and facial reenactment. Deepfake detection methods are mainly based features and machine\u00a0learning methods. There are still some challenges for deepfake detection, such as progress on deepfake generation, lack of high\u00a0quality datasets and benchmark. Future trends on deepfake detection can be efficient, robust and systematical detection methods\u00a0and high quality datasets."
          ],
          [
            "Access Date",
            "2023-06-30 23:52:04"
          ],
          [
            "Creators",
            "Tao Zhang"
          ],
          [
            "DOI",
            "10.1007/s11042-021-11733-y"
          ],
          [
            "Date",
            "2022-02-01 2022-02-01"
          ],
          [
            "ISSN",
            "1573-7721"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Multimed Tools Appl"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "6259-6276"
          ],
          [
            "Publication Title",
            "Multimedia Tools and Applications"
          ],
          [
            "Title",
            "Deepfake generation and detection, a survey"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11042-021-11733-y"
          ],
          [
            "Volume",
            "81"
          ]
        ],
        "resource": "storage/i2921.pdf",
        "selectable": false
      },
      {
        "text": "Generation and detection of manipulated multimodal audiovisual content",
        "item-id": "i3222",
        "nodes": [
          {
            "text": "Liz-L\u00f3pez et al_2024_Generation and detection of manipulated multimodal audiovisual content.pdf",
            "item-id": "i3304",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liz-L\u00f3pez et al_2024_Generation and detection of manipulated multimodal audiovisual content.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GZPZVRVS/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/3\">Methodology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Initial Analysis</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Manipulation techniques</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Video manipulation techniques</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/4\">Face morphing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/5\">Entire face synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/6\">Face Swap</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/7\">Facial Reenactment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/7\">Lip sync</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/7\">Facial attribute manipulation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/8\">Audio manipulation techniques</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/8\">Voice conversion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/8\">Text-to-speech synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/9\">Voice cloning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/9\">Voice morphing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/9\">Replay Attacks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/10\">Multimodal manipulation techniques</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/10\">Dataset of manipulated multimedia content</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/11\">Video</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/11\">Audio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Multimodal video and audio</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Multimedia data forensics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Techniques for video forensics</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/13\">Techniques for detecting manipulated metadata</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/14\">Techniques for detecting manipulated video</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/15\">Techniques for detecting manipulated video-temporal continuity features</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/18\">Techniques for detecting manipulated audio</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/18\">Audio forensics based on feature selection and extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/19\">Audio forensics based on CNN architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/22\">Audio forensics based on attention layers</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/24\">Techniques for detecting manipulated multimodal content</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/24\">Inconsistencies between modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/26\">Emotional inconsistencies</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/27\">Available tools for non-technical end-users</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/28\">Discussion and conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/28\">Answer to research questions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/29\">Future trends and challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/30\">Conclusion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Declaration of competing interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Data availability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Appendix A. Datasets download links</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/31\">Appendix B. Codes availables from forensics systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GZPZVRVS/32\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liz-L\u00f3pez et al_2024_Generation and detection of manipulated multimodal audiovisual content.pdf"
              ]
            ],
            "resource": "storage/i3304.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generation and detection of manipulated multimodal audiovisual content",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative deep learning techniques have invaded the public discourse recently. Despite the advantages, the applications to disinformation are concerning as the counter-measures advance slowly. As the manipulation of multimedia content becomes easier, faster, and more credible, developing effective forensics becomes invaluable. Other works have identified this need but neglect that disinformation is inherently multimodal. Overall in this survey, we exhaustively describe modern manipulation and forensic techniques from the lens of video, audio and their multimodal fusion. For manipulation techniques, we give a classification of the most commonly applied manipulations. Generative techniques can be exploited to generate datasets; we provide a list of current datasets useful for forensics. We have reviewed forensic techniques from 2018 to 2023, examined the usage of datasets, and given a comparative analysis of each modality. Finally, we give another comparison of end-to-end forensics tools for end-users. From our analysis clear trends are found with diffusion models, dataset granularity, explainability techniques, synchronisation improvements, and learning task diversity. We find a roadmap of deep challenges ahead, including multilinguality, multimodality, improving data quality (and variety), all in an adversarial ever-changing environment."
          ],
          [
            "Access Date",
            "2023-11-07 11:27:46"
          ],
          [
            "Creators",
            "Helena Liz-L\u00f3pez, Mamadou Keita, Abdelmalik Taleb-Ahmed, Abdenour Hadid, Javier Huertas-Tato, David Camacho"
          ],
          [
            "DOI",
            "10.1016/j.inffus.2023.102103"
          ],
          [
            "Date",
            "2024-03-01 2024-03-01"
          ],
          [
            "ISSN",
            "1566-2535"
          ],
          [
            "Journal Abbreviation",
            "Information Fusion"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "102103"
          ],
          [
            "Publication Title",
            "Information Fusion"
          ],
          [
            "Short Title",
            "Generation and detection of manipulated multimodal audiovisual content"
          ],
          [
            "Title",
            "Generation and detection of manipulated multimodal audiovisual content: Advances, trends and open challenges"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1566253523004190"
          ],
          [
            "Volume",
            "103"
          ]
        ],
        "resource": "storage/i3304.pdf",
        "selectable": false
      },
      {
        "text": "Generative Adversarial Networks for face generation",
        "item-id": "i1529",
        "nodes": [
          {
            "text": "Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf",
            "item-id": "i1550",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf"
              ]
            ],
            "resource": "storage/i1550.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative Adversarial Networks for face generation",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues."
          ],
          [
            "Access Date",
            "2022-05-12 15:31:57"
          ],
          [
            "Creators",
            "Amina Kammoun, Rim Slama, Hedi Tabia, Tarek Ouni, Mohmed Abid"
          ],
          [
            "DOI",
            "10.1145/1122445.1122456"
          ],
          [
            "Date",
            "2022-03-20 2022-03-20"
          ],
          [
            "Extra",
            "Just Accepted"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Generative Adversarial Networks for face generation"
          ],
          [
            "Title",
            "Generative Adversarial Networks for face generation: A survey"
          ],
          [
            "URL",
            "http://doi.org/10.1145/1122445.1122456"
          ]
        ],
        "resource": "storage/i1550.pdf",
        "selectable": false
      },
      {
        "text": "Generative Adversarial Networks in Computer Vision",
        "item-id": "i1220",
        "nodes": [
          {
            "text": "Wang et al_2021_Generative Adversarial Networks in Computer Vision.pdf",
            "item-id": "i1221",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2021_Generative Adversarial Networks in Computer Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2021_Generative Adversarial Networks in Computer Vision.pdf"
              ]
            ],
            "resource": "storage/i1221.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative Adversarial Networks in Computer Vision",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review."
          ],
          [
            "Access Date",
            "2021-12-06 11:31:04"
          ],
          [
            "Creators",
            "Zhengwei Wang, Qi She, Tom\u00e1s E. Ward"
          ],
          [
            "DOI",
            "10.1145/3439723"
          ],
          [
            "Date",
            "2021-02-09 2021-02-09"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "March 2022"
          ],
          [
            "Pages",
            "37:1\u201337:38"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Generative Adversarial Networks in Computer Vision"
          ],
          [
            "Title",
            "Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3439723"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i1221.pdf",
        "selectable": false
      },
      {
        "text": "Multimodal Image Synthesis and Editing",
        "item-id": "i1303",
        "nodes": [
          {
            "text": "Comment: 20 pages, 19 figures",
            "item-id": "n1359",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 20 pages, 19 figures",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 20 pages, 19 figures</div>",
            "node_type": "note"
          },
          {
            "text": "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf",
            "item-id": "i1358",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhan et al_2021_Multimodal Image Synthesis and Editing.pdf"
              ]
            ],
            "resource": "storage/i1358.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multimodal Image Synthesis and Editing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modelling the interaction among multimodal information, multimodal image synthesis and editing have become a hot research topic in recent years. Different from traditional visual guidance which provides explicit clues, multimodal guidance offers intuitive and flexible means in image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of features with inherent modality gaps, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis \\& editing and formulate taxonomies according to data modality and model architectures. We start with an introduction to different types of guidance modalities in image synthesis and editing. We then describe multimodal image synthesis and editing approaches extensively with detailed frameworks including Generative Adversarial Networks (GANs), GAN Inversion, Transformers, and other methods such as NeRF and Diffusion models. This is followed by a comprehensive description of benchmark datasets and corresponding evaluation metrics as widely adopted in multimodal image synthesis and editing, as well as detailed comparisons of different synthesis methods with analysis of respective advantages and limitations. Finally, we provide insights into the current research challenges and possible future research directions. A project associated with this survey is available at https://github.com/fnzhan/MISE"
          ],
          [
            "Access Date",
            "2022-02-18 00:42:09"
          ],
          [
            "Creators",
            "Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu"
          ],
          [
            "Date",
            "2021-12-27 2021-12-27"
          ],
          [
            "Extra",
            "arXiv: 2112.13592"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2112.13592 [cs]"
          ],
          [
            "Short Title",
            "Multimodal Image Synthesis and Editing"
          ],
          [
            "Title",
            "Multimodal Image Synthesis and Editing: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2112.13592"
          ]
        ],
        "resource": "storage/i1358.pdf",
        "selectable": false
      },
      {
        "text": "Neurosymbolic AI and its Taxonomy",
        "item-id": "i3246",
        "nodes": [
          {
            "text": "Comment: submitted to ACM Computing Surveys",
            "item-id": "n3400",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: submitted to ACM Computing Surveys",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: submitted to ACM Computing Surveys</div>",
            "node_type": "note"
          },
          {
            "text": "Gibaut et al_2023_Neurosymbolic AI and its Taxonomy.pdf",
            "item-id": "i3398",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gibaut et al_2023_Neurosymbolic AI and its Taxonomy.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WJGSXKDS/1\">1 Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/2\">1.1 Symbols</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/3\">1.2 Methodoly and Scope of this Work</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/4\">2 Knowledge Representation in Neurosymbolic Systems</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/6\">3 Learning in Neurosymbolic Systems</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/6\">3.1 Inductive Logic Programming</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/7\">3.2 Hybrid Learning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/8\">4 Reasoning in Neurosymbolic Systems</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/8\">4.1 Forward and Backward chaining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/8\">4.2 Approximate Satisfiability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/9\">4.3 Relationship reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/9\">4.4 Exploration of Practical Reasoning in DFL</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/9\">5 Explainability and Trustworthiness in Neurosymbolic Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/10\">6 Applications of neuro-symbolic Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WJGSXKDS/15\">7 Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gibaut et al_2023_Neurosymbolic AI and its Taxonomy.pdf"
              ]
            ],
            "resource": "storage/i3398.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Neurosymbolic AI and its Taxonomy",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications."
          ],
          [
            "Access Date",
            "2023-12-09 00:19:51"
          ],
          [
            "Archiveid",
            "arXiv:2305.08876"
          ],
          [
            "Creators",
            "Wandemberg Gibaut, Leonardo Pereira, Fabio Grassiotto, Alexandre Osorio, Eder Gadioli, Amparo Munoz, Sildolfo Gomes, Claudio dos Santos"
          ],
          [
            "DOI",
            "10.48550/arxiv.2305.08876"
          ],
          [
            "Date",
            "2023-05-12 2023-05-12"
          ],
          [
            "Extra",
            "arXiv:2305.08876 [cs]"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Neurosymbolic AI and its Taxonomy"
          ],
          [
            "Title",
            "Neurosymbolic AI and its Taxonomy: a survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2305.08876"
          ]
        ],
        "resource": "storage/i3398.pdf",
        "selectable": false
      },
      {
        "text": "Past, Present, and Future of Face Recognition",
        "item-id": "i2100",
        "nodes": [
          {
            "text": "Adjabi et al_2020_Past, Present, and Future of Face Recognition.pdf",
            "item-id": "i2135",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Adjabi et al_2020_Past, Present, and Future of Face Recognition.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_22XTLUVP/1\">Introduction </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/3\">Face Recognition History </a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/4\">Face Recognition Systems </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/4\">Main Steps in Face Recognition Systems </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/5\">Assessment Protocols in Face Recognition </a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/6\">Available Datasets and Protocols for 2D Face Recognition </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/7\">ORL Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/7\">FERET Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/8\">AR Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/8\">XM2VTS Database </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/9\">BANCA Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/9\">FRGC Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/10\">LFW Database </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/10\">CMU Multi PIE Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/10\">CASIA-WebFace Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/10\">IARPA Janus Benchmark-A </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/11\">MegaFace Database </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/11\">CFP Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/11\">Ms-Celeb-M1 Benchmark </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/11\">DMFD Database </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/12\">VGGFACE Database </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/12\">VGGFACE2 Database </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/12\">IARPA Janus Benchmark-B </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/13\">MF2 Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/13\">DFW Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/13\">IARPA Janus Benchmark-C </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/13\">LFR Dataset </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/13\">RMFRD and SMFRD: Masqued Face Recognition Dataset </a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/15\">Two-Dimensional Face Recognition Approaches </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/15\">Holistic Methods </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/17\">Geometric Approach </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/19\">Local-Texture Approach </a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/22\">Deep Learning Approach </a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/22\">Introduction to Deep Learning </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/22\">Convolutional Neural Networks (CNNs) </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/23\">Popular CNN Architectures </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/25\">Deep CNN-Based Methods for Face Recognition. </a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/32\">Three-Dimensional Face Recognition </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/32\">Factual Background and Acquisition Systems </a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/32\">Introduction to 3D Face Recognition </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/33\">Microsoft Kinect Technology </a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/35\">Methods and Datasets </a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/35\">Challenges of 3D Facial Recognition </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/37\">Traditional Methods of Machine Learning </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/38\">Deep Learning-Based Methods </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/38\">Three-Dimensional Face Recognition Databases </a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_22XTLUVP/39\">Open Challenges </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/39\">Face Recognition and Occlusion </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/40\">Hetegerenous Face Recognition </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/40\">Face Recognition and Ageing </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/40\">Single Sample Face Recognition </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/41\">Face Recognition in Video Surveillance </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/41\">Face Recognition and Soft Biometrics </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/42\">Face Recognition and Smartphones </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/43\">Face Recognition and Internet of Things (IoT) </a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/43\">Conclusions </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_22XTLUVP/43\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Adjabi et al_2020_Past, Present, and Future of Face Recognition.pdf"
              ]
            ],
            "resource": "storage/i2135.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Past, Present, and Future of Face Recognition",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Face recognition is one of the most active research fields of computer vision and pattern recognition, with many practical and commercial applications including identification, access control, forensics, and human-computer interactions. However, identifying a face in a crowd raises serious questions about individual freedoms and poses ethical issues. Significant methods, algorithms, approaches, and databases have been proposed over recent years to study constrained and unconstrained face recognition. 2D approaches reached some degree of maturity and reported very high rates of recognition. This performance is achieved in controlled environments where the acquisition parameters are controlled, such as lighting, angle of view, and distance between the camera\u2013subject. However, if the ambient conditions (e.g., lighting) or the facial appearance (e.g., pose or facial expression) change, this performance will degrade dramatically. 3D approaches were proposed as an alternative solution to the problems mentioned above. The advantage of 3D data lies in its invariance to pose and lighting conditions, which has enhanced recognition systems efficiency. 3D data, however, is somewhat sensitive to changes in facial expressions. This review presents the history of face recognition technology, the current state-of-the-art methodologies, and future directions. We specifically concentrate on the most recent databases, 2D and 3D face recognition methods. Besides, we pay particular attention to deep learning approach as it presents the actuality in this field. Open issues are examined and potential directions for research in facial recognition are proposed in order to provide the reader with a point of reference for topics that deserve consideration."
          ],
          [
            "Access Date",
            "2022-11-07 09:07:46"
          ],
          [
            "Creators",
            "Insaf Adjabi, Abdeldjalil Ouahabi, Amir Benzaoui, Abdelmalik Taleb-Ahmed"
          ],
          [
            "DOI",
            "10.3390/electronics9081188"
          ],
          [
            "Date",
            "2020-08-00 2020/8"
          ],
          [
            "Extra",
            "Number: 8\nPublisher: Multidisciplinary Digital Publishing Institute"
          ],
          [
            "ISSN",
            "2079-9292"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.mdpi.com"
          ],
          [
            "Pages",
            "1188"
          ],
          [
            "Publication Title",
            "Electronics"
          ],
          [
            "Rights",
            "http://creativecommons.org/licenses/by/3.0/"
          ],
          [
            "Short Title",
            "Past, Present, and Future of Face Recognition"
          ],
          [
            "Title",
            "Past, Present, and Future of Face Recognition: A Review"
          ],
          [
            "URL",
            "https://www.mdpi.com/2079-9292/9/8/1188"
          ],
          [
            "Volume",
            "9"
          ]
        ],
        "resource": "storage/i2135.pdf",
        "selectable": false
      },
      {
        "text": "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge",
        "item-id": "i1237",
        "nodes": [
          {
            "text": "Ignatov et al_2021_Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI.pdf",
            "item-id": "i1239",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ignatov et al_2021_Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ignatov et al_2021_Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI.pdf"
              ]
            ],
            "resource": "storage/i1239.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper."
          ],
          [
            "Access Date",
            "2021-12-06 13:24:41"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Andrey Ignatov, Andres Romero, Heewon Kim, Radu Timofte"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2535-2544"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge"
          ],
          [
            "Title",
            "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge: Report"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Real-Time_Video_Super-Resolution_on_Smartphones_With_Deep_Learning_Mobile_AI_CVPRW_2021_paper.html"
          ]
        ],
        "resource": "storage/i1239.pdf",
        "selectable": false
      },
      {
        "text": "Representation Learning",
        "item-id": "i2293",
        "nodes": [
          {
            "text": "Bengio et al_2013_Representation Learning.pdf",
            "item-id": "i2295",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bengio et al_2013_Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bengio et al_2013_Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i2295.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Representation Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."
          ],
          [
            "Creators",
            "Yoshua Bengio, Aaron Courville, Pascal Vincent"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2013.50"
          ],
          [
            "Date",
            "2013-08-00 2013-08"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Issue",
            "8"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1798-1828"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "Representation Learning"
          ],
          [
            "Title",
            "Representation Learning: A Review and New Perspectives"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i2295.pdf",
        "selectable": false
      },
      {
        "text": "Self-supervised Visual Feature Learning with Deep Neural Networks",
        "item-id": "i99",
        "nodes": [
          {
            "text": "Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf",
            "item-id": "i221",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf"
              ]
            ],
            "resource": "storage/i221.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Self-supervised Visual Feature Learning with Deep Neural Networks",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning."
          ],
          [
            "Creators",
            "L. Jing, Y. Tian"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2020.2992393"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "Self-supervised Visual Feature Learning with Deep Neural Networks"
          ],
          [
            "Title",
            "Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey"
          ]
        ],
        "resource": "storage/i221.pdf",
        "selectable": false
      },
      {
        "text": "Survey of Deep Learning Paradigms for Speech Processing",
        "item-id": "i2554",
        "nodes": [
          {
            "text": "Bhangale_Kothandaraman_2022_Survey of Deep Learning Paradigms for Speech Processing.pdf",
            "item-id": "i2625",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bhangale_Kothandaraman_2022_Survey of Deep Learning Paradigms for Speech Processing.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bhangale_Kothandaraman_2022_Survey of Deep Learning Paradigms for Speech Processing.pdf"
              ]
            ],
            "resource": "storage/i2625.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Survey of Deep Learning Paradigms for Speech Processing",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Over the past decades, a particular focus is given to research on machine learning techniques for speech processing applications. However, in the past few years, research has focused on using deep learning for speech processing applications. This new machine learning field has become a very attractive area of study and has remarkably better performance than the others in the various speech processing applications. This paper presents a brief survey of application deep learning for various speech processing applications such as speech separation, speech enhancement, speech recognition, speaker recognition, emotion recognition, language recognition, music recognition, speech data retrieval, etc. The survey goes on to cover the use of Auto-Encoder, Generative Adversarial Network, Restricted Boltzmann Machine, Deep Belief Network, Deep Neural Network, Convolutional Neural Network, Recurrent Neural Network and Deep Reinforcement Learning for speech processing. Additionally, it focuses on the various speech database and evaluation metrics used by deep learning algorithms for performance evaluation."
          ],
          [
            "Access Date",
            "2023-06-07 14:44:04"
          ],
          [
            "Creators",
            "Kishor Barasu Bhangale, Mohanaprasad Kothandaraman"
          ],
          [
            "DOI",
            "10.1007/s11277-022-09640-y"
          ],
          [
            "Date",
            "2022-07-01 2022-07-01"
          ],
          [
            "ISSN",
            "1572-834X"
          ],
          [
            "Issue",
            "2"
          ],
          [
            "Journal Abbreviation",
            "Wireless Pers Commun"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "1913-1949"
          ],
          [
            "Publication Title",
            "Wireless Personal Communications"
          ],
          [
            "Title",
            "Survey of Deep Learning Paradigms for Speech Processing"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11277-022-09640-y"
          ],
          [
            "Volume",
            "125"
          ]
        ],
        "resource": "storage/i2625.pdf",
        "selectable": false
      },
      {
        "text": "Text to Speech Synthesis",
        "item-id": "i2556",
        "nodes": [
          {
            "text": "ResearchGate Link",
            "item-id": "i2630",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "ResearchGate Link",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2023-06-07 14:41:10"
              ],
              [
                "Title",
                "ResearchGate Link"
              ],
              [
                "URL",
                "https://www.researchgate.net/publication/364280141_Text_to_Speech_Synthesis_A_Systematic_Review_Deep_Learning_Based_Architecture_and_Future_Research_Direction"
              ]
            ]
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Text to Speech Synthesis",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Creators",
            "Fahima Khanam, Farha Munmun, Nadia Ritu, Dr. Aloke Saha, M. Ph. D."
          ],
          [
            "DOI",
            "10.12720/jait.13.5.398-412"
          ],
          [
            "Date",
            "2022-10-00 2022-10"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Journal of Advances in Information Technology"
          ],
          [
            "Library Catalog",
            "ResearchGate"
          ],
          [
            "Pages",
            "398-412"
          ],
          [
            "Publication Title",
            "Journal of Advances in Information Technology"
          ],
          [
            "Short Title",
            "Text to Speech Synthesis"
          ],
          [
            "Title",
            "Text to Speech Synthesis: A Systematic Review, Deep Learning Based Architecture and Future Research Direction"
          ],
          [
            "Volume",
            "13"
          ]
        ],
        "selectable": false
      },
      {
        "text": "The Creation and Detection of Deepfakes",
        "item-id": "i958",
        "nodes": [
          {
            "text": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item-id": "i969",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf"
              ]
            ],
            "resource": "storage/i969.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The Creation and Detection of Deepfakes",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these \u201cdeepfakes\u201d have advanced significantly. In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention."
          ],
          [
            "Access Date",
            "2021-10-05 09:05:27"
          ],
          [
            "Creators",
            "Yisroel Mirsky, Wenke Lee"
          ],
          [
            "DOI",
            "10.1145/3425780"
          ],
          [
            "Date",
            "2021-01-02 2021-01-02"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "April 2021"
          ],
          [
            "Pages",
            "7:1\u20137:41"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "The Creation and Detection of Deepfakes"
          ],
          [
            "Title",
            "The Creation and Detection of Deepfakes: A Survey"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3425780"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i969.pdf",
        "selectable": false
      },
      {
        "text": "Transformers in Speech Processing",
        "item-id": "i2555",
        "nodes": [
          {
            "text": "Comment: under-review",
            "item-id": "n2629",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: under-review",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: under-review</div>",
            "node_type": "note"
          },
          {
            "text": "Latif et al_2023_Transformers in Speech Processing.pdf",
            "item-id": "i2628",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Latif et al_2023_Transformers in Speech Processing.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_IKFNYT3D/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-A Sequential Models for Speech Processing</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B Overview of Transformers</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B1 Self-Attention Layer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B2 Masked Self-Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/2\">II-B3 Multi-Head Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/3\">II-B4 Positional Encoding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/3\">II-C Popular Transformers for Speech</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/4\">II-C1 wav2vec</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C2 data2vec</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C3 Whisper</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C4 Tacotron</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C5 VALL-E</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/5\">II-C6 Conformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/6\">II-C7 UniSpeech</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/6\">II-C8 Speechformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/7\">II-C9 WavLM</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/7\">III Literature Review</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/7\">III-A Automatic Speech Recognition (ASR)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/8\">III-B Neural Speech Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/9\">III-C Speech Translation (ST)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/11\">III-D Speech Paralinguistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/11\">III-E Speech Enhancement and Separation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/13\">III-F Spoken Dialogue Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/15\">III-G Multi-Modal Applications</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/17\">IV Challenges and Future Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/17\">IV-A Training Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/17\">IV-B Computational Cost and Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/18\">IV-C Large Data Requirements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/18\">IV-D Generalization and Transferability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/18\">IV-E Multimodal Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/19\">IV-F Robustness</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/19\">V Summary and Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/20\">VI Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IKFNYT3D/20\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Latif et al_2023_Transformers in Speech Processing.pdf"
              ]
            ],
            "resource": "storage/i2628.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Transformers in Speech Processing",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues."
          ],
          [
            "Access Date",
            "2023-06-07 14:42:33"
          ],
          [
            "Archiveid",
            "arXiv:2303.11607"
          ],
          [
            "Creators",
            "Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, Junaid Qadir"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.11607"
          ],
          [
            "Date",
            "2023-03-21 2023-03-21"
          ],
          [
            "Extra",
            "arXiv:2303.11607 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Transformers in Speech Processing"
          ],
          [
            "Title",
            "Transformers in Speech Processing: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.11607"
          ]
        ],
        "resource": "storage/i2628.pdf",
        "selectable": false
      },
      {
        "text": "Transformers in Vision",
        "item-id": "i2383",
        "nodes": [
          {
            "text": "Khan et al_2022_Transformers in Vision.pdf",
            "item-id": "i2424",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khan et al_2022_Transformers in Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khan et al_2022_Transformers in Vision.pdf"
              ]
            ],
            "resource": "storage/i2424.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Transformers in Vision",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision."
          ],
          [
            "Access Date",
            "2023-05-23 08:00:02"
          ],
          [
            "Creators",
            "Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah"
          ],
          [
            "DOI",
            "10.1145/3505244"
          ],
          [
            "Date",
            "2022-00-13 \u4e5d\u6708 13, 2022"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "10s"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "200:1\u2013200:41"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Transformers in Vision"
          ],
          [
            "Title",
            "Transformers in Vision: A Survey"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3505244"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i2424.pdf",
        "selectable": false
      },
      {
        "text": "Video Super Resolution Based on Deep Learning",
        "item-id": "i7",
        "nodes": [
          {
            "text": "Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf",
            "item-id": "i119",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KJPJ4DX4/1\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/2\">II Background</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/3\">III Video Super-resolution Methods</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/3\">IV Methods with Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/3\">IV-A Motion Estimation and Compensation Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/4\">IV-A1 Deep-DE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/4\">IV-A2 VSRnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/5\">IV-A3 VESPCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/5\">IV-A4 DRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/6\">IV-A5 RVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/6\">IV-A6 FRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/6\">IV-A7 STTN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/7\">IV-A8 SOFVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/7\">IV-A9 TecoGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A10 TOFlow</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A11 MMCNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A12 RBPN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/8\">IV-A13 MEMC-Net</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A14 RRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A15 RTVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A16 MultiBoot VSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A17 MAFN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/9\">IV-A18 STARnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/10\">IV-B Deformable Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/10\">IV-B1 EDVR </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/10\">IV-B2 DNLN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">IV-B3 TDAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">IV-B4 D3Dnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">IV-B5 VESR-Net</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/11\">V Methods without Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-A 2D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-A1 VSRResFeatGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-A2 FFCVSR</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-B 3D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/12\">V-B1 DUF</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-B2 FSTRN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-B3 3DSRnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C Recurrent Convolutional Neural Networks (RCNNs)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C1 BRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C2 STCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/13\">V-C3 RISTN</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/14\">V-D Non-Local Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/14\">V-D1 PFNL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/14\">V-D2 MuCAN</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/15\">VI Performance Comparisons</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/15\">VI-A Datasets and Competitions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VI-B Performance of Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII Trends and Challenges</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII-A Lightweight Super-Resolution Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII-B Interpretability of Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/16\">VII-C Super-Resolution with Larger Scaling Factors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/17\">VII-D Super-Resolution with Random Scaling Factors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/17\">VII-E More Reasonable &amp;amp; Proper Degradation Process of Videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-F Unsupervised Super-Resolution Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-G More Effective Scene Change Algorithms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-H More Reasonable Evaluation Criteria for Video Quality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VII-I More Effective Methods for Leveraging Information</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/18\">VIII Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">IX Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X Methods with Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A Motion Estimation and Compensation Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A1 Deep-DE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A2 VSRnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/23\">X-A3 VESPCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A4 DRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A5 RVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A6 FRVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A7 STTN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A8 SOFVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A9 TecoGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A10 TOFlow</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A11 MMCNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A12 RBPN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/24\">X-A13 MEMC-Net</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A14 RRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A15 RTVSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A16 MultiBoot VSR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A17 MAFN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/25\">X-A18 STARnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B Deformable Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B1 EDVR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B2 DNLN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B3 TDAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B4 D3Dnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B5 VESR-Net</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/26\">X-B6 STVSR</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI Methods without Alignment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-A 2D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-A1 VSRResFeatGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-A2 FFCVSR</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-B 3D Convolution Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-B1 DUF</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/27\">XI-B2 FSTRN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-B3 3DSRnet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C Recurrent Convolutional Neural Networks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C1 BRCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C2 STCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-C3 RISTN</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-D Non-Local Methods</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-D1 PFNL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/28\">XI-D2 MuCAN</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/29\">XII Performance Comparisons</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KJPJ4DX4/29\">XII-A Performance of Methods</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i119.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Video Super Resolution Based on Deep Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning."
          ],
          [
            "Access Date",
            "2021-05-04 07:47:31"
          ],
          [
            "Creators",
            "Hongying Liu, Zhubo Ruan, Peng Zhao, Chao Dong, Fanhua Shang, Yuanyuan Liu, Linlin Yang"
          ],
          [
            "Date",
            "2020-12-20 2020-12-20"
          ],
          [
            "Extra",
            "arXiv: 2007.12928"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2007.12928 [cs, eess]"
          ],
          [
            "Short Title",
            "Video Super Resolution Based on Deep Learning"
          ],
          [
            "Title",
            "Video Super Resolution Based on Deep Learning: A Comprehensive Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2007.12928"
          ]
        ],
        "resource": "storage/i119.pdf",
        "selectable": false
      },
      {
        "text": "Video Understanding with Large Language Models",
        "item-id": "i3435",
        "nodes": [
          {
            "text": "Tang et al_2024_Video Understanding with Large Language Models.pdf",
            "item-id": "i3490",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tang et al_2024_Video Understanding with Large Language Models.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Q7V8RBBF/3\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/5\">Foundations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/7\">Vision Integration with LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/7\">Language's Roles in Video Understanding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">Other Modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">Training Strategies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">Vid-LLMs: Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/8\">LLM-based Video Agents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/11\">Vid-LLM Pretraining</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/11\">Vid-LLM Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/12\">Fine-tuning with Connective Adapters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/14\">Fine-tuning with Insertive Adapters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/14\">Fine-tuning with Hybrid Adapters</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/15\">Hybrid Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/15\">Tasks, Datasets, and Benchmarks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Recognition and Anticipation</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Dataset Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/16\">Captioning and Description</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/17\">Dataset Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/18\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/18\">Grounding and Retrieval</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/19\">Dataset Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/19\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/19\">Question Answering</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/20\">Datasets Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/20\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/21\">Video Instruction Tuning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/21\">Pretraining Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/21\">Fine-tuning Dataset</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Media and Entertainment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Interactive and User-Centric Technologies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/22\">Healthcare and Security Applications</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/23\">Future Directions and Conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/23\">Limitations and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7V8RBBF/24\">Conclusion</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tang et al_2024_Video Understanding with Large Language Models.pdf"
              ]
            ],
            "resource": "storage/i3490.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video Understanding with Large Language Models",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding."
          ],
          [
            "Access Date",
            "2024-01-08 15:56:30"
          ],
          [
            "Archiveid",
            "arXiv:2312.17432"
          ],
          [
            "Creators",
            "Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu"
          ],
          [
            "DOI",
            "10.48550/arXiv.2312.17432"
          ],
          [
            "Date",
            "2024-01-03 2024-01-03"
          ],
          [
            "Extra",
            "arXiv:2312.17432 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video Understanding with Large Language Models"
          ],
          [
            "Title",
            "Video Understanding with Large Language Models: A Survey"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2312.17432"
          ]
        ],
        "resource": "storage/i3490.pdf",
        "selectable": false
      }
    ],
    "item_title": "Survey",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Temporal Action Localization",
    "item-id": "c10,i2939",
    "nodes": [
      {
        "text": "A Survey on Temporal Action Localization",
        "item-id": "i1012",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1172",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>SS-TAD</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf",
            "item-id": "i1015",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_R3R6LIQ6/1\">INTRODUCTION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/2\">RELATED TECHNIQUES</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/2\">TRADITIONAL METHODS</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/3\">DEEP LEARNING METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/3\">TWO-STAGE LOCALIZATION METHODS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/4\">ONE-STAGE LOCALIZATION METHODS</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">BENCHMARK DATASETS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">THUMOS'14 6</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">ActivityNet 7</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">MEXaction2 47</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">MUTITHUMOS 48</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">CHARADES 8</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">AVA 9</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">EVALUATION METRICS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/5\">BASIC CONCEPTS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">ACCURACY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">RECALL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">PRECISION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">INTERSECTION-OVER-UNION (IoU)</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">EVALUATION METRICS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">AVERAGE RECALL (AR)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">MEAN AVERAGE PRECISION (mAP)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">RECENT METHODS AND DEVELOPMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">FULLY-SUPETVISED TEMPORAL ACTION LOCALIZATION (F-TAL)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/6\">FULLY-SUPETVISED LEARNING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/7\">CURRENT REPRESENTATIVE METHODS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/7\">WEAKLY-SUPETVISED TEMPORAL ACTION LOCALIZATION (W-TAL)</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/7\">WEAKLY-SUPETVISED LEARNING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/8\">CURRENT REPRESENTATIVE METHODS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/8\">INSIGHTS ON THE PROBLEM OF W-TAL</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/9\">FUTURE DIRECTIONS AND TRENDS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/9\">CONCLUSION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/9\">REFERENCES</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/11\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/11\">HUIFEN XIA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_R3R6LIQ6/11\">YONGZHAO ZHAN</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf"
              ]
            ],
            "resource": "storage/i1015.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Survey on Temporal Action Localization",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action localization is one of the most crucial and challenging problems for video understanding in computer vision. It has received a lot of attention in recent years because of the extensive application of daily life. Temporal action localization has made some significant progress, especially with the development of deep learning recently. And more demand is for temporal action localization in untrimmed videos. In this paper, our target is to survey the state-of-the-art techniques and models for video temporal action localization. It mainly includes the related techniques, some benchmark datasets and the evaluation metrics of temporal action localization. In addition, we summarize temporal action localization from two aspects: fully-supervised learning and weakly-supervised learning. And we list several representative works and compare their performances respectively. Finally, we make some deep analysis and propose potential research directions, and conclude the survey."
          ],
          [
            "Creators",
            "Huifen Xia, Yongzhao Zhan"
          ],
          [
            "DOI",
            "10.1109/ACCESS.2020.2986861"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Extra",
            "Conference Name: IEEE Access"
          ],
          [
            "ISSN",
            "2169-3536"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "70477-70487"
          ],
          [
            "Publication Title",
            "IEEE Access"
          ],
          [
            "Title",
            "A Survey on Temporal Action Localization"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i1015.pdf",
        "selectable": false
      },
      {
        "text": "ActionFormer",
        "item-id": "i2213",
        "nodes": [
          {
            "text": "Zhang et al_2022_ActionFormer.pdf",
            "item-id": "i2249",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2022_ActionFormer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_VSCNL6LF/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/3\">2 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/4\">3 ActionFormer: A Simple Transformer Model for Temporal Action Localization</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/6\">3.1 Encode Videos with Transformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/7\">3.2 Decoding Actions in Time</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/7\">3.3 ActionFormer: Model Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/8\">3.4 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/9\">4 Experiments and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/9\">4.1 Results on THUMOS14</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/11\">4.2 Results on ActivityNet-1.3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/11\">4.3 Results on EPIC-Kitchens 100</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/12\">4.4 Ablation Experiments</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/14\">5 Conclusion and Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_VSCNL6LF/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2022_ActionFormer.pdf"
              ]
            ],
            "resource": "storage/i2249.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ActionFormer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer\u2014a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU\u00a0=\u00a00.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at https://github.com/happyharrycn/actionformer_release."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Chen-Lin Zhang, Jianxin Wu, Yin Li, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19772-7_29"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19772-7"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "492-510"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "ActionFormer"
          ],
          [
            "Title",
            "ActionFormer: Localizing Moments of\u00a0Actions with\u00a0Transformers"
          ]
        ],
        "resource": "storage/i2249.pdf",
        "selectable": false
      },
      {
        "text": "Activity Graph Transformer for Temporal Action Localization",
        "item-id": "i2939",
        "nodes": [
          {
            "text": "Comment: Project webpage: https://www.sfu.ca/~mnawhal/projects/agt.html; Code available at https://github.com/Nmegha2601",
            "item-id": "n2964",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project webpage: https://www.sfu.ca/~mnawhal/projects/agt.html; Code available at https://github.com/Nmegha2601",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project webpage: https://www.sfu.ca/~mnawhal/projects/agt.html; Code available at https://github.com/Nmegha2601/activitygraph_transformer</div>",
            "node_type": "note"
          },
          {
            "text": "Nawhal_Mori_2021_Activity Graph Transformer for Temporal Action Localization.pdf",
            "item-id": "i2963",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Nawhal_Mori_2021_Activity Graph Transformer for Temporal Action Localization.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Nawhal_Mori_2021_Activity Graph Transformer for Temporal Action Localization.pdf"
              ]
            ],
            "resource": "storage/i2963.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Activity Graph Transformer for Temporal Action Localization",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have non-sequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the state-of-the-art by a considerable margin."
          ],
          [
            "Access Date",
            "2023-07-27 03:31:02"
          ],
          [
            "Archiveid",
            "arXiv:2101.08540"
          ],
          [
            "Creators",
            "Megha Nawhal, Greg Mori"
          ],
          [
            "DOI",
            "10.48550/arXiv.2101.08540"
          ],
          [
            "Date",
            "2021-01-28 2021-01-28"
          ],
          [
            "Extra",
            "arXiv:2101.08540 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Activity Graph Transformer for Temporal Action Localization"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2101.08540"
          ]
        ],
        "resource": "storage/i2963.pdf",
        "selectable": false
      },
      {
        "text": "ActivityNet",
        "item-id": "i1161",
        "nodes": [
          {
            "text": "Caba Heilbron et al_2015_ActivityNet.pdf",
            "item-id": "i1180",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Caba Heilbron et al_2015_ActivityNet.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Caba Heilbron et al_2015_ActivityNet.pdf"
              ]
            ],
            "resource": "storage/i1180.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ActivityNet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper, we introduce ActivityNet: a new large-scale video benchmark for human activity understanding. Our new benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity categories with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 hours of video. We illustrate three scenarios in which ActivityNet can be used to benchmark and compare algorithms for human activity understanding: untrimmed video classification, trimmed activity classification and activity detection."
          ],
          [
            "Access Date",
            "2021-11-01 19:43:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, Juan Carlos Niebles"
          ],
          [
            "Date",
            "2015-00-00 2015"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "961-970"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "ActivityNet"
          ],
          [
            "Title",
            "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2015/html/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html"
          ]
        ],
        "resource": "storage/i1180.pdf",
        "selectable": false
      },
      {
        "text": "An Empirical Study of End-to-End Temporal Action Detection",
        "item-id": "i2204",
        "nodes": [
          {
            "text": "Liu et al_2022_An Empirical Study of End-to-End Temporal Action Detection.pdf",
            "item-id": "i2234",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2022_An Empirical Study of End-to-End Temporal Action Detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2022_An Empirical Study of End-to-End Temporal Action Detection.pdf"
              ]
            ],
            "resource": "storage/i2234.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "An Empirical Study of End-to-End Temporal Action Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action detection (TAD) is an important yet challenging task in video understanding. It aims to simultaneously predict the semantic label and the temporal interval of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detection head upon the encoder is optimized for TAD. The effect of end-to-end learning is not systematically evaluated. Besides, there lacks an in-depth study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learning over head-only learning and observe up to 11% performance improvement. Besides, we study the effects of multiple design choices that affect the TAD performance and speed, including detection head, video encoder, and resolution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the state-of-the-art performance of end-to-end methods while running more than 4x faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field. Code and models are available at https://github.com/xlliu7/E2E-TAD."
          ],
          [
            "Access Date",
            "2023-03-10 17:02:43"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Xiaolong Liu, Song Bai, Xiang Bai"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "20010-20019"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "An Empirical Study of End-to-End Temporal Action Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Liu_An_Empirical_Study_of_End-to-End_Temporal_Action_Detection_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2234.pdf",
        "selectable": false
      },
      {
        "text": "BMN",
        "item-id": "i951",
        "nodes": [
          {
            "text": "Lin et al_2019_BMN.pdf",
            "item-id": "i981",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lin et al_2019_BMN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lin et al_2019_BMN.pdf"
              ]
            ],
            "resource": "storage/i981.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BMN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance."
          ],
          [
            "Access Date",
            "2021-09-22 09:27:03"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3889-3898"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "BMN"
          ],
          [
            "Title",
            "BMN: Boundary-Matching Network for Temporal Action Proposal Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i981.pdf",
        "selectable": false
      },
      {
        "text": "BSN",
        "item-id": "i787",
        "nodes": [
          {
            "text": "Lin et al_2018_BSN.pdf",
            "item-id": "i836",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lin et al_2018_BSN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lin et al_2018_BSN.pdf"
              ]
            ],
            "resource": "storage/i836.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BSN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts \"local to global\" fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance."
          ],
          [
            "Access Date",
            "2021-08-04 17:15:09"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, Ming Yang"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3-19"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "BSN"
          ],
          [
            "Title",
            "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i836.pdf",
        "selectable": false
      },
      {
        "text": "BSN++",
        "item-id": "i1792",
        "nodes": [
          {
            "text": "Su et al_2021_BSN++.pdf",
            "item-id": "i1806",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Su et al_2021_BSN++.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Su et al_2021_BSN++.pdf"
              ]
            ],
            "resource": "storage/i1806.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BSN++",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task."
          ],
          [
            "Access Date",
            "2022-09-29 10:36:15"
          ],
          [
            "Creators",
            "Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, Junjie Yan"
          ],
          [
            "DOI",
            "10.1609/aaai.v35i3.16363"
          ],
          [
            "Date",
            "2021-05-18 2021-05-18"
          ],
          [
            "Extra",
            "Number: 3"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Pages",
            "2602-2610"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c) 2021 Association for the Advancement of Artificial Intelligence"
          ],
          [
            "Short Title",
            "BSN++"
          ],
          [
            "Title",
            "BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/16363"
          ],
          [
            "Volume",
            "35"
          ]
        ],
        "resource": "storage/i1806.pdf",
        "selectable": false
      },
      {
        "text": "BasicTAD",
        "item-id": "i2531",
        "nodes": [
          {
            "text": "Yang et al_2023_BasicTAD.pdf",
            "item-id": "i2742",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2023_BasicTAD.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2023_BasicTAD.pdf"
              ]
            ],
            "resource": "storage/i2742.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BasicTAD",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action detection (TAD) is extensively studied in the video understanding community by generally following the object detection pipeline in images. However, complex designs are not uncommon in TAD, such as two-stream feature extraction, multi-stage training, complex temporal modeling, and global context fusion. In this paper, we do not aim to introduce any novel technique for TAD. Instead, we study a simple, straightforward, yet must-known baseline given the current status of complex design and low detection efficiency in TAD. In our simple baseline (BasicTAD), we decompose the TAD pipeline into several essential components: data sampling, backbone design, neck construction, and detection head. We extensively investigate the existing techniques in each component for this baseline and, more importantly, perform end-to-end training over the entire pipeline thanks to the simplicity of design. As a result, this simple BasicTAD yields an astounding and real-time RGB-Only baseline very close to the state-of-the-art methods with two-stream inputs. In addition, we further improve the BasicTAD by preserving more temporal and spatial information in network representation (termed as PlusTAD). Empirical results demonstrate that our PlusTAD is very efficient and significantly outperforms the previous methods on the datasets of THUMOS14 and FineAction. Meanwhile, we also perform in-depth visualization and error analysis on our proposed method and try to provide more insights into the TAD problem. Our approach can serve as a strong baseline for future TAD research. The code and model are released at https://github.com/MCG-NJU/BasicTAD."
          ],
          [
            "Access Date",
            "2023-06-07 17:58:51"
          ],
          [
            "Creators",
            "Min Yang, Guo Chen, Yin-Dong Zheng, Tong Lu, Limin Wang"
          ],
          [
            "DOI",
            "10.1016/j.cviu.2023.103692"
          ],
          [
            "Date",
            "2023-07-01 2023-07-01"
          ],
          [
            "ISSN",
            "1077-3142"
          ],
          [
            "Journal Abbreviation",
            "Computer Vision and Image Understanding"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ScienceDirect"
          ],
          [
            "Pages",
            "103692"
          ],
          [
            "Publication Title",
            "Computer Vision and Image Understanding"
          ],
          [
            "Short Title",
            "BasicTAD"
          ],
          [
            "Title",
            "BasicTAD: An astounding RGB-Only baseline for temporal action detection"
          ],
          [
            "URL",
            "https://www.sciencedirect.com/science/article/pii/S1077314223000723"
          ],
          [
            "Volume",
            "232"
          ]
        ],
        "resource": "storage/i2742.pdf",
        "selectable": false
      },
      {
        "text": "CDC",
        "item-id": "i794",
        "nodes": [
          {
            "text": "Shou et al_2017_CDC.pdf",
            "item-id": "i847",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shou et al_2017_CDC.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shou et al_2017_CDC.pdf"
              ]
            ],
            "resource": "storage/i847.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CDC",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket.org/columbiadvmm/cdc."
          ],
          [
            "Access Date",
            "2021-08-04 17:11:41"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu Chang"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5734-5743"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "CDC"
          ],
          [
            "Title",
            "CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i847.pdf",
        "selectable": false
      },
      {
        "text": "CTAP",
        "item-id": "i948",
        "nodes": [
          {
            "text": "Gao et al_2018_CTAP.pdf",
            "item-id": "i976",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gao et al_2018_CTAP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gao et al_2018_CTAP.pdf"
              ]
            ],
            "resource": "storage/i976.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "CTAP",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-09-23 07:24:26"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Jiyang Gao, Kan Chen, Ram Nevatia"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "68-83"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "CTAP"
          ],
          [
            "Title",
            "CTAP: Complementary Temporal Action Proposal Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Jiyang_Gao_CTAP_Complementary_Temporal_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i976.pdf",
        "selectable": false
      },
      {
        "text": "Cascaded Boundary Regression for Temporal Action Detection",
        "item-id": "i2528",
        "nodes": [
          {
            "text": "Gao et al_2017_Cascaded Boundary Regression for Temporal Action Detection.pdf",
            "item-id": "i2737",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gao et al_2017_Cascaded Boundary Regression for Temporal Action Detection.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gao et al_2017_Cascaded Boundary Regression for Temporal Action Detection.pdf"
              ]
            ],
            "resource": "storage/i2737.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Cascaded Boundary Regression for Temporal Action Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classi\ufb01ers on sliding windows. Although sliding windows may contain an identi\ufb01able portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and speci\ufb01c actions are detected respec- tively in the \ufb01rst and the second stage. CBR uses temporal coordinate regression to re\ufb01ne the temporal boundaries of the sliding windows. The salient aspect of the re\ufb01nement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the re\ufb01ned windows back to the system for further boundary re\ufb01nement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%"
          ],
          [
            "Access Date",
            "2023-06-07 18:10:14"
          ],
          [
            "Conference Name",
            "British Machine Vision Conference 2017"
          ],
          [
            "Creators",
            "Jiyang Gao, Zhenheng Yang, Ram Nevatia"
          ],
          [
            "DOI",
            "10.5244/C.31.52"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "ISBN",
            "978-1-901725-60-5"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "52"
          ],
          [
            "Place",
            "London, UK"
          ],
          [
            "Proceedings Title",
            "Procedings of the British Machine Vision Conference 2017"
          ],
          [
            "Publisher",
            "British Machine Vision Association"
          ],
          [
            "Title",
            "Cascaded Boundary Regression for Temporal Action Detection"
          ],
          [
            "URL",
            "http://www.bmva.org/bmvc/2017/papers/paper052/index.html"
          ]
        ],
        "resource": "storage/i2737.pdf",
        "selectable": false
      },
      {
        "text": "Efficient Action Detection in Untrimmed Videos via Multi-task Learning",
        "item-id": "i795",
        "nodes": [
          {
            "text": "Zhu_Newsam_2017_Efficient Action Detection in Untrimmed Videos via Multi-task Learning.pdf",
            "item-id": "i849",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhu_Newsam_2017_Efficient Action Detection in Untrimmed Videos via Multi-task Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhu_Newsam_2017_Efficient Action Detection in Untrimmed Videos via Multi-task Learning.pdf"
              ]
            ],
            "resource": "storage/i849.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Efficient Action Detection in Untrimmed Videos via Multi-task Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper studies the joint learning of action recognition and temporal localization in long, untrimmed videos. We employ a multi-task learning framework that performs the three highly related steps of action proposal, action recognition, and action localization refinement in parallel instead of the standard sequential pipeline that performs the steps in order. We develop a novel temporal actionness regression module that estimates what proportion of a clip contains action. We use it for temporal localization but it could have other applications like video retrieval, surveillance, summarization, etc. We also introduce random shear augmentation during training to simulate viewpoint change. We evaluate our framework on three popular video benchmarks. Results demonstrate that our joint model is efficient in terms of storage and computation in that we do not need to compute and cache dense trajectory features, and that it is several times faster than its sequential ConvNets counterpart. Yet, despite being more efficient, it outperforms stateof-the-art methods with respect to accuracy."
          ],
          [
            "Conference Name",
            "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)"
          ],
          [
            "Creators",
            "Yi Zhu, Shawn Newsam"
          ],
          [
            "DOI",
            "10.1109/WACV.2017.29"
          ],
          [
            "Date",
            "2017-03-00 2017-03"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "197-206"
          ],
          [
            "Proceedings Title",
            "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)"
          ],
          [
            "Title",
            "Efficient Action Detection in Untrimmed Videos via Multi-task Learning"
          ]
        ],
        "resource": "storage/i849.pdf",
        "selectable": false
      },
      {
        "text": "End-To-End Learning of Action Detection From Frame Glimpses in Videos",
        "item-id": "i798",
        "nodes": [
          {
            "text": "Yeung et al_2016_End-To-End Learning of Action Detection From Frame Glimpses in Videos.pdf",
            "item-id": "i856",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yeung et al_2016_End-To-End Learning of Action Detection From Frame Glimpses in Videos.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yeung et al_2016_End-To-End Learning of Action Detection From Frame Glimpses in Videos.pdf"
              ]
            ],
            "resource": "storage/i856.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "End-To-End Learning of Action Detection From Frame Glimpses in Videos",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and whether to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's task-specific decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% or less) of the video frames."
          ],
          [
            "Access Date",
            "2021-08-04 13:49:04"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2678-2687"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "End-To-End Learning of Action Detection From Frame Glimpses in Videos"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Yeung_End-To-End_Learning_of_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i856.pdf",
        "selectable": false
      },
      {
        "text": "End-to-End Temporal Action Detection With Transformer",
        "item-id": "i2214",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n2899",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotation</p>\n<p>TadTR</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Liu et al_2022_End-to-End Temporal Action Detection With Transformer.pdf",
            "item-id": "i2250",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2022_End-to-End Temporal Action Detection With Transformer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2022_End-to-End Temporal Action Detection With Transformer.pdf"
              ]
            ],
            "resource": "storage/i2250.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "End-to-End Temporal Action Detection With Transformer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR."
          ],
          [
            "Creators",
            "Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei Zhang, Song Bai, Xiang Bai"
          ],
          [
            "DOI",
            "10.1109/TIP.2022.3195321"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "5427-5441"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Title",
            "End-to-End Temporal Action Detection With Transformer"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i2250.pdf",
        "selectable": false
      },
      {
        "text": "End-to-end, single-stream temporal action detection in untrimmed videos",
        "item-id": "i786",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1170",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>SS-TAD</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Buch et al_2019_End-to-end, single-stream temporal action detection in untrimmed videos.pdf",
            "item-id": "i827",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Buch et al_2019_End-to-end, single-stream temporal action detection in untrimmed videos.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Buch et al_2019_End-to-end, single-stream temporal action detection in untrimmed videos.pdf"
              ]
            ],
            "resource": "storage/i827.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "End-to-end, single-stream temporal action detection in untrimmed videos",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this work, we present a new intuitive, end-to-end approach for temporal action detection in untrimmed videos. We introduce our new architecture for Single-Stream Temporal Action Detection (SS-TAD), which effectively integrates joint action detection with its semantic sub-tasks in a single unifying end-to-end framework. We develop a method for training our deep recurrent architecture based on enforcing semantic constraints on intermediate modules that are gradually relaxed as learning progresses. We find that such a dynamic learning scheme enables SS-TAD to achieve higher overall detection performance, with fewer training epochs. By design, our single-pass network is very efficient and can operate at 701 frames per second, while simultaneously outperforming the state-of-the-art methods for temporal action detection on THUMOS\u201914."
          ],
          [
            "Access Date",
            "2021-08-04 17:15:54"
          ],
          [
            "Creators",
            "Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, Juan Carlos Niebles"
          ],
          [
            "DOI",
            "10.5244/c.31.93"
          ],
          [
            "Date",
            "2019-05-01 2019/05/01"
          ],
          [
            "Extra",
            "Publisher: British Machine Vision Association"
          ],
          [
            "Language",
            "English (US)"
          ],
          [
            "Library Catalog",
            "research.kaust.edu.sa"
          ],
          [
            "Publication Title",
            "Procedings of the British Machine Vision Conference 2017"
          ],
          [
            "Title",
            "End-to-end, single-stream temporal action detection in untrimmed videos"
          ],
          [
            "URL",
            "https://research.kaust.edu.sa/en/publications/end-to-end-single-stream-temporal-action-detection-in-untrimmed-v"
          ]
        ],
        "resource": "storage/i827.pdf",
        "selectable": false
      },
      {
        "text": "Exploiting Informative Video Segments for Temporal Action Localization",
        "item-id": "i809",
        "nodes": [
          {
            "text": "Sun et al_2021_Exploiting Informative Video Segments for Temporal Action Localization.pdf",
            "item-id": "i878",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Sun et al_2021_Exploiting Informative Video Segments for Temporal Action Localization.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Sun et al_2021_Exploiting Informative Video Segments for Temporal Action Localization.pdf"
              ]
            ],
            "resource": "storage/i878.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exploiting Informative Video Segments for Temporal Action Localization",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognizing actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segment-level attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks: THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with the state-of-the-art methods. Moreover, compared with the baseline method that equally treats video segments, STAN achieves significant improvements with the mAP increased from 30.4% to 39.8% on the THUMOS2014 dataset and from 31.4% to 35.9% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization."
          ],
          [
            "Creators",
            "Che Sun, Hao Song, Xinxiao Wu, Yunde Jia, Jiebo Luo"
          ],
          [
            "DOI",
            "10.1109/TMM.2021.3050067"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Multimedia"
          ],
          [
            "ISSN",
            "1941-0077"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Multimedia"
          ],
          [
            "Title",
            "Exploiting Informative Video Segments for Temporal Action Localization"
          ]
        ],
        "resource": "storage/i878.pdf",
        "selectable": false
      },
      {
        "text": "Exploring Temporal Preservation Networks for Precise Temporal Action Localization",
        "item-id": "i785",
        "nodes": [
          {
            "text": "Yang et al_2018_Exploring Temporal Preservation Networks for Precise Temporal Action.pdf",
            "item-id": "i833",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2018_Exploring Temporal Preservation Networks for Precise Temporal Action.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2018_Exploring Temporal Preservation Networks for Precise Temporal Action.pdf"
              ]
            ],
            "resource": "storage/i833.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Exploring Temporal Preservation Networks for Precise Temporal Action Localization",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments pre-determined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization with minimal loss of time information. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement in both per-frame action prediction and segment-level temporal action localization."
          ],
          [
            "Access Date",
            "2021-08-04 17:16:40"
          ],
          [
            "Creators",
            "Ke Yang, Peng Qiao, Dongsheng Li, Shaohe Lv, Yong Dou"
          ],
          [
            "Date",
            "2018-04-27 2018-04-27"
          ],
          [
            "Extra",
            "Number: 1"
          ],
          [
            "ISSN",
            "2374-3468"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "ojs.aaai.org"
          ],
          [
            "Publication Title",
            "Proceedings of the AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Copyright (c)"
          ],
          [
            "Title",
            "Exploring Temporal Preservation Networks for Precise Temporal Action Localization"
          ],
          [
            "URL",
            "https://ojs.aaai.org/index.php/AAAI/article/view/12234"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i833.pdf",
        "selectable": false
      },
      {
        "text": "FineAction",
        "item-id": "i2779",
        "nodes": [
          {
            "text": "Liu et al_2022_FineAction.pdf",
            "item-id": "i2900",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2022_FineAction.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BQ3CPN52/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/2\">II Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/2\">II-A Action Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/2\">II-B Temporal Action Localization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/3\">III FineAction Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/3\">III-A Dataset Preparation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/4\">III-B Dataset Annotation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/5\">III-C Dataset Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/5\">III-D Dataset Properties</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/6\">IV Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/6\">IV-A Dataset Split</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/6\">IV-B Untrimmed Video Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/7\">IV-C Temporal Action Localization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/8\">IV-D Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/10\">V Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BQ3CPN52/10\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2022_FineAction.pdf"
              ]
            ],
            "resource": "storage/i2900.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "FineAction",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond. The dataset is available at https://deeperaction.github.io/datasets/fineaction."
          ],
          [
            "Creators",
            "Yi Liu, Limin Wang, Yali Wang, Xiao Ma, Yu Qiao"
          ],
          [
            "DOI",
            "10.1109/TIP.2022.3217368"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6937-6950"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Short Title",
            "FineAction"
          ],
          [
            "Title",
            "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i2900.pdf",
        "selectable": false
      },
      {
        "text": "G-TAD",
        "item-id": "i1160",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1174",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>GTAD</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Xu et al_2020_G-TAD.pdf",
            "item-id": "i1175",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2020_G-TAD.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2020_G-TAD.pdf"
              ]
            ],
            "resource": "storage/i1175.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "G-TAD",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design an SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves state-of-the-art performance on two detection benchmarks. On ActivityNet-1.3 it obtains an average mAP of 34.09%; on THUMOS14 it reaches 51.6% at IoU@0.5 when combined with a proposal processing method. The code has been made available at https://github.com/frostinassiky/gtad."
          ],
          [
            "Access Date",
            "2021-11-01 19:46:23"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Mengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, Bernard Ghanem"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "10156-10165"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "G-TAD"
          ],
          [
            "Title",
            "G-TAD: Sub-Graph Localization for Temporal Action Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_G-TAD_Sub-Graph_Localization_for_Temporal_Action_Detection_CVPR_2020_paper.html"
          ]
        ],
        "resource": "storage/i1175.pdf",
        "selectable": false
      },
      {
        "text": "Graph Convolutional Networks for Temporal Action Localization",
        "item-id": "i1113",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1129",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>GTAD</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Zeng et al_2019_Graph Convolutional Networks for Temporal Action Localization.pdf",
            "item-id": "i1131",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zeng et al_2019_Graph Convolutional Networks for Temporal Action Localization.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zeng et al_2019_Graph Convolutional Networks for Temporal Action Localization.pdf"
              ]
            ],
            "resource": "storage/i1131.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Graph Convolutional Networks for Temporal Action Localization",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using GraphConvolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14(49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships."
          ],
          [
            "Access Date",
            "2021-10-26 09:54:19"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, Chuang Gan"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "7094-7103"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Graph Convolutional Networks for Temporal Action Localization"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i1131.pdf",
        "selectable": false
      },
      {
        "text": "HACS",
        "item-id": "i2774",
        "nodes": [
          {
            "text": "Zhao et al_2019_HACS.pdf",
            "item-id": "i2906",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao et al_2019_HACS.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao et al_2019_HACS.pdf"
              ]
            ],
            "resource": "storage/i2906.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "HACS",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large-scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transfer learning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1M as a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations."
          ],
          [
            "Access Date",
            "2023-07-01 08:00:28"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8668-8678"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "HACS"
          ],
          [
            "Title",
            "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_HACS_Human_Action_Clips_and_Segments_Dataset_for_Recognition_and_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i2906.pdf",
        "selectable": false
      },
      {
        "text": "Hear Me out",
        "item-id": "i2783",
        "nodes": [
          {
            "text": "Bagchi et al_2022_Hear Me out.pdf",
            "item-id": "i2907",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bagchi et al_2022_Hear Me out.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bagchi et al_2022_Hear Me out.pdf"
              ]
            ],
            "resource": "storage/i2907.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Hear Me out",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data will be made available."
          ],
          [
            "Access Date",
            "2023-07-01 07:45:44"
          ],
          [
            "Conference Name",
            "17th International Conference on Computer Vision Theory and Applications"
          ],
          [
            "Creators",
            "Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, Ravi Sarvadevabhatla"
          ],
          [
            "DOI",
            "10.5220/0010832700003124"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-989-758-555-5"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "144-154"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
          ],
          [
            "Publisher",
            "SCITEPRESS - Science and Technology Publications"
          ],
          [
            "Short Title",
            "Hear Me out"
          ],
          [
            "Title",
            "Hear Me out: Fusional Approaches for Audio Augmented Temporal Action Localization:"
          ],
          [
            "URL",
            "https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010832700003124"
          ]
        ],
        "resource": "storage/i2907.pdf",
        "selectable": false
      },
      {
        "text": "KFC",
        "item-id": "i808",
        "nodes": [
          {
            "text": "Ding et al_2021_KFC.pdf",
            "item-id": "i876",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ding et al_2021_KFC.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ding et al_2021_KFC.pdf"
              ]
            ],
            "resource": "storage/i876.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "KFC",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In temporal action localization (TAL), semi-supervised learning is a promising technique to mitigate the cost of precise boundary annotations. Semi-supervised approaches employing consistency regularization (CR), encouraging models to be robust to the perturbed inputs, have achieved great success in image classification problems. The success of CR is largely depended on the perturbations, where instances are perturbed to train a robust model without altering their semantic information. However, the perturbations for image or video classification tasks are not fit to apply to TAL. Since videos in TAL are too long to train the model with raw videos in an end-to-end manner. In this paper, we devise a method named K-farthest crossover to construct perturbations based on video features and apply it to TAL. Motivated by the observation that features in the same action instance become more and more similar during the training process while those in different action instances or backgrounds become more and more divergent, we add perturbations to each feature along temporal axis and adopt CR to encourage the model to retain this observation. Specifically, for a feature, we first find the top-k dissimilar features and average them to form a perturbation. Then, similar to chromosomal crossover, we select a large part of the feature and a small part of the perturbation to recombine a perturbed feature, which preserves the feature semantics yet enough discrepancy."
          ],
          [
            "Creators",
            "Xinpeng Ding, Nannan Wang, Xinbo Gao, Jie Li, Xiaoyu Wang, Tongliang Liu"
          ],
          [
            "DOI",
            "10.1109/TIP.2021.3099407"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Image Processing"
          ],
          [
            "ISSN",
            "1941-0042"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Image Processing"
          ],
          [
            "Short Title",
            "KFC"
          ],
          [
            "Title",
            "KFC: An Efficient Framework for Semi-supervised Temporal Action Localization"
          ]
        ],
        "resource": "storage/i876.pdf",
        "selectable": false
      },
      {
        "text": "Multi-Shot Temporal Event Localization",
        "item-id": "i1114",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1132",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>MUSES</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Liu et al_2021_Multi-Shot Temporal Event Localization.pdf",
            "item-id": "i1134",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2021_Multi-Shot Temporal Event Localization.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2021_Multi-Shot Temporal Event Localization.pdf"
              ]
            ],
            "resource": "storage/i1134.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multi-Shot Temporal Event Localization",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Current developments in temporal event or action localization usually target actions captured by a single camera. However, extensive events or actions in the wild may be captured as a sequence of shots by multiple cameras at different positions. In this paper, we propose a new and challenging task called multi-shot temporal event localization, and accordingly, collect a large-scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an average of 19 shots per instance and 176 shots per video, which induces large intra-instance variations. Our comprehensive evaluations show that the state-of-the-art method in temporal action localization only achieves an mAP of 13.1% at IoU=0.5. As a minor contribution, we present a simple baseline approach for handling the intra-instance variations, which reports an mAP of 18.9% on MUSES and 56.9% on THUMOS14 at IoU=0.5. To facilitate research in this direction, we release the dataset and the project code at https://songbai.site/muses/."
          ],
          [
            "Access Date",
            "2021-10-26 09:52:31"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Xiaolong Liu, Yao Hu, Song Bai, Fei Ding, Xiang Bai, Philip H. S. Torr"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12596-12606"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Multi-Shot Temporal Event Localization"
          ],
          [
            "Title",
            "Multi-Shot Temporal Event Localization: A Benchmark"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1134.pdf",
        "selectable": false
      },
      {
        "text": "Precise Temporal Action Localization by Evolving Temporal Proposals",
        "item-id": "i783",
        "nodes": [
          {
            "text": "Qiu et al_2018_Precise Temporal Action Localization by Evolving Temporal Proposals.pdf",
            "item-id": "i830",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Qiu et al_2018_Precise Temporal Action Localization by Evolving Temporal Proposals.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G5WTYDW5/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/2\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/2\">2.1 Action Recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/2\">2.2 Temporal Action Proposal</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/2\">2.3 Temporal Action Localization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/2\">3 Evolving Temporal Proposals</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/3\">3.1 Actionness Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/4\">3.2 Refinement Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/5\">3.3 Localization Network</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/5\">4.1 Dataset and Evaluation Metric</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/6\">4.2 Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/6\">4.3 Comparison with State-of-the-arts</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/7\">4.4 Ablation Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/7\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/8\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5WTYDW5/8\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Qiu et al_2018_Precise Temporal Action Localization by Evolving Temporal Proposals.pdf"
              ]
            ],
            "resource": "storage/i830.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Precise Temporal Action Localization by Evolving Temporal Proposals",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Locating actions in long untrimmed videos has been a challenging problem in video content analysis. The performances of existing action localization approaches remain unsatisfactory in precisely determining the beginning and the end of an action. Imitating the human perception procedure with observations and refinements, we propose a novel three-phase action localization framework. Our framework is embedded with an Actionness Network to generate initial proposals through frame-wise similarity grouping, and then a Refinement Network to conduct boundary adjustment on these proposals. Finally, the refined proposals are sent to a Localization Network for further fine-grained location regression. The whole process can be deemed as multi-stage refinement using a novel non-local pyramid feature under various temporal granularities. We evaluate our framework on THUMOS14 benchmark and obtain a significant improvement over the state-of-the-arts approaches. Specifically, the performance gain is remarkable under precise localization with high IoU thresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2%."
          ],
          [
            "Access Date",
            "2021-08-04"
          ],
          [
            "Conference Name",
            "Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval"
          ],
          [
            "Creators",
            "Haonan Qiu, Yingbin Zheng, Hao Ye, Yao Lu, Feng Wang, Liang He"
          ],
          [
            "DOI",
            "10.1145/3206025.3206029"
          ],
          [
            "Date",
            "2018-06-05 June 5, 2018"
          ],
          [
            "ISBN",
            "978-1-4503-5046-4"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "388\u2013396"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "ICMR '18"
          ],
          [
            "Title",
            "Precise Temporal Action Localization by Evolving Temporal Proposals"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3206025.3206029"
          ]
        ],
        "resource": "storage/i830.pdf",
        "selectable": false
      },
      {
        "text": "Quo Vadis, Action Recognition?",
        "item-id": "i950",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n2016",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"8\"><p>Annotations</p>\n<p>I3D</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf",
            "item-id": "i979",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf"
              ]
            ],
            "resource": "storage/i979.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Quo Vadis, Action Recognition?",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-09-23 04:12:37"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Joao Carreira, Andrew Zisserman"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "6299-6308"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "Quo Vadis, Action Recognition?"
          ],
          [
            "Title",
            "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html"
          ]
        ],
        "resource": "storage/i979.pdf",
        "selectable": false
      },
      {
        "text": "R-C3D",
        "item-id": "i790",
        "nodes": [
          {
            "text": "Xu et al_2017_R-C3D.pdf",
            "item-id": "i840",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2017_R-C3D.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2017_R-C3D.pdf"
              ]
            ],
            "resource": "storage/i840.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "R-C3D",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at http://ai.bu.edu/r-c3d/."
          ],
          [
            "Access Date",
            "2021-08-04 17:13:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Huijuan Xu, Abir Das, Kate Saenko"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5783-5792"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "R-C3D"
          ],
          [
            "Title",
            "R-C3D: Region Convolutional 3D Network for Temporal Activity Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i840.pdf",
        "selectable": false
      },
      {
        "text": "Relaxed Transformer Decoders for Direct Action Proposal Generation",
        "item-id": "i1791",
        "nodes": [
          {
            "text": "Tan et al_2021_Relaxed Transformer Decoders for Direct Action Proposal Generation.pdf",
            "item-id": "i1804",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tan et al_2021_Relaxed Transformer Decoders for Direct Action Proposal Generation.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tan et al_2021_Relaxed Transformer Decoders for Direct Action Proposal Generation.pdf"
              ]
            ],
            "resource": "storage/i1804.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Relaxed Transformer Decoders for Direct Action Proposal Generation",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action instances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer encoder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/MCG-NJU/RTD-Action."
          ],
          [
            "Access Date",
            "2022-09-29 10:36:58"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Jing Tan, Jiaqi Tang, Limin Wang, Gangshan Wu"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "13526-13535"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Relaxed Transformer Decoders for Direct Action Proposal Generation"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Tan_Relaxed_Transformer_Decoders_for_Direct_Action_Proposal_Generation_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1804.pdf",
        "selectable": false
      },
      {
        "text": "Rescaling Egocentric Vision",
        "item-id": "i2782",
        "nodes": [
          {
            "text": "Damen et al_2022_Rescaling Egocentric Vision.pdf",
            "item-id": "i2904",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Damen et al_2022_Rescaling Egocentric Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Damen et al_2022_Rescaling Egocentric Vision.pdf"
              ]
            ],
            "resource": "storage/i2904.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Rescaling Egocentric Vision",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces the pipeline to extend the largest dataset in egocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100\u00a0hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (Damen in Scaling egocentric vision: ECCV, 2018), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection enables new challenges such as action detection and evaluating the \u201ctest of time\u201d\u2014i.e. whether models trained on data collected in 2018 can generalise to new footage collected two years later. The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval\u00a0(from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics."
          ],
          [
            "Access Date",
            "2023-07-01 08:04:01"
          ],
          [
            "Creators",
            "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
          ],
          [
            "DOI",
            "10.1007/s11263-021-01531-2"
          ],
          [
            "Date",
            "2022-01-01 2022-01-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "1"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "33-55"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Short Title",
            "Rescaling Egocentric Vision"
          ],
          [
            "Title",
            "Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-021-01531-2"
          ],
          [
            "Volume",
            "130"
          ]
        ],
        "resource": "storage/i2904.pdf",
        "selectable": false
      },
      {
        "text": "Rethinking the Faster R-CNN Architecture for Temporal Action Localization",
        "item-id": "i779",
        "nodes": [
          {
            "text": "Chao et al_2018_Rethinking the Faster R-CNN Architecture for Temporal Action Localization.pdf",
            "item-id": "i829",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chao et al_2018_Rethinking the Faster R-CNN Architecture for Temporal Action Localization.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chao et al_2018_Rethinking the Faster R-CNN Architecture for Temporal Action Localization.pdf"
              ]
            ],
            "resource": "storage/i829.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Rethinking the Faster R-CNN Architecture for Temporal Action Localization",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge."
          ],
          [
            "Access Date",
            "2021-08-04 17:17:48"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia Deng, Rahul Sukthankar"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1130-1139"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Rethinking the Faster R-CNN Architecture for Temporal Action Localization"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html"
          ]
        ],
        "resource": "storage/i829.pdf",
        "selectable": false
      },
      {
        "text": "SAP",
        "item-id": "i784",
        "nodes": [
          {
            "text": "Huang et al_2018_SAP.pdf",
            "item-id": "i832",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2018_SAP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2018_SAP.pdf"
              ]
            ],
            "resource": "storage/i832.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "SAP",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose a Self-Adaptive Proposal (SAP) model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at the beginning of the video and traverse the whole video by adopting a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent\u2019s decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS\u201914 validate the effectiveness of SAP, which can achieve competitive performance with current action detection algorithms via much fewer proposals."
          ],
          [
            "Access Date",
            "2021-08-04 17:17:13"
          ],
          [
            "Conference Name",
            "Thirty-Second AAAI Conference on Artificial Intelligence"
          ],
          [
            "Creators",
            "Jingjia Huang, Nannan Li, Tao Zhang, Ge Li, Tiejun Huang, Wen Gao"
          ],
          [
            "Date",
            "2018-04-27 2018/04/27"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "www.aaai.org"
          ],
          [
            "Proceedings Title",
            "Thirty-Second AAAI Conference on Artificial Intelligence"
          ],
          [
            "Rights",
            "Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys\u2019 fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author\u2019s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author\u2019s employer, and then only on the author\u2019s or the employer\u2019s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author\u2019s or the employer\u2019s creation (including tables of contents with links to other papers) without AAAI\u2019s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void."
          ],
          [
            "Short Title",
            "SAP"
          ],
          [
            "Title",
            "SAP: Self-Adaptive Proposal Model for Temporal Action Detection Based on Reinforcement Learning"
          ],
          [
            "URL",
            "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16109"
          ]
        ],
        "resource": "storage/i832.pdf",
        "selectable": false
      },
      {
        "text": "Scaling Egocentric Vision",
        "item-id": "i2780",
        "nodes": [
          {
            "text": "Damen et al_2018_Scaling Egocentric Vision.pdf",
            "item-id": "i2905",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Damen et al_2018_Scaling Egocentric Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Damen et al_2018_Scaling Egocentric Vision.pdf"
              ]
            ],
            "resource": "storage/i2905.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Scaling Egocentric Vision",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "First-person vision is gaining interest as it offers a unique viewpoint on people\u2019s interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict non-scripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens."
          ],
          [
            "Access Date",
            "2023-07-01 08:03:51"
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "720-736"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Short Title",
            "Scaling Egocentric Vision"
          ],
          [
            "Title",
            "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html"
          ]
        ],
        "resource": "storage/i2905.pdf",
        "selectable": false
      },
      {
        "text": "Single Shot Temporal Action Detection",
        "item-id": "i788",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n1171",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"5\"><p>Annotations</p>\n<p>SSAD</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Lin et al_2017_Single Shot Temporal Action Detection.pdf",
            "item-id": "i837",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lin et al_2017_Single Shot Temporal Action Detection.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_48DWRK48/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_48DWRK48/3\">3 Our Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/3\">3.1 Problem Definition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/3\">3.2 Extracting of Snippet-level Action Scores</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/4\">3.3 SSAD Network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/5\">3.4 Training of SSAD network</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/6\">3.5 Prediction and post-processing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_48DWRK48/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/6\">4.1 Dataset and setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/6\">4.2 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/6\">4.3 Comparison with state-of-the-art systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/7\">4.4 Model Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/8\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_48DWRK48/9\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lin et al_2017_Single Shot Temporal Action Detection.pdf"
              ]
            ],
            "resource": "storage/i837.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Single Shot Temporal Action Detection",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the \"detection by classification\" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from $19.0%$ to $24.6%$ on THUMOS 2014 and from 7.4% to $11.0%$ on MEXaction2."
          ],
          [
            "Access Date",
            "2021-08-04"
          ],
          [
            "Conference Name",
            "Proceedings of the 25th ACM international conference on Multimedia"
          ],
          [
            "Creators",
            "Tianwei Lin, Xu Zhao, Zheng Shou"
          ],
          [
            "DOI",
            "10.1145/3123266.3123343"
          ],
          [
            "Date",
            "2017-10-19 October 19, 2017"
          ],
          [
            "ISBN",
            "978-1-4503-4906-2"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "988\u2013996"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 25th ACM international conference on Multimedia"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "MM '17"
          ],
          [
            "Title",
            "Single Shot Temporal Action Detection"
          ],
          [
            "URL",
            "http://doi.org/10.1145/3123266.3123343"
          ]
        ],
        "resource": "storage/i837.pdf",
        "selectable": false
      },
      {
        "text": "TSP",
        "item-id": "i1111",
        "nodes": [
          {
            "text": "Alwassel et al_2021_TSP.pdf",
            "item-id": "i1127",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Alwassel et al_2021_TSP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Alwassel et al_2021_TSP.pdf"
              ]
            ],
            "resource": "storage/i1127.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TSP",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Due to the large memory footprint of untrimmed videos, current state-of-the-art video localization methods operate atop precomputed video clip features. These features are extracted from video encoders typically trained for trimmed action classification tasks, making such features not necessarily suitable for temporal localization. In this work, we propose a novel supervised pretraining paradigm for clip features that not only trains to classify activities but also considers background clips and global video information to improve temporal sensitivity. Extensive experiments show that using features trained with our novel pretraining strategy significantly improves the performance of recent state-of-the-art methods on three tasks: Temporal Action Localization, Action Proposal Generation, and Dense Video Captioning. We also show that our pretraining approach is effective across three encoder architectures and two pretraining datasets. We believe video feature encoding is an important building block for localization algorithms, and extracting temporally-sensitive features should be of paramount importance in building more accurate models. The code and pretrained models are available on our project website."
          ],
          [
            "Access Date",
            "2021-10-26 11:14:48"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Humam Alwassel, Silvio Giancola, Bernard Ghanem"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3173-3183"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "TSP"
          ],
          [
            "Title",
            "TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Alwassel_TSP_Temporally-Sensitive_Pretraining_of_Video_Encoders_for_Localization_Tasks_ICCVW_2021_paper.html"
          ]
        ],
        "resource": "storage/i1127.pdf",
        "selectable": false
      },
      {
        "text": "TURN TAP",
        "item-id": "i949",
        "nodes": [
          {
            "text": "Gao et al_2017_TURN TAP.pdf",
            "item-id": "i977",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gao et al_2017_TURN TAP.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gao et al_2017_TURN TAP.pdf"
              ]
            ],
            "resource": "storage/i977.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TURN TAP",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2021-09-23 07:20:16"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, Ram Nevatia"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3628-3636"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Short Title",
            "TURN TAP"
          ],
          [
            "Title",
            "TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Gao_TURN_TAP_Temporal_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i977.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Action Detection With Structured Segment Networks",
        "item-id": "i793",
        "nodes": [
          {
            "text": "Zhao et al_2017_Temporal Action Detection With Structured Segment Networks.pdf",
            "item-id": "i846",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhao et al_2017_Temporal Action Detection With Structured Segment Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhao et al_2017_Temporal Action Detection With Structured Segment Networks.pdf"
              ]
            ],
            "resource": "storage/i846.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Action Detection With Structured Segment Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS'14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures."
          ],
          [
            "Access Date",
            "2021-08-04 17:12:17"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, Dahua Lin"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "2914-2923"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Temporal Action Detection With Structured Segment Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Zhao_Temporal_Action_Detection_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i846.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Action Localization With Pyramid of Score Distribution Features",
        "item-id": "i797",
        "nodes": [
          {
            "text": "Yuan et al_2016_Temporal Action Localization With Pyramid of Score Distribution Features.pdf",
            "item-id": "i854",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yuan et al_2016_Temporal Action Localization With Pyramid of Score Distribution Features.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yuan et al_2016_Temporal Action Localization With Pyramid of Score Distribution Features.pdf"
              ]
            ],
            "resource": "storage/i854.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Action Localization With Pyramid of Score Distribution Features",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We investigate the feature design and classification architectures in temporal action localization. This application focuses on detecting and labeling actions in untrimmed videos, which brings more challenge than classifying pre-segmented videos. The major difficulty for action localization is the uncertainty of action occurrence and utilization of information from different scales. Two innovations are proposed to address this issue. First, we propose a Pyramid of Score Distribution Feature (PSDF) to capture the motion information at multiple resolutions centered at each detection window. This novel feature mitigates the influence of unknown action position and duration, and shows significant performance gain over previous detection approaches. Second, inter-frame consistency is further explored by incorporating PSDF into the state-of-the-art Recurrent Neural Networks, which gives additional performance gain in detecting actions in temporally untrimmed videos. We tested our action localization framework on the THUMOS'15 and MPII Cooking Activities Dataset, both of which show a large performance improvement over previous attempts."
          ],
          [
            "Access Date",
            "2021-08-04 13:50:03"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Jun Yuan, Bingbing Ni, Xiaokang Yang, Ashraf A. Kassim"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3093-3102"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Temporal Action Localization With Pyramid of Score Distribution Features"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Yuan_Temporal_Action_Localization_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i854.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs",
        "item-id": "i796",
        "nodes": [
          {
            "text": "Shou et al_2016_Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs.pdf",
            "item-id": "i852",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shou et al_2016_Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shou et al_2016_Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs.pdf"
              ]
            ],
            "resource": "storage/i852.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We address temporal action localization in untrimmed long videos. This is important because videos in real applications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal action localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as initialization for the localization network; and (3) a localization network fine-tunes the learned classification network to localize each action instance. We propose a novel loss function for the localization network to explicitly consider temporal overlap and achieve high temporal localization accuracy. In the end, only the proposal network and the localization network are used during prediction. On two large-scale benchmarks, our approach achieves significantly superior performances compared with other state-of-the-art systems: mAP increases from 1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014."
          ],
          [
            "Access Date",
            "2021-08-04 13:53:08"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zheng Shou, Dongang Wang, Shih-Fu Chang"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "1049-1058"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_cvpr_2016/html/Shou_Temporal_Action_Localization_CVPR_2016_paper.html"
          ]
        ],
        "resource": "storage/i852.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Action Segmentation From Timestamp Supervision",
        "item-id": "i986",
        "nodes": [
          {
            "text": "Li et al_2021_Temporal Action Segmentation From Timestamp Supervision.pdf",
            "item-id": "i995",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2021_Temporal Action Segmentation From Timestamp Supervision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2021_Temporal Action Segmentation From Timestamp Supervision.pdf"
              ]
            ],
            "resource": "storage/i995.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Action Segmentation From Timestamp Supervision",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action segmentation approaches have been very successful recently. However, annotating videos with frame-wise labels to train such models is very expensive and time consuming. While weakly supervised methods trained using only ordered action lists require less annotation effort, the performance is still worse than fully supervised approaches. In this paper, we propose to use timestamp supervision for the temporal action segmentation task. Timestamps require a comparable annotation effort to weakly supervised approaches, and yet provide a more supervisory signal. To demonstrate the effectiveness of timestamp supervision, we propose an approach to train a segmentation model using only timestamps annotations. Our approach uses the model output and the annotated timestamps to generate frame-wise labels by detecting the action changes. We further introduce a confidence loss that forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This ensures that all and not only the most distinctive frames of an action are learned during training. The evaluation on four datasets shows that models trained with timestamps annotations achieve comparable performance to the fully supervised approaches."
          ],
          [
            "Access Date",
            "2021-10-10 12:45:52"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zhe Li, Yazan Abu Farha, Jurgen Gall"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "8365-8374"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Temporal Action Segmentation From Timestamp Supervision"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Li_Temporal_Action_Segmentation_From_Timestamp_Supervision_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i995.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Context Aggregation Network for Temporal Action Proposal Refinement",
        "item-id": "i782",
        "nodes": [
          {
            "text": "Qing et al_2021_Temporal Context Aggregation Network for Temporal Action Proposal Refinement.pdf",
            "item-id": "i825",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Qing et al_2021_Temporal Context Aggregation Network for Temporal Action Proposal Refinement.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Qing et al_2021_Temporal Context Aggregation Network for Temporal Action Proposal Refinement.pdf"
              ]
            ],
            "resource": "storage/i825.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Context Aggregation Network for Temporal Action Proposal Refinement",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through local and global temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both local and global temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task."
          ],
          [
            "Access Date",
            "2021-08-06 03:17:35"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, Wei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao, Nong Sang"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "485-494"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Temporal Context Aggregation Network for Temporal Action Proposal Refinement"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Qing_Temporal_Context_Aggregation_Network_for_Temporal_Action_Proposal_Refinement_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i825.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Context Network for Activity Localization in Videos",
        "item-id": "i792",
        "nodes": [
          {
            "text": "Dai et al_2017_Temporal Context Network for Activity Localization in Videos.pdf",
            "item-id": "i844",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dai et al_2017_Temporal Context Network for Activity Localization in Videos.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dai et al_2017_Temporal Context Network for Activity Localization in Videos.pdf"
              ]
            ],
            "resource": "storage/i844.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Context Network for Activity Localization in Videos",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a Temporal Context Network (TCN) for precise temporal localization of human activities. Similar to the Faster-RCNN architecture, proposals are placed at equal intervals in a video which span multiple temporal scales. We propose a novel representation for ranking these proposals. Since pooling features only inside a segment is not sufficient to predict activity boundaries, we construct a representation which explicitly captures context around a proposal for ranking it. For each temporal segment inside a proposal, features are uniformly sampled at a pair of scales and are input to a temporal convolutional neural network for classification. After ranking proposals, non-maximum suppression is applied and classification is performed to obtain final detections. TCN outperforms state-of-the-art methods on the ActivityNet dataset and the THUMOS14 dataset."
          ],
          [
            "Access Date",
            "2021-08-04 17:12:40"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S. Davis, Yan Qiu Chen"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "5793-5802"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE International Conference on Computer Vision"
          ],
          [
            "Title",
            "Temporal Context Network for Activity Localization in Videos"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Temporal_Context_Network_ICCV_2017_paper.html"
          ]
        ],
        "resource": "storage/i844.pdf",
        "selectable": false
      },
      {
        "text": "Temporal Segment Networks",
        "item-id": "i1112",
        "nodes": [
          {
            "text": "Wang et al_2016_Temporal Segment Networks.pdf",
            "item-id": "i1128",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2016_Temporal Segment Networks.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WYBNEX6F/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/3\">2 Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/4\">3 Action Recognition with Temporal Segment Networks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/4\">3.1 Temporal Segment Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/6\">3.2 Learning Temporal Segment Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/8\">3.3 Testing Temporal Segment Networks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/9\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/9\">4.1 Datasets and Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/10\">4.2 Exploration Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/11\">4.3 Evaluation of Temporal Segment Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/12\">4.4 Comparison with the State of the Art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/13\">4.5 Model Visualization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/14\">5 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WYBNEX6F/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2016_Temporal Segment Networks.pdf"
              ]
            ],
            "resource": "storage/i1128.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Temporal Segment Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 (69.4%69.4% 69.4\\,\\% ) and UCF101 (94.2%94.2% 94.2\\,\\% ). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks)."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling"
          ],
          [
            "DOI",
            "10.1007/978-3-319-46484-8_2"
          ],
          [
            "Date",
            "2016-00-00 2016"
          ],
          [
            "ISBN",
            "978-3-319-46484-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "20-36"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "Temporal Segment Networks"
          ],
          [
            "Title",
            "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition"
          ]
        ],
        "resource": "storage/i1128.pdf",
        "selectable": false
      },
      {
        "text": "The EPIC-KITCHENS Dataset",
        "item-id": "i2781",
        "nodes": [
          {
            "text": "Damen et al_2021_The EPIC-KITCHENS Dataset.pdf",
            "item-id": "i2902",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Damen et al_2021_The EPIC-KITCHENS Dataset.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8VY4TJB5/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/2\">2 Related Datasets</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/3\">3 The EPIC-KITCHENS Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/3\">3.1 Data Collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/4\">3.2 Action Segment Annotations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/6\">3.3 Active Object Bounding Box Annotations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/6\">3.4 Verb and Noun Classes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/7\">3.5 Annotation Visualisations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/8\">3.6 Annotation Quality Assurance</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/8\">4 Benchmarks and Baseline Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/9\">4.1 Object Detection Benchmark</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/10\">4.2 Action Recognition Benchmark</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/13\">4.3 Action Anticipation Benchmark</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/14\">5 Conclusion and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/15\">References</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Dima Damen</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Hazel Doughty</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Giovanni Maria Farinella</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Sanja Fidler</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Antonino Furnari</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Evangelos Kazakos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Davide Moltisanti</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Jonathan Munro</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Toby Perrett</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Will Price</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/16\">Michael Wray</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8VY4TJB5/18\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Damen et al_2021_The EPIC-KITCHENS Dataset.pdf"
              ]
            ],
            "resource": "storage/i2902.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The EPIC-KITCHENS Dataset",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the largest egocentric video benchmark, offering a unique viewpoint on people\u2019s interaction with objects, their attention, and even intention. In this paper, we detail how this large-scale dataset was captured by 32 participants in their native kitchen environments, and densely annotated with actions and object interactions. Our videos depict nonscripted daily activities, as recording is started every time a participant entered their kitchen. Recording took place in four countries by participants belonging to ten different nationalities, resulting in highly diverse kitchen habits and cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.2K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. We introduce new baselines that highlight the multimodal nature of the dataset and the importance of explicit temporal modelling to discriminate fine-grained actions (e.g., \u2018closing a tap\u2019 from \u2018opening\u2019 it up)."
          ],
          [
            "Creators",
            "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
          ],
          [
            "DOI",
            "10.1109/TPAMI.2020.2991965"
          ],
          [
            "Date",
            "2021-11-00 2021-11"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "ISSN",
            "1939-3539"
          ],
          [
            "Issue",
            "11"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "4125-4141"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence"
          ],
          [
            "Short Title",
            "The EPIC-KITCHENS Dataset"
          ],
          [
            "Title",
            "The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines"
          ],
          [
            "Volume",
            "43"
          ]
        ],
        "resource": "storage/i2902.pdf",
        "selectable": false
      },
      {
        "text": "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\"",
        "item-id": "i1162",
        "nodes": [
          {
            "text": "Comment: Preprint submitted to Computer Vision and Image Understanding",
            "item-id": "n1181",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Preprint submitted to Computer Vision and Image Understanding",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Preprint submitted to Computer Vision and Image Understanding</div>",
            "node_type": "note"
          },
          {
            "text": "Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf",
            "item-id": "i1178",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf"
              ]
            ],
            "resource": "storage/i1178.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\"",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Automatically recognizing and localizing wide ranges of human actions has crucial importance for video understanding. Towards this goal, the THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including THUMOS challenge, had focused primarily on the classification of pre-segmented (i.e., trimmed) videos, which is an artificial task. In THUMOS 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include `background videos' which share similar scenes and backgrounds as action videos, but are devoid of the specific actions. The three editions of the challenge organized in 2013--2015 have made THUMOS a common benchmark for action classification and detection and the annual challenge is widely attended by teams from around the world. In this paper we describe the THUMOS benchmark in detail and give an overview of data collection and annotation procedures. We present the evaluation protocols used to quantify results in the two THUMOS tasks of action classification and temporal detection. We also present results of submissions to the THUMOS 2015 challenge and review the participating approaches. Additionally, we include a comprehensive empirical study evaluating the differences in action recognition between trimmed and untrimmed videos, and how well methods trained on trimmed videos generalize to untrimmed videos. We conclude by proposing several directions and improvements for future THUMOS challenges."
          ],
          [
            "Access Date",
            "2021-11-01 19:43:05"
          ],
          [
            "Creators",
            "Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, Mubarak Shah"
          ],
          [
            "DOI",
            "10.1016/j.cviu.2016.10.018"
          ],
          [
            "Date",
            "2017-02-00 02/2017"
          ],
          [
            "Extra",
            "arXiv: 1604.06182"
          ],
          [
            "ISSN",
            "10773142"
          ],
          [
            "Journal Abbreviation",
            "Computer Vision and Image Understanding"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Pages",
            "1-23"
          ],
          [
            "Publication Title",
            "Computer Vision and Image Understanding"
          ],
          [
            "Title",
            "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\""
          ],
          [
            "URL",
            "http://arxiv.org/abs/1604.06182"
          ],
          [
            "Volume",
            "155"
          ]
        ],
        "resource": "storage/i1178.pdf",
        "selectable": false
      },
      {
        "text": "TriDet",
        "item-id": "i2530",
        "nodes": [
          {
            "text": "Shi et al_2023_TriDet.pdf",
            "item-id": "i2740",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shi et al_2023_TriDet.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_84THEV6N/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/2\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_84THEV6N/2\">3 . Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/3\">3.1 . Method Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/3\">3.2 . Feature Pyramid with SGP Layer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/4\">3.3 . Trident-head with Relative Boundary Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/5\">3.4 . Training and Inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_84THEV6N/5\">4 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/6\">4.1 . Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/6\">4.2 . Main Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/7\">4.3 . Ablation Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/8\">5 . Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_84THEV6N/11\">A . Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/11\">A.1 . Network Architecture in Feature Pyramid</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/11\">A.2 . The rank loss problem in Transformer.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/13\">A.3 . Error Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_84THEV6N/14\">A.4 . Qualitative Analysis</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shi et al_2023_TriDet.pdf"
              ]
            ],
            "resource": "storage/i2740.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TriDet",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we present a one-stage framework TriDet for temporal action detection. Existing methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose a Scalable-Granularity Perception (SGP) layer to aggregate information across different temporal granularities, which is much more efficient than the recent transformer-based feature pyramid. Benefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. For example, TriDet hits an average mAP of 69.3% on THUMOS14, outperforming the previous best by 2.5%, but with only 74.6% of its latency."
          ],
          [
            "Access Date",
            "2023-06-07 18:00:03"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, Dacheng Tao"
          ],
          [
            "Date",
            "2023-00-00 2023"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "18857-18866"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "TriDet"
          ],
          [
            "Title",
            "TriDet: Temporal Action Detection With Relative Boundary Modeling"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2023/html/Shi_TriDet_Temporal_Action_Detection_With_Relative_Boundary_Modeling_CVPR_2023_paper.html"
          ]
        ],
        "resource": "storage/i2740.pdf",
        "selectable": false
      }
    ],
    "item_title": "Temporal Action Localization",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Text Style Transfer",
    "item-id": "c16,i1432",
    "nodes": [
      {
        "text": "A Review of Text Style Transfer using Deep Learning",
        "item-id": "i1256",
        "nodes": [
          {
            "text": "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf",
            "item-id": "i1259",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Toshevska_Gievska_2021_A Review of Text Style Transfer using Deep Learning.pdf"
              ]
            ],
            "resource": "storage/i1259.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "A Review of Text Style Transfer using Deep Learning",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves, however, they adjust their speaking and writing style to a social context, an audience, an interlocutor or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence. A systematic review of text style transfer methodologies using deep learning is presented in this paper. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field."
          ],
          [
            "Creators",
            "Martina Toshevska, Sonja Gievska"
          ],
          [
            "DOI",
            "10.1109/TAI.2021.3115992"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Extra",
            "Conference Name: IEEE Transactions on Artificial Intelligence"
          ],
          [
            "ISSN",
            "2691-4581"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "1-1"
          ],
          [
            "Publication Title",
            "IEEE Transactions on Artificial Intelligence"
          ],
          [
            "Title",
            "A Review of Text Style Transfer using Deep Learning"
          ]
        ],
        "resource": "storage/i1259.pdf",
        "selectable": false
      },
      {
        "text": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer",
        "item-id": "i1286",
        "nodes": [
          {
            "text": "Comment: COLING 2020",
            "item-id": "n1291",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: COLING 2020",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: COLING 2020</div>",
            "node_type": "note"
          },
          {
            "text": "Huang et al_2020_Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer.pdf",
            "item-id": "i1290",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Huang et al_2020_Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_K7PWPWPC/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/2\">2 Related work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/4\">3 CAE: Cycle-consistent Adversarial Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/4\">3.1 LSTM autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/4\">3.2 Adversarial style transfer networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/5\">3.3 Cycle-consistent constraint</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/5\">3.4 Training and inference</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1 Experimental setup</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.2 Baselines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.3 Hyper-parameter settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/6\">4.1.4 Evaluation metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/7\">4.2 Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/7\">4.2.1 Yelp restaurant reviews sentiment transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/7\">4.2.2 Yahoo questions topic transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.2.3 Human evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.3 Ablation study</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.4 Analyses</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/8\">4.4.1 Style-transferred sentences</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/9\">4.4.2 Comparison with the nearest neighbour sequences from training data</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K7PWPWPC/9\">5 Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Huang et al_2020_Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer.pdf"
              ]
            ],
            "resource": "storage/i1290.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Unsupervised text style transfer is full of challenges due to the lack of parallel data and difficulties in content preservation. In this paper, we propose a novel neural approach to unsupervised text style transfer, which we refer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from non-parallel data. CAE consists of three essential components: (1) LSTM autoencoders that encode a text in one style into its latent representation and decode an encoded representation into its original text or a transferred representation into a style-transferred text, (2) adversarial style transfer networks that use an adversarially trained generator to transform a latent representation in one style into a representation in another style, and (3) a cycle-consistent constraint that enhances the capacity of the adversarial style transfer networks in content preservation. The entire CAE with these three components can be trained end-to-end. Extensive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation."
          ],
          [
            "Access Date",
            "2022-01-16 11:58:46"
          ],
          [
            "Creators",
            "Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, Feiyu Xu"
          ],
          [
            "Date",
            "2020-10-01 2020-10-01"
          ],
          [
            "Extra",
            "arXiv: 2010.00735"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2010.00735 [cs]"
          ],
          [
            "Title",
            "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2010.00735"
          ]
        ],
        "resource": "storage/i1290.pdf",
        "selectable": false
      },
      {
        "text": "Dear Sir or Madam, May I introduce the GYAFC Dataset",
        "item-id": "i1432",
        "nodes": [
          {
            "text": "Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human ",
            "item-id": "n1482",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human ",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: To appear in the proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf",
            "item-id": "i1481",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Rao_Tetreault_2018_Dear Sir or Madam, May I introduce the GYAFC Dataset.pdf"
              ]
            ],
            "resource": "storage/i1481.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Dear Sir or Madam, May I introduce the GYAFC Dataset",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics."
          ],
          [
            "Access Date",
            "2022-03-29 13:28:17"
          ],
          [
            "Creators",
            "Sudha Rao, Joel Tetreault"
          ],
          [
            "Date",
            "2018-04-16 2018-04-16"
          ],
          [
            "Extra",
            "arXiv: 1803.06535"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1803.06535 [cs]"
          ],
          [
            "Short Title",
            "Dear Sir or Madam, May I introduce the GYAFC Dataset"
          ],
          [
            "Title",
            "Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1803.06535"
          ]
        ],
        "resource": "storage/i1481.pdf",
        "selectable": false
      },
      {
        "text": "Delete, Retrieve, Generate",
        "item-id": "i1277",
        "nodes": [
          {
            "text": "Comment: NAACL 2018",
            "item-id": "n1284",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NAACL 2018",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NAACL 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Li et al_2018_Delete, Retrieve, Generate.pdf",
            "item-id": "i1283",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2018_Delete, Retrieve, Generate.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2018_Delete, Retrieve, Generate.pdf"
              ]
            ],
            "resource": "storage/i1283.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Delete, Retrieve, Generate",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing \"screen is just the right size\" to \"screen is too small\"). Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., \"too small\"). Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous."
          ],
          [
            "Access Date",
            "2021-12-21 04:47:12"
          ],
          [
            "Creators",
            "Juncen Li, Robin Jia, He He, Percy Liang"
          ],
          [
            "Date",
            "2018-04-17 2018-04-17"
          ],
          [
            "Extra",
            "arXiv: 1804.06437"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1804.06437 [cs]"
          ],
          [
            "Short Title",
            "Delete, Retrieve, Generate"
          ],
          [
            "Title",
            "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1804.06437"
          ]
        ],
        "resource": "storage/i1283.pdf",
        "selectable": false
      },
      {
        "text": "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer",
        "item-id": "i1430",
        "nodes": [
          {
            "text": "Comment: ACL 2018",
            "item-id": "n1476",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: ACL 2018",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: ACL 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Santos et al_2018_Fighting Offensive Language on Social Media with Unsupervised Text Style.pdf",
            "item-id": "i1475",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Santos et al_2018_Fighting Offensive Language on Social Media with Unsupervised Text Style.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Santos et al_2018_Fighting Offensive Language on Social Media with Unsupervised Text Style.pdf"
              ]
            ],
            "resource": "storage/i1475.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences."
          ],
          [
            "Access Date",
            "2022-03-29 13:30:14"
          ],
          [
            "Creators",
            "Cicero Nogueira dos Santos, Igor Melnyk, Inkit Padhi"
          ],
          [
            "Date",
            "2018-05-19 2018-05-19"
          ],
          [
            "Extra",
            "arXiv: 1805.07685"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1805.07685 [cs]"
          ],
          [
            "Title",
            "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1805.07685"
          ]
        ],
        "resource": "storage/i1475.pdf",
        "selectable": false
      },
      {
        "text": "How Positive Are You",
        "item-id": "i1287",
        "nodes": [
          {
            "text": "Kim_Sohn_2020_How Positive Are You.pdf",
            "item-id": "i1292",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Kim_Sohn_2020_How Positive Are You.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Kim_Sohn_2020_How Positive Are You.pdf"
              ]
            ],
            "resource": "storage/i1292.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "How Positive Are You",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The prevalent approach for unsupervised text style transfer is disentanglement between content and style. However, it is difficult to completely separate style information from the content. Other approaches allow the latent text representation to contain style and the target style to affect the generated output more than the latent representation does. In both approaches, however, it is impossible to adjust the strength of the style in the generated output. Moreover, those previous approaches typically perform both the sentence reconstruction and style control tasks in a single model, which complicates the overall architecture. In this paper, we address these issues by separating the model into a sentence reconstruction module and a style module. We use the Transformer-based autoencoder model for sentence reconstruction and the adaptive style embedding is learned directly in the style module. Because of this separation, each module can better focus on its own task. Moreover, we can vary the style strength of the generated sentence by changing the style of the embedding expression. Therefore, our approach not only controls the strength of the style, but also simplifies the model architecture. Experimental results show that our approach achieves better style transfer performance and content preservation than previous approaches."
          ],
          [
            "Access Date",
            "2022-01-16 11:44:50"
          ],
          [
            "Conference Name",
            "COLING 2020"
          ],
          [
            "Creators",
            "Heejin Kim, Kyung-Ah Sohn"
          ],
          [
            "DOI",
            "10.18653/v1/2020.coling-main.191"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "2115\u20132125"
          ],
          [
            "Place",
            "Barcelona, Spain (Online)"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 28th International Conference on Computational Linguistics"
          ],
          [
            "Publisher",
            "International Committee on Computational Linguistics"
          ],
          [
            "Short Title",
            "How Positive Are You"
          ],
          [
            "Title",
            "How Positive Are You: Text Style Transfer using Adaptive Style Embedding"
          ],
          [
            "URL",
            "https://aclanthology.org/2020.coling-main.191"
          ]
        ],
        "resource": "storage/i1292.pdf",
        "selectable": false
      },
      {
        "text": "Multiple-Attribute Text Rewriting",
        "item-id": "i1275",
        "nodes": [
          {
            "text": "Lample et al_2018_Multiple-Attribute Text Rewriting.pdf",
            "item-id": "i1279",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lample et al_2018_Multiple-Attribute Text Rewriting.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CG3FYHA9/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/3\">Controllable Text Rewriting</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/3\">Are Adversarial Models really Doing Disentanglement?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/4\">Our Approach</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/5\">Implementation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/6\">Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/7\">Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/7\">Model selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/8\">Comparisons to Prior Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/9\">Evaluating Multiple Attribute Control</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/9\">Ablation study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/10\">Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/14\">Supplementary Material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/14\">Training Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/14\">Dataset Creation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CG3FYHA9/16\">Additional Qualitative Examples</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lample et al_2018_Multiple-Attribute Text Rewriting.pdf"
              ]
            ],
            "resource": "storage/i1279.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multiple-Attribute Text Rewriting",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "A system for rewriting text conditioned on multiple controllable attributes"
          ],
          [
            "Access Date",
            "2021-12-21 04:54:30"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Guillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc'Aurelio Ranzato, Y.-Lan Boureau"
          ],
          [
            "Date",
            "2018-09-27 2018/09/27"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Title",
            "Multiple-Attribute Text Rewriting"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=H1g2NhC5KQ"
          ]
        ],
        "resource": "storage/i1279.pdf",
        "selectable": false
      },
      {
        "text": "Politeness Transfer",
        "item-id": "i1431",
        "nodes": [
          {
            "text": "Comment: To appear at ACL 2020",
            "item-id": "n1479",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: To appear at ACL 2020",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: To appear at ACL 2020</div>",
            "node_type": "note"
          },
          {
            "text": "Madaan et al_2020_Politeness Transfer.pdf",
            "item-id": "i1478",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Madaan et al_2020_Politeness Transfer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Madaan et al_2020_Politeness Transfer.pdf"
              ]
            ],
            "resource": "storage/i1478.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Politeness Transfer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate."
          ],
          [
            "Access Date",
            "2022-03-29 13:29:34"
          ],
          [
            "Creators",
            "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black, Shrimai Prabhumoye"
          ],
          [
            "Date",
            "2020-05-01 2020-05-01"
          ],
          [
            "Extra",
            "arXiv: 2004.14257"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:2004.14257 [cs]"
          ],
          [
            "Short Title",
            "Politeness Transfer"
          ],
          [
            "Title",
            "Politeness Transfer: A Tag and Generate Approach"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2004.14257"
          ]
        ],
        "resource": "storage/i1478.pdf",
        "selectable": false
      },
      {
        "text": "Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis",
        "item-id": "i1252",
        "nodes": [
          {
            "text": "Lee et al_2018_Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance.pdf",
            "item-id": "i1254",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lee et al_2018_Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lee et al_2018_Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance.pdf"
              ]
            ],
            "resource": "storage/i1254.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Conventional seq2seq chatbot models only try to find the sentences with the highest probabilities conditioned on the input sequences, without considering the sentiment of the output sentences. Some research works trying to modify the sentiment of the output sequences were reported. In this paper, we propose five models to scale or adjust the sentiment of the chatbot response: persona-based model, reinforcement learning, plug and play model, sentiment transformation network and cycleGAN, all based on the conventional seq2seq model. We also develop two evaluation metrics to estimate if the responses are reasonable given the input. These metrics together with other two popularly used metrics were used to analyze the performance of the five proposed models on different aspects, and reinforcement learning and cycleGAN were shown to be very attractive. The evaluation metrics were also found to be well correlated with human evaluation."
          ],
          [
            "Conference Name",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Creators",
            "Chih-Wei Lee, Yau-Shian Wang, Tsung-Yuan Hsu, Kuan-Yu Chen, Hung-Yi Lee, Lin-Shan Lee"
          ],
          [
            "DOI",
            "10.1109/ICASSP.2018.8461377"
          ],
          [
            "Date",
            "2018-04-00 2018-04"
          ],
          [
            "Extra",
            "ISSN: 2379-190X"
          ],
          [
            "Library Catalog",
            "IEEE Xplore"
          ],
          [
            "Pages",
            "6164-6168"
          ],
          [
            "Proceedings Title",
            "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
          ],
          [
            "Title",
            "Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis"
          ]
        ],
        "resource": "storage/i1254.pdf",
        "selectable": false
      },
      {
        "text": "Style Transformer",
        "item-id": "i1276",
        "nodes": [
          {
            "text": "Dai et al_2019_Style Transformer.pdf",
            "item-id": "i1281",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dai et al_2019_Style Transformer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dai et al_2019_Style Transformer.pdf"
              ]
            ],
            "resource": "storage/i1281.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Style Transformer",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation."
          ],
          [
            "Access Date",
            "2021-12-21 04:52:37"
          ],
          [
            "Creators",
            "Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang"
          ],
          [
            "Date",
            "2019-08-20 2019-08-20"
          ],
          [
            "Extra",
            "arXiv: 1905.05621"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1905.05621 [cs]"
          ],
          [
            "Short Title",
            "Style Transformer"
          ],
          [
            "Title",
            "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1905.05621"
          ]
        ],
        "resource": "storage/i1281.pdf",
        "selectable": false
      },
      {
        "text": "Unsupervised Text Style Transfer using Language Models as Discriminators",
        "item-id": "i1418",
        "nodes": [
          {
            "text": "Yang et al_2018_Unsupervised Text Style Transfer using Language Models as Discriminators.pdf",
            "item-id": "i1451",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yang et al_2018_Unsupervised Text Style Transfer using Language Models as Discriminators.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yang et al_2018_Unsupervised Text Style Transfer using Language Models as Discriminators.pdf"
              ]
            ],
            "resource": "storage/i1451.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Unsupervised Text Style Transfer using Language Models as Discriminators",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2022-04-04 08:33:01"
          ],
          [
            "Creators",
            "Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, Taylor Berg-Kirkpatrick"
          ],
          [
            "Date",
            "2018-00-00 2018"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Unsupervised Text Style Transfer using Language Models as Discriminators"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2018/hash/398475c83b47075e8897a083e97eb9f0-Abstract.html"
          ],
          [
            "Volume",
            "31"
          ]
        ],
        "resource": "storage/i1451.pdf",
        "selectable": false
      },
      {
        "text": "Yelp Dataset Challenge",
        "item-id": "i1429",
        "nodes": [
          {
            "text": "Asghar_2016_Yelp Dataset Challenge.pdf",
            "item-id": "i1473",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Asghar_2016_Yelp Dataset Challenge.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Asghar_2016_Yelp Dataset Challenge.pdf"
              ]
            ],
            "resource": "storage/i1473.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Yelp Dataset Challenge",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models."
          ],
          [
            "Access Date",
            "2022-03-29 13:31:08"
          ],
          [
            "Creators",
            "Nabiha Asghar"
          ],
          [
            "Date",
            "2016-05-17 2016-05-17"
          ],
          [
            "Extra",
            "arXiv: 1605.05362"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Publication Title",
            "arXiv:1605.05362 [cs]"
          ],
          [
            "Short Title",
            "Yelp Dataset Challenge"
          ],
          [
            "Title",
            "Yelp Dataset Challenge: Review Rating Prediction"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1605.05362"
          ]
        ],
        "resource": "storage/i1473.pdf",
        "selectable": false
      }
    ],
    "item_title": "Text Style Transfer",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Text to Image",
    "item-id": "c25,i1851",
    "nodes": [
      {
        "text": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "item-id": "i1851",
        "nodes": [
          {
            "text": "Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf",
            "item-id": "i2073",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BF5HN5U5/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/3\">2 Imagen</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/3\">2.1 Pretrained text encoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/4\">2.2 Diffusion models and classifier-free guidance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/4\">2.3 Large guidance weight samplers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/5\">2.4 Robust cascaded diffusion models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/5\">2.5 Neural network architecture</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/5\">3 Evaluating Text-to-Image Models</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/6\">4.1 Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/7\">4.2 Results on COCO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/7\">4.3 Results on DrawBench</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/7\">4.4 Analysis of Imagen</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/8\">5 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/9\">6 Conclusions, Limitations and Societal Impact</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/10\">7 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/20\">A Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/20\">B Architecture Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/20\">B.1 Efficient U-Net</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/21\">C DrawBench</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/21\">D Imagen Detailed Abalations and Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/21\">D.1 Pre-trained Text Encoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/22\">D.2 Classifier-free Guidance and the Alignment-Fidelity Trade-off</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/25\">D.3 Impact of Model Size</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/25\">D.3.1 Impact of Text Conditioning Schemas</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/26\">D.3.2 Comparison of U-Net vs Efficient U-Net</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/26\">E Comparison to GLIDE and DALL-E 2</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/44\">F Implementation Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/44\">F.1 6464</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/45\">F.2 6464 256256</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BF5HN5U5/45\">F.3 256256 10241024</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf"
              ]
            ],
            "resource": "storage/i2073.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."
          ],
          [
            "Access Date",
            "2022-10-13 21:03:34"
          ],
          [
            "Archiveid",
            "arXiv:2205.11487"
          ],
          [
            "Creators",
            "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, Mohammad Norouzi"
          ],
          [
            "DOI",
            "10.48550/arXiv.2205.11487"
          ],
          [
            "Date",
            "2022-05-23 2022-05-23"
          ],
          [
            "Extra",
            "arXiv:2205.11487 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2205.11487"
          ]
        ],
        "resource": "storage/i2073.pdf",
        "selectable": false
      }
    ],
    "item_title": "Text to Image",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Transformers",
    "item-id": "c5,i3651",
    "nodes": [
      {
        "text": "AST",
        "item-id": "i2208",
        "nodes": [
          {
            "text": "Gong et al_2021_AST.pdf",
            "item-id": "i2241",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Gong et al_2021_AST.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Gong et al_2021_AST.pdf"
              ]
            ],
            "resource": "storage/i2241.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AST",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-02-25 06:28:16"
          ],
          [
            "Conference Name",
            "Interspeech 2021"
          ],
          [
            "Creators",
            "Yuan Gong, Yu-An Chung, James Glass"
          ],
          [
            "DOI",
            "10.21437/Interspeech.2021-698"
          ],
          [
            "Date",
            "2021-08-30 2021-8-30"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "DOI.org (Crossref)"
          ],
          [
            "Pages",
            "571-575"
          ],
          [
            "Proceedings Title",
            "Interspeech 2021"
          ],
          [
            "Publisher",
            "ISCA"
          ],
          [
            "Short Title",
            "AST"
          ],
          [
            "Title",
            "AST: Audio Spectrogram Transformer"
          ],
          [
            "URL",
            "https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html"
          ]
        ],
        "resource": "storage/i2241.pdf",
        "selectable": false
      },
      {
        "text": "An Image is Worth 16x16 Words",
        "item-id": "i1574",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n152",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Vision Transformer (ViT)</p>\n<p>In previous, there is few methods using transformer to replace CNN using in the computer vision image analysis tasks. Or it is still hard to be accelerated.</p>\n<p>The authors assume to use image patches to replace NLP tokens in transformer.</p>\n<p>The structure of ViT is very similar to the transformer encoder part in NLP area. For the input image, it will be divided as patches, and the patches are like tokens added by trainable positional encoding to be sent into transformer. There is another trainable token in the first place to be sent into the transformer. In the output of the transformer encoder, the output for that trainable token is used for classification.</p>\n<p>ILSVRC-2012 ImageNet dataset, ImageNet-21k and JFT are used.</p>\n<p>In the large dataset, the transformers with more parameters have advantages comparing to the less one and CNN.</p>\n<p>+ve: Better performance with pretraining in large dataset compared to CNN.</p>\n<p>-ve: Perform bad when data is not enough. Didn't try to implement ViT to other computer vision tasks. Need to continue exploring self-supervised pre-training methods.</p>\n<p>In my opinion, this paper provides a great method for using the original structure of transformer. The tokens in NLP is replaced with image patches in computer vision. From the result of image classification, we can expect this structure works in other CV tasks.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf",
            "item-id": "i1618",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf"
              ]
            ],
            "resource": "storage/i1618.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "An Image is Worth 16x16 Words",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied..."
          ],
          [
            "Access Date",
            "2022-05-20 06:38:03"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Short Title",
            "An Image is Worth 16x16 Words"
          ],
          [
            "Title",
            "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=YicbFdNTTy&utm_campaign=f86497ed3a-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_source=Deep%20Learning%20Weekly&utm_term=0_384567b42d-f86497ed3a-72965345"
          ]
        ],
        "resource": "storage/i1618.pdf",
        "selectable": false
      },
      {
        "text": "Attention Augmented Convolutional Networks",
        "item-id": "i19",
        "nodes": [
          {
            "text": "Annotations",
            "item-id": "n743",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotations",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"3\"><p>Annotations</p>\n<p>Attention-Augmented Convolution (AA).</p>\n<p>The convolution layers has limited local receptive fields and translation equivalence via weight sharing. Both are the bias for modelling images.</p>\n<p>Self-attention can capture long range interactions but not applied to computer vision tasks yet. Combine the self-attention and convolution can generate the best result.</p>\n<p>First, flatten the input feature map through spacial dimensions with a positional encoding added, then sent it to multi-head self-attention layer. The positional encoding used is a relative position encodings. It is a trainable encoding based on relative position. Two encodings, for Width and Height, will be added to the K vectors. The results from both self-attention and convolutions will be concatenated.</p>\n<p>CIFAR-100 and ImageNet for classification. COCO for object detection.</p>\n<p>The results for classification and object detection are both better than previous methods. The positional encoding also contributes a lot.</p>\n<p>+ve: Better performance compared to other purely convolutional neural networks in classification and object detection.</p>\n<p>-ve: </p>\n<p>This paper uses the self-attention layers to enhance the traditional convolutional layers. They used a simple concat to fusion two features but provides a innovative relative positional encoding, which is very important in the future.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Bello et al_2019_Attention Augmented Convolutional Networks.pdf",
            "item-id": "i163",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bello et al_2019_Attention Augmented Convolutional Networks.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bello et al_2019_Attention Augmented Convolutional Networks.pdf"
              ]
            ],
            "resource": "storage/i163.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attention Augmented Convolutional Networks",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V. Le"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "3286-3295"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Attention Augmented Convolutional Networks"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_ICCV_2019/html/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html"
          ]
        ],
        "resource": "storage/i163.pdf",
        "selectable": false
      },
      {
        "text": "Attention is all you need",
        "item-id": "i24",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n107",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Self-attention and transformer</p>\n<p>The previous recurrent model structures cannot compute parallelly.</p>\n<p>The attention mechanisms can allow the dependencies without regard to their distances in the sequences. The transformer structure can parallel the computation, and reach a better quarlity.</p>\n<p>They proposed the Transformer with Multi-Head Attention blocks. The tansformer is a encoder-decoder structure, and the attention layers are the main part of this structure. The Multi-head attention layer has 3 imput, Query, Key and Value. The Q, K and V will be used to calculate the output of attention layer.</p>\n<p>The standard WMT 2014 English-German dataset and WMT 2014 English-French dataset.</p>\n<p>The results shows the performance is better than other models and the training cost is less than others.</p>\n<p>+ve: Faster train. Better performance.</p>\n<p>-ve: Only used for text data. No locality focus.</p>\n<p>This paper is the origin of self-attention and transformer, which replaces the traditional RNN based structures for NLP tasks, with faster training speed and better perforance. Based on this method, lots of work also focus on the transformer for computer vision.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Vaswani et al_2017_Attention is all you need.pdf",
            "item-id": "i209",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Vaswani et al_2017_Attention is all you need.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Vaswani et al_2017_Attention is all you need.pdf"
              ]
            ],
            "resource": "storage/i209.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Attention is all you need",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature."
          ],
          [
            "Conference Name",
            "Proceedings of the 31st International Conference on Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin"
          ],
          [
            "Date",
            "2017-12-04 December 4, 2017"
          ],
          [
            "ISBN",
            "978-1-5108-6096-4"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "6000\u20136010"
          ],
          [
            "Place",
            "Red Hook, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 31st International Conference on Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates Inc."
          ],
          [
            "Series",
            "NIPS'17"
          ],
          [
            "Title",
            "Attention is all you need"
          ]
        ],
        "resource": "storage/i209.pdf",
        "selectable": false
      },
      {
        "text": "BART",
        "item-id": "i1860",
        "nodes": [
          {
            "text": "Lewis et al_2019_BART.pdf",
            "item-id": "i1957",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Lewis et al_2019_BART.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Lewis et al_2019_BART.pdf"
              ]
            ],
            "resource": "storage/i1957.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "BART",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance."
          ],
          [
            "Access Date",
            "2022-11-01 15:57:37"
          ],
          [
            "Archiveid",
            "arXiv:1910.13461"
          ],
          [
            "Creators",
            "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer"
          ],
          [
            "DOI",
            "10.48550/arXiv.1910.13461"
          ],
          [
            "Date",
            "2019-10-29 2019-10-29"
          ],
          [
            "Extra",
            "arXiv:1910.13461 [cs, stat]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "BART"
          ],
          [
            "Title",
            "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1910.13461"
          ]
        ],
        "resource": "storage/i1957.pdf",
        "selectable": false
      },
      {
        "text": "BERT",
        "item-id": "i27",
        "nodes": [
          {
            "text": "Devlin et al_2019_BERT.pdf",
            "item-id": "i151",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Devlin et al_2019_BERT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Devlin et al_2019_BERT.pdf"
              ]
            ],
            "resource": "storage/i151.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "BERT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
          ],
          [
            "Access Date",
            "2021-04-27 14:50:52"
          ],
          [
            "Conference Name",
            "NAACL-HLT 2019"
          ],
          [
            "Creators",
            "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
          ],
          [
            "DOI",
            "10.18653/v1/N19-1423"
          ],
          [
            "Date",
            "2019-06-00 2019-06"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "4171\u20134186"
          ],
          [
            "Place",
            "Minneapolis, Minnesota"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "BERT"
          ],
          [
            "Title",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
          ],
          [
            "URL",
            "https://www.aclweb.org/anthology/N19-1423"
          ]
        ],
        "resource": "storage/i151.pdf",
        "selectable": false
      },
      {
        "text": "Emerging Properties in Self-Supervised Vision Transformers",
        "item-id": "i1863",
        "nodes": [
          {
            "text": "Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf",
            "item-id": "i1963",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf"
              ]
            ],
            "resource": "storage/i1963.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Emerging Properties in Self-Supervised Vision Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base."
          ],
          [
            "Access Date",
            "2022-11-01 15:53:53"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9650-9660"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Emerging Properties in Self-Supervised Vision Transformers"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1963.pdf",
        "selectable": false
      },
      {
        "text": "End-to-End Object Detection with Transformers",
        "item-id": "i35",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n751",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"3\"><p>Annotation</p>\n<p>Detection Transformer (DETR)</p>\n<p>The previous modern object detection algorithms relies on proposals, anchors, and windows to predict object boxes, which is easily influenced by preprocessing.</p>\n<p>As the E2E solutions are developped well in other area, it is necessary to use E2E in object detection.</p>\n<p>First step is a CNN backbone to extract features from images. Then is a transformer encoder takes the flattened feature maps as input sequence and generate the sequence output. Then the output will be feed into the middle part decoder for prediction. The object queries are trainable parameter for predicting the result boxes. Notice in each self attention layer, the positional encoding is applied. After the decoder, there is FFN to predict the bounding box and class.</p>\n<p>COCO</p>\n<p>The results is better than Faster-RCNN in object detection, also better than other dataset for segmentation.</p>\n<p>+ve: Better performance than Faster R-CNN. Easy to implement and flexible. Also can be used for segmentation. </p>\n<p>-ve: Not good for small objects. Training too difficult.</p>\n<p>This method used transformer to implement to E2E object detection and segmentation tasks, and perform great results. This is an elegant way to achieve E2E, which is very innovative.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Carion et al_2020_End-to-End Object Detection with Transformers.pdf",
            "item-id": "i175",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Carion et al_2020_End-to-End Object Detection with Transformers.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NHV5VHBB/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/3\">2.1 Set Prediction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/3\">2.2 Transformers and Parallel Decoding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/4\">2.3 Object Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/4\">3 The DETR Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/5\">3.1 Object Detection Set Prediction Loss</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/6\">3.2 DETR Architecture</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/8\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/9\">4.1 Comparison with Faster R-CNN and RetinaNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/10\">4.2 Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/13\">4.3 DETR for Panoptic Segmentation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/15\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NHV5VHBB/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Carion et al_2020_End-to-End Object Detection with Transformers.pdf"
              ]
            ],
            "resource": "storage/i175.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "End-to-End Object Detection with Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm"
          ],
          [
            "DOI",
            "10.1007/978-3-030-58452-8_13"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "ISBN",
            "978-3-030-58452-8"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "213-229"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer International Publishing"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "End-to-End Object Detection with Transformers"
          ]
        ],
        "resource": "storage/i175.pdf",
        "selectable": false
      },
      {
        "text": "Generative Pretraining From Pixels",
        "item-id": "i1561",
        "nodes": [
          {
            "text": "Chen et al_2020_Generative Pretraining From Pixels.pdf",
            "item-id": "i1594",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2020_Generative Pretraining From Pixels.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2020_Generative Pretraining From Pixels.pdf"
              ]
            ],
            "resource": "storage/i1594.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Generative Pretraining From Pixels",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features."
          ],
          [
            "Access Date",
            "2022-05-21 03:25:55"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever"
          ],
          [
            "Date",
            "2020-11-21 2020-11-21"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "1691-1703"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 37th International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Generative Pretraining From Pixels"
          ],
          [
            "URL",
            "https://proceedings.mlr.press/v119/chen20s.html"
          ]
        ],
        "resource": "storage/i1594.pdf",
        "selectable": false
      },
      {
        "text": "Image Transformer",
        "item-id": "i28",
        "nodes": [
          {
            "text": "Parmar et al_2018_Image Transformer.pdf",
            "item-id": "i154",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Parmar et al_2018_Image Transformer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Parmar et al_2018_Image Transformer.pdf"
              ]
            ],
            "resource": "storage/i154.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Image Transformer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual seq..."
          ],
          [
            "Access Date",
            "2021-04-23 10:52:07"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran"
          ],
          [
            "Date",
            "2018-07-03 2018/07/03"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "4055-4064"
          ],
          [
            "Proceedings Title",
            "International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Image Transformer"
          ],
          [
            "URL",
            "http://proceedings.mlr.press/v80/parmar18a.html"
          ]
        ],
        "resource": "storage/i154.pdf",
        "selectable": false
      },
      {
        "text": "Intriguing Properties of Vision Transformers",
        "item-id": "i1814",
        "nodes": [
          {
            "text": "Naseer et al_2021_Intriguing Properties of Vision Transformers.pdf",
            "item-id": "i1817",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Naseer et al_2021_Intriguing Properties of Vision Transformers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Naseer et al_2021_Intriguing Properties of Vision Transformers.pdf"
              ]
            ],
            "resource": "storage/i1817.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Intriguing Properties of Vision Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble,  leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms.  We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released."
          ],
          [
            "Access Date",
            "2022-10-12 07:00:26"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "23296\u201323308"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Intriguing Properties of Vision Transformers"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i1817.pdf",
        "selectable": false
      },
      {
        "text": "Long Video Generation with\u00a0Time-Agnostic VQGAN and\u00a0Time-Sensitive Transformer",
        "item-id": "i3208",
        "nodes": [
          {
            "text": "Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf",
            "item-id": "i3277",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_63TA6BXK/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_63TA6BXK/4\">2 Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/4\">2.1 Extending the VQGAN Framework for Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/7\">2.2 Time-Agnostic VQGAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/8\">2.3 Time-Sensitive Transformer</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_63TA6BXK/9\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/9\">3.1 Experimental Setups</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/9\">3.2 Quantitative Evaluation on Short Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/11\">3.3 Quantitative Evaluation on Long Video Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/12\">3.4 Qualitative Evaluation on Long Video Generation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/14\">4 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/15\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_63TA6BXK/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf"
              ]
            ],
            "resource": "storage/i3277.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Long Video Generation with\u00a0Time-Agnostic VQGAN and\u00a0Time-Sensitive Transformer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Videos are created to express emotion, exchange information, and share experiences. Video synthesis has intrigued researchers for a long time. Despite the rapid progress driven by advances in visual synthesis, most existing studies focus on improving the frames\u2019 quality and the transitions between them, while little progress has been made in generating longer videos. In this paper, we present a method that builds on 3D-VQGAN and transformers to generate videos with thousands of frames. Our evaluation shows that our model trained on 16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse, and Taichi-HD datasets can generate diverse, coherent, and high-quality long videos. We also showcase conditional extensions of our approach for generating meaningful long videos by incorporating temporal information with text and audio. Videos and code can be found at https://songweige.github.io/projects/tats."
          ],
          [
            "Conference Name",
            "Computer Vision \u2013 ECCV 2022"
          ],
          [
            "Creators",
            "Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, Devi Parikh, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19790-1_7"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19790-1"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "102-118"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Computer Vision \u2013 ECCV 2022"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Title",
            "Long Video Generation with\u00a0Time-Agnostic VQGAN and\u00a0Time-Sensitive Transformer"
          ]
        ],
        "resource": "storage/i3277.pdf",
        "selectable": false
      },
      {
        "text": "MViTv2",
        "item-id": "i1826",
        "nodes": [
          {
            "text": "Li et al_2022_MViTv2.pdf",
            "item-id": "i2069",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Li et al_2022_MViTv2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Li et al_2022_MViTv2.pdf"
              ]
            ],
            "resource": "storage/i2069.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MViTv2",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTv2s' pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 58.7 boxAP on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models are available at https://github.com/facebookresearch/mvit."
          ],
          [
            "Access Date",
            "2022-10-22 04:27:51"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "4804-4814"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MViTv2"
          ],
          [
            "Title",
            "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i2069.pdf",
        "selectable": false
      },
      {
        "text": "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation",
        "item-id": "i1705",
        "nodes": [
          {
            "text": "Comment: 22 pages, 8 figures. Under the review process",
            "item-id": "n1757",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: 22 pages, 8 figures. Under the review process",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: 22 pages, 8 figures. Under the review process</div>",
            "node_type": "note"
          },
          {
            "text": "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf",
            "item-id": "i1756",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G7GYACED/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/3\">2 Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/4\">2.1 Masked Autoencoders (MAE)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3 Masked Spectrogram Modeling using Masked Autoencoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3.1 Input Audio Duration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/5\">3.2 Patch Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">3.3 Feature Calculation for Downstream Tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4.1 Experimental Details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/6\">4.1.1 Pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/7\">4.1.2 Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/7\">4.2 Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/8\">4.3 Experimental Results: Comparison with the HEAR 2021 Results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4 Experimental Results: Impact of Design Choices on Performance</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4.1 Impact of Input Audio Duration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/9\">4.4.2 Impact of Patch Size</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/11\">4.4.3 Impact of Input Splitting: Patches vs. Strips</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G7GYACED/12\">4.5 Qualitative Analysis with Visualizations</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/12\">4.5.1 Reconstructions with Random Masks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/13\">4.5.2 Reconstructions with Patterned Masks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/13\">4.5.3 Reconstructions with Various Mask Ratios</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/15\">4.5.4 Self-Attention Map Visualizations</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/17\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G7GYACED/21\">A Reconstruction Examples of Various MSM-MAE Models</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Niizumi et al_2022_Masked Spectrogram Modeling using Masked Autoencoders for Learning.pdf"
              ]
            ],
            "resource": "storage/i1756.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent general-purpose audio representations show state-of-the-art performance on various audio tasks. These representations are pre-trained by self-supervised learning methods that create training signals from the input. For example, typical audio contrastive learning uses temporal relationships among input sounds to create training signals, whereas some methods use a difference among input views created by data augmentations. However, these training signals do not provide information derived from the intact input sound, which we think is suboptimal for learning representation that describes the input as it is. In this paper, we seek to learn audio representations from the input itself as supervision using a pretext task of auto-encoding of masked spectrogram patches, Masked Spectrogram Modeling (MSM, a variant of Masked Image Modeling applied to audio spectrogram). To implement MSM, we use Masked Autoencoders (MAE), an image self-supervised learning method. MAE learns to efficiently encode the small number of visible patches into latent representations to carry essential information for reconstructing a large number of masked patches. While training, MAE minimizes the reconstruction error, which uses the input as training signal, consequently achieving our goal. We conducted experiments on our MSM using MAE (MSM-MAE) models under the evaluation benchmark of the HEAR 2021 NeurIPS Challenge. Our MSM-MAE models outperformed the HEAR 2021 Challenge results on seven out of 15 tasks (e.g., accuracies of 73.4% on CREMA-D and 85.8% on LibriCount), while showing top performance on other tasks where specialized models perform better. We also investigate how the design choices of MSM-MAE impact the performance and conduct qualitative analysis of visualization outcomes to gain an understanding of learned representations. We make our code available online."
          ],
          [
            "Access Date",
            "2022-07-17 14:53:45"
          ],
          [
            "Archiveid",
            "arXiv:2204.12260"
          ],
          [
            "Creators",
            "Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.12260"
          ],
          [
            "Date",
            "2022-04-26 2022-04-26"
          ],
          [
            "Extra",
            "arXiv:2204.12260 [cs, eess]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.12260"
          ]
        ],
        "resource": "storage/i1756.pdf",
        "selectable": false
      },
      {
        "text": "MeMViT",
        "item-id": "i1870",
        "nodes": [
          {
            "text": "Wu et al_2022_MeMViT.pdf",
            "item-id": "i1975",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2022_MeMViT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2022_MeMViT.pdf"
              ]
            ],
            "resource": "storage/i1975.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MeMViT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "While today's video recognition systems parse snapshots or short clips accurately, they cannot connect the dots and reason across a longer range of time yet. Most existing video architectures can only process <5 seconds of a video without hitting the computation or memory bottlenecks. In this paper, we propose a new strategy to overcome this challenge. Instead of trying to process more frames at once like most existing methods, we propose to process videos in an online fashion and cache \"memory\" at each iteration. Through the memory, the model can reference prior context for long-term modeling, with only a marginal cost. Based on this idea, we build MeMViT, a Memory-augmented Multiscale Vision Transformer, that has a temporal support 30x longer than existing models with only 4.5 more compute; traditional methods need >3,000% more compute to do the same. On a wide range of settings, the increased temporal support enabled by MeMViT brings large gains in recognition accuracy consistently. MeMViT obtains state-of-the-art results on the AVA, EPIC-Kitchens-100 action classification, and action anticipation datasets."
          ],
          [
            "Access Date",
            "2022-10-31 04:10:11"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "13587-13597"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MeMViT"
          ],
          [
            "Title",
            "MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1975.pdf",
        "selectable": false
      },
      {
        "text": "MiniViT",
        "item-id": "i1868",
        "nodes": [
          {
            "text": "Zhang et al_2022_MiniViT.pdf",
            "item-id": "i1972",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2022_MiniViT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2022_MiniViT.pdf"
              ]
            ],
            "resource": "storage/i1972.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "MiniViT",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Vision Transformer (ViT) models have recently drawn much attention in computer vision due to their high model capability. However, ViT models suffer from huge number of parameters, restricting their applicability on devices with limited computation. To alleviate this problem, we propose MiniViT, a new compression framework, which achieves parameter reduction in vision transformers while retaining the same performance. The central idea of MiniViT is to multiplex the weights of consecutive transformer blocks. More specifically, we make the weights shared across layers, while imposing a transformation on the weights to increase diversity. Weight distillation over self-attention is also applied to transfer knowledge from large-scale ViT models to weight-multiplexed compact models. Comprehensive experiments demonstrate the efficacy of MiniViT, showing that it can reduce the size of the pre-trained Swin-B transformer by 48%, while achieving an increase of 1.0% in top-1 accuracy on ImageNet. Moreover, using a single-layer parameters, MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters, without seriously compromising the performance. Finally, we verify the transferability of MiniViT by reporting its performance on downstream benchmarks. Code and models are available at here."
          ],
          [
            "Access Date",
            "2022-10-31 04:12:23"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, Lu Yuan"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12145-12154"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Short Title",
            "MiniViT"
          ],
          [
            "Title",
            "MiniViT: Compressing Vision Transformers With Weight Multiplexing"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_MiniViT_Compressing_Vision_Transformers_With_Weight_Multiplexing_CVPR_2022_paper.html"
          ]
        ],
        "resource": "storage/i1972.pdf",
        "selectable": false
      },
      {
        "text": "MobileViT",
        "item-id": "i1522",
        "nodes": [
          {
            "text": "Comment: Accepted at ICLR'22",
            "item-id": "n1533",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Accepted at ICLR'22",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Accepted at ICLR'22</div>",
            "node_type": "note"
          },
          {
            "text": "Mehta_Rastegari_2022_MobileViT.pdf",
            "item-id": "i1532",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Mehta_Rastegari_2022_MobileViT.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MN6DWYTK/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/3\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/4\">3 MobileViT: A Light-weight Transformer</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/4\">3.1 MobileViT Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/6\">3.2 Multi-scale Sampler for Training Efficiency</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/7\">4 Experimental Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/7\">4.1 Image Classification on the ImageNet-1k Dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/8\">4.2 MobileViT as a General-purpose Backbone</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/8\">4.2.1 Mobile Object Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/8\">4.2.2 Mobile Semantic Segmentation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/9\">4.3 Performance on Mobile Devices</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/10\">5 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/14\">A MobileViT Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/14\">B Multi-scale sampler</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/15\">C Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/17\">D Training Details for SSDLite and DeepLabv3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/18\">E Extended Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/19\">F Qualitative Results on the Task of Object Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MN6DWYTK/23\">G Semantic Segmentation Results on an Unseen Dataset</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Mehta_Rastegari_2022_MobileViT.pdf"
              ]
            ],
            "resource": "storage/i1532.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MobileViT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets"
          ],
          [
            "Access Date",
            "2022-05-18 03:41:06"
          ],
          [
            "Archiveid",
            "arXiv:2110.02178"
          ],
          [
            "Creators",
            "Sachin Mehta, Mohammad Rastegari"
          ],
          [
            "Date",
            "2022-03-04 2022-03-04"
          ],
          [
            "Extra",
            "arXiv:2110.02178 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MobileViT"
          ],
          [
            "Title",
            "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2110.02178"
          ]
        ],
        "resource": "storage/i1532.pdf",
        "selectable": false
      },
      {
        "text": "MultiMAE",
        "item-id": "i1683",
        "nodes": [
          {
            "text": "Bachmann et al_2022_MultiMAE.pdf",
            "item-id": "i1710",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Bachmann et al_2022_MultiMAE.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2LUPTYTA/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/2\">2 . Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/3\">3 . Method Description</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/3\">3.1 . Multi-modal encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/4\">3.2 . Decoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/4\">3.3 . Multi-modal masking strategies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/4\">3.4 . Pseudo labeled multi-task training dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/5\">3.5 . Pre-training details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4.1 . Transfer tasks and datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4.2 . Transfers with RGB-only</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/6\">4.3 . Transfers with multiple modalities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/7\">4.4 . Influence of pre-training task choices and masking on transfer performance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/8\">4.5 . Cross-modal exchange of information</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/9\">5 . Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/10\"></a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\"> </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">A . Additional pre-training implementation details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">B . Transfer implementation details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">B.1 . ImageNet classification fine-tuning setting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/15\">B.2 . Semantic segmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/17\">B.3 . NYUv2 depth estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/17\">B.4 . Taskonomy dense regression tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/17\">C . Mask sampling strategies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/18\">D . Detailed Taskonomy transfer results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/18\">E . Robustness evaluation on ImageNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/18\">F . Comparison of MAE variants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/19\">G . Comparison of pre-training time </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2LUPTYTA/19\">H . Additional visualizations</a></li></ul></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Bachmann et al_2022_MultiMAE.pdf"
              ]
            ],
            "resource": "storage/i1710.pdf"
          },
          {
            "text": "Comment: Project page at https://multimae.epfl.ch",
            "item-id": "n1713",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project page at https://multimae.epfl.ch",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project page at https://multimae.epfl.ch</div>",
            "node_type": "note"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "MultiMAE",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We propose a pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two key aspects: I) it can optionally accept additional modalities of information in the input besides the RGB image (hence \"multi-modal\"), and II) its training objective accordingly includes predicting multiple outputs besides the RGB image (hence \"multi-task\"). We make use of masking (across image patches and input modalities) to make training MultiMAE tractable as well as to ensure cross-modality predictive coding is indeed learned by the network. We show this pre-training strategy leads to a flexible, simple, and efficient framework with improved transfer results to downstream tasks. In particular, the same exact pre-trained network can be flexibly used when additional information besides RGB images is available or when no information other than RGB is available - in all configurations yielding competitive to or significantly better results than the baselines. To avoid needing training datasets with multiple modalities and tasks, we train MultiMAE entirely using pseudo labeling, which makes the framework widely applicable to any RGB dataset. The experiments are performed on multiple transfer tasks (image classification, semantic segmentation, depth estimation) and datasets (ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an intriguingly impressive capability by the model in cross-modal/task predictive coding and transfer."
          ],
          [
            "Access Date",
            "2022-07-29 07:39:07"
          ],
          [
            "Archiveid",
            "arXiv:2204.01678"
          ],
          [
            "Creators",
            "Roman Bachmann, David Mizrahi, Andrei Atanov, Amir Zamir"
          ],
          [
            "DOI",
            "10.48550/arXiv.2204.01678"
          ],
          [
            "Date",
            "2022-04-04 2022-04-04"
          ],
          [
            "Extra",
            "arXiv:2204.01678 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "MultiMAE"
          ],
          [
            "Title",
            "MultiMAE: Multi-modal Multi-task Masked Autoencoders"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2204.01678"
          ]
        ],
        "resource": "storage/i1710.pdf",
        "selectable": false
      },
      {
        "text": "Multiscale Vision Transformers",
        "item-id": "i1569",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n150",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>Multi-scale ViT (MViT)</p>\n<p>The previous vision transformer (ViT) only used one scale to analyze the images, lack of the thoughts of \"pyramid\" strategies, which need to go to different scales to get variety of features from different scales.</p>\n<p>The \"pyramid\" strategies is a multi-scale processing, which means working in a lower resolution can cover the features with better \"sense\" of context. The \"depth\" of modern deep neural networks can help. Authors hope to connect the multi-scale feature hierarchies concept with the transformer. And this concept is beneficial to the transformer structures.</p>\n<p>They proposed Multiscale Vision Transformer (MViT) and it is built by Multi-Head Pooling Attention (MHPA) layers. Compared to the Muti-Head Attention layer, the MHPA has pooling layers to each route (identity, Q, K and V). In the MViT, the dimension of feature maps will decrease through the MHPA layers to detect multi-scale features.</p>\n<p>The dataset used is Kinetics-400 and Kinetics-600 for image classification. And Something-Something-v2, Charades and AVA for transfer learning.+</p>\n<p>The results shows the MViT has better accuracy for video classification than others. Also it has less parameters and less computational complexity than ViT. In image recognition with transfer learning, the MViT also performs the best comparing to other models.</p>\n<p>+ve: Expand the feature complexity. Big advantage than single-scale vision transformer.</p>\n<p>-ve:</p>\n<p>In my opinion, this MViT brings the pyramid concepts to transformer models, also provide a method to work with video. With higher performance and lower computational complexity and parameters. This MViT is very useful for video analysis.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Fan et al_2021_Multiscale Vision Transformers.pdf",
            "item-id": "i1608",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Fan et al_2021_Multiscale Vision Transformers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Fan et al_2021_Multiscale Vision Transformers.pdf"
              ]
            ],
            "resource": "storage/i1608.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Multiscale Vision Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10 more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast."
          ],
          [
            "Access Date",
            "2022-05-20 06:44:54"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Creators",
            "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "6824-6835"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision"
          ],
          [
            "Title",
            "Multiscale Vision Transformers"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/ICCV2021/html/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.html"
          ]
        ],
        "resource": "storage/i1608.pdf",
        "selectable": false
      },
      {
        "text": "Neural Discrete Representation Learning",
        "item-id": "i1527",
        "nodes": [
          {
            "text": "van den Oord et al_2017_Neural Discrete Representation Learning.pdf",
            "item-id": "i1544",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "van den Oord et al_2017_Neural Discrete Representation Learning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "van den Oord et al_2017_Neural Discrete Representation Learning.pdf"
              ]
            ],
            "resource": "storage/i1544.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Neural Discrete Representation Learning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of posterior collapse'' -\u2014 where the latents are ignored when they are paired with a powerful autoregressive decoder -\u2014 typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations."
          ],
          [
            "Access Date",
            "2022-05-12 15:41:24"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Aaron van den Oord, Oriol Vinyals, koray kavukcuoglu"
          ],
          [
            "Date",
            "2017-00-00 2017"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Neural Discrete Representation Learning"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html"
          ],
          [
            "Volume",
            "30"
          ]
        ],
        "resource": "storage/i1544.pdf",
        "selectable": false
      },
      {
        "text": "N\u00dcWA",
        "item-id": "i1869",
        "nodes": [
          {
            "text": "Wu et al_2022_N\u00dcWA.pdf",
            "item-id": "i1973",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wu et al_2022_N\u00dcWA.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CVY39CHI/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CVY39CHI/3\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/3\">2.1 Visual Auto-regressive Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/3\">2.2 Visual Sparse Self-Attention</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CVY39CHI/4\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/4\">3.1 3D Data Representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/5\">3.2 3D Nearby Self-Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/7\">3.3 3D Encoder-Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/7\">3.4 Training Objective</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CVY39CHI/8\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/8\">4.1 Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/8\">4.2 Comparison with State-of-the-art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/12\">4.3 Ablation Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/15\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CVY39CHI/15\">References</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wu et al_2022_N\u00dcWA.pdf"
              ]
            ],
            "resource": "storage/i1973.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "N\u00dcWA",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "This paper presents a unified multimodal pre-trained model called N\u00dcWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate N\u00dcWA on 8 downstream tasks. Compared to several strong baselines, N\u00dcWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks."
          ],
          [
            "Conference Name",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Creators",
            "Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, Tal Hassner"
          ],
          [
            "DOI",
            "10.1007/978-3-031-19787-1_41"
          ],
          [
            "Date",
            "2022-00-00 2022"
          ],
          [
            "ISBN",
            "978-3-031-19787-1"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "720-736"
          ],
          [
            "Place",
            "Cham"
          ],
          [
            "Proceedings Title",
            "Proceedings of the European Conference on Computer Vision (ECCV)"
          ],
          [
            "Publisher",
            "Springer Nature Switzerland"
          ],
          [
            "Series",
            "Lecture Notes in Computer Science"
          ],
          [
            "Short Title",
            "N\u00dcWA"
          ],
          [
            "Title",
            "N\u00dcWA: Visual Synthesis Pre-training for\u00a0Neural visUal World creAtion"
          ]
        ],
        "resource": "storage/i1973.pdf",
        "selectable": false
      },
      {
        "text": "On Layer Normalization in the Transformer Architecture",
        "item-id": "i683",
        "nodes": [
          {
            "text": "Xiong et al_2020_On Layer Normalization in the Transformer Architecture.pdf",
            "item-id": "i689",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xiong et al_2020_On Layer Normalization in the Transformer Architecture.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xiong et al_2020_On Layer Normalization in the Transformer Architecture.pdf"
              ]
            ],
            "resource": "storage/i689.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "On Layer Normalization in the Transformer Architecture",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial..."
          ],
          [
            "Access Date",
            "2021-06-02 07:30:34"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tieyan Liu"
          ],
          [
            "Date",
            "2020-11-21 2020/11/21"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "10524-10533"
          ],
          [
            "Proceedings Title",
            "International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "On Layer Normalization in the Transformer Architecture"
          ],
          [
            "URL",
            "http://proceedings.mlr.press/v119/xiong20b.html"
          ]
        ],
        "resource": "storage/i689.pdf",
        "selectable": false
      },
      {
        "text": "PVT v2",
        "item-id": "i3651",
        "nodes": [
          {
            "text": "Wang et al_2022_PVT v2.pdf",
            "item-id": "i3675",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wang et al_2022_PVT v2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wang et al_2022_PVT v2.pdf"
              ]
            ],
            "resource": "storage/i3675.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "PVT v2",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Transformers have recently lead to encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs: (i) a linear complexity attention layer, (ii) an overlapping patch embedding, and (iii) a convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linearity and provides significant improvements on fundamental vision tasks such as classification, detection, and segmentation. In particular, PVT v2 achieves comparable or better performance than recent work such as the Swin transformer. We hope this work will facilitate state-of-the-art transformer research in computer vision. Code is available at https://github.com/whai362/PVT."
          ],
          [
            "Access Date",
            "2024-01-15 16:35:41"
          ],
          [
            "Creators",
            "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao"
          ],
          [
            "DOI",
            "10.1007/s41095-022-0274-8"
          ],
          [
            "Date",
            "2022-09-01 2022-09-01"
          ],
          [
            "ISSN",
            "2096-0662"
          ],
          [
            "Issue",
            "3"
          ],
          [
            "Journal Abbreviation",
            "Comp. Visual Media"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "415-424"
          ],
          [
            "Publication Title",
            "Computational Visual Media"
          ],
          [
            "Short Title",
            "PVT v2"
          ],
          [
            "Title",
            "PVT v2: Improved baselines with Pyramid Vision Transformer"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s41095-022-0274-8"
          ],
          [
            "Volume",
            "8"
          ]
        ],
        "resource": "storage/i3675.pdf",
        "selectable": false
      },
      {
        "text": "Pre-Trained Image Processing Transformer",
        "item-id": "i1526",
        "nodes": [
          {
            "text": "Chen et al_2021_Pre-Trained Image Processing Transformer.pdf",
            "item-id": "i1541",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Chen et al_2021_Pre-Trained Image Processing Transformer.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Chen et al_2021_Pre-Trained Image Processing Transformer.pdf"
              ]
            ],
            "resource": "storage/i1541.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Pre-Trained Image Processing Transformer",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the constructive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT"
          ],
          [
            "Access Date",
            "2022-05-16 03:54:47"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12299-12310"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Pre-Trained Image Processing Transformer"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1541.pdf",
        "selectable": false
      },
      {
        "text": "Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers",
        "item-id": "i1570",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n600",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>SETR, Segmentation Transformer</p>\n<p>Hard to learn the long-range dependency information because of the limited receptive fields of CNN in segmentation task.</p>\n<p>The encoder-decoder structure </p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Zheng et al_2021_Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With.pdf",
            "item-id": "i1610",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zheng et al_2021_Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zheng et al_2021_Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With.pdf"
              ]
            ],
            "resource": "storage/i1610.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission."
          ],
          [
            "Access Date",
            "2022-05-20 06:44:09"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, Li Zhang"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "6881-6890"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1610.pdf",
        "selectable": false
      },
      {
        "text": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones",
        "item-id": "i1571",
        "nodes": [
          {
            "text": "Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf",
            "item-id": "i1612",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf"
              ]
            ],
            "resource": "storage/i1612.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutions."
          ],
          [
            "Access Date",
            "2022-05-20 06:43:36"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12894-12904"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Vaswani_Scaling_Local_Self-Attention_for_Parameter_Efficient_Visual_Backbones_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1612.pdf",
        "selectable": false
      },
      {
        "text": "Self-Attention with Relative Position Representations",
        "item-id": "i750",
        "nodes": [
          {
            "text": "Comment: NAACL 2018",
            "item-id": "n769",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: NAACL 2018",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: NAACL 2018</div>",
            "node_type": "note"
          },
          {
            "text": "Shaw et al_2018_Self-Attention with Relative Position Representations.pdf",
            "item-id": "i768",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Shaw et al_2018_Self-Attention with Relative Position Representations.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Shaw et al_2018_Self-Attention with Relative Position Representations.pdf"
              ]
            ],
            "resource": "storage/i768.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Self-Attention with Relative Position Representations",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs."
          ],
          [
            "Access Date",
            "2021-07-24 23:48:13"
          ],
          [
            "Archiveid",
            "arXiv:1803.02155"
          ],
          [
            "Creators",
            "Peter Shaw, Jakob Uszkoreit, Ashish Vaswani"
          ],
          [
            "DOI",
            "10.48550/arXiv.1803.02155"
          ],
          [
            "Date",
            "2018-04-12 2018-04-12"
          ],
          [
            "Extra",
            "arXiv:1803.02155 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Title",
            "Self-Attention with Relative Position Representations"
          ],
          [
            "URL",
            "http://arxiv.org/abs/1803.02155"
          ]
        ],
        "resource": "storage/i768.pdf",
        "selectable": false
      },
      {
        "text": "Stand-Alone Self-Attention in Vision Models",
        "item-id": "i1578",
        "nodes": [
          {
            "text": "Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf",
            "item-id": "i1621",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf"
              ]
            ],
            "resource": "storage/i1621.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Stand-Alone Self-Attention in Vision Models",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Convolutions are a fundamental building block of modern  computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention to ResNet-50 produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a fully self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox."
          ],
          [
            "Access Date",
            "2022-05-20 06:35:23"
          ],
          [
            "Creators",
            "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jon Shlens"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Title",
            "Stand-Alone Self-Attention in Vision Models"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html"
          ],
          [
            "Volume",
            "32"
          ]
        ],
        "resource": "storage/i1621.pdf",
        "selectable": false
      },
      {
        "text": "Taming Transformers for High-Resolution Image Synthesis",
        "item-id": "i1528",
        "nodes": [
          {
            "text": "Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf",
            "item-id": "i1549",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf"
              ]
            ],
            "resource": "storage/i1549.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Taming Transformers for High-Resolution Image Synthesis",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers."
          ],
          [
            "Access Date",
            "2022-05-12 15:37:07"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Creators",
            "Patrick Esser, Robin Rombach, Bjorn Ommer"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "12873-12883"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          [
            "Title",
            "Taming Transformers for High-Resolution Image Synthesis"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html"
          ]
        ],
        "resource": "storage/i1549.pdf",
        "selectable": false
      },
      {
        "text": "Training data-efficient image transformers & distillation through attention",
        "item-id": "i740",
        "nodes": [
          {
            "text": "Supplementary PDF",
            "item-id": "i741",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Supplementary PDF",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Access Date",
                "2021-07-21 16:18:50"
              ],
              [
                "Title",
                "Supplementary PDF"
              ],
              [
                "URL",
                "http://proceedings.mlr.press/v139/touvron21a/touvron21a-supp.pdf"
              ]
            ],
            "resource": "storage/i741.pdf"
          },
          {
            "text": "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf",
            "item-id": "i742",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf"
              ]
            ],
            "resource": "storage/i742.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Training data-efficient image transformers & distillation through attention",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models."
          ],
          [
            "Access Date",
            "2021-07-21 16:18:49"
          ],
          [
            "Conference Name",
            "International Conference on Machine Learning"
          ],
          [
            "Creators",
            "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou"
          ],
          [
            "Date",
            "2021-07-01 2021-07-01"
          ],
          [
            "Extra",
            "ISSN: 2640-3498"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "proceedings.mlr.press"
          ],
          [
            "Pages",
            "10347-10357"
          ],
          [
            "Proceedings Title",
            "International Conference on Machine Learning"
          ],
          [
            "Publisher",
            "PMLR"
          ],
          [
            "Title",
            "Training data-efficient image transformers & distillation through attention"
          ],
          [
            "URL",
            "http://proceedings.mlr.press/v139/touvron21a.html"
          ]
        ],
        "selectable": false
      },
      {
        "text": "TransGAN",
        "item-id": "i1573",
        "nodes": [
          {
            "text": "Annotation",
            "item-id": "n557",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Annotation",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\"><div data-schema-version=\"2\"><p>Annotation</p>\n<p>TransGAN. Pure transformer GAN.</p>\n<p>The convolution-based models cannot capture the features in global statistics. Can we build a strong GAN with completely self-attention layers/transformers? </p>\n<p>The transformer has strong representation capability for global view. Transformer is universal for multiple tasks.</p>\n<p>The Transformer encoder is used as basic block in TransGAN, which contains MHA(multi-head attention) and MLP with GELU, layer normalization and residual connections. In G, it is formed by transformer encoders and upsampling layers with increasing resolutions. The D is a ViT. For training, data augmentation, co-training with self-supervised auxiliary task, locality-aware initialization for self-attention are used to improve the performance. Also, scaling up the model benefits the performance.</p>\n<p>CIFAR-10. Also STL-10 and CelebA for higher resolution task.</p>\n<p>In CIFAR-10, it is better than other models except StyleGAN2. In STL-10 and CelebA, this model is better than other models.</p>\n<p>+ve: The performance is achieving the same level of previous CNN-based models. Unify the task pipeline as transformer structure is generic. </p>\n<p>-ve: Need more sophisticated tokenizing. Need pre-training transformers using pretext tasks. Need stronger attention forms. Need stronger and efficient self-attention forms. Need conditional image generation.</p>\n<p>In my opinion, the TransGAN provides a basic transformer-based GAN structure and several training tricks for better performance. This paper build the fundamental knowledge for transformer-based GAN, leading to more possibilities of generative networks on transformer.</p>\n</div></div>",
            "node_type": "note"
          },
          {
            "text": "Jiang et al_2021_TransGAN.pdf",
            "item-id": "i1614",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Jiang et al_2021_TransGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Jiang et al_2021_TransGAN.pdf"
              ]
            ],
            "resource": "storage/i1614.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "TransGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2022-05-20 06:41:59"
          ],
          [
            "Creators",
            "Yifan Jiang, Shiyu Chang, Zhangyang Wang"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "14745\u201314758"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "TransGAN"
          ],
          [
            "Title",
            "TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper/2021/hash/7c220a2091c26a7f5e9f1cfb099511e3-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i1614.pdf",
        "selectable": false
      },
      {
        "text": "Transformers",
        "item-id": "i1559",
        "nodes": [
          {
            "text": "Wolf et al_2020_Transformers.pdf",
            "item-id": "i1592",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wolf et al_2020_Transformers.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wolf et al_2020_Transformers.pdf"
              ]
            ],
            "resource": "storage/i1592.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Transformers",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers."
          ],
          [
            "Access Date",
            "2022-05-21 04:53:07"
          ],
          [
            "Conference Name",
            "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
          ],
          [
            "Creators",
            "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush"
          ],
          [
            "DOI",
            "10.18653/v1/2020.emnlp-demos.6"
          ],
          [
            "Date",
            "2020-00-00 2020"
          ],
          [
            "Library Catalog",
            "ACLWeb"
          ],
          [
            "Pages",
            "38\u201345"
          ],
          [
            "Place",
            "Online"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
          ],
          [
            "Publisher",
            "Association for Computational Linguistics"
          ],
          [
            "Short Title",
            "Transformers"
          ],
          [
            "Title",
            "Transformers: State-of-the-Art Natural Language Processing"
          ],
          [
            "URL",
            "https://aclanthology.org/2020.emnlp-demos.6"
          ]
        ],
        "resource": "storage/i1592.pdf",
        "selectable": false
      },
      {
        "text": "Transformers in Vision",
        "item-id": "i2383",
        "nodes": [
          {
            "text": "Khan et al_2022_Transformers in Vision.pdf",
            "item-id": "i2424",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Khan et al_2022_Transformers in Vision.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Khan et al_2022_Transformers in Vision.pdf"
              ]
            ],
            "resource": "storage/i2424.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Transformers in Vision",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision."
          ],
          [
            "Access Date",
            "2023-05-23 08:00:02"
          ],
          [
            "Creators",
            "Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah"
          ],
          [
            "DOI",
            "10.1145/3505244"
          ],
          [
            "Date",
            "2022-00-13 \u4e5d\u6708 13, 2022"
          ],
          [
            "ISSN",
            "0360-0300"
          ],
          [
            "Issue",
            "10s"
          ],
          [
            "Journal Abbreviation",
            "ACM Comput. Surv."
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "200:1\u2013200:41"
          ],
          [
            "Publication Title",
            "ACM Computing Surveys"
          ],
          [
            "Short Title",
            "Transformers in Vision"
          ],
          [
            "Title",
            "Transformers in Vision: A Survey"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/3505244"
          ],
          [
            "Volume",
            "54"
          ]
        ],
        "resource": "storage/i2424.pdf",
        "selectable": false
      },
      {
        "text": "Vector-quantized Image Modeling with Improved VQGAN",
        "item-id": "i1567",
        "nodes": [
          {
            "text": "Yu et al_2021_Vector-quantized Image Modeling with Improved VQGAN.pdf",
            "item-id": "i1603",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yu et al_2021_Vector-quantized Image Modeling with Improved VQGAN.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yu et al_2021_Vector-quantized Image Modeling with Improved VQGAN.pdf"
              ]
            ],
            "resource": "storage/i1603.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Vector-quantized Image Modeling with Improved VQGAN",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and..."
          ],
          [
            "Access Date",
            "2022-05-20 06:47:20"
          ],
          [
            "Conference Name",
            "International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu"
          ],
          [
            "Date",
            "2021-09-29 2021/09/29"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Proceedings Title",
            "International Conference on Learning Representations"
          ],
          [
            "Title",
            "Vector-quantized Image Modeling with Improved VQGAN"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=pfNyExj7z2"
          ]
        ],
        "resource": "storage/i1603.pdf",
        "selectable": false
      },
      {
        "text": "ViTAE",
        "item-id": "i2806",
        "nodes": [
          {
            "text": "Xu et al_2021_ViTAE.pdf",
            "item-id": "i2863",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Xu et al_2021_ViTAE.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_9THDSS62/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9THDSS62/3\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/3\">CNNs with intrinsic IB</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/3\">Vision transformers with learned IB</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9THDSS62/3\">Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/3\">Revisit vision transformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/4\">Overview architecture of ViTAE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/5\">Reduction cell</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/5\">Normal cell</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/5\">Model details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_9THDSS62/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/6\">Implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/6\">Comparison with the state-of-the-art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/6\">Ablation study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/8\">Data efficiency and training efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/9\">Generalization on downstream tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/9\">Visual inspection of ViTAE</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/10\">Limitation and discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_9THDSS62/10\">Conclusion</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Xu et al_2021_ViTAE.pdf"
              ]
            ],
            "resource": "storage/i2863.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ViTAE",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a new Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at https://github.com/Annbless/ViTAE."
          ],
          [
            "Access Date",
            "2023-07-17 06:01:21"
          ],
          [
            "Conference Name",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Creators",
            "Yufei Xu, Qiming ZHANG, Jing Zhang, Dacheng Tao"
          ],
          [
            "Date",
            "2021-00-00 2021"
          ],
          [
            "Library Catalog",
            "Neural Information Processing Systems"
          ],
          [
            "Pages",
            "28522\u201328535"
          ],
          [
            "Proceedings Title",
            "Advances in Neural Information Processing Systems"
          ],
          [
            "Publisher",
            "Curran Associates, Inc."
          ],
          [
            "Short Title",
            "ViTAE"
          ],
          [
            "Title",
            "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"
          ],
          [
            "URL",
            "https://proceedings.neurips.cc/paper_files/paper/2021/hash/efb76cff97aaf057654ef2f38cd77d73-Abstract.html"
          ],
          [
            "Volume",
            "34"
          ]
        ],
        "resource": "storage/i2863.pdf",
        "selectable": false
      },
      {
        "text": "ViTAEv2",
        "item-id": "i2805",
        "nodes": [
          {
            "text": "Zhang et al_2023_ViTAEv2.pdf",
            "item-id": "i2861",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2023_ViTAEv2.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2023_ViTAEv2.pdf"
              ]
            ],
            "resource": "storage/i2861.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "ViTAEv2",
        "item_type": "journalArticle",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we leverage the two IBs and propose the ViTAE transformer, which utilizes a reduction cell for multi-scale feature and a normal cell for locality. The two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline and representative models. Besides, we scale up our ViTAE model to 644\u00a0M parameters and obtain the state-of-the-art classification performance, i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the best 91.2% Top-1 classification accuracy on ImageNet Real validation set, without using extra private data. It demonstrates that the introduced inductive bias still helps when the model size becomes large. The source code and pretrained models are publicly available atcode."
          ],
          [
            "Access Date",
            "2023-07-17 06:02:38"
          ],
          [
            "Creators",
            "Qiming Zhang, Yufei Xu, Jing Zhang, Dacheng Tao"
          ],
          [
            "DOI",
            "10.1007/s11263-022-01739-w"
          ],
          [
            "Date",
            "2023-05-01 2023-05-01"
          ],
          [
            "ISSN",
            "1573-1405"
          ],
          [
            "Issue",
            "5"
          ],
          [
            "Journal Abbreviation",
            "Int J Comput Vis"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "Springer Link"
          ],
          [
            "Pages",
            "1141-1162"
          ],
          [
            "Publication Title",
            "International Journal of Computer Vision"
          ],
          [
            "Short Title",
            "ViTAEv2"
          ],
          [
            "Title",
            "ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond"
          ],
          [
            "URL",
            "https://doi.org/10.1007/s11263-022-01739-w"
          ],
          [
            "Volume",
            "131"
          ]
        ],
        "resource": "storage/i2861.pdf",
        "selectable": false
      },
      {
        "text": "VideoGPT",
        "item-id": "i1555",
        "nodes": [
          {
            "text": "Comment: Project website: https://wilson1yan.github.io/videogpt/index.html",
            "item-id": "n1585",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Project website: https://wilson1yan.github.io/videogpt/index.html",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Project website: https://wilson1yan.github.io/videogpt/index.html</div>",
            "node_type": "note"
          },
          {
            "text": "Yan et al_2021_VideoGPT.pdf",
            "item-id": "i1584",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Yan et al_2021_VideoGPT.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Yan et al_2021_VideoGPT.pdf"
              ]
            ],
            "resource": "storage/i1584.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "VideoGPT",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html"
          ],
          [
            "Access Date",
            "2022-05-25 02:07:20"
          ],
          [
            "Archiveid",
            "arXiv:2104.10157"
          ],
          [
            "Creators",
            "Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas"
          ],
          [
            "Date",
            "2021-09-14 2021-09-14"
          ],
          [
            "Extra",
            "arXiv:2104.10157 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "VideoGPT"
          ],
          [
            "Title",
            "VideoGPT: Video Generation using VQ-VAE and Transformers"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2104.10157"
          ]
        ],
        "resource": "storage/i1584.pdf",
        "selectable": false
      }
    ],
    "item_title": "Transformers",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "User Interface",
    "item-id": "c33,i2334",
    "nodes": [
      {
        "text": "AFFDEX SDK",
        "item-id": "i2334",
        "nodes": [
          {
            "text": "McDuff et al_2016_AFFDEX SDK.pdf",
            "item-id": "i2347",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "McDuff et al_2016_AFFDEX SDK.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "McDuff et al_2016_AFFDEX SDK.pdf"
              ]
            ],
            "resource": "storage/i2347.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "AFFDEX SDK",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present a real-time facial expression recognition toolkit that can automatically code the expressions of multiple people simultaneously. The toolkit is available across major mobile and desktop platforms (Android, iOS, Windows). The system is trained on the world's largest dataset of facial expressions and has been optimized to operate on mobile devices and with very few false detections. The toolkit offers the potential for the design of novel interfaces that respond to users' emotional states based on their facial expressions. We present a demonstration application that provides real-time visualization of the expressions captured by the camera."
          ],
          [
            "Access Date",
            "2023-05-02"
          ],
          [
            "Creators",
            "Daniel McDuff, Abdelrahman Mahmoud, Mohammad Mavadati, May Amr, Jay Turcot, Rana el Kaliouby"
          ],
          [
            "DOI",
            "10.1145/2851581.2890247"
          ],
          [
            "Date",
            "2016-00-07 \u4e94\u6708 7, 2016"
          ],
          [
            "ISBN",
            "978-1-4503-4082-3"
          ],
          [
            "Library Catalog",
            "ACM Digital Library"
          ],
          [
            "Pages",
            "3723\u20133726"
          ],
          [
            "Place",
            "New York, NY, USA"
          ],
          [
            "Proceedings Title",
            "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems"
          ],
          [
            "Publisher",
            "Association for Computing Machinery"
          ],
          [
            "Series",
            "CHI EA '16"
          ],
          [
            "Short Title",
            "AFFDEX SDK"
          ],
          [
            "Title",
            "AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit"
          ],
          [
            "URL",
            "https://dl.acm.org/doi/10.1145/2851581.2890247"
          ]
        ],
        "resource": "storage/i2347.pdf",
        "selectable": false
      }
    ],
    "item_title": "User Interface",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Video Captioning",
    "item-id": "c36,i2391",
    "nodes": [
      {
        "text": "Audio-Visual Interpretable and Controllable Video Captioning",
        "item-id": "i2391",
        "nodes": [
          {
            "text": "Tian et al_2019_Audio-Visual Interpretable and Controllable Video Captioning.pdf",
            "item-id": "i2441",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Tian et al_2019_Audio-Visual Interpretable and Controllable Video Captioning.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Tian et al_2019_Audio-Visual Interpretable and Controllable Video Captioning.pdf"
              ]
            ],
            "resource": "storage/i2441.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Audio-Visual Interpretable and Controllable Video Captioning",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Access Date",
            "2023-05-23 06:09:18"
          ],
          [
            "Conference Name",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Creators",
            "Yapeng Tian, Chenxiao Guan, Goodman Justin, Marc Moore, Chenliang Xu"
          ],
          [
            "Date",
            "2019-00-00 2019"
          ],
          [
            "Library Catalog",
            "openaccess.thecvf.com"
          ],
          [
            "Pages",
            "9-12"
          ],
          [
            "Proceedings Title",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
          ],
          [
            "Title",
            "Audio-Visual Interpretable and Controllable Video Captioning"
          ],
          [
            "URL",
            "https://openaccess.thecvf.com/content_CVPRW_2019/html/Sight_and_Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.html"
          ]
        ],
        "resource": "storage/i2441.pdf",
        "selectable": false
      }
    ],
    "item_title": "Video Captioning",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  },
  {
    "text": "Visual Grounding",
    "item-id": "c48,i3639",
    "nodes": [
      {
        "text": "DINO",
        "item-id": "i3639",
        "nodes": [
          {
            "text": "Zhang et al_2022_DINO.pdf",
            "item-id": "i3641",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Zhang et al_2022_DINO.pdf",
            "item_type": "attachment",
            "item_note": "<div class=\"zotero-note znv1\"><p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_AP9SWK7Q/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/3\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/3\">DINO: DETR with Improved DeNoising Anchor Boxes</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/3\">Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/4\">Model Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/5\">Contrastive DeNoising Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/6\">Look Forward Twice</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/7\">Mixed Query Selection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/7\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/7\">Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/7\">Main Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/8\">Comparison with SOTA Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/9\">Ablation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/9\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/13\">Optimized DN-Deformable DETR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/13\">Results using SwinL backbone without pre-training on Object 365</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/13\">Test Time Augmentations (TTA)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/14\">Training Efficiency</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/14\">Additional Analysis on our Model Components</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">More Implementation Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">Adaptive DN groups</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">Large-Scale Model Pre-trianing</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">Other Implementation Details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">Basic hyper-parameters.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">Loss function.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/15\">Detailed model components.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/16\">Training augmentation.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/16\">Multi-scale setting.</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/16\">Detailed Hyper-parameters</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/17\">Inference Speed and GFLOPs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/17\">Why DINO improves AP on small objects by large</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/17\">More details about Look Forward Twice (LFT)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/18\">Visualizations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AP9SWK7Q/18\">LVIS results</a></li></ul></div>",
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Zhang et al_2022_DINO.pdf"
              ]
            ],
            "resource": "storage/i3641.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "DINO",
        "item_type": "conferencePaper",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "We present DINO (DETR with Improved deNoising anchOr boxes), a strong end-to-end object detector. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a look forward twice scheme for box prediction, and a mixed query selection method for anchor initialization. DINO achieves 49.4AP in 12 epochs and 51.3AP in 24 epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of +6.0AP and +2.7AP, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO val2017 (63.2AP) and test-dev (63.3AP) with model size under 1 billion parameters. Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. The code will be available."
          ],
          [
            "Access Date",
            "2024-01-12 06:48:13"
          ],
          [
            "Conference Name",
            "The Eleventh International Conference on Learning Representations"
          ],
          [
            "Creators",
            "Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, Heung-Yeung Shum"
          ],
          [
            "Date",
            "2022-09-29 2022/09/29"
          ],
          [
            "Language",
            "en"
          ],
          [
            "Library Catalog",
            "openreview.net"
          ],
          [
            "Short Title",
            "DINO"
          ],
          [
            "Title",
            "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"
          ],
          [
            "URL",
            "https://openreview.net/forum?id=3mRwyG5one"
          ]
        ],
        "resource": "storage/i3641.pdf",
        "selectable": false
      },
      {
        "text": "Grounding DINO",
        "item-id": "i2796",
        "nodes": [
          {
            "text": "Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO",
            "item-id": "n2841",
            "icon": "glyphicon glyphicon-text-background",
            "item_title": "Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO",
            "item_type": "note",
            "item_note": "<div class=\"zotero-note znv1\">Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO</div>",
            "node_type": "note"
          },
          {
            "text": "Liu et al_2023_Grounding DINO.pdf",
            "item-id": "i2840",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Liu et al_2023_Grounding DINO.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Liu et al_2023_Grounding DINO.pdf"
              ]
            ],
            "resource": "storage/i2840.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Grounding DINO",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \\url{https://github.com/IDEA-Research/GroundingDINO}."
          ],
          [
            "Access Date",
            "2023-07-18 01:38:30"
          ],
          [
            "Archiveid",
            "arXiv:2303.05499"
          ],
          [
            "Creators",
            "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang"
          ],
          [
            "DOI",
            "10.48550/arXiv.2303.05499"
          ],
          [
            "Date",
            "2023-03-20 2023-03-20"
          ],
          [
            "Extra",
            "arXiv:2303.05499 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Grounding DINO"
          ],
          [
            "Title",
            "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2303.05499"
          ]
        ],
        "resource": "storage/i2840.pdf",
        "selectable": false
      },
      {
        "text": "Video-GroundingDINO",
        "item-id": "i3588",
        "nodes": [
          {
            "text": "Wasim et al_2023_Video-GroundingDINO.pdf",
            "item-id": "i3590",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Wasim et al_2023_Video-GroundingDINO.pdf",
            "item_type": "attachment",
            "item_note": null,
            "node_type": "item",
            "metadata": [
              [
                "Title",
                "Wasim et al_2023_Video-GroundingDINO.pdf"
              ]
            ],
            "resource": "storage/i3590.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-log-in",
        "item_title": "Video-GroundingDINO",
        "item_type": "preprint",
        "item_note": null,
        "node_type": "item",
        "metadata": [
          [
            "Abstract Note",
            "Video grounding aims to localize a spatio-temporal section in a video corresponding to an input text query. This paper addresses a critical limitation in current video grounding methodologies by introducing an Open-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent closed-set approaches that struggle with open-vocabulary scenarios due to limited training data and predefined vocabularies, our model leverages pre-trained representations from foundational spatial grounding models. This empowers it to effectively bridge the semantic gap between natural language and diverse visual content, achieving strong performance in closed-set and open-vocabulary settings. Our contributions include a novel spatio-temporal video grounding model, surpassing state-of-the-art results in closed-set evaluations on multiple datasets and demonstrating superior performance in open-vocabulary scenarios. Notably, the proposed model outperforms state-of-the-art methods in closed-set settings on VidSTG (Declarative and Interrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in open-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model surpasses the recent best-performing models by $4.26$ m_vIoU and $1.83\\%$ accuracy, demonstrating its efficacy in handling diverse linguistic and visual concepts for improved video understanding. Our codes will be released at https://github.com/TalalWasim/Video-GroundingDINO."
          ],
          [
            "Access Date",
            "2024-01-09 04:41:09"
          ],
          [
            "Archiveid",
            "arXiv:2401.00901"
          ],
          [
            "Creators",
            "Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan"
          ],
          [
            "Date",
            "2023-12-31 2023-12-31"
          ],
          [
            "Extra",
            "arXiv:2401.00901 [cs]"
          ],
          [
            "Library Catalog",
            "arXiv.org"
          ],
          [
            "Repository",
            "arXiv"
          ],
          [
            "Short Title",
            "Video-GroundingDINO"
          ],
          [
            "Title",
            "Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding"
          ],
          [
            "URL",
            "http://arxiv.org/abs/2401.00901"
          ]
        ],
        "resource": "storage/i3590.pdf",
        "selectable": false
      }
    ],
    "item_title": "Visual Grounding",
    "item_type": null,
    "item_note": null,
    "node_type": "collection",
    "selectable": false
  }
]