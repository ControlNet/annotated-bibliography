
@inproceedings{mildenhall_nerf_2020,
	address = {Springer, Cham},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58451-1},
	doi = {10.1007},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Mildenhall, Ben and Srinivasan, Pratul and Tancik, Matthew and Barron, Jonathan and Ramamoorthi, Ravi and Ng, Ren},
	year = {2020},
	doi = {10.1007/978-3-030-58452-8_24},
	pages = {405--421},
	file = {Mildenhall et al_2020_NeRF.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Mildenhall et al_2020_NeRF.pdf:application/pdf},
}

@article{cootes_active_1995,
	title = {Active {Shape} {Models}-{Their} {Training} and {Application}},
	volume = {61},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314285710041},
	doi = {10.1006/cviu.1995.1004},
	abstract = {Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our Active Shape Models can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images.},
	language = {en},
	number = {1},
	urldate = {2021-03-16},
	journal = {Computer Vision and Image Understanding},
	author = {Cootes, T. F. and Taylor, C. J. and Cooper, D. H. and Graham, J.},
	month = jan,
	year = {1995},
	keywords = {Viewed},
	pages = {38--59},
	file = {Cootes et al_1995_Active Shape Models-Their Training and Application.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Cootes et al_1995_Active Shape Models-Their Training and Application.pdf:application/pdf;ScienceDirect Snapshot:/Users/controlnet/Zotero/storage/A9CBSJ67/S1077314285710041.html:text/html},
}

@article{vincent_stacked_2010,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	volume = {11},
	issn = {1532-4435},
	shorttitle = {Stacked {Denoising} {Autoencoders}},
	abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
	journal = {The Journal of Machine Learning Research},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	month = dec,
	year = {2010},
	keywords = {Viewed},
	pages = {3371--3408},
	file = {Vincent et al_2010_Stacked Denoising Autoencoders.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Vincent et al_2010_Stacked Denoising Autoencoders.pdf:application/pdf},
}

@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
	language = {en},
	number = {5786},
	urldate = {2021-03-16},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	note = {Publisher: American Association for the Advancement of Science
Section: Report},
	keywords = {Viewed},
	pages = {504--507},
	file = {Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/MVMYC94D/504.html:text/html},
}

@article{kramer_nonlinear_1991,
	title = {Nonlinear principal component analysis using autoassociative neural networks},
	volume = {37},
	copyright = {Copyright © 1991 American Institute of Chemical Engineers},
	issn = {1547-5905},
	url = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690370209},
	doi = {https://doi.org/10.1002/aic.690370209},
	abstract = {Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.},
	language = {en},
	number = {2},
	urldate = {2021-03-16},
	journal = {AIChE Journal},
	author = {Kramer, Mark A.},
	year = {1991},
	note = {\_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690370209},
	keywords = {Viewed},
	pages = {233--243},
	file = {Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Kramer_1991_Nonlinear principal component analysis using autoassociative neural networks.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/TA7ULUCD/aic.html:text/html},
}

@article{liu_video_2020,
	title = {Video {Super} {Resolution} {Based} on {Deep} {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Video {Super} {Resolution} {Based} on {Deep} {Learning}},
	url = {http://arxiv.org/abs/2007.12928},
	abstract = {In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning.},
	urldate = {2021-05-04},
	journal = {arXiv:2007.12928 [cs, eess]},
	author = {Liu, Hongying and Ruan, Zhubo and Zhao, Peng and Dong, Chao and Shang, Fanhua and Liu, Yuanyuan and Yang, Linlin},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.12928},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/YFSWND62/2007.html:text/html;Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Liu et al_2020_Video Super Resolution Based on Deep Learning.pdf:application/pdf},
}

@inproceedings{meishvili_learning_2020,
	title = {Learning to {Have} an {Ear} for {Face} {Super}-{Resolution}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html},
	abstract = {We propose a novel method to use both audio and a low-resolution image to perform extreme face super-resolution (a 16x increase of the input size). When the resolution of the input image is very low (e.g., 8x8 pixels), the loss of information is so dire that important details of the original identity have been lost and audio can aid the recovery of a plausible high-resolution image. In fact, audio carries information about facial attributes, such as gender and age. To combine the aural and visual modalities, we propose a method to first build the latent representations of a face from the lone audio track and then from the lone low-resolution image. We then train a network to fuse these two representations. We show experimentally that audio can assist in recovering attributes such as the gender, the age and the identity, and thus improve the correctness of the high-resolution image reconstruction process. Our procedure does not make use of human annotation and thus can be easily trained with existing video datasets. Moreover, we show that our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations.},
	urldate = {2021-05-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Meishvili, Givi and Jenni, Simon and Favaro, Paolo},
	year = {2020},
	keywords = {Viewed},
	pages = {1364--1374},
	file = {Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Meishvili et al_2020_Learning to Have an Ear for Face Super-Resolution.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/3I29U2DH/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.html:text/html},
}

@inproceedings{chen_fsrnet_2018,
	title = {{FSRNet}: {End}-to-{End} {Learning} {Face} {Super}-{Resolution} {With} {Facial} {Priors}},
	shorttitle = {{FSRNet}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html},
	abstract = {Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to superresolve very low-resolution (LR) face images without wellaligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.},
	urldate = {2021-05-04},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chen, Yu and Tai, Ying and Liu, Xiaoming and Shen, Chunhua and Yang, Jian},
	year = {2018},
	pages = {2492--2501},
	file = {Chen et al_2018_FSRNet.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chen et al_2018_FSRNet.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/U6JM6ABA/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html:text/html},
}

@article{jiang_deep_2021,
	title = {Deep {Learning}-based {Face} {Super}-resolution: {A} {Survey}},
	shorttitle = {Deep {Learning}-based {Face} {Super}-resolution},
	url = {http://arxiv.org/abs/2101.03749},
	abstract = {Face super-resolution, also known as face hallucination, which is aimed at enhancing the resolution of low-resolution (LR) one or a sequence of face images to generate the corresponding high-resolution (HR) face images, is a domain-specific image super-resolution problem. Recently, face super-resolution has received considerable attention, and witnessed dazzling advances with deep learning techniques. To date, few summaries of the studies on the deep learning-based face super-resolution are available. In this survey, we present a comprehensive review of deep learning techniques in face super-resolution in a systematic manner. First, we summarize the problem formulation of face super-resolution. Second, we compare the differences between generic image super-resolution and face super-resolution. Third, datasets and performance metrics commonly used in facial hallucination are presented. Fourth, we roughly categorize existing methods according to the utilization of face-specific information. In each category, we start with a general description of design principles, present an overview of representative approaches, and compare the similarities and differences among various methods. Finally, we envision prospects for further technical advancement in this field.},
	urldate = {2021-05-04},
	journal = {arXiv:2101.03749 [cs]},
	author = {Jiang, Junjun and Wang, Chenyang and Liu, Xianming and Ma, Jiayi},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.03749},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/NQA699NJ/2101.html:text/html;Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jiang et al_2021_Deep Learning-based Face Super-resolution.pdf:application/pdf},
}

@inproceedings{ledig_photo-realistic_2017,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	urldate = {2021-05-03},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	year = {2017},
	pages = {4681--4690},
	file = {Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/IN3MZN46/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html:text/html},
}

@inproceedings{wang_esrgan_2018,
	title = {{ESRGAN}: {Enhanced} {Super}-{Resolution} {Generative} {Adversarial} {Networks}},
	shorttitle = {{ESRGAN}},
	url = {https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html},
	abstract = {The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.},
	urldate = {2021-05-03},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV}) {Workshops}},
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},
	year = {2018},
	pages = {0--0},
	file = {Snapshot:/Users/controlnet/Zotero/storage/GQHG2PUB/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html:text/html;Wang et al_2018_ESRGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2018_ESRGAN.pdf:application/pdf},
}

@inproceedings{lim_enhanced_2017,
	title = {Enhanced {Deep} {Residual} {Networks} for {Single} {Image} {Super}-{Resolution}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html},
	abstract = {Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.},
	urldate = {2021-05-03},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Mu Lee, Kyoung},
	year = {2017},
	pages = {136--144},
	file = {Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lim et al_2017_Enhanced Deep Residual Networks for Single Image Super-Resolution.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/RN6MA3UV/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html:text/html},
}

@inproceedings{dong_learning_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning a {Deep} {Convolutional} {Network} for {Image} {Super}-{Resolution}},
	isbn = {978-3-319-10593-2},
	doi = {10.1007/978-3-319-10593-2_13},
	abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {deep convolutional neural networks, Super-resolution},
	pages = {184--199},
	file = {Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Dong et al_2014_Learning a Deep Convolutional Network for Image Super-Resolution.pdf:application/pdf},
}

@article{wang_deep_2020,
	title = {Deep {Learning} for {Image} {Super}-resolution: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Deep {Learning} for {Image} {Super}-resolution},
	doi = {10.1109/TPAMI.2020.2982166},
	abstract = {Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Zhihao and Chen, Jian and Hoi, Steven C.H.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Animals, Benchmark testing, Convolutional Neural Networks (CNN), Deep learning, Deep Learning, Degradation, Generative Adversarial Nets (GAN), Image Super-resolution, Measurement},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/88H69X25/9044873.html:text/html;Wang et al_2020_Deep Learning for Image Super-resolution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2020_Deep Learning for Image Super-resolution.pdf:application/pdf},
}

@article{he_eigengan_2021,
	title = {{EigenGAN}: {Layer}-{Wise} {Eigen}-{Learning} for {GANs}},
	shorttitle = {{EigenGAN}},
	url = {http://arxiv.org/abs/2104.12476},
	abstract = {Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via the adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of "eigen-dimensions" at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, under the linear circumstance, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow.},
	journal = {arXiv:2104.12476 [cs, stat]},
	author = {He, Zhenliang and Kan, Meina and Shan, Shiguang},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12476},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/ZHZ55574/2104.html:text/html;He et al_2021_EigenGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/He et al_2021_EigenGAN.pdf:application/pdf},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/JWTXXQUI/2010.html:text/html;Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf:application/pdf},
}

@inproceedings{bello_attention_2019,
	title = {Attention {Augmented} {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V.},
	year = {2019},
	keywords = {Viewed},
	pages = {3286--3295},
	file = {Bello et al_2019_Attention Augmented Convolutional Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Bello et al_2019_Attention Augmented Convolutional Networks.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/4VKKC22S/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html:text/html},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the t...},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	keywords = {Viewed},
	pages = {448--456},
	file = {Ioffe_Szegedy_2015_Batch Normalization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Ioffe_Szegedy_2015_Batch Normalization.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/T2BGYQWB/ioffe15.html:text/html},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/39KTZ95D/1607.html:text/html;Ba et al_2016_Layer Normalization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Ba et al_2016_Layer Normalization.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {Viewed},
	pages = {770--778},
	file = {He et al_2016_Deep Residual Learning for Image Recognition.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/He et al_2016_Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/R4UWJIIU/1406.html:text/html;Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	keywords = {Viewed},
	pages = {6000--6010},
	file = {Vaswani et al_2017_Attention is all you need.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Vaswani et al_2017_Attention is all you need.pdf:application/pdf},
}

@article{fan_multiscale_2021,
	title = {Multiscale {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.11227},
	abstract = {We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast},
	journal = {arXiv:2104.11227 [cs]},
	author = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11227},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/3CH8Q456/2104.html:text/html;Fan et al_2021_Multiscale Vision Transformers.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Fan et al_2021_Multiscale Vision Transformers.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2021-05-01},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/3LCNJLQ5/Long-Short-Term-Memory.html:text/html},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://www.aclweb.org/anthology/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Devlin et al_2019_BERT.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Devlin et al_2019_BERT.pdf:application/pdf},
}

@inproceedings{parmar_image_2018,
	title = {Image {Transformer}},
	url = {http://proceedings.mlr.press/v80/parmar18a.html},
	abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual seq...},
	language = {en},
	urldate = {2021-04-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {4055--4064},
	file = {Parmar et al_2018_Image Transformer.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Parmar et al_2018_Image Transformer2.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/5JCVU8L8/parmar18a.html:text/html},
}

@article{khan_transformers_2021,
	title = {Transformers in {Vision}: {A} {Survey}},
	shorttitle = {Transformers in {Vision}},
	url = {http://arxiv.org/abs/2101.01169},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
	urldate = {2021-04-23},
	journal = {arXiv:2101.01169 [cs]},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	month = feb,
	year = {2021},
	note = {arXiv: 2101.01169},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/GWYVPP4U/2101.html:text/html;Khan et al_2021_Transformers in Vision.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Khan et al_2021_Transformers in Vision.pdf:application/pdf},
}

@incollection{sharma_survey_2021,
	address = {Cham},
	series = {Intelligent {Systems} {Reference} {Library}},
	title = {A {Survey} on {Automatic} {Multimodal} {Emotion} {Recognition} in the {Wild}},
	isbn = {978-3-030-51870-7},
	url = {https://doi.org/10.1007/978-3-030-51870-7_3},
	abstract = {Affective computing has been an active area of research for the past two decades. One of the major component of affective computing is automatic emotion recognition. This chapter gives a detailed overview of different emotion recognition techniques and the predominantly used signal modalities. The discussion starts with the different emotion representations and their limitations. Given that affective computing is a data-driven research area, a thorough comparison of standard emotion labelled databases is presented. Based on the source of the data, feature extraction and analysis techniques are presented for emotion recognition. Further, applications of automatic emotion recognition are discussed along with current and important issues such as privacy and fairness.},
	language = {en},
	urldate = {2021-04-23},
	booktitle = {Advances in {Data} {Science}: {Methodologies} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Sharma, Garima and Dhall, Abhinav},
	editor = {Phillips-Wren, Gloria and Esposito, Anna and Jain, Lakhmi C.},
	year = {2021},
	doi = {10.1007/978-3-030-51870-7_3},
	pages = {35--64},
	file = {Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Sharma_Dhall_2021_A Survey on Automatic Multimodal Emotion Recognition in the Wild.pdf:application/pdf},
}

@inproceedings{chung_perfect_2019,
	title = {Perfect {Match}: {Improved} {Cross}-modal {Embeddings} for {Audio}-visual {Synchronisation}},
	shorttitle = {Perfect {Match}},
	doi = {10.1109/ICASSP.2019.8682524},
	abstract = {This paper proposes a new strategy for learning powerful cross-modal embeddings for audio-to-video synchronisation. Here, we set up the problem as one of cross-modal retrieval, where the objective is to find the most relevant audio segment given a short video clip. The method builds on the recent advances in learning representations from cross-modal self-supervision. The main contributions of this paper are as follows: (1) we propose a new learning strategy where the embeddings are learnt via a multi-way matching problem, as opposed to a binary classification (matching or non-matching) problem as proposed by recent papers; (2) we demonstrate that performance of this method far exceeds the existing baselines on the synchronisation task; (3) we use the learnt embeddings for visual speech recognition in self-supervision, and show that the performance matches the representations learnt end-to-end in a fully-supervised manner.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chung, Soo-Whan and Chung, Joon Son and Kang, Hong-Goo},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {audio-visual synchronisation, cross-modal embedding, Cross-modal supervision, Lips, self-supervised learning, Speech recognition, Streaming media, Synchronization, Task analysis, Training, Visualization},
	pages = {3965--3969},
	file = {Chung et al_2019_Perfect Match.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chung et al_2019_Perfect Match.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/CCLS68BR/8682524.html:text/html},
}

@inproceedings{chung_out_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Out of {Time}: {Automated} {Lip} {Sync} in the {Wild}},
	isbn = {978-3-319-54427-4},
	shorttitle = {Out of {Time}},
	doi = {10.1007/978-3-319-54427-4_19},
	abstract = {The goal of this work is to determine the audio-video synchronisation between mouth motion and speech in a video.We propose a two-stream ConvNet architecture that enables the mapping between the sound and the mouth images to be trained end-to-end from unlabelled data. The trained network is used to determine the lip-sync error in a video.We apply the network to two further tasks: active speaker detection and lip reading. On both tasks we set a new state-of-the-art on standard benchmark datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Chung, Joon Son and Zisserman, Andrew},
	editor = {Chen, Chu-Song and Lu, Jiwen and Ma, Kai-Kuang},
	year = {2017},
	keywords = {Audio Stream, Convolutional Neural Network, Equal Error Rate, Mouth Shape, Phoneme Recognition},
	pages = {251--263},
	file = {Chung_Zisserman_2017_Out of Time.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chung_Zisserman_2017_Out of Time.pdf:application/pdf},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Carion et al_2020_End-to-End Object Detection with Transformers.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Carion et al_2020_End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{wang_high-resolution_2018,
	title = {High-{Resolution} {Image} {Synthesis} and {Semantic} {Manipulation} {With} {Conditional} {GANs}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html},
	abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
	urldate = {2021-04-22},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	year = {2018},
	pages = {8798--8807},
	file = {Snapshot:/Users/controlnet/Zotero/storage/3FUDQRMP/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html:text/html;Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2018_High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs.pdf:application/pdf},
}

@inproceedings{saito_temporal_2017,
	title = {Temporal {Generative} {Adversarial} {Nets} {With} {Singular} {Value} {Clipping}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html},
	abstract = {In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.},
	urldate = {2021-04-22},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Saito, Masaki and Matsumoto, Eiichi and Saito, Shunta},
	year = {2017},
	pages = {2830--2839},
	file = {Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Saito et al_2017_Temporal Generative Adversarial Nets With Singular Value Clipping.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/IZS7A9V9/Saito_Temporal_Generative_Adversarial_ICCV_2017_paper.html:text/html},
}

@inproceedings{arandjelovic_look_2017,
	title = {Look, {Listen} and {Learn}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html},
	abstract = {We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.},
	urldate = {2021-04-22},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Arandjelovic, Relja and Zisserman, Andrew},
	year = {2017},
	pages = {609--617},
	file = {Arandjelovic_Zisserman_2017_Look, Listen and Learn.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Arandjelovic_Zisserman_2017_Look, Listen and Learn.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/AU3848SY/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html:text/html},
}

@inproceedings{kumar_robust_2020,
	title = {Robust {One} {Shot} {Audio} to {Video} {Generation}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html},
	urldate = {2021-04-22},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Kumar, Neeraj and Goel, Srishti and Narang, Ankur and Hasan, Mujtaba},
	year = {2020},
	keywords = {Viewed},
	pages = {770--771},
	file = {Kumar et al_2020_Robust One Shot Audio to Video Generation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Kumar et al_2020_Robust One Shot Audio to Video Generation.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/Q9YPKPYG/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html:text/html},
}

@inproceedings{amodei_deep_2016,
	title = {Deep {Speech} 2 : {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {http://proceedings.mlr.press/v48/amodei16.html},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-eng...},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi and Jiang, Bing and Ju, Cai and Jun, Billy and LeGresley, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {173--182},
	file = {Amodei et al_2016_Deep Speech 2.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Amodei et al_2016_Deep Speech 2.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/XUINVS2H/amodei16.html:text/html},
}

@inproceedings{park_semantic_2019,
	title = {Semantic {Image} {Synthesis} {With} {Spatially}-{Adaptive} {Normalization}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html},
	urldate = {2021-04-22},
	booktitle = {We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. {Previous} methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. {Instead}, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. {Experiments} on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. {Finally}, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. {Code} is available upon publication.},
	author = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
	year = {2019},
	pages = {2337--2346},
	file = {Park et al_2019_Semantic Image Synthesis With Spatially-Adaptive Normalization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Park et al_2019_Semantic Image Synthesis With Spatially-Adaptive Normalization.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/G8CYMQAZ/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html:text/html},
}

@inproceedings{vondrick_generating_2016,
	title = {Generating {Videos} with {Scene} {Dynamics}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf},
	abstract = {We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {613--621},
	file = {Snapshot:/Users/controlnet/Zotero/storage/LUFQTG57/04025959b191f8f9de3f924f0940515f-Abstract.html:text/html;Vondrick et al_2016_Generating Videos with Scene Dynamics.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Vondrick et al_2016_Generating Videos with Scene Dynamics.pdf:application/pdf},
}

@inproceedings{wang_video--video_2018,
	title = {Video-to-{Video} {Synthesis}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf},
	abstract = {We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Liu, Guilin and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1144--1156},
	file = {Snapshot:/Users/controlnet/Zotero/storage/TS53RTJY/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html:text/html;Wang et al_2018_Video-to-Video Synthesis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2018_Video-to-Video Synthesis.pdf:application/pdf},
}

@article{suwajanakorn_synthesizing_2017,
	title = {Synthesizing {Obama}: learning lip sync from audio},
	volume = {36},
	issn = {0730-0301},
	shorttitle = {Synthesizing {Obama}},
	url = {http://doi.org/10.1145/3072959.3073640},
	doi = {10.1145/3072959.3073640},
	abstract = {Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.},
	number = {4},
	urldate = {2021-04-22},
	journal = {ACM Transactions on Graphics},
	author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
	month = jul,
	year = {2017},
	keywords = {audio, audiovisual speech, big data, face synthesis, lip sync, LSTM, RNN, uncanny valley, videos},
	pages = {95:1--95:13},
	file = {Suwajanakorn et al_2017_Synthesizing Obama.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Suwajanakorn et al_2017_Synthesizing Obama.pdf:application/pdf},
}

@article{aneja_real-time_2019,
	title = {Real-{Time} {Lip} {Sync} for {Live} {2D} {Animation}},
	url = {http://arxiv.org/abs/1910.08685},
	abstract = {The emergence of commercial tools for real-time performance-based 2D animation has enabled 2D characters to appear on live broadcasts and streaming platforms. A key requirement for live animation is fast and accurate lip sync that allows characters to respond naturally to other actors or the audience through the voice of a human performer. In this work, we present a deep learning based interactive system that automatically generates live lip sync for layered 2D characters using a Long Short Term Memory (LSTM) model. Our system takes streaming audio as input and produces viseme sequences with less than 200ms of latency (including processing time). Our contributions include specific design decisions for our feature definition and LSTM configuration that provide a small but useful amount of lookahead to produce accurate lip sync. We also describe a data augmentation procedure that allows us to achieve good results with a very small amount of hand-animated training data (13-20 minutes). Extensive human judgement experiments show that our results are preferred over several competing methods, including those that only support offline (non-live) processing. Video summary and supplementary results at GitHub link: https://github.com/deepalianeja/CharacterLipSync2D},
	urldate = {2021-04-22},
	journal = {arXiv:1910.08685 [cs]},
	author = {Aneja, Deepali and Li, Wilmot},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.08685},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics, Computer Science - Human-Computer Interaction},
	file = {Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Aneja_Li_2019_Real-Time Lip Sync for Live 2D Animation.pdf:application/pdf;arXiv.org Snapshot:/Users/controlnet/Zotero/storage/7I4RYARH/1910.html:text/html},
}

@inproceedings{tian_audio2face_2019,
	title = {{Audio2Face}: {Generating} {Speech}/{Face} {Animation} from {Single} {Audio} with {Attention}-{Based} {Bidirectional} {LSTM} {Networks}},
	shorttitle = {{Audio2Face}},
	doi = {10.1109/ICMEW.2019.00069},
	abstract = {We propose an end to end deep learning approach for generating real-time facial animation from just audio. Specifically, our deep architecture employs deep bidirectional long short-term memory network and attention mechanism to discover the latent representations of time-varying contextual information within the speech and recognize the significance of different information contributed to certain face status. Therefore, our model is able to drive different levels of facial movements at inference and automatically keep up with the corresponding pitch and latent speaking style in the input audio, with no assumption or further human intervention. Evaluation results show that our method could not only generate accurate lip movements from audio, but also successfully regress the speaker's time-varying facial movements.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Multimedia} {Expo} {Workshops} ({ICMEW})},
	author = {Tian, Guanzhong and Yuan, Yi and Liu, Yong},
	month = jul,
	year = {2019},
	keywords = {Animation, Long short-term memory network, Attention mechanism},
	pages = {366--371},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/YQE2BYHQ/8795082.html:text/html;Tian et al_2019_Audio2Face.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tian et al_2019_Audio2Face.pdf:application/pdf},
}

@article{ramachandran_stand-alone_2019,
	title = {Stand-{Alone} {Self}-{Attention} in {Vision} {Models}},
	url = {http://arxiv.org/abs/1906.05909},
	abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12\% fewer FLOPS and 29\% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39\% fewer FLOPS and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
	urldate = {2021-04-09},
	journal = {arXiv:1906.05909 [cs]},
	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05909},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/ZHYHSP49/1906.html:text/html;Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf:application/pdf},
}

@article{vaswani_scaling_2021,
	title = {Scaling {Local} {Self}-{Attention} for {Parameter} {Efficient} {Visual} {Backbones}},
	url = {http://arxiv.org/abs/2103.12731},
	abstract = {Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.},
	urldate = {2021-04-09},
	journal = {arXiv:2103.12731 [cs]},
	author = {Vaswani, Ashish and Ramachandran, Prajit and Srinivas, Aravind and Parmar, Niki and Hechtman, Blake and Shlens, Jonathon},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.12731},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/Y8LLUYWL/2103.html:text/html;Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Vaswani et al_2021_Scaling Local Self-Attention for Parameter Efficient Visual Backbones.pdf:application/pdf},
}

@inproceedings{huang_arbitrary_2017,
	title = {Arbitrary {Style} {Transfer} in {Real}-{Time} {With} {Adaptive} {Instance} {Normalization}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html},
	urldate = {2021-03-29},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Huang, Xun and Belongie, Serge},
	year = {2017},
	keywords = {Unreviewed},
	pages = {1501--1510},
	file = {Huang_Belongie_2017_Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Huang_Belongie_2017_Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/ESXAEPLT/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html:text/html},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2021-04-08},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/VPGCMDSE/1406.html:text/html;Goodfellow et al_2014_Generative Adversarial Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Goodfellow et al_2014_Generative Adversarial Networks.pdf:application/pdf},
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} {With} {Convolutions}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification.},
	urldate = {2021-03-16},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	keywords = {Viewed},
	pages = {1--9},
	file = {Snapshot:/Users/controlnet/Zotero/storage/PTFRKBPB/Szegedy_Going_Deeper_With_2015_CVPR_paper.html:text/html;Szegedy et al_2015_Going Deeper With Convolutions.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Szegedy et al_2015_Going Deeper With Convolutions.pdf:application/pdf},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2021-03-16},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/9VVQ6FR8/1409.html:text/html;Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf},
}

@article{lin_network_2014,
	title = {Network {In} {Network}},
	url = {http://arxiv.org/abs/1312.4400},
	abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
	urldate = {2021-03-16},
	journal = {arXiv:1312.4400 [cs]},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	month = mar,
	year = {2014},
	note = {arXiv: 1312.4400},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/GBJXNLGT/1312.html:text/html;Lin et al_2014_Network In Network.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lin et al_2014_Network In Network.pdf:application/pdf},
}

@article{cootes_active_2001,
	title = {Active appearance models},
	volume = {23},
	issn = {1939-3539},
	doi = {10.1109/34.927467},
	abstract = {We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cootes, T. F. and Edwards, G. J. and Taylor, C. J.},
	month = jun,
	year = {2001},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {active appearance models, Active shape model, Deformable models, deformable template, gray-level variation, Image generation, image matching, Image reconstruction, Image segmentation, image texture, Iterative algorithms, iterative method, iterative methods, learning, learning (artificial intelligence), model matching, optimisation, Optimization methods, Robustness, Shape control, shape matching, statistical analysis, statistical models, Surface fitting, texture matching, Viewed},
	pages = {681--685},
	file = {Cootes et al_2001_Active appearance models.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Cootes et al_2001_Active appearance models.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/2Y22Q7QV/927467.html:text/html},
}

@article{frs_liii_1901,
	title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
	volume = {2},
	issn = {1941-5982},
	url = {https://doi.org/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	number = {11},
	urldate = {2021-04-08},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {F.R.S, Karl Pearson},
	month = nov,
	year = {1901},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/14786440109462720},
	pages = {559--572},
	file = {F.R.S_1901_LIII.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/F.R.S_1901_LIII.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/NTCZUUMU/14786440109462720.html:text/html},
}

@article{makhzani_adversarial_2016,
	title = {Adversarial {Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
	urldate = {2021-03-16},
	journal = {arXiv:1511.05644 [cs]},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = may,
	year = {2016},
	note = {arXiv: 1511.05644},
	keywords = {Computer Science - Machine Learning, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/HFBFFJF8/1511.html:text/html;Makhzani et al_2016_Adversarial Autoencoders.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Makhzani et al_2016_Adversarial Autoencoders.pdf:application/pdf},
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2021-03-16},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.0580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/PCYSDJFS/1207.html:text/html;Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf:application/pdf},
}

@article{doersch_tutorial_2021,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2021-03-16},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = jan,
	year = {2021},
	note = {arXiv: 1606.05908},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Unreviewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/GBJ99QGT/1606.html:text/html;Doersch_2021_Tutorial on Variational Autoencoders.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Doersch_2021_Tutorial on Variational Autoencoders.pdf:application/pdf},
}

@article{zhu_knowledge_2020,
	title = {Knowledge {Distillation} for {Face} {Photo}-{Sketch} {Synthesis}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2020.3030536},
	abstract = {Significant progress has been made with face photo-sketch synthesis in recent years due to the development of deep convolutional neural networks, particularly generative adversarial networks (GANs). However, the performance of existing methods is still limited because of the lack of training data (photo-sketch pairs). To address this challenge, we investigate the effect of knowledge distillation (KD) on training neural networks for the face photo-sketch synthesis task and propose an effective KD model to improve the performance of synthetic images. In particular, we utilize a teacher network trained on a large amount of data in a related task to separately learn knowledge of the face photo and knowledge of the face sketch and simultaneously transfer this knowledge to two student networks designed for the face photo-sketch synthesis task. In addition to assimilating the knowledge from the teacher network, the two student networks can mutually transfer their own knowledge to further enhance their learning. To further enhance the perception quality of the synthetic image, we propose a KD+ model that combines GANs with KD. The generator can produce images with more realistic textures and less noise under the guide of knowledge. Extensive experiments and a user study demonstrate the superiority of our models over the state-of-the-art methods.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhu, M. and Li, J. and Wang, N. and Gao, X.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Task analysis, Unreviewed, Data models, Face photo-sketch synthesis, Face recognition, Faces, Gallium nitride, generative adversarial networks (GANs), knowledge distillation (KD), Knowledge engineering, teacher-student model., Training data},
	pages = {1--14},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/FZIX6WUE/9240973.html:text/html;Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhu et al_2020_Knowledge Distillation for Face Photo-Sketch Synthesis.pdf:application/pdf},
}

@article{zheng_survey_2020,
	title = {A {Survey} of {Deep} {Facial} {Attribute} {Analysis}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01308-z},
	doi = {10.1007/s11263-020-01308-z},
	abstract = {Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.},
	language = {en},
	number = {8},
	urldate = {2021-03-16},
	journal = {International Journal of Computer Vision},
	author = {Zheng, Xin and Guo, Yanqing and Huang, Huaibo and Li, Yi and He, Ran},
	month = sep,
	year = {2020},
	keywords = {Unreviewed},
	pages = {2002--2034},
	file = {Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zheng et al_2020_A Survey of Deep Facial Attribute Analysis.pdf:application/pdf},
}

@article{yuan_ckd_2020,
	title = {{CKD}: {Cross}-{Task} {Knowledge} {Distillation} for {Text}-to-{Image} {Synthesis}},
	volume = {22},
	issn = {1941-0077},
	shorttitle = {{CKD}},
	doi = {10.1109/TMM.2019.2951463},
	abstract = {Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.},
	number = {8},
	journal = {IEEE Transactions on Multimedia},
	author = {Yuan, M. and Peng, Y.},
	month = aug,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Task analysis, Visualization, Unreviewed, learning (artificial intelligence), approximate the real data distribution, CKD, complex RGB pixel space, consistent semantic contents, cross-task knowledge distillation approach, Generative adversarial networks, highly challenging task, huge pixel space, Image color analysis, image colour analysis, image contents, image semantic understanding, Image synthesis, knowledge distillation, multiple image semantic understanding tasks, multistage knowledge distillation paradigm, Neural networks, pattern recognition, photo-realistic images, realistic images, semantic consistency issue, semantic representation, semantic space, Semantics, synthetic images, T2IS, text analysis, text description, text-to-image synthesis, Text-to-image synthesis, transfer learning, visual quality issue},
	pages = {1955--1968},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/DTFEDK7M/8890866.html:text/html;Yuan_Peng_2020_CKD.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yuan_Peng_2020_CKD.pdf:application/pdf},
}

@inproceedings{viazovetskyi_stylegan2_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{StyleGAN2} {Distillation} for {Feed}-{Forward} {Image} {Manipulation}},
	isbn = {978-3-030-58542-6},
	doi = {10.1007/978-3-030-58542-6_11},
	abstract = {StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces’ transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Viazovetskyi, Yuri and Ivashkin, Vladimir and Kashin, Evgeny},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Unreviewed, Computer vision, Distillation, StyleGAN2, Synthetic data},
	pages = {170--186},
	file = {Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Viazovetskyi et al_2020_StyleGAN2 Distillation for Feed-Forward Image Manipulation.pdf:application/pdf},
}

@article{pihlgren_pretraining_2020,
	title = {Pretraining {Image} {Encoders} without {Reconstruction} via {Feature} {Prediction} {Loss}},
	url = {http://arxiv.org/abs/2003.07441},
	abstract = {This work investigates three methods for calculating loss for autoencoder-based pretraining of image encoders: The commonly used reconstruction loss, the more recently introduced deep perceptual similarity loss, and a feature prediction loss proposed here; the latter turning out to be the most efficient choice. Standard auto-encoder pretraining for deep learning tasks is done by comparing the input image and the reconstructed image. Recent work shows that predictions based on embeddings generated by image autoencoders can be improved by training with perceptual loss, i.e., by adding a loss network after the decoding step. So far the autoencoders trained with loss networks implemented an explicit comparison of the original and reconstructed images using the loss network. However, given such a loss network we show that there is no need for the time-consuming task of decoding the entire image. Instead, we propose to decode the features of the loss network, hence the name "feature prediction loss". To evaluate this method we perform experiments on three standard publicly available datasets (LunarLander-v2, STL-10, and SVHN) and compare six different procedures for training image encoders (pixel-wise, perceptual similarity, and feature prediction losses; combined with two variations of image and feature encoding/decoding). The embedding-based prediction results show that encoders trained with feature prediction loss is as good or better than those trained with the other two losses. Additionally, the encoder is significantly faster to train using feature prediction loss in comparison to the other losses. The method implementation used in this work is available online: https://github.com/guspih/Perceptual-Autoencoders},
	urldate = {2021-03-18},
	journal = {arXiv:2003.07441 [cs]},
	author = {Pihlgren, Gustav Grund and Sandin, Fredrik and Liwicki, Marcus},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.07441},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Unreviewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/TYYGD6EZ/2003.html:text/html;Pihlgren et al_2020_Pretraining Image Encoders without Reconstruction via Feature Prediction Loss.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Pihlgren et al_2020_Pretraining Image Encoders without Reconstruction via Feature Prediction Loss.pdf:application/pdf},
}

@inproceedings{pidhorskyi_adversarial_2020,
	title = {Adversarial {Latent} {Autoencoders}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html},
	abstract = {Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.},
	urldate = {2021-03-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pidhorskyi, Stanislav and Adjeroh, Donald A. and Doretto, Gianfranco},
	year = {2020},
	keywords = {Unreviewed},
	pages = {14104--14113},
	file = {Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Pidhorskyi et al_2020_Adversarial Latent Autoencoders.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/8TBAEC8L/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.html:text/html},
}

@inproceedings{li_semantic_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Semantic {Relation} {Preserving} {Knowledge} {Distillation} for {Image}-to-{Image} {Translation}},
	isbn = {978-3-030-58574-7},
	doi = {10.1007/978-3-030-58574-7_39},
	abstract = {Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher’s feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Zeqi and Jiang, Ruowei and Aarabi, Parham},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Unreviewed, Generative adversarial networks, Image-to-image translation, Knowledge distillation, Model compression},
	pages = {648--663},
	file = {Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li et al_2020_Semantic Relation Preserving Knowledge Distillation for Image-to-Image.pdf:application/pdf},
}

@article{li_towards_2020,
	title = {Towards {Cross}-{Modality} {Medical} {Image} {Segmentation} with {Online} {Mutual} {Knowledge} {Distillation}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5421},
	doi = {10.1609/aaai.v34i01.5421},
	language = {en},
	number = {01},
	urldate = {2021-03-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Kang and Yu, Lequan and Wang, Shujun and Heng, Pheng-Ann},
	month = apr,
	year = {2020},
	note = {Number: 01},
	keywords = {Unreviewed},
	pages = {775--783},
	file = {Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li et al_2020_Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/K5LHY756/5421.html:text/html},
}

@inproceedings{hong_distilling_2020,
	title = {Distilling {Image} {Dehazing} {With} {Heterogeneous} {Task} {Imitation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html},
	urldate = {2021-03-18},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hong, Ming and Xie, Yuan and Li, Cuihua and Qu, Yanyun},
	year = {2020},
	keywords = {Unreviewed},
	pages = {3462--3471},
	file = {Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Hong et al_2020_Distilling Image Dehazing With Heterogeneous Task Imitation.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/ABU8RUMW/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.html:text/html},
}

@inproceedings{zhai_lifelong_2019,
	title = {Lifelong {GAN}: {Continual} {Learning} for {Conditional} {Image} {Generation}},
	shorttitle = {Lifelong {GAN}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html},
	urldate = {2021-03-22},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Zhai, Mengyao and Chen, Lei and Tung, Frederick and He, Jiawei and Nawhal, Megha and Mori, Greg},
	year = {2019},
	keywords = {Unreviewed},
	pages = {2759--2768},
	file = {Snapshot:/Users/controlnet/Zotero/storage/MCSJBUDY/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html:text/html;Zhai et al_2019_Lifelong GAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhai et al_2019_Lifelong GAN.pdf:application/pdf},
}

@article{wang_deepvid_2019,
	title = {{DeepVID}: {Deep} {Visual} {Interpretation} and {Diagnosis} for {Image} {Classifiers} via {Knowledge} {Distillation}},
	volume = {25},
	issn = {1941-0506},
	shorttitle = {{DeepVID}},
	doi = {10.1109/TVCG.2019.2903943},
	abstract = {Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, J. and Gou, L. and Zhang, W. and Yang, H. and Shen, H.},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Deep learning, Training, Unreviewed, learning (artificial intelligence), Data models, knowledge distillation, Neural networks, Semantics, Analytical models, deep generative model, deep learning experts, Deep neural networks, Deep Neural Networks, deep visual diagnosis, deep visual interpretation, DeepVID, DNN, generative model, image classification, image classifiers, model interpretation, neural nets, safety-critical applications, visual analytics, Visual analytics},
	pages = {2168--2180},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/ZF8Z5MGH/8667661.html:text/html;Wang et al_2019_DeepVID.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2019_DeepVID.pdf:application/pdf},
}

@inproceedings{tung_similarity-preserving_2019,
	title = {Similarity-{Preserving} {Knowledge} {Distillation}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html},
	urldate = {2021-03-22},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Tung, Frederick and Mori, Greg},
	year = {2019},
	keywords = {Unreviewed},
	pages = {1365--1374},
	file = {Snapshot:/Users/controlnet/Zotero/storage/NXTWWDCS/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html:text/html;Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tung_Mori_2019_Similarity-Preserving Knowledge Distillation.pdf:application/pdf},
}

@article{li_reliable_2019,
	title = {Reliable {Crowdsourcing} and {Deep} {Locality}-{Preserving} {Learning} for {Unconstrained} {Facial} {Expression} {Recognition}},
	volume = {28},
	issn = {1941-0042},
	doi = {10.1109/TIP.2018.2868382},
	abstract = {Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30 000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI, and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning-based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community.},
	number = {1},
	journal = {IEEE Transactions on Image Processing},
	author = {Li, S. and Deng, W.},
	month = jan,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Unreviewed, learning (artificial intelligence), Face recognition, basic emotion, CK+ database, compound emotion, Compounds, convolution, cross-database study, crowdsourcing annotation, Databases, deep learning, deep learning-based methods, deep locality-preserving convolutional neural network method, descriptor encodings, DLP-CNN, emotion labels, emotion recognition, expectation-maximisation algorithm, expectation-maximization algorithm, express compound, Expression recognition, Face, face recognition, facial behavior posed, facial expression database, facial images, feedforward neural nets, inter-class scatter, laboratory-controlled emotions, locality closeness, Machine learning, mixture emotions, multimodal expressions recognition, RAF database, RAF-DB, Reactive power, real-world affective face database, real-world emotions, Reliability, unconstrained facial expression recognition, visual databases},
	pages = {356--370},
	file = {Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li_Deng_2019_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained.pdf:application/pdf},
}

@inproceedings{gao_image_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Image {Super}-{Resolution} {Using} {Knowledge} {Distillation}},
	isbn = {978-3-030-20890-5},
	doi = {10.1007/978-3-030-20890-5_34},
	abstract = {The significant improvements in image super-resolution (SR) in recent years is majorly resulted from the use of deeper and deeper convolutional neural networks (CNN). However, both computational time and memory consumption simultaneously increase with the utilization of very deep CNN models, posing challenges to deploy SR models in realtime on computationally limited devices. In this work, we propose a novel strategy that uses a teacher-student network to improve the image SR performance. The training of a small but efficient student network is guided by a deep and powerful teacher network. We have evaluated the performance using different ways of knowledge distillation. Through the validations on four datasets, the proposed method significantly improves the SR performance of a student network without changing its structure. This means that the computational time and the memory consumption do not increase during the testing stage while the SR performance is significantly improved.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2018},
	publisher = {Springer International Publishing},
	author = {Gao, Qinquan and Zhao, Yan and Li, Gen and Tong, Tong},
	editor = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2019},
	keywords = {Super-resolution, Unreviewed, Knowledge distillation, Convolutional neural networks, Teacher-student network},
	pages = {527--541},
	file = {Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Gao et al_2019_Image Super-Resolution Using Knowledge Distillation.pdf:application/pdf},
}

@article{aguinaldo_compressing_2019,
	title = {Compressing {GANs} using {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/1902.00159},
	abstract = {Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super resolution, and synthetic data generation. State-of-the-art GANs often use tens of millions of parameters, making them expensive to deploy for applications in low SWAP (size, weight, and power) hardware, such as mobile devices, and for applications with real time capabilities. There has been no work found to reduce the number of parameters used in GANs. Therefore, we propose a method to compress GANs using knowledge distillation techniques, in which a smaller "student" GAN learns to mimic a larger "teacher" GAN. We show that the distillation methods used on MNIST, CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1, 58:1, and 87:1, respectively, while retaining the quality of the generated image. From our experiments, we observe a qualitative limit for GAN's compression. Moreover, we observe that, with a fixed parameter budget, compressed GANs outperform GANs trained using standard training methods. We conjecture that this is partially owing to the optimization landscape of over-parameterized GANs which allows efficient training using alternating gradient descent. Thus, training an over-parameterized GAN followed by our proposed compression scheme provides a high quality generative model with a small number of parameters.},
	urldate = {2021-03-26},
	journal = {arXiv:1902.00159 [cs, stat]},
	author = {Aguinaldo, Angeline and Chiang, Ping-Yeh and Gain, Alex and Patil, Ameya and Pearson, Kolten and Feizi, Soheil},
	month = jan,
	year = {2019},
	note = {arXiv: 1902.00159},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Unreviewed},
	file = {Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Aguinaldo et al_2019_Compressing GANs using Knowledge Distillation.pdf:application/pdf;arXiv.org Snapshot:/Users/controlnet/Zotero/storage/QQ42XCWA/1902.html:text/html},
}

@article{zhu_toward_2018,
	title = {Toward {Multimodal} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1711.11586},
	abstract = {Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a {\textbackslash}emph\{distribution\} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.},
	urldate = {2021-03-22},
	journal = {arXiv:1711.11586 [cs, stat]},
	author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
	month = oct,
	year = {2018},
	note = {arXiv: 1711.11586},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Graphics, Unreviewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/4JC6HZMQ/1711.html:text/html;Zhu et al_2018_Toward Multimodal Image-to-Image Translation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhu et al_2018_Toward Multimodal Image-to-Image Translation.pdf:application/pdf},
}

@inproceedings{woo_cbam_2018,
	title = {{CBAM}: {Convolutional} {Block} {Attention} {Module}},
	shorttitle = {{CBAM}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html},
	urldate = {2021-03-18},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	year = {2018},
	keywords = {Unreviewed},
	pages = {3--19},
	file = {Snapshot:/Users/controlnet/Zotero/storage/JEERU7SI/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html:text/html;Woo et al_2018_CBAM.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Woo et al_2018_CBAM.pdf:application/pdf},
}

@techreport{rhue_racial_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Racial {Influence} on {Automated} {Perceptions} of {Emotions}},
	url = {https://papers.ssrn.com/abstract=3281765},
	abstract = {The practical applications of artificial intelligence are expanding into various elements of society, leading to a growing interest in the potential biases of such algorithms. Facial analysis, one application of artificial intelligence, is increasingly used in real-word situations. For example, some organizations tell candidates to answer predefined questions in a recorded video and use facial recognition to analyze the potential applicant faces. In addition, some companies are developing facial recognition software to scan the faces in crowds and assess threats, specifically mentioning doubt and anger as emotions that indicate threats.  This study provides evidence that facial recognition software interprets emotions differently based on the person’s race. Using a publically available data set of professional basketball players’ pictures, I compare the emotional analysis from two different facial recognition services, Face   and Microsoft's Face API. Both services interpret black players as having more negative emotions than white players; however, there are two different mechanisms. Face   consistently interprets black players as angrier than white players, even controlling for their degree of smiling. Microsoft registers contempt instead of anger, and it interprets black players as more contemptuous when their facial expressions are ambiguous. As the players’ smile widens, the disparity disappears. This finding has implications for individuals, organizations, and society, and it contributes to the growing literature of bias and/or disparate impact in AI.},
	language = {en},
	number = {ID 3281765},
	urldate = {2021-03-16},
	institution = {Social Science Research Network},
	author = {Rhue, Lauren},
	month = nov,
	year = {2018},
	doi = {10.2139/ssrn.3281765},
	keywords = {Unreviewed, artificial intelligence, bias, coarsened exact matching, econometrics, facial recognition, race},
	file = {Rhue_2018_Racial Influence on Automated Perceptions of Emotions.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Rhue_2018_Racial Influence on Automated Perceptions of Emotions.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/LJCMBRU6/papers.html:text/html},
}

@inproceedings{bao_cvae-gan_2017,
	title = {{CVAE}-{GAN}: {Fine}-{Grained} {Image} {Generation} {Through} {Asymmetric} {Training}},
	shorttitle = {{CVAE}-{GAN}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html},
	urldate = {2021-03-22},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
	year = {2017},
	keywords = {Unreviewed},
	pages = {2745--2754},
	file = {Bao et al_2017_CVAE-GAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Bao et al_2017_CVAE-GAN.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/HJSQ369H/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.html:text/html},
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2021-03-16},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Unreviewed},
	file = {Arjovsky et al_2017_Wasserstein GAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Arjovsky et al_2017_Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/controlnet/Zotero/storage/PWZ8UTT5/1701.html:text/html},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2021-03-16},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Unreviewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/DQWLAZHB/1511.html:text/html;Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf:application/pdf},
}

@inproceedings{larsen_autoencoding_2016,
	title = {Autoencoding beyond pixels using a learned similarity metric},
	url = {http://proceedings.mlr.press/v48/larsen16.html},
	abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GA...},
	language = {en},
	urldate = {2021-03-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	keywords = {Unreviewed},
	pages = {1558--1566},
	file = {Larsen et al_2016_Autoencoding beyond pixels using a learned similarity metric.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Larsen et al_2016_Autoencoding beyond pixels using a learned similarity metric.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/4IHWY4CY/larsen16.html:text/html},
}

@inproceedings{johnson_perceptual_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	isbn = {978-3-319-46475-6},
	doi = {10.1007/978-3-319-46475-6_43},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Super-resolution, Deep learning, Unreviewed, Style transfer},
	pages = {694--711},
	file = {Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:application/pdf},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2021-03-17},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Unreviewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/8KILIT88/1503.html:text/html;Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf:application/pdf},
}

@article{shlens_tutorial_2014,
	title = {A {Tutorial} on {Principal} {Component} {Analysis}},
	url = {http://arxiv.org/abs/1404.1100},
	abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
	urldate = {2021-03-16},
	journal = {arXiv:1404.1100 [cs, stat]},
	author = {Shlens, Jonathon},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.1100},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Unreviewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/CR8D93IJ/1404.html:text/html;Shlens_2014_A Tutorial on Principal Component Analysis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shlens_2014_A Tutorial on Principal Component Analysis.pdf:application/pdf},
}

@article{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	journal = {Advances in neural information processing systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
	month = jan,
	year = {2012},
	keywords = {Unreviewed},
	pages = {1097--1105},
	file = {Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf},
}

@article{bengio_learning_2009,
	title = {Learning {Deep} {Architectures} for {AI}},
	volume = {2},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-006},
	doi = {10.1561/2200000006},
	abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
	language = {en},
	number = {1},
	urldate = {2021-03-16},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Bengio, Y.},
	year = {2009},
	keywords = {Unreviewed},
	pages = {1--127},
	file = {Bengio_2009_Learning Deep Architectures for AI.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Bengio_2009_Learning Deep Architectures for AI.pdf:application/pdf},
}

@inproceedings{rudovic_shape-constrained_2011,
	title = {Shape-constrained {Gaussian} process regression for facial-point-based head-pose normalization},
	doi = {10.1109/ICCV.2011.6126407},
	abstract = {Given the facial points extracted from an image of a face in an arbitrary pose, the goal of facial-point-based head-pose normalization is to obtain the corresponding facial points in a predefined pose (e.g., frontal). This involves inference of complex and high-dimensional mappings due to the large number of the facial points employed, and due to differences in head-pose and facial expression. Most regression-based approaches for learning such mappings focus on modeling correlations only between the inputs (i.e., the facial points in a non-frontal pose) and the outputs (i.e., the facial points in the frontal pose), but not within the inputs and the outputs of the model. This makes these models prone to errors due to noise and outliers in test data, often resulting in anatomically impossible facial configurations formed by their predictions. To address this, we propose Shape-constrained Gaussian Process (SC-GP) regression for facial-point-based head-pose normalization. Specifically, a deformable face-shape model is used to learn a face-shape prior, which is placed on both the input and the output of GP regression in order to constrain the model predictions to anatomically feasible facial configurations. Our extensive experiments on both synthetic and real image data show that the proposed approach generalizes well across poses and handles successfully noise and outliers in test data. In addition, the proposed model outperforms previously proposed approaches to facial-point-based head-pose normalization.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Rudovic, O. and Pantic, M.},
	month = nov,
	year = {2011},
	note = {ISSN: 2380-7504},
	keywords = {Training, Unreviewed, Deformable models, Data models, face recognition, arbitrary pose, Computational modeling, deformable face-shape model, facial point extraction, facial-point-based head-pose normalization, feature extraction, Gaussian processes, high-dimensional mappings, pose estimation, Principal component analysis, regression analysis, Shape, shape-constrained Gaussian process regression, Three dimensional displays},
	pages = {1495--1502},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/T82G2MK8/6126407.html:text/html;Rudovic_Pantic_2011_Shape-constrained Gaussian process regression for facial-point-based head-pose.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Rudovic_Pantic_2011_Shape-constrained Gaussian process regression for facial-point-based head-pose.pdf:application/pdf},
}

@inproceedings{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html},
	urldate = {2021-03-29},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	year = {2019},
	pages = {4401--4410},
	file = {Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/8LKELHY9/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html:text/html},
}

@article{jing_self-supervised_2020,
	title = {Self-supervised {Visual} {Feature} {Learning} with {Deep} {Neural} {Networks}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Self-supervised {Visual} {Feature} {Learning} with {Deep} {Neural} {Networks}},
	doi = {10.1109/TPAMI.2020.2992393},
	abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jing, L. and Tian, Y.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep Learning, Task analysis, Training, Visualization, Convolutional Neural Network, Annotations, Feature extraction, Learning systems, Self-supervised Learning, Transfer Learning, Unsupervised Learning, Videos},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/X8PV7RTB/9086055.html:text/html;Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jing_Tian_2020_Self-supervised Visual Feature Learning with Deep Neural Networks.pdf:application/pdf},
}

@inproceedings{wang_collaborative_2020,
	title = {Collaborative {Distillation} for {Ultra}-{Resolution} {Universal} {Style} {Transfer}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html},
	abstract = {Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.},
	urldate = {2021-03-29},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Huan and Li, Yijun and Wang, Yuehai and Hu, Haoji and Yang, Ming-Hsuan},
	year = {2020},
	pages = {1860--1869},
	file = {Snapshot:/Users/controlnet/Zotero/storage/VZ2DNRMF/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.html:text/html;Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2020_Collaborative Distillation for Ultra-Resolution Universal Style Transfer.pdf:application/pdf},
}

@article{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2021-03-29},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = feb,
	year = {2019},
	note = {arXiv: 1809.11096},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/KYRR8X9E/1809.html:text/html;Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf:application/pdf},
}

@article{zhang_shufflenet_2017,
	title = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
	shorttitle = {{ShuffleNet}},
	url = {http://arxiv.org/abs/1707.01083},
	abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves {\textasciitilde}13x actual speedup over AlexNet while maintaining comparable accuracy.},
	urldate = {2021-03-29},
	journal = {arXiv:1707.01083 [cs]},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	month = dec,
	year = {2017},
	note = {arXiv: 1707.01083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/NJXA8YRI/1707.html:text/html;Zhang et al_2017_ShuffleNet.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhang et al_2017_ShuffleNet.pdf:application/pdf},
}

@inproceedings{yin_dreaming_2020,
	title = {Dreaming to {Distill}: {Data}-{Free} {Knowledge} {Transfer} via {DeepInversion}},
	shorttitle = {Dreaming to {Distill}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html},
	abstract = {We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We "invert" a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.},
	urldate = {2021-03-29},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M. and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K. and Kautz, Jan},
	year = {2020},
	pages = {8715--8724},
	file = {Yin et al_2020_Dreaming to Distill.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yin et al_2020_Dreaming to Distill.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Neural networks, convolution, Machine learning, Principal component analysis, Feature extraction, 2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, segmentation recognition, Viewed},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/8RPNJDWS/726791.html:text/html;Lecun et al_1998_Gradient-based learning applied to document recognition.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lecun et al_1998_Gradient-based learning applied to document recognition.pdf:application/pdf},
}

@article{pumarola_d-nerf_2020,
	title = {D-{NeRF}: {Neural} {Radiance} {Fields} for {Dynamic} {Scenes}},
	shorttitle = {D-{NeRF}},
	url = {http://arxiv.org/abs/2011.13961},
	abstract = {Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a {\textbackslash}emph\{single\} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.},
	urldate = {2021-03-16},
	journal = {arXiv:2011.13961 [cs]},
	author = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.13961},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/4M84DPVT/2011.html:text/html;Pumarola et al_2020_D-NeRF.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Pumarola et al_2020_D-NeRF.pdf:application/pdf},
}

@article{jiang_transgan_2021,
	title = {{TransGAN}: {Two} {Transformers} {Can} {Make} {One} {Strong} {GAN}},
	shorttitle = {{TransGAN}},
	url = {http://arxiv.org/abs/2102.07074},
	abstract = {The recent explosive interest on transformers has suggested their potential to become powerful "universal" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN {\textbackslash}textbf\{completely free of convolutions\}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed {\textbackslash}textbf\{TransGAN\}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets {\textbackslash}textbf\{new state-of-the-art\} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA \$64{\textbackslash}times64\$, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at {\textbackslash}url\{https://github.com/VITA-Group/TransGAN\}.},
	urldate = {2021-05-07},
	journal = {arXiv:2102.07074 [cs]},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.07074},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Viewed},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/J7FQDBCT/2102.html:text/html;Jiang et al_2021_TransGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jiang et al_2021_TransGAN.pdf:application/pdf},
}

@inproceedings{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html},
	abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
	urldate = {2021-05-10},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
	pages = {8110--8119},
	file = {Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/PF3FYJ5N/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html:text/html},
}

@article{zheng_rethinking_2021,
	title = {Rethinking {Semantic} {Segmentation} from a {Sequence}-to-{Sequence} {Perspective} with {Transformers}},
	url = {http://arxiv.org/abs/2012.15840},
	abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
	urldate = {2021-05-12},
	journal = {arXiv:2012.15840 [cs]},
	author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.15840},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/FM3HRLK8/2012.html:text/html;Zheng et al_2021_Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zheng et al_2021_Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with.pdf:application/pdf},
}

@inproceedings{shi_real-time_2016,
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
	urldate = {2021-05-13},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Shi, Wenzhe and Caballero, Jose and Huszar, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	year = {2016},
	pages = {1874--1883},
	file = {Shi et al_2016_Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shi et al_2016_Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/6R3JMNB7/Shi_Real-Time_Single_Image_CVPR_2016_paper.html:text/html},
}

@article{zhu_deep_2021,
	title = {Deep {Audio}-visual {Learning}: {A} {Survey}},
	volume = {18},
	issn = {1751-8520},
	shorttitle = {Deep {Audio}-visual {Learning}},
	url = {https://doi.org/10.1007/s11633-021-1293-0},
	doi = {10.1007/s11633-021-1293-0},
	abstract = {Audio-visual learning, aimed at exploiting the relationship between audio and visual modalities, has drawn considerable attention since deep learning started to be used successfully. Researchers tend to leverage these two modalities to improve the performance of previously considered single-modality tasks or address new challenging problems. In this paper, we provide a comprehensive survey of recent audio-visual learning development. We divide the current audio-visual learning tasks into four different subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. State-of-the-art methods, as well as the remaining challenges of each subfield, are further discussed. Finally, we summarize the commonly used datasets and challenges.},
	language = {en},
	number = {3},
	urldate = {2021-05-14},
	journal = {International Journal of Automation and Computing},
	author = {Zhu, Hao and Luo, Man-Di and Wang, Rui and Zheng, Ai-Hua and He, Ran},
	month = jun,
	year = {2021},
	pages = {351--376},
	file = {Zhu et al_2021_Deep Audio-visual Learning.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhu et al_2021_Deep Audio-visual Learning.pdf:application/pdf},
}

@inproceedings{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {https://openreview.net/forum?id=Hk99zCeAb},
	abstract = {We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality.},
	language = {en},
	urldate = {2021-05-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = feb,
	year = {2018},
	file = {Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/RXZQC545/forum.html:text/html},
}

@inproceedings{oh_speech2face_2019,
	title = {{Speech2Face}: {Learning} the {Face} {Behind} a {Voice}},
	shorttitle = {{Speech2Face}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Oh_Speech2Face_Learning_the_Face_Behind_a_Voice_CVPR_2019_paper.html},
	abstract = {How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/Youtube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers.},
	urldate = {2021-05-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Oh, Tae-Hyun and Dekel, Tali and Kim, Changil and Mosseri, Inbar and Freeman, William T. and Rubinstein, Michael and Matusik, Wojciech},
	year = {2019},
	pages = {7539--7548},
	file = {Oh et al_2019_Speech2Face.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Oh et al_2019_Speech2Face.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/C3SLLYKV/Oh_Speech2Face_Learning_the_Face_Behind_a_Voice_CVPR_2019_paper.html:text/html},
}

@article{jahn_high-resolution_2021,
	title = {High-{Resolution} {Complex} {Scene} {Synthesis} with {Transformers}},
	url = {http://arxiv.org/abs/2105.06458},
	abstract = {The use of coarse-grained layouts for controllable synthesis of complex scene images via deep generative models has recently gained popularity. However, results of current approaches still fall short of their promise of high-resolution synthesis. We hypothesize that this is mostly due to the highly engineered nature of these approaches which often rely on auxiliary losses and intermediate steps such as mask generators. In this note, we present an orthogonal approach to this task, where the generative model is based on pure likelihood training without additional objectives. To do so, we first optimize a powerful compression model with adversarial training which learns to reconstruct its inputs via a discrete latent bottleneck and thereby effectively strips the latent representation of high-frequency details such as texture. Subsequently, we train an autoregressive transformer model to learn the distribution of the discrete image representations conditioned on a tokenized version of the layouts. Our experiments show that the resulting system is able to synthesize high-quality images consistent with the given layouts. In particular, we improve the state-of-the-art FID score on COCO-Stuff and on Visual Genome by up to 19\% and 53\% and demonstrate the synthesis of images up to 512 x 512 px on COCO and Open Images.},
	urldate = {2021-05-16},
	journal = {arXiv:2105.06458 [cs]},
	author = {Jahn, Manuel and Rombach, Robin and Ommer, Björn},
	month = may,
	year = {2021},
	note = {arXiv: 2105.06458},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/C3ZTADTD/2105.html:text/html;Jahn et al_2021_High-Resolution Complex Scene Synthesis with Transformers.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jahn et al_2021_High-Resolution Complex Scene Synthesis with Transformers.pdf:application/pdf},
}

@article{lee-thorp_fnet_2021,
	title = {{FNet}: {Mixing} {Tokens} with {Fourier} {Transforms}},
	shorttitle = {{FNet}},
	url = {http://arxiv.org/abs/2105.03824},
	abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate "efficient" Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
	urldate = {2021-05-16},
	journal = {arXiv:2105.03824 [cs]},
	author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03824},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/9Q7GQAVI/2105.html:text/html;Lee-Thorp et al_2021_FNet.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lee-Thorp et al_2021_FNet.pdf:application/pdf},
}

@article{li_involution_2021,
	title = {Involution: {Inverting} the {Inherence} of {Convolution} for {Visual} {Recognition}},
	shorttitle = {Involution},
	url = {http://arxiv.org/abs/2103.06255},
	abstract = {Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6\% top-1 accuracy, 2.5\% and 2.4\% bounding box AP, and 4.7\% mean IoU absolutely while compressing the computational cost to 66\%, 65\%, 72\%, and 57\% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at https://github.com/d-li14/involution.},
	urldate = {2021-05-17},
	journal = {arXiv:2103.06255 [cs]},
	author = {Li, Duo and Hu, Jie and Wang, Changhu and Li, Xiangtai and She, Qi and Zhu, Lei and Zhang, Tong and Chen, Qifeng},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.06255},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/MVAKS3MS/2103.html:text/html;Li et al_2021_Involution.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li et al_2021_Involution.pdf:application/pdf},
}

@article{esser_taming_2021,
	title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://compvis.github.io/taming-transformers/ .},
	urldate = {2021-05-17},
	journal = {arXiv:2012.09841 [cs]},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	month = feb,
	year = {2021},
	note = {arXiv: 2012.09841},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/7NCDD3LK/2012.html:text/html;Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf:application/pdf},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring sp...},
	language = {en},
	urldate = {2021-05-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/D3MGZFGC/chen20j.html:text/html},
}

@article{strudel_segmenter_2021,
	title = {Segmenter: {Transformer} for {Semantic} {Segmentation}},
	shorttitle = {Segmenter},
	url = {http://arxiv.org/abs/2105.05633},
	abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution based approaches, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.},
	urldate = {2021-05-28},
	journal = {arXiv:2105.05633 [cs]},
	author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
	month = may,
	year = {2021},
	note = {arXiv: 2105.05633},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/HG2QG8K4/2105.html:text/html;Strudel et al_2021_Segmenter.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Strudel et al_2021_Segmenter.pdf:application/pdf},
}

@article{shor_towards_2020,
	title = {Towards {Learning} a {Universal} {Non}-{Semantic} {Representation} of {Speech}},
	url = {http://arxiv.org/abs/2002.12764},
	doi = {10.21437/Interspeech.2020-1242},
	abstract = {The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark, models, and evaluation code are publicly released.},
	urldate = {2021-06-05},
	journal = {Interspeech 2020},
	author = {Shor, Joel and Jansen, Aren and Maor, Ronnie and Lang, Oran and Tuval, Omry and Quitry, Felix de Chaumont and Tagliasacchi, Marco and Shavitt, Ira and Emanuel, Dotan and Haviv, Yinnon},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.12764},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {140--144},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/5BDT64YF/2002.html:text/html;Shor et al_2020_Towards Learning a Universal Non-Semantic Representation of Speech.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shor et al_2020_Towards Learning a Universal Non-Semantic Representation of Speech.pdf:application/pdf},
}

@article{liu_multi-task_2020,
	title = {Multi-{Task} {Temporal} {Shift} {Attention} {Networks} for {On}-{Device} {Contactless} {Vitals} {Measurement}},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/e1228be46de6a0234ac22ded31417bc7-Abstract.html},
	abstract = {Telehealth and remote health monitoring have become increasingly important during the SARS-CoV-2 pandemic and it is widely expected that this will have a lasting impact on healthcare practices. These tools can help reduce the risk of exposing patients and medical staff to infection, make healthcare services more accessible, and allow providers to see more patients. However, objective measurement of vital signs is challenging without direct contact with a patient. We present a video-based and on-device optical cardiopulmonary vital sign measurement approach. It leverages a novel multi-task temporal shift convolutional attention network (MTTS-CAN) and enables real-time cardiovascular and respiratory measurements on mobile platforms. We evaluate our system on an Advanced RISC Machine (ARM) CPU and achieve state-of-the-art accuracy while running at over 150 frames per second which enables real-time applications. Systematic experimentation on large benchmark datasets reveals that our approach leads to substantial (20\%-50\%) reductions in error and generalizes well across datasets.},
	language = {en},
	urldate = {2021-06-02},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Xin and Fromm, Josh and Patel, Shwetak and McDuff, Daniel},
	year = {2020},
	pages = {19400--19411},
	file = {Liu et al_2020_Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Liu et al_2020_Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/BPCRD48F/e1228be46de6a0234ac22ded31417bc7-Abstract.html:text/html},
}

@inproceedings{xiong_layer_2020,
	title = {On {Layer} {Normalization} in the {Transformer} {Architecture}},
	url = {http://proceedings.mlr.press/v119/xiong20b.html},
	abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial...},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10524--10533},
	file = {Snapshot:/Users/controlnet/Zotero/storage/Z4VA752I/xiong20b.html:text/html;Xiong et al_2020_On Layer Normalization in the Transformer Architecture.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Xiong et al_2020_On Layer Normalization in the Transformer Architecture.pdf:application/pdf},
}

@article{zhou_vision-infused_2019,
	title = {Vision-{Infused} {Deep} {Audio} {Inpainting}},
	url = {http://arxiv.org/abs/1910.10997},
	abstract = {Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, {\textbackslash}ie synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://hangz-nju-cuhk.github.io/projects/AudioInpainting},
	urldate = {2021-06-08},
	journal = {arXiv:1910.10997 [cs]},
	author = {Zhou, Hang and Liu, Ziwei and Xu, Xudong and Luo, Ping and Wang, Xiaogang},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.10997},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/PLC8QGEF/1910.html:text/html;Zhou et al_2019_Vision-Infused Deep Audio Inpainting.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhou et al_2019_Vision-Infused Deep Audio Inpainting.pdf:application/pdf},
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR- 10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
	urldate = {2021-06-30},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {4700--4708},
	file = {Huang et al_2017_Densely Connected Convolutional Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Huang et al_2017_Densely Connected Convolutional Networks.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/859NCWVS/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html:text/html},
}

@article{lindemann_survey_2021,
	title = {A {Survey} on {Anomaly} {Detection} for {Technical} {Systems} using {LSTM} {Networks}},
	url = {http://arxiv.org/abs/2105.13810},
	abstract = {Anomalies represent deviations from the intended system operation and can lead to decreased efficiency as well as partial or complete system failure. As the causes of anomalies are often unknown due to complex system dynamics, efficient anomaly detection is necessary. Conventional detection approaches rely on statistical and time-invariant methods that fail to address the complex and dynamic nature of anomalies. With advances in artificial intelligence and increasing importance for anomaly detection and prevention in various domains, artificial neural network approaches enable the detection of more complex anomaly types while considering temporal and contextual characteristics. In this article, a survey on state-of-the-art anomaly detection using deep neural and especially long short-term memory networks is conducted. The investigated approaches are evaluated based on the application scenario, data and anomaly types as well as further metrics. To highlight the potential of upcoming anomaly detection techniques, graph-based and transfer learning approaches are also included in the survey, enabling the analysis of heterogeneous data as well as compensating for its shortage and improving the handling of dynamic processes.},
	urldate = {2021-07-12},
	journal = {arXiv:2105.13810 [cs, stat]},
	author = {Lindemann, Benjamin and Maschler, Benjamin and Sahlab, Nada and Weyrich, Michael},
	month = may,
	year = {2021},
	note = {arXiv: 2105.13810},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/623SNNTF/2105.html:text/html;Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lindemann et al_2021_A Survey on Anomaly Detection for Technical Systems using LSTM Networks.pdf:application/pdf},
}

@article{ahmed_survey_2016,
	title = {A survey of network anomaly detection techniques},
	volume = {60},
	issn = {1084-8045},
	url = {https://www.sciencedirect.com/science/article/pii/S1084804515002891},
	doi = {10.1016/j.jnca.2015.11.016},
	abstract = {Information and Communication Technology (ICT) has a great impact on social wellbeing, economic growth and national security in todays world. Generally, ICT includes computers, mobile communication devices and networks. ICT is also embraced by a group of people with malicious intent, also known as network intruders, cyber criminals, etc. Confronting these detrimental cyber activities is one of the international priorities and important research area. Anomaly detection is an important data analysis task which is useful for identifying the network intrusions. This paper presents an in-depth analysis of four major categories of anomaly detection techniques which include classification, statistical, information theory and clustering. The paper also discusses research challenges with the datasets used for network intrusion detection.},
	language = {en},
	urldate = {2021-07-12},
	journal = {Journal of Network and Computer Applications},
	author = {Ahmed, Mohiuddin and Naser Mahmood, Abdun and Hu, Jiankun},
	month = jan,
	year = {2016},
	keywords = {Anomaly detection, Classification, Clustering, Computer security, Information theory, Intrusion detection},
	pages = {19--31},
	file = {Ahmed et al_2016_A survey of network anomaly detection techniques.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Ahmed et al_2016_A survey of network anomaly detection techniques.pdf:application/pdf;ScienceDirect Snapshot:/Users/controlnet/Zotero/storage/DMU8HV86/S1084804515002891.html:text/html},
}

@article{khan_anomaly_2021,
	title = {Anomaly {Detection} {Approach} to {Identify} {Early} {Cases} in a {Pandemic} using {Chest} {X}-rays},
	url = {http://arxiv.org/abs/2010.02814},
	abstract = {The current COVID-19 pandemic is now getting contained, albeit at the cost of morethan2.3million human lives. A critical phase in any pandemic is the early detection of cases to develop preventive treatments and strategies. In the case of COVID-19,several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such asCOVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of data from infected persons could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate the problem of identifying early cases in a pandemic as an anomaly detection problem, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present several unsupervised deep learning approaches, including convolutional and adversarially trained autoencoder. We tested two settings on a publicly available dataset (COVIDx)by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. Afterperforming3-fold cross validation, we obtain a ROC-AUC of0.765. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays},
	urldate = {2021-07-12},
	journal = {arXiv:2010.02814 [cs, eess]},
	author = {Khan, Shehroz S. and Khoshbakhtian, Faraz and Ashraf, Ahmed Bilal},
	month = apr,
	year = {2021},
	note = {arXiv: 2010.02814},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/56RRUJEC/2010.html:text/html;Khan et al_2021_Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Khan et al_2021_Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest.pdf:application/pdf},
}

@article{dolhansky_deepfake_2020,
	title = {The {DeepFake} {Detection} {Challenge} ({DFDC}) {Dataset}},
	url = {http://arxiv.org/abs/2006.07397},
	abstract = {Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real "in-the-wild" Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https://ai.facebook.com/datasets/dfdc.},
	urldate = {2021-07-19},
	journal = {arXiv:2006.07397 [cs]},
	author = {Dolhansky, Brian and Bitton, Joanna and Pflaum, Ben and Lu, Jikuo and Howes, Russ and Wang, Menglin and Ferrer, Cristian Canton},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.07397},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/2BB7ZX3G/2006.html:text/html;Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Dolhansky et al_2020_The DeepFake Detection Challenge (DFDC) Dataset.pdf:application/pdf},
}

@inproceedings{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {http://proceedings.mlr.press/v139/touvron21a.html},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
	language = {en},
	urldate = {2021-07-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10347--10357},
	file = {Supplementary PDF:/Users/controlnet/Zotero/storage/6FCVZIZP/Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf;Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf:application/pdf},
}

@inproceedings{rezatofighi_generalized_2019,
	title = {Generalized {Intersection} {Over} {Union}: {A} {Metric} and a {Loss} for {Bounding} {Box} {Regression}},
	shorttitle = {Generalized {Intersection} {Over} {Union}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.html},
	abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
	urldate = {2021-07-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
	year = {2019},
	pages = {658--666},
	file = {Rezatofighi et al_2019_Generalized Intersection Over Union.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Rezatofighi et al_2019_Generalized Intersection Over Union.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/95BMGDRC/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.html:text/html},
}

@article{shaw_self-attention_2018,
	title = {Self-{Attention} with {Relative} {Position} {Representations}},
	url = {http://arxiv.org/abs/1803.02155},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
	urldate = {2021-07-24},
	journal = {arXiv:1803.02155 [cs]},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.02155},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/9NECYPWK/1803.html:text/html;Shaw et al_2018_Self-Attention with Relative Position Representations.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shaw et al_2018_Self-Attention with Relative Position Representations.pdf:application/pdf},
}

@article{carbonneau_multiple_2018,
	title = {Multiple instance learning: {A} survey of problem characteristics and applications},
	volume = {77},
	issn = {0031-3203},
	shorttitle = {Multiple instance learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317304065},
	doi = {10.1016/j.patcog.2017.10.009},
	abstract = {Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research. Code is available on-line at https://github.com/macarbonneau/MILSurvey.},
	language = {en},
	urldate = {2021-08-02},
	journal = {Pattern Recognition},
	author = {Carbonneau, Marc-André and Cheplygina, Veronika and Granger, Eric and Gagnon, Ghyslain},
	month = may,
	year = {2018},
	keywords = {Computer vision, Classification, Computer aided diagnosis, Document classification, Drug activity prediction, Multi-instance learning, Multiple instance learning, Weakly supervised learning},
	pages = {329--353},
	file = {ScienceDirect Full Text PDF:/Users/controlnet/Zotero/storage/KRCUABCA/Carbonneau et al. - 2018 - Multiple instance learning A survey of problem ch.pdf:application/pdf;ScienceDirect Snapshot:/Users/controlnet/Zotero/storage/SXF3JQUP/S0031320317304065.html:text/html},
}

@inproceedings{jia_transfer_2018,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {Transfer learning from speaker verification to multispeaker text-to-speech synthesis},
	abstract = {We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
	urldate = {2021-08-06},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Jia, Ye and Zhang, Yu and Weiss, Ron J. and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui},
	month = dec,
	year = {2018},
	pages = {4485--4495},
	file = {Jia et al_2018_Transfer learning from speaker verification to multispeaker text-to-speech.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jia et al_2018_Transfer learning from speaker verification to multispeaker text-to-speech.pdf:application/pdf},
}

@inproceedings{shen_natural_2018,
	title = {Natural {TTS} {Synthesis} by {Conditioning} {Wavenet} on {MEL} {Spectrogram} {Predictions}},
	doi = {10.1109/ICASSP.2018.8461368},
	abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and Saurous, Rif A. and Agiomvrgiannakis, Yannis and Wu, Yonghui},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Training, Acoustics, Decoding, Linguistics, Spectrogram, Tacotron 2, text-to-speech, Time-domain analysis, Vocoders, WaveNet},
	pages = {4779--4783},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/E5WQYRWA/8461368.html:text/html;Shen et al_2018_Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shen et al_2018_Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions.pdf:application/pdf},
}

@article{wang_tacotron_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Tacotron},
	url = {http://arxiv.org/abs/1703.10135},
	abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
	urldate = {2021-08-06},
	journal = {arXiv:1703.10135 [cs]},
	author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.10135},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/8HYCZNNI/1703.html:text/html;Wang et al_2017_Tacotron.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2017_Tacotron.pdf:application/pdf},
}

@inproceedings{prajwal_lip_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {A {Lip} {Sync} {Expert} {Is} {All} {You} {Need} for {Speech} to {Lip} {Generation} {In} the {Wild}},
	isbn = {978-1-4503-7988-5},
	url = {http://doi.org/10.1145/3394171.3413532},
	doi = {10.1145/3394171.3413532},
	abstract = {In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model, and also publicly release the code, models, and evaluation benchmarks on our website.},
	urldate = {2021-08-06},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
	month = oct,
	year = {2020},
	keywords = {lip sync, talking face generation, video generation},
	pages = {484--492},
	file = {Prajwal et al_2020_A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Prajwal et al_2020_A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild.pdf:application/pdf},
}

@inproceedings{chao_rethinking_2018,
	title = {Rethinking the {Faster} {R}-{CNN} {Architecture} for {Temporal} {Action} {Localization}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html},
	abstract = {We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chao, Yu-Wei and Vijayanarasimhan, Sudheendra and Seybold, Bryan and Ross, David A. and Deng, Jia and Sukthankar, Rahul},
	year = {2018},
	pages = {1130--1139},
	file = {Chao et al_2018_Rethinking the Faster R-CNN Architecture for Temporal Action Localization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chao et al_2018_Rethinking the Faster R-CNN Architecture for Temporal Action Localization.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/9J99MCZT/Chao_Rethinking_the_Faster_CVPR_2018_paper.html:text/html},
}

@article{su_bsn_2021,
	title = {{BSN}++: {Complementary} {Boundary} {Regressor} with {Scale}-{Balanced} {Relation} {Modeling} for {Temporal} {Action} {Proposal} {Generation}},
	shorttitle = {{BSN}++},
	url = {http://arxiv.org/abs/2009.07641},
	abstract = {Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.},
	urldate = {2021-08-06},
	journal = {arXiv:2009.07641 [cs]},
	author = {Su, Haisheng and Gan, Weihao and Wu, Wei and Qiao, Yu and Yan, Junjie},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.07641},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/KDXASUC4/2009.html:text/html;Su et al_2021_BSN++.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Su et al_2021_BSN++.pdf:application/pdf},
}

@article{tan_relaxed_2021,
	title = {Relaxed {Transformer} {Decoders} for {Direct} {Action} {Proposal} {Generation}},
	url = {http://arxiv.org/abs/2102.01894},
	abstract = {Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action instances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and end-to-end learnable framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer encoder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models will be made available at https://github.com/MCG-NJU/RTD-Action.},
	urldate = {2021-08-06},
	journal = {arXiv:2102.01894 [cs]},
	author = {Tan, Jing and Tang, Jiaqi and Wang, Limin and Wu, Gangshan},
	month = apr,
	year = {2021},
	note = {arXiv: 2102.01894},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/HV3MYFBC/2102.html:text/html;Tan et al_2021_Relaxed Transformer Decoders for Direct Action Proposal Generation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tan et al_2021_Relaxed Transformer Decoders for Direct Action Proposal Generation.pdf:application/pdf},
}

@inproceedings{qing_temporal_2021,
	title = {Temporal {Context} {Aggregation} {Network} for {Temporal} {Action} {Proposal} {Refinement}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Qing_Temporal_Context_Aggregation_Network_for_Temporal_Action_Proposal_Refinement_CVPR_2021_paper.html},
	abstract = {Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through local and global temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both local and global temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task.},
	language = {en},
	urldate = {2021-08-06},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Qing, Zhiwu and Su, Haisheng and Gan, Weihao and Wang, Dongliang and Wu, Wei and Wang, Xiang and Qiao, Yu and Yan, Junjie and Gao, Changxin and Sang, Nong},
	year = {2021},
	pages = {485--494},
	file = {Qing et al_2021_Temporal Context Aggregation Network for Temporal Action Proposal Refinement.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Qing et al_2021_Temporal Context Aggregation Network for Temporal Action Proposal Refinement.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/DXUYT3DW/Qing_Temporal_Context_Aggregation_Network_for_Temporal_Action_Proposal_Refinement_CVPR_2021_pap.html:text/html},
}

@inproceedings{qiu_precise_2018,
	address = {New York, NY, USA},
	series = {{ICMR} '18},
	title = {Precise {Temporal} {Action} {Localization} by {Evolving} {Temporal} {Proposals}},
	isbn = {978-1-4503-5046-4},
	url = {http://doi.org/10.1145/3206025.3206029},
	doi = {10.1145/3206025.3206029},
	abstract = {Locating actions in long untrimmed videos has been a challenging problem in video content analysis. The performances of existing action localization approaches remain unsatisfactory in precisely determining the beginning and the end of an action. Imitating the human perception procedure with observations and refinements, we propose a novel three-phase action localization framework. Our framework is embedded with an Actionness Network to generate initial proposals through frame-wise similarity grouping, and then a Refinement Network to conduct boundary adjustment on these proposals. Finally, the refined proposals are sent to a Localization Network for further fine-grained location regression. The whole process can be deemed as multi-stage refinement using a novel non-local pyramid feature under various temporal granularities. We evaluate our framework on THUMOS14 benchmark and obtain a significant improvement over the state-of-the-arts approaches. Specifically, the performance gain is remarkable under precise localization with high IoU thresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2\%.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 2018 {ACM} on {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Qiu, Haonan and Zheng, Yingbin and Ye, Hao and Lu, Yao and Wang, Feng and He, Liang},
	month = jun,
	year = {2018},
	keywords = {action localization, deep neural network, temporal proposal},
	pages = {388--396},
	file = {Qiu et al_2018_Precise Temporal Action Localization by Evolving Temporal Proposals.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Qiu et al_2018_Precise Temporal Action Localization by Evolving Temporal Proposals.pdf:application/pdf},
}

@inproceedings{huang_sap_2018,
	title = {{SAP}: {Self}-{Adaptive} {Proposal} {Model} for {Temporal} {Action} {Detection} {Based} on {Reinforcement} {Learning}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	shorttitle = {{SAP}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16109},
	abstract = {Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose a Self-Adaptive Proposal (SAP) model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at the beginning of the video and traverse the whole video by adopting a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent’s decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS’14 validate the effectiveness of SAP, which can achieve competitive performance with current action detection algorithms via much fewer proposals.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Huang, Jingjia and Li, Nannan and Zhang, Tao and Li, Ge and Huang, Tiejun and Gao, Wen},
	month = apr,
	year = {2018},
	file = {Huang et al_2018_SAP.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Huang et al_2018_SAP.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/AP454MZS/16109.html:text/html},
}

@article{yang_exploring_2018,
	title = {Exploring {Temporal} {Preservation} {Networks} for {Precise} {Temporal} {Action} {Localization}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12234},
	abstract = {Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments pre-determined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization with minimal loss of time information. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement in both per-frame action prediction and segment-level temporal action localization.},
	language = {en},
	number = {1},
	urldate = {2021-08-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yang, Ke and Qiao, Peng and Li, Dongsheng and Lv, Shaohe and Dou, Yong},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Yang et al_2018_Exploring Temporal Preservation Networks for Precise Temporal Action.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yang et al_2018_Exploring Temporal Preservation Networks for Precise Temporal Action.pdf:application/pdf},
}

@article{buch_end--end_2019,
	title = {End-to-end, single-stream temporal action detection in untrimmed videos},
	url = {https://research.kaust.edu.sa/en/publications/end-to-end-single-stream-temporal-action-detection-in-untrimmed-v},
	doi = {10.5244/c.31.93},
	abstract = {In this work, we present a new intuitive, end-to-end approach for temporal action detection in untrimmed videos. We introduce our new architecture for Single-Stream Temporal Action Detection (SS-TAD), which effectively integrates joint action detection with its semantic sub-tasks in a single unifying end-to-end framework. We develop a method for training our deep recurrent architecture based on enforcing semantic constraints on intermediate modules that are gradually relaxed as learning progresses. We find that such a dynamic learning scheme enables SS-TAD to achieve higher overall detection performance, with fewer training epochs. By design, our single-pass network is very efficient and can operate at 701 frames per second, while simultaneously outperforming the state-of-the-art methods for temporal action detection on THUMOS’14.},
	language = {English (US)},
	urldate = {2021-08-04},
	journal = {Procedings of the British Machine Vision Conference 2017},
	author = {Buch, Shyamal and Escorcia, Victor and Ghanem, Bernard and Fei-Fei, Li and Niebles, Juan Carlos},
	month = may,
	year = {2019},
	note = {Publisher: British Machine Vision Association},
	file = {Buch et al_2019_End-to-end, single-stream temporal action detection in untrimmed videos.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Buch et al_2019_End-to-end, single-stream temporal action detection in untrimmed videos.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/YUVIFUW9/end-to-end-single-stream-temporal-action-detection-in-untrimmed-v.html:text/html},
}

@inproceedings{lin_bsn_2018,
	title = {{BSN}: {Boundary} {Sensitive} {Network} for {Temporal} {Action} {Proposal} {Generation}},
	shorttitle = {{BSN}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper.html},
	abstract = {Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts "local to global" fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Lin, Tianwei and Zhao, Xu and Su, Haisheng and Wang, Chongjing and Yang, Ming},
	year = {2018},
	pages = {3--19},
	file = {Lin et al_2018_BSN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lin et al_2018_BSN.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/VH4WAC5E/Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper.html:text/html},
}

@inproceedings{lin_single_2017,
	address = {New York, NY, USA},
	series = {{MM} '17},
	title = {Single {Shot} {Temporal} {Action} {Detection}},
	isbn = {978-1-4503-4906-2},
	url = {http://doi.org/10.1145/3123266.3123343},
	doi = {10.1145/3123266.3123343},
	abstract = {Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the "detection by classification" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from \$19.0\%\$ to \$24.6\%\$ on THUMOS 2014 and from 7.4\% to \$11.0\%\$ on MEXaction2.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 25th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Tianwei and Zhao, Xu and Shou, Zheng},
	month = oct,
	year = {2017},
	keywords = {ssad network, temporal action detection, untrimmed video},
	pages = {988--996},
	file = {Lin et al_2017_Single Shot Temporal Action Detection.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lin et al_2017_Single Shot Temporal Action Detection.pdf:application/pdf},
}

@inproceedings{xu_r-c3d_2017,
	title = {R-{C3D}: {Region} {Convolutional} {3D} {Network} for {Temporal} {Activity} {Detection}},
	shorttitle = {R-{C3D}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.html},
	abstract = {We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at http://ai.bu.edu/r-c3d/.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Xu, Huijuan and Das, Abir and Saenko, Kate},
	year = {2017},
	pages = {5783--5792},
	file = {Snapshot:/Users/controlnet/Zotero/storage/II5QYZN3/Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.html:text/html;Xu et al_2017_R-C3D.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Xu et al_2017_R-C3D.pdf:application/pdf},
}

@article{gao_cascaded_2017,
	title = {Cascaded {Boundary} {Regression} for {Temporal} {Action} {Detection}},
	url = {http://arxiv.org/abs/1705.01180},
	abstract = {Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classifiers on sliding windows. Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and specific actions are detected respectively in the first and the second stage. CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows. The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0\% to 31.0\%.},
	urldate = {2021-08-04},
	journal = {arXiv:1705.01180 [cs]},
	author = {Gao, Jiyang and Yang, Zhenheng and Nevatia, Ram},
	month = may,
	year = {2017},
	note = {arXiv: 1705.01180},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/N6897AEH/1705.html:text/html;Gao et al_2017_Cascaded Boundary Regression for Temporal Action Detection.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Gao et al_2017_Cascaded Boundary Regression for Temporal Action Detection.pdf:application/pdf},
}

@inproceedings{dai_temporal_2017,
	title = {Temporal {Context} {Network} for {Activity} {Localization} in {Videos}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Temporal_Context_Network_ICCV_2017_paper.html},
	abstract = {We present a Temporal Context Network (TCN) for precise temporal localization of human activities. Similar to the Faster-RCNN architecture, proposals are placed at equal intervals in a video which span multiple temporal scales. We propose a novel representation for ranking these proposals. Since pooling features only inside a segment is not sufficient to predict activity boundaries, we construct a representation which explicitly captures context around a proposal for ranking it. For each temporal segment inside a proposal, features are uniformly sampled at a pair of scales and are input to a temporal convolutional neural network for classification. After ranking proposals, non-maximum suppression is applied and classification is performed to obtain final detections. TCN outperforms state-of-the-art methods on the ActivityNet dataset and the THUMOS14 dataset.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Dai, Xiyang and Singh, Bharat and Zhang, Guyue and Davis, Larry S. and Qiu Chen, Yan},
	year = {2017},
	pages = {5793--5802},
	file = {Dai et al_2017_Temporal Context Network for Activity Localization in Videos.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Dai et al_2017_Temporal Context Network for Activity Localization in Videos.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/C2H25BAL/Dai_Temporal_Context_Network_ICCV_2017_paper.html:text/html},
}

@inproceedings{zhao_temporal_2017,
	title = {Temporal {Action} {Detection} {With} {Structured} {Segment} {Networks}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Zhao_Temporal_Action_Detection_ICCV_2017_paper.html},
	abstract = {Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS'14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Zhao, Yue and Xiong, Yuanjun and Wang, Limin and Wu, Zhirong and Tang, Xiaoou and Lin, Dahua},
	year = {2017},
	pages = {2914--2923},
	file = {Snapshot:/Users/controlnet/Zotero/storage/I2YM839G/Zhao_Temporal_Action_Detection_ICCV_2017_paper.html:text/html;Zhao et al_2017_Temporal Action Detection With Structured Segment Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhao et al_2017_Temporal Action Detection With Structured Segment Networks.pdf:application/pdf},
}

@inproceedings{shou_cdc_2017,
	title = {{CDC}: {Convolutional}-{De}-{Convolutional} {Networks} for {Precise} {Temporal} {Action} {Localization} in {Untrimmed} {Videos}},
	shorttitle = {{CDC}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html},
	abstract = {Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket.org/columbiadvmm/cdc.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Shou, Zheng and Chan, Jonathan and Zareian, Alireza and Miyazawa, Kazuyuki and Chang, Shih-Fu},
	year = {2017},
	pages = {5734--5743},
	file = {Shou et al_2017_CDC.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shou et al_2017_CDC.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/U9BC2LLV/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html:text/html},
}

@inproceedings{zhu_efficient_2017,
	title = {Efficient {Action} {Detection} in {Untrimmed} {Videos} via {Multi}-task {Learning}},
	doi = {10.1109/WACV.2017.29},
	abstract = {This paper studies the joint learning of action recognition and temporal localization in long, untrimmed videos. We employ a multi-task learning framework that performs the three highly related steps of action proposal, action recognition, and action localization refinement in parallel instead of the standard sequential pipeline that performs the steps in order. We develop a novel temporal actionness regression module that estimates what proportion of a clip contains action. We use it for temporal localization but it could have other applications like video retrieval, surveillance, summarization, etc. We also introduce random shear augmentation during training to simulate viewpoint change. We evaluate our framework on three popular video benchmarks. Results demonstrate that our joint model is efficient in terms of storage and computation in that we do not need to compute and cache dense trajectory features, and that it is several times faster than its sequential ConvNets counterpart. Yet, despite being more efficient, it outperforms stateof-the-art methods with respect to accuracy.},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Zhu, Yi and Newsam, Shawn},
	month = mar,
	year = {2017},
	keywords = {Training, Training data, Computational modeling, Videos, Proposals, Three-dimensional displays},
	pages = {197--206},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/HYAUW7KQ/7926612.html:text/html;Zhu_Newsam_2017_Efficient Action Detection in Untrimmed Videos via Multi-task Learning.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhu_Newsam_2017_Efficient Action Detection in Untrimmed Videos via Multi-task Learning.pdf:application/pdf},
}

@inproceedings{shou_temporal_2016,
	title = {Temporal {Action} {Localization} in {Untrimmed} {Videos} via {Multi}-{Stage} {CNNs}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Shou_Temporal_Action_Localization_CVPR_2016_paper.html},
	abstract = {We address temporal action localization in untrimmed long videos. This is important because videos in real applications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal action localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as initialization for the localization network; and (3) a localization network fine-tunes the learned classification network to localize each action instance. We propose a novel loss function for the localization network to explicitly consider temporal overlap and achieve high temporal localization accuracy. In the end, only the proposal network and the localization network are used during prediction. On two large-scale benchmarks, our approach achieves significantly superior performances compared with other state-of-the-art systems: mAP increases from 1.7\% to 7.4\% on MEXaction2 and increases from 15.0\% to 19.0\% on THUMOS 2014.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Shou, Zheng and Wang, Dongang and Chang, Shih-Fu},
	year = {2016},
	pages = {1049--1058},
	file = {Shou et al_2016_Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Shou et al_2016_Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/H9IJ85TD/Shou_Temporal_Action_Localization_CVPR_2016_paper.html:text/html},
}

@inproceedings{yuan_temporal_2016,
	title = {Temporal {Action} {Localization} {With} {Pyramid} of {Score} {Distribution} {Features}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Yuan_Temporal_Action_Localization_CVPR_2016_paper.html},
	abstract = {We investigate the feature design and classification architectures in temporal action localization. This application focuses on detecting and labeling actions in untrimmed videos, which brings more challenge than classifying pre-segmented videos. The major difficulty for action localization is the uncertainty of action occurrence and utilization of information from different scales. Two innovations are proposed to address this issue. First, we propose a Pyramid of Score Distribution Feature (PSDF) to capture the motion information at multiple resolutions centered at each detection window. This novel feature mitigates the influence of unknown action position and duration, and shows significant performance gain over previous detection approaches. Second, inter-frame consistency is further explored by incorporating PSDF into the state-of-the-art Recurrent Neural Networks, which gives additional performance gain in detecting actions in temporally untrimmed videos. We tested our action localization framework on the THUMOS'15 and MPII Cooking Activities Dataset, both of which show a large performance improvement over previous attempts.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yuan, Jun and Ni, Bingbing and Yang, Xiaokang and Kassim, Ashraf A.},
	year = {2016},
	pages = {3093--3102},
	file = {Snapshot:/Users/controlnet/Zotero/storage/2YX7GE92/Yuan_Temporal_Action_Localization_CVPR_2016_paper.html:text/html;Yuan et al_2016_Temporal Action Localization With Pyramid of Score Distribution Features.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yuan et al_2016_Temporal Action Localization With Pyramid of Score Distribution Features.pdf:application/pdf},
}

@inproceedings{yeung_end--end_2016,
	title = {End-{To}-{End} {Learning} of {Action} {Detection} {From} {Frame} {Glimpses} in {Videos}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Yeung_End-To-End_Learning_of_CVPR_2016_paper.html},
	abstract = {In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and whether to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's task-specific decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2\% or less) of the video frames.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
	year = {2016},
	pages = {2678--2687},
	file = {Snapshot:/Users/controlnet/Zotero/storage/QUFKHF9E/Yeung_End-To-End_Learning_of_CVPR_2016_paper.html:text/html;Yeung et al_2016_End-To-End Learning of Action Detection From Frame Glimpses in Videos.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yeung et al_2016_End-To-End Learning of Action Detection From Frame Glimpses in Videos.pdf:application/pdf},
}

@inproceedings{kim_nlnl_2019,
	title = {{NLNL}: {Negative} {Learning} for {Noisy} {Labels}},
	shorttitle = {{NLNL}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.html},
	urldate = {2021-08-04},
	author = {Kim, Youngdong and Yim, Junho and Yun, Juseung and Kim, Junmo},
	year = {2019},
	pages = {101--110},
	file = {Kim et al_2019_NLNL.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Kim et al_2019_NLNL.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/G3KPZ8CR/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.html:text/html},
}

@inproceedings{salimans_improved_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Improved techniques for training {GANs}},
	isbn = {978-1-5108-3881-9},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = dec,
	year = {2016},
	pages = {2234--2242},
	file = {Salimans et al_2016_Improved techniques for training GANs.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Salimans et al_2016_Improved techniques for training GANs.pdf:application/pdf},
}

@article{sohn_fixmatch_2020,
	title = {{FixMatch}: {Simplifying} {Semi}-{Supervised} {Learning} with {Consistency} and {Confidence}},
	shorttitle = {{FixMatch}},
	url = {http://arxiv.org/abs/2001.07685},
	abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
	urldate = {2021-08-04},
	journal = {arXiv:2001.07685 [cs, stat]},
	author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
	month = nov,
	year = {2020},
	note = {arXiv: 2001.07685},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/H6BGNMWR/2001.html:text/html;Sohn et al_2020_FixMatch.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Sohn et al_2020_FixMatch.pdf:application/pdf},
}

@article{xie_unsupervised_2020,
	title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
	url = {http://arxiv.org/abs/1904.12848},
	abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
	urldate = {2021-08-04},
	journal = {arXiv:1904.12848 [cs, stat]},
	author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
	month = nov,
	year = {2020},
	note = {arXiv: 1904.12848},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/IPQ6IQFI/1904.html:text/html;Xie et al_2020_Unsupervised Data Augmentation for Consistency Training.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Xie et al_2020_Unsupervised Data Augmentation for Consistency Training.pdf:application/pdf},
}

@article{tarvainen_mean_2018,
	title = {Mean teachers are better role models: {Weight}-averaged consistency targets improve semi-supervised deep learning results},
	shorttitle = {Mean teachers are better role models},
	url = {http://arxiv.org/abs/1703.01780},
	abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
	urldate = {2021-08-04},
	journal = {arXiv:1703.01780 [cs, stat]},
	author = {Tarvainen, Antti and Valpola, Harri},
	month = apr,
	year = {2018},
	note = {arXiv: 1703.01780},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/W7GAKXQ5/1703.html:text/html;Tarvainen_Valpola_2018_Mean teachers are better role models.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tarvainen_Valpola_2018_Mean teachers are better role models.pdf:application/pdf},
}

@article{laine_temporal_2017,
	title = {Temporal {Ensembling} for {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/1610.02242},
	abstract = {In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44\% to 7.05\% in SVHN with 500 labels and from 18.63\% to 16.55\% in CIFAR-10 with 4000 labels, and further to 5.12\% and 12.16\% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.},
	urldate = {2021-08-04},
	journal = {arXiv:1610.02242 [cs]},
	author = {Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.02242},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/N3BE367I/1610.html:text/html;Laine_Aila_2017_Temporal Ensembling for Semi-Supervised Learning.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Laine_Aila_2017_Temporal Ensembling for Semi-Supervised Learning.pdf:application/pdf},
}

@article{yang_survey_2021,
	title = {A {Survey} on {Deep} {Semi}-supervised {Learning}},
	url = {http://arxiv.org/abs/2103.00550},
	abstract = {Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from model design perspectives and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we offer a detailed comparison of these methods in terms of the type of losses, contributions, and architecture differences. In addition to the past few years' progress, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.},
	urldate = {2021-08-03},
	journal = {arXiv:2103.00550 [cs]},
	author = {Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00550},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/C9UNYFVA/2103.html:text/html;Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yang et al_2021_A Survey on Deep Semi-supervised Learning.pdf:application/pdf},
}

@article{ding_kfc_2021,
	title = {{KFC}: {An} {Efficient} {Framework} for {Semi}-supervised {Temporal} {Action} {Localization}},
	issn = {1941-0042},
	shorttitle = {{KFC}},
	doi = {10.1109/TIP.2021.3099407},
	abstract = {In temporal action localization (TAL), semi-supervised learning is a promising technique to mitigate the cost of precise boundary annotations. Semi-supervised approaches employing consistency regularization (CR), encouraging models to be robust to the perturbed inputs, have achieved great success in image classification problems. The success of CR is largely depended on the perturbations, where instances are perturbed to train a robust model without altering their semantic information. However, the perturbations for image or video classification tasks are not fit to apply to TAL. Since videos in TAL are too long to train the model with raw videos in an end-to-end manner. In this paper, we devise a method named K-farthest crossover to construct perturbations based on video features and apply it to TAL. Motivated by the observation that features in the same action instance become more and more similar during the training process while those in different action instances or backgrounds become more and more divergent, we add perturbations to each feature along temporal axis and adopt CR to encourage the model to retain this observation. Specifically, for a feature, we first find the top-k dissimilar features and average them to form a perturbation. Then, similar to chromosomal crossover, we select a large part of the feature and a small part of the perturbation to recombine a perturbed feature, which preserves the feature semantics yet enough discrepancy.},
	journal = {IEEE Transactions on Image Processing},
	author = {Ding, Xinpeng and Wang, Nannan and Gao, Xinbo and Li, Jie and Wang, Xiaoyu and Liu, Tongliang},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Training, Semantics, Annotations, Feature extraction, Location awareness, Perturbation methods, semi-supervised learning, Semisupervised learning, Temporal Action Localization, video understanding},
	pages = {1--1},
	file = {Ding et al_2021_KFC.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Ding et al_2021_KFC.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/36WANIUK/9500051.html:text/html},
}

@article{sun_exploiting_2021,
	title = {Exploiting {Informative} {Video} {Segments} for {Temporal} {Action} {Localization}},
	issn = {1941-0077},
	doi = {10.1109/TMM.2021.3050067},
	abstract = {We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognizing actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segment-level attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks: THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with the state-of-the-art methods. Moreover, compared with the baseline method that equally treats video segments, STAN achieves significant improvements with the mAP increased from 30.4\% to 39.8\% on the THUMOS2014 dataset and from 31.4\% to 35.9\% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization.},
	journal = {IEEE Transactions on Multimedia},
	author = {Sun, Che and Song, Hao and Wu, Xinxiao and Jia, Yunde and Luo, Jiebo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Image segmentation, Feature extraction, Proposals, Location awareness, Temporal Action Localization, Aggregates, Attention Mechanism, Generators, Informative Video Segments, Motion segmentation, Supervised Temporal Attention Network},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/9BWABMEP/9316901.html:text/html;Sun et al_2021_Exploiting Informative Video Segments for Temporal Action Localization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Sun et al_2021_Exploiting Informative Video Segments for Temporal Action Localization.pdf:application/pdf},
}

@article{tan_survey_2021,
	title = {A {Survey} on {Neural} {Speech} {Synthesis}},
	url = {http://arxiv.org/abs/2106.15561},
	abstract = {Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.},
	urldate = {2021-08-11},
	journal = {arXiv:2106.15561 [cs, eess]},
	author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
	month = jul,
	year = {2021},
	note = {arXiv: 2106.15561},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/7X5H8MFN/2106.html:text/html;Tan et al_2021_A Survey on Neural Speech Synthesis.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tan et al_2021_A Survey on Neural Speech Synthesis.pdf:application/pdf},
}

@article{hao_gancraft_2021,
	title = {{GANcraft}: {Unsupervised} {3D} {Neural} {Rendering} of {Minecraft} {Worlds}},
	shorttitle = {{GANcraft}},
	url = {http://arxiv.org/abs/2104.07659},
	abstract = {We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .},
	urldate = {2021-08-11},
	journal = {arXiv:2104.07659 [cs]},
	author = {Hao, Zekun and Mallya, Arun and Belongie, Serge and Liu, Ming-Yu},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.07659},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/BT3FKSJT/2104.html:text/html;Hao et al_2021_GANcraft.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Hao et al_2021_GANcraft.pdf:application/pdf},
}

@article{chen_adaspeech_2021,
	title = {{AdaSpeech}: {Adaptive} {Text} to {Speech} for {Custom} {Voice}},
	shorttitle = {{AdaSpeech}},
	url = {http://arxiv.org/abs/2103.00993},
	abstract = {Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech data. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions that could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we use two acoustic encoders to extract an utterance-level vector and a sequence of phoneme-level vectors from the target speech during training; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. Audio samples are available at https://speechresearch.github.io/adaspeech/.},
	urldate = {2021-08-11},
	journal = {arXiv:2103.00993 [cs, eess]},
	author = {Chen, Mingjian and Tan, Xu and Li, Bohan and Liu, Yanqing and Qin, Tao and Zhao, Sheng and Liu, Tie-Yan},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.00993},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/6F86Z8ZC/2103.html:text/html;Chen et al_2021_AdaSpeech.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chen et al_2021_AdaSpeech.pdf:application/pdf},
}

@article{casanova_sc-glowtts_2021,
	title = {{SC}-{GlowTTS}: an {Efficient} {Zero}-{Shot} {Multi}-{Speaker} {Text}-{To}-{Speech} {Model}},
	shorttitle = {{SC}-{GlowTTS}},
	url = {http://arxiv.org/abs/2104.05557},
	abstract = {In this paper, we propose SC-GlowTTS: an efficient zero-shot multi-speaker text-to-speech model that improves similarity for speakers unseen during training. We propose a speaker-conditional architecture that explores a flow-based decoder that works in a zero-shot scenario. As text encoders, we explore a dilated residual convolutional-based encoder, gated convolutional-based encoder, and transformer-based encoder. Additionally, we have shown that adjusting a GAN-based vocoder for the spectrograms predicted by the TTS model on the training dataset can significantly improve the similarity and speech quality for new speakers. Our model converges using only 11 speakers, reaching state-of-the-art results for similarity with new speakers, as well as high speech quality.},
	urldate = {2021-08-11},
	journal = {arXiv:2104.05557 [cs, eess]},
	author = {Casanova, Edresson and Shulby, Christopher and Gölge, Eren and Müller, Nicolas Michael and de Oliveira, Frederico Santos and Junior, Arnaldo Candido and Soares, Anderson da Silva and Aluisio, Sandra Maria and Ponti, Moacir Antonelli},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.05557},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/WTFRCGKT/2104.html:text/html;Casanova et al_2021_SC-GlowTTS.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Casanova et al_2021_SC-GlowTTS.pdf:application/pdf},
}

@article{qian_speech_2021,
	title = {Speech {Drives} {Templates}: {Co}-{Speech} {Gesture} {Synthesis} with {Learned} {Templates}},
	shorttitle = {Speech {Drives} {Templates}},
	url = {http://arxiv.org/abs/2108.08020},
	abstract = {Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we design a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity. For our method, the template vector determines the general appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both indispensable for synthesizing a realistic gesture sequence. Due to the intractability of an objective metric for gesture-speech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization ability of our model. Extensive experiments show the superiority of our method in both objective and subjective evaluations on fidelity and synchronization.},
	urldate = {2021-08-24},
	journal = {arXiv:2108.08020 [cs]},
	author = {Qian, Shenhan and Tu, Zhi and Zhi, YiHao and Liu, Wen and Gao, Shenghua},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.08020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/A7B4P3A8/2108.html:text/html;Qian et al_2021_Speech Drives Templates.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Qian et al_2021_Speech Drives Templates.pdf:application/pdf},
}

@inproceedings{zhou_pose-controllable_2021,
	title = {Pose-{Controllable} {Talking} {Face} {Generation} by {Implicitly} {Modularized} {Audio}-{Visual} {Representation}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Representation_CVPR_2021_paper.html},
	abstract = {While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on non-aligned raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework. Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are controllable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhou, Hang and Sun, Yasheng and Wu, Wayne and Loy, Chen Change and Wang, Xiaogang and Liu, Ziwei},
	year = {2021},
	pages = {4176--4186},
	file = {Snapshot:/Users/controlnet/Zotero/storage/FJNUDUHN/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Represent.html:text/html;Zhou et al_2021_Pose-Controllable Talking Face Generation by Implicitly Modularized.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhou et al_2021_Pose-Controllable Talking Face Generation by Implicitly Modularized.pdf:application/pdf},
}

@article{zhang_facial_2021,
	title = {{FACIAL}: {Synthesizing} {Dynamic} {Talking} {Face} with {Implicit} {Attribute} {Learning}},
	shorttitle = {{FACIAL}},
	url = {http://arxiv.org/abs/2108.07938},
	abstract = {In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the attention map of eye blinks as input to generate the photo-realistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods.},
	urldate = {2021-08-24},
	journal = {arXiv:2108.07938 [cs]},
	author = {Zhang, Chenxu and Zhao, Yifan and Huang, Yifei and Zeng, Ming and Ni, Saifeng and Budagavi, Madhukar and Guo, Xiaohu},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.07938},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/2SX6VRU8/2108.html:text/html;Zhang et al_2021_FACIAL.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zhang et al_2021_FACIAL.pdf:application/pdf},
}

@article{liu_high-speed_2021,
	title = {High-{Speed} and {High}-{Quality} {Text}-to-{Lip} {Generation}},
	url = {http://arxiv.org/abs/2107.06831},
	abstract = {As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video. Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner. However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation. This encourages the research of parallel T2L generation. In this work, we propose a novel parallel decoding model for high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner. Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem. Extensive experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L generates lip movements with competitive quality compared with the state-of-the-art AR T2L model DualLip and exceeds the baseline AR model TransformerT2L by a notable margin benefiting from the mitigation of the error propagation problem; and 2) exhibits distinct superiority in inference speed (an average speedup of 19\${\textbackslash}times\$ than DualLip on TCD-TIMIT).},
	urldate = {2021-08-24},
	journal = {arXiv:2107.06831 [cs]},
	author = {Liu, Jinglin and Zhu, Zhiying and Ren, Yi and Zhao, Zhou},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.06831},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/VR9ITL8S/2107.html:text/html;Liu et al_2021_High-Speed and High-Quality Text-to-Lip Generation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Liu et al_2021_High-Speed and High-Quality Text-to-Lip Generation.pdf:application/pdf},
}

@inproceedings{lahiri_lipsync3d_2021,
	title = {{LipSync3D}: {Data}-{Efficient} {Learning} of {Personalized} {3D} {Talking} {Faces} {From} {Video} {Using} {Pose} and {Lighting} {Normalization}},
	shorttitle = {{LipSync3D}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_paper.html},
	abstract = {In this paper, we present a video-based learning framework for animating personalized 3D talking faces from audio. We introduce two training-time data normalizations that significantly improve data sample efficiency. First, we isolate and represent faces in a normalized space that decouples 3D geometry, head pose, and texture. This decomposes the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo constancy of skin to isolate and remove spatiotemporal lighting variations. Together, these normalizations allow simple networks to generate high fidelity lip-sync videos under novel ambient illumination while training with just a single video (of usually {\textless} 5 minutes). Further, to stabilize temporal dynamics, we introduce an auto-regressive approach that conditions the model on its previous visual state. Human ratings and objective metrics demonstrate that our method outperforms contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications enabled by our framework.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Lahiri, Avisek and Kwatra, Vivek and Frueh, Christian and Lewis, John and Bregler, Chris},
	year = {2021},
	pages = {2755--2764},
	file = {Lahiri et al_2021_LipSync3D.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lahiri et al_2021_LipSync3D.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/KNN54D4I/Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_.html:text/html},
}

@article{khalid_fakeavceleb_2021,
	title = {{FakeAVCeleb}: {A} {Novel} {Audio}-{Video} {Multimodal} {Deepfake} {Dataset}},
	shorttitle = {{FakeAVCeleb}},
	url = {http://arxiv.org/abs/2108.05080},
	abstract = {With the significant advancements made in generation of forged video and audio, commonly known as deepfakes, using deep learning technologies, the problem of its misuse is a well-known issue now. Recently, a new problem of generating cloned or synthesized human voice of a person is emerging. AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake videos and audios, new deepfake detectors are need that focuses on both, video and audio. Detecting deepfakes is a challenging task and researchers have made numerous attempts and proposed several deepfake detection methods. To develop a good deepfake detector, a handsome amount of good quality dataset is needed that captures the real world scenarios. Many researchers have contributed in this cause and provided several deepfake dataset, self generated and in-the-wild. However, almost all of these datasets either contains deepfake videos or audio. Moreover, the recent deepfake datasets proposed by researchers have racial bias issues. Hence, there is a crucial need of a good deepfake video and audio deepfake dataset. To fill this gap, we propose a novel Audio-Video Deepfake dataset (FakeAVCeleb) that not only contains deepfake videos but respective synthesized cloned audios as well. We generated our dataset using recent most popular deepfake generation methods and the videos and audios are perfectly lip-synced with each other. To generate a more realistic dataset, we selected real YouTube videos of celebrities having four racial backgrounds (Caucasian, Black, East Asian and South Asian) to counter the racial bias issue. Lastly, we propose a novel multimodal detection method that detects deepfake videos and audios based on our multimodal Audio-Video deepfake dataset.},
	urldate = {2021-08-17},
	journal = {arXiv:2108.05080 [cs]},
	author = {Khalid, Hasam and Tariq, Shahroz and Woo, Simon S.},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.05080},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, I.4.9, I.5.4},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/DI4HARK9/2108.html:text/html;Khalid et al_2021_FakeAVCeleb.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Khalid et al_2021_FakeAVCeleb.pdf:application/pdf},
}

@article{bagchi_hear_2021,
	title = {Hear {Me} {Out}: {Fusional} {Approaches} for {Audio} {Augmented} {Temporal} {Action} {Localization}},
	shorttitle = {Hear {Me} {Out}},
	url = {http://arxiv.org/abs/2106.14118},
	abstract = {State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available at https://github.com/skelemoa/tal-hmo.},
	urldate = {2021-09-15},
	journal = {arXiv:2106.14118 [cs]},
	author = {Bagchi, Anurag and Mahmood, Jazib and Fernandes, Dolton and Sarvadevabhatla, Ravi Kiran},
	month = aug,
	year = {2021},
	note = {arXiv: 2106.14118
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/K87WNXUL/2106.html:text/html;Bagchi et al_2021_Hear Me Out.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Bagchi et al_2021_Hear Me Out.pdf:application/pdf},
}

@article{heo_deepfake_2021,
	title = {Deepfake {Detection} {Scheme} {Based} on {Vision} {Transformer} and {Distillation}},
	url = {http://arxiv.org/abs/2104.01353},
	abstract = {Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition.},
	urldate = {2021-09-15},
	journal = {arXiv:2104.01353 [cs]},
	author = {Heo, Young-Jin and Choi, Young-Ju and Lee, Young-Woon and Kim, Byung-Gyu},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.01353},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/JYYL6RQ7/2104.html:text/html;Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Heo et al_2021_Deepfake Detection Scheme Based on Vision Transformer and Distillation.pdf:application/pdf},
}

@article{coccomini_combining_2021,
	title = {Combining {EfficientNet} and {Vision} {Transformers} for {Video} {Deepfake} {Detection}},
	url = {http://arxiv.org/abs/2107.02612},
	abstract = {Deepfakes are the result of digital manipulation to obtain credible videos in order to deceive the viewer. This is done through deep learning techniques based on autoencoders or GANs that become more accessible and accurate year after year, resulting in fake videos that are very difficult to distinguish from real ones. Traditionally, CNN networks have been used to perform deepfake detection, with the best results obtained using methods based on EfficientNet B7. In this study, we combine various types of Vision Transformers with a convolutional EfficientNet B0 used as a feature extractor, obtaining comparable results with some very recent methods that use Vision Transformers. Differently from the state-of-the-art approaches, we use neither distillation nor ensemble methods. The best model achieved an AUC of 0.951 and an F1 score of 88.0\%, very close to the state-of-the-art on the DeepFake Detection Challenge (DFDC).},
	urldate = {2021-09-15},
	journal = {arXiv:2107.02612 [cs]},
	author = {Coccomini, Davide and Messina, Nicola and Gennaro, Claudio and Falchi, Fabrizio},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.02612
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/BVI89ZBE/2107.html:text/html;Coccomini et al_2021_Combining EfficientNet and Vision Transformers for Video Deepfake Detection.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Coccomini et al_2021_Combining EfficientNet and Vision Transformers for Video Deepfake Detection.pdf:application/pdf},
}

@article{gong_ast_2021,
	title = {{AST}: {Audio} {Spectrogram} {Transformer}},
	shorttitle = {{AST}},
	url = {http://arxiv.org/abs/2104.01778},
	abstract = {In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6\% accuracy on ESC-50, and 98.1\% accuracy on Speech Commands V2.},
	urldate = {2021-09-24},
	journal = {arXiv:2104.01778 [cs]},
	author = {Gong, Yuan and Chung, Yu-An and Glass, James},
	month = jul,
	year = {2021},
	note = {arXiv: 2104.01778},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/WJ4RM58S/2104.html:text/html;Gong et al_2021_AST.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Gong et al_2021_AST.pdf:application/pdf},
}

@inproceedings{chi_audio_2021,
	title = {Audio {Albert}: {A} {Lite} {Bert} for {Self}-{Supervised} {Learning} of {Audio} {Representation}},
	shorttitle = {Audio {Albert}},
	doi = {10.1109/SLT48900.2021.9383575},
	abstract = {Self-supervised speech models are powerful speech representation extractors for downstream applications. Recently, larger models have been utilized in acoustic model training to achieve better performance. We propose Audio ALBERT, a lite version of the self-supervised speech representation model. We apply the lightweight representation extractor to two downstream tasks, speaker classification and phoneme classification. We show that Audio ALBERT achieves performance comparable with massive pre-trained networks in the downstream tasks while having 91\% fewer parameters. Moreover, we design probing models to measure how much the latent representations can encode the speaker's and phoneme's information. We find that the representations encoded in internal layers of Audio ALBERT contain more information for both phoneme and speaker than the last layer, which is generally used for downstream tasks. Our findings provide a new avenue for using self-supervised networks to achieve better performance and efficiency.},
	booktitle = {2021 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Chi, Po-Han and Chung, Pei-Hung and Wu, Tsung-Han and Hsieh, Chun-Cheng and Chen, Yen-Hao and Li, Shang-Wen and Lee, Hung-yi},
	month = jan,
	year = {2021},
	keywords = {Task analysis, Training, Computational modeling, Feature extraction, Acoustics, Spectrogram, Encoding, Network compression, Self-supervised learning, Speech representation learning, transformer, Weight sharing},
	pages = {344--350},
	file = {Chi et al_2021_Audio Albert.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chi et al_2021_Audio Albert.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/KAFAUK8V/9383575.html:text/html},
}

@inproceedings{gao_ctap_2018,
	title = {{CTAP}: {Complementary} {Temporal} {Action} {Proposal} {Generation}},
	shorttitle = {{CTAP}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Jiyang_Gao_CTAP_Complementary_Temporal_ECCV_2018_paper.html},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Gao, Jiyang and Chen, Kan and Nevatia, Ram},
	year = {2018},
	pages = {68--83},
	file = {Gao et al_2018_CTAP.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Gao et al_2018_CTAP.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/R57RK6B5/Jiyang_Gao_CTAP_Complementary_Temporal_ECCV_2018_paper.html:text/html},
}

@inproceedings{gao_turn_2017,
	title = {{TURN} {TAP}: {Temporal} {Unit} {Regression} {Network} for {Temporal} {Action} {Proposals}},
	shorttitle = {{TURN} {TAP}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Gao_TURN_TAP_Temporal_ICCV_2017_paper.html},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Gao, Jiyang and Yang, Zhenheng and Chen, Kan and Sun, Chen and Nevatia, Ram},
	year = {2017},
	pages = {3628--3636},
	file = {Gao et al_2017_TURN TAP.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Gao et al_2017_TURN TAP.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/4XIYKQKX/Gao_TURN_TAP_Temporal_ICCV_2017_paper.html:text/html},
}

@inproceedings{carreira_quo_2017,
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Carreira, Joao and Zisserman, Andrew},
	year = {2017},
	pages = {6299--6308},
	file = {Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/DSIRZDBC/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html:text/html},
}

@inproceedings{lin_bmn_2019,
	title = {{BMN}: {Boundary}-{Matching} {Network} for {Temporal} {Action} {Proposal} {Generation}},
	shorttitle = {{BMN}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.html},
	abstract = {Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
	year = {2019},
	pages = {3889--3898},
	file = {Lin et al_2019_BMN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Lin et al_2019_BMN.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/JKJ9ZUX6/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.html:text/html},
}

@inproceedings{chung_voxceleb2_2018,
	title = {{VoxCeleb2}: {Deep} {Speaker} {Recognition}},
	abstract = {The objective of this paper is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.},
	booktitle = {{INTERSPEECH}},
	author = {Chung, J. S. and Nagrani, A. and Zisserman, A.},
	year = {2018},
	file = {Chung et al_2018_VoxCeleb2.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chung et al_2018_VoxCeleb2.pdf:application/pdf},
}

@inproceedings{rossler_faceforensics_2019,
	title = {{FaceForensics}++: {Learning} to {Detect} {Manipulated} {Facial} {Images}},
	shorttitle = {{FaceForensics}++},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.html},
	abstract = {The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Rossler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Niessner, Matthias},
	year = {2019},
	pages = {1--11},
	file = {Rossler et al_2019_FaceForensics++.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Rossler et al_2019_FaceForensics++.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/TCDGD4LI/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.html:text/html},
}

@inproceedings{jiang_deeperforensics-10_2020,
	title = {{DeeperForensics}-1.0: {A} {Large}-{Scale} {Dataset} for {Real}-{World} {Face} {Forgery} {Detection}},
	shorttitle = {{DeeperForensics}-1.0},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Jiang_DeeperForensics-1.0_A_Large-Scale_Dataset_for_Real-World_Face_Forgery_Detection_CVPR_2020_paper.html},
	abstract = {We present our on-going effort of constructing a large- scale benchmark for face forgery detection. The first version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far, with 60, 000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. We further contribute a comprehensive study that evaluates five representative detection baselines and make a thorough analysis of different settings.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Jiang, Liming and Li, Ren and Wu, Wayne and Qian, Chen and Loy, Chen Change},
	year = {2020},
	pages = {2889--2898},
	file = {Jiang et al_2020_DeeperForensics-1.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jiang et al_2020_DeeperForensics-1.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/7M9TY2I6/Jiang_DeeperForensics-1.0_A_Large-Scale_Dataset_for_Real-World_Face_Forgery_Detection_CVPR_2020.html:text/html},
}

@inproceedings{li_celeb-df_2020,
	title = {Celeb-{DF}: {A} {Large}-{Scale} {Challenging} {Dataset} for {DeepFake} {Forensics}},
	shorttitle = {Celeb-{DF}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Celeb-DF_A_Large-Scale_Challenging_Dataset_for_DeepFake_Forensics_CVPR_2020_paper.html},
	abstract = {AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for datasets of DeepFake videos. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Yuezun and Yang, Xin and Sun, Pu and Qi, Honggang and Lyu, Siwei},
	year = {2020},
	pages = {3207--3216},
	file = {Li et al_2020_Celeb-DF.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li et al_2020_Celeb-DF.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/6N4GNUIT/Li_Celeb-DF_A_Large-Scale_Challenging_Dataset_for_DeepFake_Forensics_CVPR_2020_paper.html:text/html},
}

@article{mirsky_creation_2021,
	title = {The {Creation} and {Detection} of {Deepfakes}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {The {Creation} and {Detection} of {Deepfakes}},
	url = {http://doi.org/10.1145/3425780},
	doi = {10.1145/3425780},
	abstract = {Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these “deepfakes” have advanced significantly. In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention.},
	number = {1},
	urldate = {2021-10-05},
	journal = {ACM Computing Surveys},
	author = {Mirsky, Yisroel and Lee, Wenke},
	month = jan,
	year = {2021},
	keywords = {deep fake, Deepfake, face swap, generative AI, impersonation, reenactment, replacement, social engineering},
	pages = {7:1--7:41},
	file = {Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Mirsky_Lee_2021_The Creation and Detection of Deepfakes.pdf:application/pdf},
}

@inproceedings{li_temporal_2021,
	title = {Temporal {Action} {Segmentation} {From} {Timestamp} {Supervision}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Li_Temporal_Action_Segmentation_From_Timestamp_Supervision_CVPR_2021_paper.html},
	abstract = {Temporal action segmentation approaches have been very successful recently. However, annotating videos with frame-wise labels to train such models is very expensive and time consuming. While weakly supervised methods trained using only ordered action lists require less annotation effort, the performance is still worse than fully supervised approaches. In this paper, we propose to use timestamp supervision for the temporal action segmentation task. Timestamps require a comparable annotation effort to weakly supervised approaches, and yet provide a more supervisory signal. To demonstrate the effectiveness of timestamp supervision, we propose an approach to train a segmentation model using only timestamps annotations. Our approach uses the model output and the annotated timestamps to generate frame-wise labels by detecting the action changes. We further introduce a confidence loss that forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This ensures that all and not only the most distinctive frames of an action are learned during training. The evaluation on four datasets shows that models trained with timestamps annotations achieve comparable performance to the fully supervised approaches.},
	language = {en},
	urldate = {2021-10-10},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Zhe and Abu Farha, Yazan and Gall, Jurgen},
	year = {2021},
	pages = {8365--8374},
	file = {Li et al_2021_Temporal Action Segmentation From Timestamp Supervision.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li et al_2021_Temporal Action Segmentation From Timestamp Supervision.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/D8T4SYAM/Li_Temporal_Action_Segmentation_From_Timestamp_Supervision_CVPR_2021_paper.html:text/html},
}

@misc{nick_contributing_2019,
	title = {Contributing {Data} to {Deepfake} {Detection} {Research}},
	url = {http://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html},
	abstract = {Posted by Nick Dufour, Google Research and Andrew Gully, Jigsaw   Deep learning has given rise to technologies that would have been thought ...},
	language = {en},
	urldate = {2021-10-10},
	journal = {Google AI Blog},
	author = {Nick, Dufou and Andrew, Jigsaw},
	month = sep,
	year = {2019},
	file = {Snapshot:/Users/controlnet/Zotero/storage/XJKPJHKJ/contributing-data-to-deepfake-detection.html:text/html},
}

@inproceedings{yang_exposing_2019,
	title = {Exposing {Deep} {Fakes} {Using} {Inconsistent} {Head} {Poses}},
	doi = {10.1109/ICASSP.2019.8683164},
	abstract = {In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes). Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue. Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yang, Xin and Li, Yuezun and Lyu, Siwei},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Neural networks, Face, Videos, Three-dimensional displays, Cameras, DeepFake Detection, Head Pose Estimation, Media Forensics, Support vector machines},
	pages = {8261--8265},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/Z6UXA47D/8683164.html:text/html;Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Yang et al_2019_Exposing Deep Fakes Using Inconsistent Head Poses.pdf:application/pdf},
}

@article{korshunov_deepfakes_2018,
	title = {{DeepFakes}: a {New} {Threat} to {Face} {Recognition}? {Assessment} and {Detection}},
	shorttitle = {{DeepFakes}},
	url = {http://arxiv.org/abs/1812.08685},
	abstract = {It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62\% and 95.00\% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97\% equal error rate on high quality Deepfakes. Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so.},
	urldate = {2021-10-10},
	journal = {arXiv:1812.08685 [cs]},
	author = {Korshunov, Pavel and Marcel, Sebastien},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.08685},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/V43ASNRP/1812.html:text/html;Korshunov_Marcel_2018_DeepFakes.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Korshunov_Marcel_2018_DeepFakes.pdf:application/pdf},
}

@article{xia_survey_2020,
	title = {A {Survey} on {Temporal} {Action} {Localization}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2986861},
	abstract = {Temporal action localization is one of the most crucial and challenging problems for video understanding in computer vision. It has received a lot of attention in recent years because of the extensive application of daily life. Temporal action localization has made some significant progress, especially with the development of deep learning recently. And more demand is for temporal action localization in untrimmed videos. In this paper, our target is to survey the state-of-the-art techniques and models for video temporal action localization. It mainly includes the related techniques, some benchmark datasets and the evaluation metrics of temporal action localization. In addition, we summarize temporal action localization from two aspects: fully-supervised learning and weakly-supervised learning. And we list several representative works and compare their performances respectively. Finally, we make some deep analysis and propose potential research directions, and conclude the survey.},
	journal = {IEEE Access},
	author = {Xia, Huifen and Zhan, Yongzhao},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Task analysis, Computer vision, Machine learning, Feature extraction, Proposals, Action detection, computer vision, fully-supervised learning, Licenses, temporal action localization, Trajectory, weakly-supervised learning},
	pages = {70477--70487},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/A7KQNSV4/9062498.html:text/html;Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Xia_Zhan_2020_A Survey on Temporal Action Localization.pdf:application/pdf},
}

@article{nagrani_voxceleb_2017,
	title = {{VoxCeleb}: a large-scale speaker identification dataset},
	shorttitle = {{VoxCeleb}},
	url = {http://arxiv.org/abs/1706.08612},
	doi = {10.21437/Interspeech.2017-950},
	abstract = {Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.},
	urldate = {2021-10-12},
	journal = {Interspeech 2017},
	author = {Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
	month = aug,
	year = {2017},
	note = {arXiv: 1706.08612},
	keywords = {Computer Science - Sound},
	pages = {2616--2620},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/T8C7QXIE/1706.html:text/html;Nagrani et al_2017_VoxCeleb.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Nagrani et al_2017_VoxCeleb.pdf:application/pdf},
}

@article{tang_zero-shot_2021,
	title = {Zero-{Shot} {Text}-to-{Speech} for {Text}-{Based} {Insertion} in {Audio} {Narration}},
	url = {http://arxiv.org/abs/2109.05426},
	abstract = {Given a piece of speech and its transcript text, text-based speech editing aims to generate speech that can be seamlessly inserted into the given speech by editing the transcript. Existing methods adopt a two-stage approach: synthesize the input text using a generic text-to-speech (TTS) engine and then transform the voice to the desired voice using voice conversion (VC). A major problem of this framework is that VC is a challenging problem which usually needs a moderate amount of parallel training data to work satisfactorily. In this paper, we propose a one-stage context-aware framework to generate natural and coherent target speech without any training data of the target speaker. In particular, we manage to perform accurate zero-shot duration prediction for the inserted text. The predicted duration is used to regulate both text embedding and speech embedding. Then, based on the aligned cross-modality input, we directly generate the mel-spectrogram of the edited speech with a transformer-based decoder. Subjective listening tests show that despite the lack of training data for the speaker, our method has achieved satisfactory results. It outperforms a recent zero-shot TTS engine by a large margin.},
	urldate = {2021-10-12},
	journal = {arXiv:2109.05426 [cs, eess]},
	author = {Tang, Chuanxin and Luo, Chong and Zhao, Zhiyuan and Yin, Dacheng and Zhao, Yucheng and Zeng, Wenjun},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.05426},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/NQCCIKDM/2109.html:text/html;Tang et al_2021_Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tang et al_2021_Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration.pdf:application/pdf},
}

@article{jamaludin_you_2019,
	title = {You {Said} {That}?: {Synthesising} {Talking} {Faces} from {Audio}},
	volume = {127},
	issn = {1573-1405},
	shorttitle = {You {Said} {That}?},
	url = {https://doi.org/10.1007/s11263-019-01150-y},
	doi = {10.1007/s11263-019-01150-y},
	abstract = {We describe a method for generating a video of a talking face. The method takes still images of the target face and an audio speech segment as inputs, and generates a video of the target face lip synched with the audio. The method runs in real time and is applicable to faces and audio not seen at training time. To achieve this we develop an encoder–decoder convolutional neural network (CNN) model that uses a joint embedding of the face and audio to generate synthesised talking face video frames. The model is trained on unlabelled videos using cross-modal self-supervision. We also propose methods to re-dub videos by visually blending the generated face into the source video frame using a multi-stream CNN model.},
	language = {en},
	number = {11},
	urldate = {2021-10-17},
	journal = {International Journal of Computer Vision},
	author = {Jamaludin, Amir and Chung, Joon Son and Zisserman, Andrew},
	month = dec,
	year = {2019},
	pages = {1767--1779},
	file = {Jamaludin et al_2019_You Said That.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Jamaludin et al_2019_You Said That.pdf:application/pdf},
}

@inproceedings{k_r_towards_2019,
	address = {New York, NY, USA},
	series = {{MM} '19},
	title = {Towards {Automatic} {Face}-to-{Face} {Translation}},
	isbn = {978-1-4503-6889-6},
	url = {http://doi.org/10.1145/3343031.3351066},
	doi = {10.1145/3343031.3351066},
	abstract = {In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as "Face-to-Face Translation". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards "Face-to-Face Translation" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {K R, Prajwal and Mukhopadhyay, Rudrabha and Philip, Jerin and Jha, Abhishek and Namboodiri, Vinay and Jawahar, C V},
	month = oct,
	year = {2019},
	keywords = {cross-language talking face generation, lip synthesis, neural machine translation, speech to speech translation, translation systems, voice transfer},
	pages = {1428--1436},
	file = {K R et al_2019_Towards Automatic Face-to-Face Translation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/K R et al_2019_Towards Automatic Face-to-Face Translation.pdf:application/pdf},
}

@inproceedings{bodla_soft-nms_2017,
	title = {Soft-{NMS} -- {Improving} {Object} {Detection} {With} {One} {Line} of {Code}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Bodla_Soft-NMS_--_Improving_ICCV_2017_paper.html},
	abstract = {Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC 2007 (1.7\% for both R-FCN and Faster-RCNN) and MS-COCO (1.3\% for R-FCN and 1.1\% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8\% to 40.9\% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub},
	urldate = {2021-10-15},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
	year = {2017},
	pages = {5561--5569},
	file = {Bodla et al_2017_Soft-NMS -- Improving Object Detection With One Line of Code.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Bodla et al_2017_Soft-NMS -- Improving Object Detection With One Line of Code.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/NPIEWLXM/Bodla_Soft-NMS_--_Improving_ICCV_2017_paper.html:text/html},
}

@inproceedings{wan_generalized_2018,
	title = {Generalized {End}-to-{End} {Loss} for {Speaker} {Verification}},
	doi = {10.1109/ICASSP.2018.8462665},
	abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., “OK Google” and “Hey Google”) as well as multiple dialects.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Training, Neural networks, Feature extraction, Spectrogram, Adaptation models, end-to-end loss, Google, keyword detection, Mathematical model, Multi-Reader, Speaker verification},
	pages = {4879--4883},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/A9QCPBX3/8462665.html:text/html;Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wan et al_2018_Generalized End-to-End Loss for Speaker Verification.pdf:application/pdf},
}

@book{bird_natural_2009,
	title = {Natural {Language} {Processing} with {Python}: {Analyzing} {Text} with the {Natural} {Language} {Toolkit}},
	isbn = {978-0-596-55571-9},
	shorttitle = {Natural {Language} {Processing} with {Python}},
	abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.Packed with examples and exercises, Natural Language Processing with Python will help you:Extract information from unstructured text, either to guess the topic or identify "named entities"Analyze linguistic structure in text, including parsing and semantic analysisAccess popular linguistic databases, including WordNet and treebanksIntegrate techniques drawn from fields as diverse as linguistics and artificial intelligenceThis book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.},
	language = {en},
	publisher = {O'Reilly Media, Inc.},
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	month = jun,
	year = {2009},
	note = {Google-Books-ID: KGIbfiiP1i4C},
	keywords = {Computers / General, Computers / Languages / General, Computers / Languages / JavaScript, Computers / Languages / Python, Computers / Software Development \& Engineering / General},
}

@article{king_dlib-ml_2009,
	title = {Dlib-ml: {A} {Machine} {Learning} {Toolkit}},
	volume = {10},
	issn = {1532-4435},
	shorttitle = {Dlib-ml},
	abstract = {There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.},
	journal = {The Journal of Machine Learning Research},
	author = {King, Davis E.},
	month = dec,
	year = {2009},
	pages = {1755--1758},
	file = {King_2009_Dlib-ml.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/King_2009_Dlib-ml.pdf:application/pdf},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2021-10-18},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/7YSDKMQX/1609.html:text/html;Oord et al_2016_WaveNet.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Oord et al_2016_WaveNet.pdf:application/pdf},
}

@techreport{rumelhart_learning_1985,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	url = {https://apps.dtic.mil/sti/citations/ADA164453},
	abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytems performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords Learning networks Perceptrons Adaptive systems Learning machines and Back propagation.},
	language = {en},
	urldate = {2021-10-18},
	institution = {CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = sep,
	year = {1985},
	note = {Section: Technical Reports},
	file = {Rumelhart et al_1985_Learning Internal Representations by Error Propagation.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Rumelhart et al_1985_Learning Internal Representations by Error Propagation.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/NU88YMCW/ADA164453.html:text/html},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782},
	url = {http://doi.org/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	number = {11},
	urldate = {2021-10-18},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2020},
	pages = {139--144},
	file = {Goodfellow et al_2020_Generative adversarial networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Goodfellow et al_2020_Generative adversarial networks.pdf:application/pdf},
}

@article{wodajo_deepfake_2021,
	title = {Deepfake {Video} {Detection} {Using} {Convolutional} {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2102.11126},
	abstract = {The rapid advancement of deep learning models that can generate and synthesis hyper-realistic videos known as Deepfakes and their ease of access to the general public have raised concern from all concerned bodies to their possible malicious intent use. Deep learning techniques can now generate faces, swap faces between two subjects in a video, alter facial expressions, change gender, and alter facial features, to list a few. These powerful video manipulation methods have potential use in many fields. However, they also pose a looming threat to everyone if used for harmful purposes such as identity theft, phishing, and scam. In this work, we propose a Convolutional Vision Transformer for the detection of Deepfakes. The Convolutional Vision Transformer has two components: Convolutional Neural Network (CNN) and Vision Transformer (ViT). The CNN extracts learnable features while the ViT takes in the learned features as input and categorizes them using an attention mechanism. We trained our model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5 percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our contribution is that we have added a CNN module to the ViT architecture and have achieved a competitive result on the DFDC dataset.},
	urldate = {2021-10-18},
	journal = {arXiv:2102.11126 [cs]},
	author = {Wodajo, Deressa and Atnafu, Solomon},
	month = mar,
	year = {2021},
	note = {arXiv: 2102.11126},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/N7F2WJMC/2102.html:text/html;Wodajo_Atnafu_2021_Deepfake Video Detection Using Convolutional Vision Transformer.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wodajo_Atnafu_2021_Deepfake Video Detection Using Convolutional Vision Transformer.pdf:application/pdf},
}

@article{brandon_there_2019,
	chapter = {Social Media},
	title = {There {Are} {Now} 15,000 {Deepfake} {Videos} on {Social} {Media}. {Yes}, {You} {Should} {Worry}.},
	url = {https://www.forbes.com/sites/johnbbrandon/2019/10/08/there-are-now-15000-deepfake-videos-on-social-media-yes-you-should-worry/},
	abstract = {After someone superimposes the face of a president or celebrity onto someone else’s body (often in a pornographic movie), they can upload them to Facebook or any other social media platform.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Forbes},
	author = {Brandon, John},
	month = oct,
	year = {2019},
	file = {Snapshot:/Users/controlnet/Zotero/storage/9GL9JDGS/there-are-now-15000-deepfake-videos-on-social-media-yes-you-should-worry.html:text/html},
}

@article{thomas_deepfakes_2020,
	chapter = {Business},
	title = {Deepfakes: {A} threat to democracy or just a bit of fun?},
	shorttitle = {Deepfakes},
	url = {https://www.bbc.com/news/business-51204954},
	abstract = {Deepfakes, or computer-generated images of people, can be dangerous for democracy, warn experts.},
	language = {auto},
	urldate = {2021-10-18},
	journal = {BBC News},
	author = {Thomas, Daniel},
	month = jan,
	year = {2020},
	file = {Snapshot:/Users/controlnet/Zotero/storage/WF496AEP/business-51204954.html:text/html},
}

@inproceedings{thies_neural_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Voice} {Puppetry}: {Audio}-{Driven} {Facial} {Reenactment}},
	isbn = {978-3-030-58517-4},
	shorttitle = {Neural {Voice} {Puppetry}},
	doi = {10.1007/978-3-030-58517-4_42},
	abstract = {We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis (Video, Code and Demo: https://justusthies.github.io/posts/neural-voice-puppetry/). Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Thies, Justus and Elgharib, Mohamed and Tewari, Ayush and Theobalt, Christian and Nießner, Matthias},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {716--731},
	file = {Thies et al_2020_Neural Voice Puppetry.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Thies et al_2020_Neural Voice Puppetry.pdf:application/pdf},
}

@inproceedings{zakharov_few-shot_2019,
	title = {Few-{Shot} {Adversarial} {Learning} of {Realistic} {Neural} {Talking} {Head} {Models}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.html},
	abstract = {Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and Lempitsky, Victor},
	year = {2019},
	pages = {9459--9468},
	file = {Snapshot:/Users/controlnet/Zotero/storage/P4JNMGY3/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.html:text/html;Zakharov et al_2019_Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zakharov et al_2019_Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.pdf:application/pdf},
}

@article{kim_deep_2018,
	title = {Deep video portraits},
	volume = {37},
	issn = {0730-0301},
	url = {http://doi.org/10.1145/3197517.3201283},
	doi = {10.1145/3197517.3201283},
	abstract = {We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect.},
	number = {4},
	urldate = {2021-10-18},
	journal = {ACM Transactions on Graphics},
	author = {Kim, Hyeongwoo and Garrido, Pablo and Tewari, Ayush and Xu, Weipeng and Thies, Justus and Niessner, Matthias and Pérez, Patrick and Richardt, Christian and Zollhöfer, Michael and Theobalt, Christian},
	month = jul,
	year = {2018},
	keywords = {deep learning, conditional GAN, dubbing, facial reenactment, rendering-to-video translation, video portraits},
	pages = {163:1--163:14},
	file = {Kim et al_2018_Deep video portraits.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Kim et al_2018_Deep video portraits.pdf:application/pdf},
}

@article{sample_what_2020,
	chapter = {News},
	title = {What are deepfakes – and how can you spot them?},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them},
	abstract = {AI-generated fake videos are becoming more common (and convincing). Here’s why we should be worried},
	language = {en-GB},
	urldate = {2021-10-18},
	journal = {The Guardian},
	author = {Sample, Ian},
	month = jan,
	year = {2020},
	keywords = {Internet, Technology, UK news, US news, World news},
	file = {Snapshot:/Users/controlnet/Zotero/storage/VHBJAL32/what-are-deepfakes-and-how-can-you-spot-them.html:text/html},
}

@article{li_faceshifter_2020,
	title = {{FaceShifter}: {Towards} {High} {Fidelity} {And} {Occlusion} {Aware} {Face} {Swapping}},
	shorttitle = {{FaceShifter}},
	url = {http://arxiv.org/abs/1912.13457},
	abstract = {In this work, we propose a novel two-stage framework, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, our framework, in its first stage, generates the swapped face in high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. We propose a novel attributes encoder for extracting multi-level target face attributes, and a new generator with carefully designed Adaptive Attentional Denormalization (AAD) layers to adaptively integrate the identity and the attributes for face synthesis. To address the challenging facial occlusions, we append a second stage consisting of a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is trained to recover anomaly regions in a self-supervised way without any manual annotations. Extensive experiments on wild faces demonstrate that our face swapping results are not only considerably more perceptually appealing, but also better identity preserving in comparison to other state-of-the-art methods.},
	urldate = {2021-10-18},
	journal = {arXiv:1912.13457 [cs]},
	author = {Li, Lingzhi and Bao, Jianmin and Yang, Hao and Chen, Dong and Wen, Fang},
	month = sep,
	year = {2020},
	note = {arXiv: 1912.13457},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/TQ3WJLTM/1912.html:text/html;Li et al_2020_FaceShifter.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Li et al_2020_FaceShifter.pdf:application/pdf},
}

@inproceedings{agarwal_protecting_2019,
	title = {Protecting {World} {Leaders} {Against} {Deep} {Fakes}.},
	url = {https://openreview.net/forum?id=roZ-eTMgOpH},
	abstract = {The creation of sophisticated fake videos has been largely relegated to Hollywood studios or state actors. Recent advances in deep learning, however, have made it significantly easier to create sophisticated and compelling fake videos. With relatively modest amounts of data and computing power, the average person can, for example, create a video of a world leader confessing to illegal activity leading to a constitutional crisis, a military leader saying something racially insensitive leading to civil unrest in an area of military activity, or a corporate titan claiming that their profits are weak leading to global stock manipulation. These so called deep fakes pose a significant threat to our democracy, national security, and society. To contend with this growing threat, we describe a forensic technique that models facial expressions and movements that typify an individual's speaking pattern. Although not visually apparent, these correlations are often violated by the nature of how deep-fake videos are created and can, therefore, be used for authentication.},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {{CVPR} {Workshops}},
	author = {Agarwal, Shruti and Farid, Hany and Gu, Yuming and He, Mingming and Nagano, Koki and Li, Hao},
	month = jan,
	year = {2019},
	file = {Agarwal et al_2019_Protecting World Leaders Against Deep Fakes.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Agarwal et al_2019_Protecting World Leaders Against Deep Fakes.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/YP6FJKGE/forum.html:text/html},
}

@inproceedings{thies_face2face_2016,
	title = {{Face2Face}: {Real}-{Time} {Face} {Capture} and {Reenactment} of {RGB} {Videos}},
	shorttitle = {{Face2Face}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.html},
	abstract = {We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Thies, Justus and Zollhofer, Michael and Stamminger, Marc and Theobalt, Christian and Niessner, Matthias},
	year = {2016},
	pages = {2387--2395},
	file = {Snapshot:/Users/controlnet/Zotero/storage/2HPWZA8B/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.html:text/html;Thies et al_2016_Face2Face.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Thies et al_2016_Face2Face.pdf:application/pdf},
}

@inproceedings{nirkin_fsgan_2019,
	title = {{FSGAN}: {Subject} {Agnostic} {Face} {Swapping} and {Reenactment}},
	shorttitle = {{FSGAN}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.html},
	abstract = {We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Nirkin, Yuval and Keller, Yosi and Hassner, Tal},
	year = {2019},
	pages = {7184--7193},
	file = {Nirkin et al_2019_FSGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Nirkin et al_2019_FSGAN.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/7FQ953WQ/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.html:text/html},
}

@inproceedings{korshunova_fast_2017,
	title = {Fast {Face}-{Swap} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Korshunova_Fast_Face-Swap_Using_ICCV_2017_paper.html},
	abstract = {We consider the problem of face swapping in images, where an input identity is transformed into a target identity while preserving pose, facial expression and lighting. To perform this mapping, we use convolutional neural networks trained to capture the appearance of the target identity from an unstructured collection of his/her photographs. This approach is enabled by framing the face swapping problem in terms of style transfer, where the goal is to render an image in the style of another one. Building on recent advances in this area, we devise a new loss function that enables the network to produce highly photorealistic results. By combining neural networks with simple pre- and post-processing steps, we aim at making face swap work in real-time with no input from the user.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Korshunova, Iryna and Shi, Wenzhe and Dambre, Joni and Theis, Lucas},
	year = {2017},
	pages = {3677--3685},
	file = {Korshunova et al_2017_Fast Face-Swap Using Convolutional Neural Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Korshunova et al_2017_Fast Face-Swap Using Convolutional Neural Networks.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/VBXMWYPG/Korshunova_Fast_Face-Swap_Using_ICCV_2017_paper.html:text/html},
}

@article{schwartz_you_2018,
	chapter = {Technology},
	title = {You thought fake news was bad? {Deep} fakes are where truth goes to die},
	issn = {0261-3077},
	shorttitle = {You thought fake news was bad?},
	url = {https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth},
	abstract = {Technology can make it look as if anyone has said or done anything. Is it the next wave of (mis)information warfare?},
	language = {en-GB},
	urldate = {2021-10-18},
	journal = {The Guardian},
	author = {Schwartz, Oscar},
	month = nov,
	year = {2018},
	keywords = {Technology, UK news, Artificial intelligence (AI), Computing, Consciousness, Digital media, Human biology, Media, Neuroscience, Politics, Psychology, Science, Social media},
	file = {Snapshot:/Users/controlnet/Zotero/storage/SECYZ8TC/deep-fakes-fake-news-truth.html:text/html},
}

@inproceedings{chugh_not_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {Not made for each other- {Audio}-{Visual} {Dissonance}-based {Deepfake} {Detection} and {Localization}},
	isbn = {978-1-4503-7988-5},
	url = {http://doi.org/10.1145/3394171.3413700},
	doi = {10.1145/3394171.3413700},
	abstract = {We propose detection of deepfake videos based on the dissimilarity between the audio and visual modalities, termed as the Modality Dissonance Score (MDS). We hypothesize that manipulation of either modality will lead to dis-harmony between the two modalities, e.g., loss of lip-sync, unnatural facial and lip movements, etc. MDS is computed as the mean aggregate of dissimilarity scores between audio and visual segments in a video. Discriminative features are learnt for the audio and visual channels in a chunk-wise manner, employing the cross-entropy loss for individual modalities, and a contrastive loss that models inter-modality similarity. Extensive experiments on the DFDC and DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art by up to 7\%. We also demonstrate temporal forgery localization, and show how our technique identifies the manipulated video segments.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Chugh, Komal and Gupta, Parul and Dhall, Abhinav and Subramanian, Ramanathan},
	month = oct,
	year = {2020},
	keywords = {contrastive loss, deepfake detection and localization, modality dissonance, neural networks},
	pages = {439--447},
	file = {Chugh et al_2020_Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chugh et al_2020_Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and.pdf:application/pdf},
}

@article{neekhara_expressive_2021,
	title = {Expressive {Neural} {Voice} {Cloning}},
	url = {http://arxiv.org/abs/2102.00151},
	abstract = {Voice cloning is the task of learning to synthesize the voice of an unseen speaker from a few samples. While current voice cloning methods achieve promising results in Text-to-Speech (TTS) synthesis for a new voice, these approaches lack the ability to control the expressiveness of synthesized audio. In this work, we propose a controllable voice cloning method that allows fine-grained control over various style aspects of the synthesized speech for an unseen speaker. We achieve this by explicitly conditioning the speech synthesis model on a speaker encoding, pitch contour and latent style tokens during training. Through both quantitative and qualitative evaluations, we show that our framework can be used for various expressive voice cloning tasks using only a few transcribed or untranscribed speech samples for a new speaker. These cloning tasks include style transfer from a reference speech, synthesizing speech directly from text, and fine-grained style control by manipulating the style conditioning variables during inference.},
	urldate = {2021-10-18},
	journal = {arXiv:2102.00151 [cs, eess]},
	author = {Neekhara, Paarth and Hussain, Shehzeen and Dubnov, Shlomo and Koushanfar, Farinaz and McAuley, Julian},
	month = jan,
	year = {2021},
	note = {arXiv: 2102.00151},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/HS8MS8G2/2102.html:text/html;Neekhara et al_2021_Expressive Neural Voice Cloning.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Neekhara et al_2021_Expressive Neural Voice Cloning.pdf:application/pdf},
}

@inproceedings{kwon_kodf_2021,
	title = {{KoDF}: {A} {Large}-{Scale} {Korean} {DeepFake} {Detection} {Dataset}},
	shorttitle = {{KoDF}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Kwon_KoDF_A_Large-Scale_Korean_DeepFake_Detection_Dataset_ICCV_2021_paper.html},
	abstract = {A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratizing the face synthesis technology to a great extent. Videos generated as such have come to be called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we provide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detection datasets, and underline the importance of using multiple datasets for real-world generalization. KoDF is publicly available at https://moneybrain-research.github.io/kodf in its entirety (i.e. real clips, synthesized clips, clips with adversarial attack, and metadata).},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Kwon, Patrick and You, Jaeseong and Nam, Gyuhyeon and Park, Sungwoo and Chae, Gyeongsu},
	year = {2021},
	pages = {10744--10753},
	file = {Kwon et al_2021_KoDF.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Kwon et al_2021_KoDF.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/BFYD3SU8/Kwon_KoDF_A_Large-Scale_Korean_DeepFake_Detection_Dataset_ICCV_2021_paper.html:text/html},
}

@inproceedings{le_openforensics_2021,
	title = {{OpenForensics}: {Large}-{Scale} {Challenging} {Dataset} for {Multi}-{Face} {Forgery} {Detection} and {Segmentation} {In}-the-{Wild}},
	shorttitle = {{OpenForensics}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Le_OpenForensics_Large-Scale_Challenging_Dataset_for_Multi-Face_Forgery_Detection_and_Segmentation_ICCV_2021_paper.html},
	abstract = {The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely OpenForensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios.},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Le, Trung-Nghia and Nguyen, Huy H. and Yamagishi, Junichi and Echizen, Isao},
	year = {2021},
	pages = {10117--10127},
	file = {Le et al_2021_OpenForensics.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Le et al_2021_OpenForensics.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/DCUB94GD/Le_OpenForensics_Large-Scale_Challenging_Dataset_for_Multi-Face_Forgery_Detection_and_Segmentat.html:text/html},
}

@inproceedings{alwassel_tsp_2021,
	title = {{TSP}: {Temporally}-{Sensitive} {Pretraining} of {Video} {Encoders} for {Localization} {Tasks}},
	shorttitle = {{TSP}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Alwassel_TSP_Temporally-Sensitive_Pretraining_of_Video_Encoders_for_Localization_Tasks_ICCVW_2021_paper.html},
	abstract = {Due to the large memory footprint of untrimmed videos, current state-of-the-art video localization methods operate atop precomputed video clip features. These features are extracted from video encoders typically trained for trimmed action classification tasks, making such features not necessarily suitable for temporal localization. In this work, we propose a novel supervised pretraining paradigm for clip features that not only trains to classify activities but also considers background clips and global video information to improve temporal sensitivity. Extensive experiments show that using features trained with our novel pretraining strategy significantly improves the performance of recent state-of-the-art methods on three tasks: Temporal Action Localization, Action Proposal Generation, and Dense Video Captioning. We also show that our pretraining approach is effective across three encoder architectures and two pretraining datasets. We believe video feature encoding is an important building block for localization algorithms, and extracting temporally-sensitive features should be of paramount importance in building more accurate models. The code and pretrained models are available on our project website.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},
	year = {2021},
	pages = {3173--3183},
	file = {Alwassel et al_2021_TSP.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Alwassel et al_2021_TSP.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/6EWLAPB7/Alwassel_TSP_Temporally-Sensitive_Pretraining_of_Video_Encoders_for_Localization_Tasks_ICCVW_20.html:text/html},
}

@inproceedings{wang_temporal_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Temporal {Segment} {Networks}: {Towards} {Good} {Practices} for {Deep} {Action} {Recognition}},
	isbn = {978-3-319-46484-8},
	shorttitle = {Temporal {Segment} {Networks}},
	doi = {10.1007/978-3-319-46484-8_2},
	abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 (69.4\%69.4\% 69.4{\textbackslash},{\textbackslash}\% ) and UCF101 (94.2\%94.2\% 94.2{\textbackslash},{\textbackslash}\% ). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Action recognition, ConvNets, Good practices, Temporal segment networks},
	pages = {20--36},
	file = {Wang et al_2016_Temporal Segment Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2016_Temporal Segment Networks.pdf:application/pdf},
}

@inproceedings{zeng_graph_2019,
	title = {Graph {Convolutional} {Networks} for {Temporal} {Action} {Localization}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.html},
	abstract = {Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using GraphConvolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14(49.1\% versus 42.8\%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships.},
	urldate = {2021-10-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Zeng, Runhao and Huang, Wenbing and Tan, Mingkui and Rong, Yu and Zhao, Peilin and Huang, Junzhou and Gan, Chuang},
	year = {2019},
	pages = {7094--7103},
	file = {Snapshot:/Users/controlnet/Zotero/storage/PQ2CUNC8/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.html:text/html;Zeng et al_2019_Graph Convolutional Networks for Temporal Action Localization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zeng et al_2019_Graph Convolutional Networks for Temporal Action Localization.pdf:application/pdf},
}

@inproceedings{liu_multi-shot_2021,
	title = {Multi-{Shot} {Temporal} {Event} {Localization}: {A} {Benchmark}},
	shorttitle = {Multi-{Shot} {Temporal} {Event} {Localization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.html},
	abstract = {Current developments in temporal event or action localization usually target actions captured by a single camera. However, extensive events or actions in the wild may be captured as a sequence of shots by multiple cameras at different positions. In this paper, we propose a new and challenging task called multi-shot temporal event localization, and accordingly, collect a large-scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an average of 19 shots per instance and 176 shots per video, which induces large intra-instance variations. Our comprehensive evaluations show that the state-of-the-art method in temporal action localization only achieves an mAP of 13.1\% at IoU=0.5. As a minor contribution, we present a simple baseline approach for handling the intra-instance variations, which reports an mAP of 18.9\% on MUSES and 56.9\% on THUMOS14 at IoU=0.5. To facilitate research in this direction, we release the dataset and the project code at https://songbai.site/muses/.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Xiaolong and Hu, Yao and Bai, Song and Ding, Fei and Bai, Xiang and Torr, Philip H. S.},
	year = {2021},
	pages = {12596--12606},
	file = {Liu et al_2021_Multi-Shot Temporal Event Localization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Liu et al_2021_Multi-Shot Temporal Event Localization.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/WZHIEZXT/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.html:text/html},
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2021-10-24},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/GDB6K7SQ/1409.html:text/html;Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf},
}

@article{khalid_evaluation_2021,
	title = {Evaluation of an {Audio}-{Video} {Multimodal} {Deepfake} {Dataset} using {Unimodal} and {Multimodal} {Detectors}},
	url = {http://arxiv.org/abs/2109.02993},
	doi = {10.1145/3476099.3484315},
	abstract = {Significant advancements made in the generation of deepfakes have caused security and privacy issues. Attackers can easily impersonate a person's identity in an image by replacing his face with the target person's face. Moreover, a new domain of cloning human voices using deep-learning technologies is also emerging. Now, an attacker can generate realistic cloned voices of humans using only a few seconds of audio of the target person. With the emerging threat of potential harm deepfakes can cause, researchers have proposed deepfake detection methods. However, they only focus on detecting a single modality, i.e., either video or audio. On the other hand, to develop a good deepfake detector that can cope with the recent advancements in deepfake generation, we need to have a detector that can detect deepfakes of multiple modalities, i.e., videos and audios. To build such a detector, we need a dataset that contains video and respective audio deepfakes. We were able to find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized fake audios as well. We used this multimodal deepfake dataset and performed detailed baseline experiments using state-of-the-art unimodal, ensemble-based, and multimodal detection methods to evaluate it. We conclude through detailed experimentation that unimodals, addressing only a single modality, video or audio, do not perform well compared to ensemble-based methods. Whereas purely multimodal-based baselines provide the worst performance.},
	urldate = {2021-10-24},
	journal = {Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection},
	author = {Khalid, Hasam and Kim, Minha and Tariq, Shahroz and Woo, Simon S.},
	month = oct,
	year = {2021},
	note = {arXiv: 2109.02993},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia, I.4.9, I.5.4},
	pages = {7--15},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/KEXPARW9/2109.html:text/html;Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Khalid et al_2021_Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and.pdf:application/pdf},
}

@inproceedings{tulyakov_mocogan_2018,
	title = {{MoCoGAN}: {Decomposing} {Motion} and {Content} for {Video} {Generation}},
	shorttitle = {{MoCoGAN}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html},
	abstract = {Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion. Our code is available at https://github.com/sergeytulyakov/mocogan.},
	urldate = {2021-10-24},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
	year = {2018},
	pages = {1526--1535},
	file = {Snapshot:/Users/controlnet/Zotero/storage/HS5SXXEH/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html:text/html;Tulyakov et al_2018_MoCoGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Tulyakov et al_2018_MoCoGAN.pdf:application/pdf},
}

@inproceedings{wang_real-esrgan_2021,
	title = {Real-{ESRGAN}: {Training} {Real}-{World} {Blind} {Super}-{Resolution} {With} {Pure} {Synthetic} {Data}},
	shorttitle = {Real-{ESRGAN}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021_paper.html},
	abstract = {Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly. Code: https://github.com/xinntao/Real-ESRGAN},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
	year = {2021},
	pages = {1905--1914},
	file = {Snapshot:/Users/controlnet/Zotero/storage/25NVT4MN/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021.html:text/html;Wang et al_2021_Real-ESRGAN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2021_Real-ESRGAN.pdf:application/pdf},
}

@inproceedings{chan_basicvsr_2021,
	title = {{BasicVSR}: {The} {Search} for {Essential} {Components} in {Video} {Super}-{Resolution} and {Beyond}},
	shorttitle = {{BasicVSR}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chan_BasicVSR_The_Search_for_Essential_Components_in_Video_Super-Resolution_and_CVPR_2021_paper.html},
	abstract = {Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chan, Kelvin C. K. and Wang, Xintao and Yu, Ke and Dong, Chao and Loy, Chen Change},
	year = {2021},
	pages = {4947--4956},
	file = {Chan et al_2021_BasicVSR.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chan et al_2021_BasicVSR.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/FQC7YGZ2/Chan_BasicVSR_The_Search_for_Essential_Components_in_Video_Super-Resolution_and_CVPR_2021_paper.html:text/html},
}

@inproceedings{mehta_motion_2021,
	title = {Motion and {Region} {Aware} {Adversarial} {Learning} for {Fall} {Detection} with {Thermal} {Imaging}},
	doi = {10.1109/ICPR48806.2021.9412632},
	abstract = {Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline.},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Mehta, Vineet and Dhall, Abhinav and Pal, Sujata and Khan, Shehroz S.},
	month = jan,
	year = {2021},
	note = {ISSN: 1051-4651},
	keywords = {Pattern recognition, Three-dimensional displays, Cameras, adversarial learning, Fall detection, Health and safety, Privacy, thermal, Tracking},
	pages = {6321--6328},
	file = {IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/KC2ZC5SN/9412632.html:text/html;Mehta et al_2021_Motion and Region Aware Adversarial Learning for Fall Detection with Thermal.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Mehta et al_2021_Motion and Region Aware Adversarial Learning for Fall Detection with Thermal.pdf:application/pdf},
}

@inproceedings{zi_wilddeepfake_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {{WildDeepfake}: {A} {Challenging} {Real}-{World} {Dataset} for {Deepfake} {Detection}},
	isbn = {978-1-4503-7988-5},
	shorttitle = {{WildDeepfake}},
	url = {http://doi.org/10.1145/3394171.3413769},
	doi = {10.1145/3394171.3413769},
	abstract = {In recent years, the abuse of a face swap technique called deepfake has raised enormous public concerns. So far, a large number of deepfake videos (known as "deepfakes") have been crafted and uploaded to the internet, calling for effective countermeasures. One promising countermeasure against deepfakes is deepfake detection. Several deepfake datasets have been released to support the training and testing of deepfake detectors, such as DeepfakeDetection [1] and FaceForensics++ [23]. While this has greatly advanced deepfake detection, most of the real videos in these datasets are filmed with a few volunteer actors in limited scenes, and the fake videos are crafted by researchers using a few popular deepfake softwares. Detectors developed on these datasets may become less effective against real-world deepfakes on the internet. To better support detection against real-world deepfakes, in this paper, we introduce a new dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707 deepfake videos collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop and test the effectiveness of deepfake detectors against real-world deepfakes. We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a more challenging dataset, where the detection performance can decrease drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention masks on real/fake faces for improved detection. We empirically verify the effectiveness of ADDNets on both existing datasets and WildDeepfake. The dataset is available at: https://github.com/deepfakeinthewild/deepfake-in-the-wild.},
	urldate = {2021-10-20},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zi, Bojia and Chang, Minghao and Chen, Jingjing and Ma, Xingjun and Jiang, Yu-Gang},
	month = oct,
	year = {2020},
	keywords = {deep learning, datasets, deepfake detection},
	pages = {2382--2390},
	file = {Zi et al_2020_WildDeepfake.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Zi et al_2020_WildDeepfake.pdf:application/pdf},
}

@inproceedings{chen_simswap_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {{SimSwap}: {An} {Efficient} {Framework} {For} {High} {Fidelity} {Face} {Swapping}},
	isbn = {978-1-4503-7988-5},
	shorttitle = {{SimSwap}},
	url = {http://doi.org/10.1145/3394171.3413630},
	doi = {10.1145/3394171.3413630},
	abstract = {We propose an efficient framework, called Simple Swap (SimSwap), aiming for generalized and high fidelity face swapping. In contrast to previous approaches that either lack the ability to generalize to arbitrary identity or fail to preserve attributes like facial expression and gaze direction, our framework is capable of transferring the identity of an arbitrary source face into an arbitrary target face while preserving the attributes of the target face. We overcome the above defects in the following two ways. First, we present the ID Injection Module (IIM) which transfers the identity information of the source face into the target face at feature level. By using this module, we extend the architecture of an identity-specific face swapping algorithm to a framework for arbitrary face swapping. Second, we propose the Weak Feature Matching Loss which efficiently helps our framework to preserve the facial attributes in an implicit way. Extensive experiments on wild faces demonstrate that our SimSwap is able to achieve competitive identity performance while preserving attributes better than previous state-of-the-art methods.},
	urldate = {2021-10-20},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Renwang and Chen, Xuanhong and Ni, Bingbing and Ge, Yanhao},
	month = oct,
	year = {2020},
	keywords = {face swapping, generative adversarial network, image translation},
	pages = {2003--2011},
	file = {Chen et al_2020_SimSwap.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Chen et al_2020_SimSwap.pdf:application/pdf},
}

@inproceedings{xu_g-tad_2020,
	title = {G-{TAD}: {Sub}-{Graph} {Localization} for {Temporal} {Action} {Detection}},
	shorttitle = {G-{TAD}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_G-TAD_Sub-Graph_Localization_for_Temporal_Action_Detection_CVPR_2020_paper.html},
	abstract = {Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design an SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves state-of-the-art performance on two detection benchmarks. On ActivityNet-1.3 it obtains an average mAP of 34.09\%; on THUMOS14 it reaches 51.6\% at IoU@0.5 when combined with a proposal processing method. The code has been made available at https://github.com/frostinassiky/gtad.},
	urldate = {2021-11-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xu, Mengmeng and Zhao, Chen and Rojas, David S. and Thabet, Ali and Ghanem, Bernard},
	year = {2020},
	pages = {10156--10165},
	file = {Snapshot:/Users/controlnet/Zotero/storage/PRQXK5VJ/Xu_G-TAD_Sub-Graph_Localization_for_Temporal_Action_Detection_CVPR_2020_paper.html:text/html;Xu et al_2020_G-TAD.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Xu et al_2020_G-TAD.pdf:application/pdf},
}

@inproceedings{caba_heilbron_activitynet_2015,
	title = {{ActivityNet}: {A} {Large}-{Scale} {Video} {Benchmark} for {Human} {Activity} {Understanding}},
	shorttitle = {{ActivityNet}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html},
	abstract = {In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper, we introduce ActivityNet: a new large-scale video benchmark for human activity understanding. Our new benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity categories with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 hours of video. We illustrate three scenarios in which ActivityNet can be used to benchmark and compare algorithms for human activity understanding: untrimmed video classification, trimmed activity classification and activity detection.},
	urldate = {2021-11-01},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
	year = {2015},
	pages = {961--970},
	file = {Caba Heilbron et al_2015_ActivityNet.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Caba Heilbron et al_2015_ActivityNet.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/7M6GPYXW/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html:text/html},
}

@article{idrees_thumos_2017,
	title = {The {THUMOS} {Challenge} on {Action} {Recognition} for {Videos} "in the {Wild}"},
	volume = {155},
	issn = {10773142},
	url = {http://arxiv.org/abs/1604.06182},
	doi = {10.1016/j.cviu.2016.10.018},
	abstract = {Automatically recognizing and localizing wide ranges of human actions has crucial importance for video understanding. Towards this goal, the THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including THUMOS challenge, had focused primarily on the classification of pre-segmented (i.e., trimmed) videos, which is an artificial task. In THUMOS 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include `background videos' which share similar scenes and backgrounds as action videos, but are devoid of the specific actions. The three editions of the challenge organized in 2013--2015 have made THUMOS a common benchmark for action classification and detection and the annual challenge is widely attended by teams from around the world. In this paper we describe the THUMOS benchmark in detail and give an overview of data collection and annotation procedures. We present the evaluation protocols used to quantify results in the two THUMOS tasks of action classification and temporal detection. We also present results of submissions to the THUMOS 2015 challenge and review the participating approaches. Additionally, we include a comprehensive empirical study evaluating the differences in action recognition between trimmed and untrimmed videos, and how well methods trained on trimmed videos generalize to untrimmed videos. We conclude by proposing several directions and improvements for future THUMOS challenges.},
	urldate = {2021-11-01},
	journal = {Computer Vision and Image Understanding},
	author = {Idrees, Haroon and Zamir, Amir R. and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
	month = feb,
	year = {2017},
	note = {arXiv: 1604.06182},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--23},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/NEN84JX6/1604.html:text/html;Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Idrees et al_2017_The THUMOS Challenge on Action Recognition for Videos in the Wild.pdf:application/pdf},
}

@article{wang_m2tr_2021,
	title = {{M2TR}: {Multi}-modal {Multi}-scale {Transformers} for {Deepfake} {Detection}},
	shorttitle = {{M2TR}},
	url = {http://arxiv.org/abs/2104.09770},
	abstract = {The widespread dissemination of forged images generated by Deepfake techniques has posed a serious threat to the trustworthiness of digital information. This demands effective approaches that can detect perceptually convincing Deepfakes generated by advanced manipulation techniques. Most existing approaches combat Deepfakes with deep neural networks by mapping the input image to a binary prediction without capturing the consistency among different pixels. In this paper, we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection. We achieve this with transformer models, which have recently demonstrated superior performance in modeling dependencies between pixels for a variety of recognition tasks in computer vision. In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale transformer that operates on patches of different sizes to detect the local inconsistency at different spatial levels. To improve the detection results and enhance the robustness of our method to image compression, M2TR also takes frequency information, which is further combined with RGB features using a cross modality fusion module. Developing and evaluating Deepfake detection methods requires large-scale datasets. However, we observe that samples in existing benchmarks contain severe artifacts and lack diversity. This motivates us to introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. On three Deepfake datasets, we conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods.},
	urldate = {2021-11-01},
	journal = {arXiv:2104.09770 [cs]},
	author = {Wang, Junke and Wu, Zuxuan and Chen, Jingjing and Jiang, Yu-Gang},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.09770},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/Y6BMPQHN/2104.html:text/html;Wang et al_2021_M2TR.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Wang et al_2021_M2TR.pdf:application/pdf},
}

@inproceedings{guarnera_deepfake_2020,
	title = {{DeepFake} {Detection} by {Analyzing} {Convolutional} {Traces}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w39/Guarnera_DeepFake_Detection_by_Analyzing_Convolutional_Traces_CVPRW_2020_paper.html},
	abstract = {The Deepfake phenomenon has become very popular nowadays thanks to the possibility to create incredibly realistic images using deep learning tools, based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we focus on the analysis of Deepfakes of human faces with the objective of creating a new detection method able to detect a forensics trace hidden in images: a sort of fingerprint left in the image generation process. The proposed technique, by means of an Expectation Maximization (EM) algorithm, extracts a set of local features specifically addressed to model the underlying convolutional generative process. Ad-hoc validation has been employed through experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as ground-truth for non-fakes. Results demonstrated the effectiveness of the technique in distinguishing the different architectures and the corresponding generation process.},
	urldate = {2021-11-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Guarnera, Luca and Giudice, Oliver and Battiato, Sebastiano},
	year = {2020},
	pages = {666--667},
	file = {Guarnera et al_2020_DeepFake Detection by Analyzing Convolutional Traces.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Guarnera et al_2020_DeepFake Detection by Analyzing Convolutional Traces.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/F7SX49WE/Guarnera_DeepFake_Detection_by_Analyzing_Convolutional_Traces_CVPRW_2020_paper.html:text/html},
}

@inproceedings{amerini_deepfake_2019,
	title = {Deepfake {Video} {Detection} through {Optical} {Flow} {Based} {CNN}},
	url = {https://openaccess.thecvf.com/content_ICCVW_2019/html/HBU/Amerini_Deepfake_Video_Detection_through_Optical_Flow_Based_CNN_ICCVW_2019_paper.html},
	abstract = {Recent advances in visual media technology have led to new tools for processing and, above all, generating multimedia contents. In particular, modern AI-based technologies have provided easy-to-use tools to create extremely realistic manipulated videos. Such synthetic videos, named Deep Fakes, may constitute a serious threat to attack the reputation of public subjects or to address the general opinion on a certain event. According to this, being able to individuate this kind of fake information becomes fundamental. In this work, a new forensic technique able to discern between fake and original video sequences is given; unlike other state-of-the-art methods which resorts at single video frames, we propose the adoption of optical flow fields to exploit possible inter-frame dissimilarities. Such a clue is then used as feature to be learned by CNN classifiers. Preliminary results obtained on FaceForensics++ dataset highlight very promising performances.},
	urldate = {2021-11-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops}},
	author = {Amerini, Irene and Galteri, Leonardo and Caldelli, Roberto and Del Bimbo, Alberto},
	year = {2019},
	pages = {0--0},
	file = {Amerini et al_2019_Deepfake Video Detection through Optical Flow Based CNN.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Amerini et al_2019_Deepfake Video Detection through Optical Flow Based CNN.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/RQPQYEYT/Amerini_Deepfake_Video_Detection_through_Optical_Flow_Based_CNN_ICCVW_2019_paper.html:text/html},
}

@article{de_lima_deepfake_2020,
	title = {Deepfake {Detection} using {Spatiotemporal} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/2006.14749},
	abstract = {Better generative models and larger datasets have led to more realistic fake videos that can fool the human eye but produce temporal and spatial artifacts that deep learning approaches can detect. Most current Deepfake detection methods only use individual video frames and therefore fail to learn from temporal information. We created a benchmark of the performance of spatiotemporal convolutional methods using the Celeb-DF dataset. Our methods outperformed state-of-the-art frame-based detection methods. Code for our paper is publicly available at https://github.com/oidelima/Deepfake-Detection.},
	urldate = {2021-11-01},
	journal = {arXiv:2006.14749 [cs, eess]},
	author = {de Lima, Oscar and Franklin, Sean and Basu, Shreshtha and Karwoski, Blake and George, Annet},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.14749},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/L2AV3FP7/2006.html:text/html;de Lima et al_2020_Deepfake Detection using Spatiotemporal Convolutional Networks.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/de Lima et al_2020_Deepfake Detection using Spatiotemporal Convolutional Networks.pdf:application/pdf},
}

@inproceedings{montserrat_deepfakes_2020,
	title = {Deepfakes {Detection} {With} {Automatic} {Face} {Weighting}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w39/Montserrat_Deepfakes_Detection_With_Automatic_Face_Weighting_CVPRW_2020_paper.html},
	abstract = {Altered and manipulated multimedia is increasingly present and widely distributed via social media platforms. Advanced video manipulation tools enable the generation of highly realistic-looking altered multimedia. While many methods have been presented to detect manipulations, most of them fail when evaluated with data outside of the datasets used in research environments. In order to address this problem, the Deepfake Detection Challenge (DFDC) provides a large dataset of videos containing realistic manipulations and an evaluation system that ensures that methods work quickly and accurately, even when faced with challenging data. In this paper, we introduce a method based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that extracts visual and temporal features from faces present in videos to accurately detect manipulations. The method is evaluated with the DFDC dataset, providing competitive results compared to other techniques.},
	urldate = {2021-11-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Montserrat, Daniel Mas and Hao, Hanxiang and Yarlagadda, Sri K. and Baireddy, Sriram and Shao, Ruiting and Horvath, Janos and Bartusiak, Emily and Yang, Justin and Guera, David and Zhu, Fengqing and Delp, Edward J.},
	year = {2020},
	pages = {668--669},
	file = {Montserrat et al_2020_Deepfakes Detection With Automatic Face Weighting.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Montserrat et al_2020_Deepfakes Detection With Automatic Face Weighting.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/X86TBXGN/Montserrat_Deepfakes_Detection_With_Automatic_Face_Weighting_CVPRW_2020_paper.html:text/html},
}

@article{nawhal_activity_2021,
	title = {Activity {Graph} {Transformer} for {Temporal} {Action} {Localization}},
	url = {http://arxiv.org/abs/2101.08540},
	abstract = {We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have non-sequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the state-of-the-art by a considerable margin.},
	urldate = {2021-10-30},
	journal = {arXiv:2101.08540 [cs]},
	author = {Nawhal, Megha and Mori, Greg},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.08540},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/controlnet/Zotero/storage/BRV3YRUG/2101.html:text/html;Nawhal_Mori_2021_Activity Graph Transformer for Temporal Action Localization.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Nawhal_Mori_2021_Activity Graph Transformer for Temporal Action Localization.pdf:application/pdf},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2021-11-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Paszke et al_2019_PyTorch.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Paszke et al_2019_PyTorch.pdf:application/pdf},
}

@inproceedings{he_forgerynet_2021,
	title = {{ForgeryNet}: {A} {Versatile} {Benchmark} for {Comprehensive} {Forgery} {Analysis}},
	shorttitle = {{ForgeryNet}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.html},
	abstract = {The rapid progress of photorealistic synthesis techniques has reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis. To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations.We hope that the scale, quality, and variety of ForgeryNet dataset will foster further research and innovation in the area of face forgery classification, spatial and temporal forgery localization etc.},
	language = {en},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Yinan and Gan, Bei and Chen, Siyu and Zhou, Yichun and Yin, Guojun and Song, Luchuan and Sheng, Lu and Shao, Jing and Liu, Ziwei},
	year = {2021},
	pages = {4360--4369},
	file = {Full Text PDF:/Users/controlnet/Zotero/storage/78AB3Y4M/He et al. - 2021 - ForgeryNet A Versatile Benchmark for Comprehensiv.pdf:application/pdf;Snapshot:/Users/controlnet/Zotero/storage/EMAXTGGP/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.html:text/html},
}

@inproceedings{mittal_emotions_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {Emotions {Don}'t {Lie}: {An} {Audio}-{Visual} {Deepfake} {Detection} {Method} using {Affective} {Cues}},
	isbn = {978-1-4503-7988-5},
	shorttitle = {Emotions {Don}'t {Lie}},
	url = {http://doi.org/10.1145/3394171.3413570},
	doi = {10.1145/3394171.3413570},
	abstract = {We present a learning-based method for detecting real and fake deepfake multimedia content. To maximize information for learning, we extract and analyze the similarity between the two audio and visual modalities from within the same video. Additionally, we extract and compare affective cues corresponding to perceived emotion from the two modalities within a video to infer whether the input video is "real" or "fake". We propose a deep learning network, inspired by the Siamese network architecture and the triplet loss. To validate our model, we report the AUC metric on two large-scale deepfake detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach with several SOTA deepfake detection methods and report per-video AUC of 84.4\% on the DFDC and 96.6\% on the DF-TIMIT datasets, respectively. To the best of our knowledge, ours is the first approach that simultaneously exploits audio and video modalities and also perceived emotions from the two modalities for deepfake detection.},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Mittal, Trisha and Bhattacharya, Uttaran and Chandra, Rohan and Bera, Aniket and Manocha, Dinesh},
	month = oct,
	year = {2020},
	keywords = {affective computing, audio-visual, deepfakes, emotions, multimedia forensics},
	pages = {2823--2832},
	file = {Mittal et al_2020_Emotions Don't Lie.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Mittal et al_2020_Emotions Don't Lie.pdf:application/pdf},
}

@inproceedings{li_exposing_2019,
	title = {Exposing {DeepFake} {Videos} {By} {Detecting} {Face} {Warping} {Artifacts}},
	abstract = {In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as DeepFake videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classiﬁer, our method does not need DeepFake generated images as negative training examples since we target the artifacts in afﬁne face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Li, Yuezun and Lyu, Siwei},
	year = {2019},
	pages = {7},
	file = {Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf:/Users/controlnet/Zotero/storage/FA4E7GRE/Li and Lyu - Exposing DeepFake Videos By Detecting Face Warping.pdf:application/pdf},
}

@inproceedings{afchar_mesonet_2018,
	title = {{MesoNet}: a {Compact} {Facial} {Video} {Forgery} {Detection} {Network}},
	shorttitle = {{MesoNet}},
	doi = {10.1109/WIFS.2018.8630761},
	abstract = {This paper presents a method to automatically and efficiently detect face tampering in videos, and particularly focuses on two recent techniques used to generate hyper-realistic forged videos: Deepfake and Face2Face. Traditional image forensics techniques are usually not well suited to videos due to the compression that strongly degrades the data. Thus, this paper follows a deep learning approach and presents two networks, both with a low number of layers to focus on the mesoscopic properties of images. We evaluate those fast networks on both an existing dataset and a dataset we have constituted from online videos. The tests demonstrate a very successful detection rate with more than 98\% for Deepfake and 95\% for Face2Face.},
	booktitle = {2018 {IEEE} {International} {Workshop} on {Information} {Forensics} and {Security} ({WIFS})},
	author = {Afchar, Darius and Nozick, Vincent and Yamagishi, Junichi and Echizen, Isao},
	month = dec,
	year = {2018},
	note = {ISSN: 2157-4774},
	keywords = {Deep learning, Training, Face, Decoding, Convolutional codes, Forgery},
	pages = {1--7},
	file = {Afchar et al_2018_MesoNet.pdf:/Volumes/GoogleDrive/My Drive/Bibliography/Afchar et al_2018_MesoNet.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/controlnet/Zotero/storage/ME6GUJFU/8630761.html:text/html},
}

@article{tomar_converting_2006,
	title = {Converting video formats with {FFmpeg}},
	volume = {2006},
	issn = {1075-3583},
	abstract = {FFmpeg is a mini Swiss Army knife of format conversion tools.},
	number = {146},
	journal = {Linux Journal},
	author = {Tomar, Suramya},
	month = jun,
	year = {2006},
	pages = {10},
}
